<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 04:07</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0407</div>
    <div class="row"><div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将现实世界可验证环境扩展到百万级</div>
<div class="mono" style="margin-top:8px">我们提出了SWE-Universe，这是一个可扩展且高效的框架，用于从GitHub拉取请求（PRs）自动构建现实世界软件工程（SWE）可验证环境。为了解决自动构建中普遍存在的挑战，如低生产率、弱验证器和高昂成本，我们的框架采用了一个由高效自定义训练模型驱动的构建代理。该代理通过迭代自我验证和循环内黑客检测，确保生成高保真、可验证任务的可靠性。通过这种方法，我们将现实世界多语言SWE环境的数量扩展到百万级（807,693）。我们通过大规模代理中间训练和强化学习展示了这些环境的深远价值。最后，我们将该技术应用于Qwen3-Max-Thinking，并在SWE-Bench Verified上取得了75.3%的得分。我们的工作为推进下一代编码代理提供了关键资源和稳健的方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the limitations of existing methods in automatically constructing high-quality, verifiable software engineering environments, including low production yield, weak verification capabilities, and high costs. The framework introduces a building agent trained on a custom model to iteratively self-verify and detect hacking during the environment creation process, ensuring reliable and high-fidelity task generation. Through this approach, the framework successfully scales to over 800,000 real-world multilingual SWE environments and achieves a 75.3% score on the SWE-Bench Verified benchmark when applied to Qwen3-Max-Thinking, demonstrating its effectiveness in enhancing the training and evaluation of coding agents.</div>
<div class="mono" style="margin-top:8px">本研究提出SWE-Universe框架，旨在解决自动构建高质量可验证软件工程环境的挑战。该框架引入了一个基于自定义训练模型的构建代理，通过迭代自验证和循环内漏洞检测确保任务生成的可靠性和高保真度。研究通过该方法构建了超过80万个真实世界多语言软件工程环境，并通过将其应用于Qwen3-Max-Thinking，在SWE-Bench Verified上取得了75.3%的得分，验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image</div>
<div class="meta-line">Authors: Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik</div>
<div class="meta-line">First: 2025-09-02T13:17:11+00:00 · Latest: 2026-02-02T16:22:10+00:00</div>
<div class="meta-line">Comments: Project page is at https://learn-to-steer-paper.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02295v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://learn-to-steer-paper.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model&#x27;s internal representations.
  We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model&#x27;s cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces in the cross-attention maps, rather than learning true spatial patterns. We solve this by augmenting our training data with samples generated using prompts with incorrect relation words, which encourages the classifier to avoid linguistic shortcuts and learn spatial patterns from the attention maps. Our method dramatically improves spatial accuracy: from 20% to 61% on FLUX.1-dev and from 7% to 54% on SD2.1 across standard benchmarks. It also generalizes to multiple relations with significantly improved accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像推理时优化的数据驱动损失函数</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型可以生成令人惊叹的视觉效果，但在儿童看来微不足道的任务上却常常失败，例如将狗放在泰迪熊的右侧而不是左侧。当组合变得更加不寻常时，例如将长颈鹿放在飞机上方，这些失败会更加明显。现有方法通过模型微调或使用手工设计的次优损失函数在推理时进行优化，以解决这些空间推理问题。我们提出的方法是直接从模型的内部表示中学习这些目标，而不是强加我们对空间编码的假设。我们引入了Learn-to-Steer，一个新颖的框架，用于学习数据驱动的推理时优化目标，而非手工设计。我们的关键洞察是训练一个轻量级分类器，从扩散模型的交叉注意力图中解码空间关系，然后在推理过程中将该分类器作为学习得到的损失函数进行部署。训练此类分类器面临一个令人惊讶的挑战：它们可能通过检测交叉注意力图中的语言痕迹来走捷径，而不是学习真正的空间模式。我们通过在训练数据中加入使用错误关系词提示生成的样本，来解决这一问题，这促使分类器避免语言捷径，并从注意力图中学习空间模式。我们的方法显著提升了空间准确性：在FLUX.1-dev上从20%提升至61%，在SD2.1上从7%提升至54%。它还能够显著提升准确率地推广到多种关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of spatial reasoning failures in text-to-image diffusion models, which struggle with simple relative positioning tasks. The authors propose Learn-to-Steer, a framework that learns data-driven loss functions for inference-time optimization by training a lightweight classifier to decode spatial relationships from cross-attention maps. To prevent the classifier from relying on linguistic cues, the training data is augmented with images generated from prompts containing incorrect relation words. The method significantly improves spatial accuracy, achieving 61% on FLUX.1-dev and 54% on SD2.1 across standard benchmarks, demonstrating strong generalization to various spatial relations.</div>
<div class="mono" style="margin-top:8px">本文针对文本到图像扩散模型在空间推理任务中的失败问题，如难以正确放置物体（如狗在泰迪熊右侧而非左侧）进行研究。作者提出Learn-to-Steer框架，通过训练一个轻量级分类器，从扩散模型的交叉注意力图中解码空间关系，并将该分类器作为学习得到的损失函数用于推理阶段。为防止分类器依赖语言线索，训练数据通过添加错误关系词的提示进行增强。实验结果显示，该方法显著提升了空间准确性，在FLUX.1-dev和SD2.1基准测试中分别达到61%和54%，并能有效推广到多种空间关系。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCode: A Benchmark for Evaluating Software Engineering Agents</div>
<div class="meta-line">Authors: Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta</div>
<div class="meta-line">First: 2026-02-02T16:04:10+00:00 · Latest: 2026-02-02T16:04:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02262v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02262v1">PDF</a> · <a href="https://github.com/seal-research/OmniCode">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCode：评估软件工程代理的基准</div>
<div class="mono" style="margin-top:8px">LLM驱动的编码代理正在重新定义现实世界软件的开发方式。为了推动对更优编码代理的研究，我们需要能够严格评估此类代理执行各种软件工程任务能力的挑战性基准。然而，像HumanEval和SWE-Bench这样的流行编码基准主要关注狭窄范围的任务，如编程竞赛和补丁生成。实际上，软件工程师在现实世界软件开发中需要处理更广泛的任务。为了解决这一差距，我们提出了OmniCode，一个包含超越代码或补丁生成的更广泛和多样化任务类别的新型软件工程基准。总体而言，OmniCode包含1794个任务，涵盖三种编程语言（Python、Java和C++）以及四个关键类别：错误修复、测试生成、代码审查修复和风格修复。与以往的软件工程基准相比，OmniCode的任务（1）经过人工验证以消除定义不清的问题，（2）通过合成构建或最近整理以避免数据泄露问题，提供了一种从有限现实数据中合成生成多样化软件任务的新框架。我们使用流行的代理框架如SWE-Agent对OmniCode进行了评估，结果显示虽然它们在Python错误修复方面表现良好，但在测试生成任务以及C++和Java等语言中表现不足。例如，在Java测试生成任务中，SWE-Agent使用DeepSeek-V3.1最多仅达到20.9%。OmniCode旨在成为一种稳健的基准，推动开发能够在软件开发不同方面表现良好的代理。代码和数据可在https://github.com/seal-research/OmniCode获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of LLM-powered coding agents is transforming real-world software engineering practices, but existing benchmarks like HumanEval and SWE-Bench are limited in scope, focusing mainly on competition programming and patch generation. To address this, OmniCode is introduced as a more comprehensive benchmark that includes four categories: bug fixing, test generation, code review fixing, and style fixing, covering 1794 tasks across Python, Java, and C++. The tasks are manually validated and synthetically generated to avoid data leakage, ensuring a rigorous evaluation of agents&#x27; capabilities. Evaluation with frameworks such as SWE-Agent shows that while they perform reasonably on Python bug fixing, they struggle with test generation in Java and C++.</div>
<div class="mono" style="margin-top:8px">LLM驱动的编码代理正在改变现实世界的软件开发方式，但现有的基准如HumanEval和SWE-Bench在任务范围上较为狭窄，主要集中在编程竞赛和补丁生成。为弥补这一不足，OmniCode被提出作为一项更全面的评估基准，涵盖四个关键类别：错误修复、测试生成、代码审查修复和风格调整，共包含1794个任务，覆盖Python、Java和C++三种编程语言。这些任务经过人工验证并合成生成，以确保其准确性和避免数据泄露问题，提供了更贴近实际的评估环境。评估结果显示，尽管像SWE-Agent这样的代理在Python错误修复方面表现尚可，但在Java和C++的测试生成任务中却存在明显不足。</div>
</details>
</div>
<div class="card">
<div class="title">U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</div>
<div class="meta-line">Authors: Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</div>
<div class="meta-line">First: 2025-05-23T11:48:48+00:00 · Latest: 2026-02-02T13:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17779v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.17779v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U2-BENCH：在超声理解上评估大视觉-语言模型的基准</div>
<div class="mono" style="margin-top:8px">超声是一种广泛使用的成像方式，对全球医疗至关重要，但由于其图像质量在操作者、噪声和解剖结构上的差异，其解读仍具挑战性。尽管大视觉-语言模型（LVLMs）在自然和医学领域展示了令人印象深刻的多模态能力，但其在超声方面的表现仍鲜有研究。我们引入了U2-BENCH，这是首个全面评估LVLMs在超声理解上的基准，涵盖分类、检测、回归和文本生成等任务。U2-BENCH汇集了7,241个案例，涉及15个解剖区域，并在50个超声应用场景中定义了8个临床启发式任务，如诊断、视图识别、病灶定位、临床价值评估和报告生成。我们评估了23个最先进的LVLMs，包括开源和闭源、通用型和医学专用型模型。我们的结果表明，在图像分类任务上表现强劲，但在空间推理和临床语言生成方面仍存在持续挑战。U2-BENCH建立了一个严格且统一的测试平台，用于评估和加速医学超声成像这一独特多模态领域的LVLM研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the lack of evaluation frameworks for large vision-language models (LVLMs) in the context of ultrasound understanding, which is crucial for global healthcare but challenging due to image variability and noise. U2-BENCH is introduced as the first comprehensive benchmark that assesses LVLMs across multiple tasks including classification, detection, regression, and text generation. It includes 7,241 cases from 15 anatomical regions and covers 50 ultrasound application scenarios with 8 clinically relevant tasks. The evaluation of 23 state-of-the-art LVLMs shows strong performance in image-level classification but highlights persistent difficulties in spatial reasoning and clinical language generation.</div>
<div class="mono" style="margin-top:8px">该研究提出了U2-BENCH，这是首个用于评估大视觉-语言模型（LVLM）在超声理解方面表现的全面基准。U2-BENCH包含15个解剖区域的7241个案例，并定义了8个临床相关的任务，涵盖分类、检测、回归和文本生成。该基准评估了23个最先进的LVLM，结果显示在图像分类任务上表现良好，但在空间推理和临床语言生成方面仍存在持续挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models</div>
<div class="meta-line">Authors: Cristian Sbrolli, Matteo Matteucci, Toshihiko Yamasaki</div>
<div class="meta-line">First: 2026-02-02T12:39:39+00:00 · Latest: 2026-02-02T12:39:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02043v1">PDF</a> · <a href="https://huggingface.co/AutoComp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing &quot;a red cube and a blue sphere&quot; with &quot;a blue cube and a red sphere&quot;. Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., &quot;a monitor to the left of a bicycle on a white background&quot;) and LLM-generated Contextual captions (e.g., &quot;In a brightly lit photography studio, a monitor is positioned to the left of a bicycle&quot;), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel &quot;Confusion Benchmark&quot; reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Auto-Comp：用于对比视觉-语言模型可扩展组合探查的自动化流水线</div>
<div class="mono" style="margin-top:8px">现代视觉-语言模型（VLMs）在组合推理方面存在关键缺陷，常常将 &quot;一个红色立方体和一个蓝色球体&quot; 与 &quot;一个蓝色立方体和一个红色球体&quot; 混淆。分离这些失败的视觉和语言根源是实现稳健评估的基本挑战。为了实现细粒度、可控的分析，我们引入了 Auto-Comp，这是一个完全自动化且合成的流水线，用于生成可扩展的基准测试。其可控性是解构和隔离不同推理能力的关键。Auto-Comp 从最小化描述（例如：&quot;一个显示器位于白色背景上的自行车左侧&quot;）和大语言模型生成的上下文描述（例如：&quot;在一个明亮的摄影工作室中，显示器位于自行车左侧&quot;）生成配对图像，从而允许进行受控的 A/B 测试，以区分核心绑定能力与视觉-语言复杂性。我们在新的基准测试上评估了 20 个 VLMs，发现 CLIP 和 SigLIP 模型家族在颜色绑定和空间关系方面都存在普遍的组合失败。关键的是，我们提出的新型 &quot;混淆基准&quot; 揭示了一个比简单属性交换更深层次的缺陷：模型对低熵干扰项（例如重复的物体或颜色）高度敏感，表明其组合失败不仅限于已知的词袋模型限制。我们发现了一个令人惊讶的权衡：视觉-语言上下文虽然提供了全局场景线索，有助于空间推理，但同时通过引入视觉杂乱而阻碍了局部属性绑定。我们发布了 Auto-Comp 流水线，以促进未来基准测试的创建，并发布了所有生成的基准测试（https://huggingface.co/AutoComp）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the critical issue of compositional reasoning in Vision-Language Models (VLMs), where models often confuse attribute swaps such as color and shape in object descriptions. To enable detailed and controlled analysis, the authors propose Auto-Comp, an automated pipeline that generates synthetic benchmarks by pairing minimal images with contextual captions. Evaluation of 20 VLMs on these benchmarks shows widespread compositional failures in both CLIP and SigLIP models, particularly in color binding and spatial relations. A key finding is that visio-linguistic context improves spatial reasoning but impairs attribute binding due to visual clutter, highlighting a trade-off in model performance. The pipeline and benchmarks are released to support future research in this area.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在组合推理中的关键缺陷，即模型常混淆属性交换的描述，如将&#x27;一个红色立方体和一个蓝色球体&#x27;误认为&#x27;一个蓝色立方体和一个红色球体&#x27;。为实现细粒度且可控的分析，作者提出了Auto-Comp，一个自动化生成合成基准的管道。通过将最小图像与上下文描述配对，该方法支持A/B测试以区分核心绑定能力与视觉语言复杂性。在对20个VLMs的新基准评估中，发现CLIP和SigLIP模型家族均存在普遍的组合推理失败。研究还引入了&#x27;混淆基准&#x27;，揭示了模型对低熵干扰项（如重复物体或颜色）的高度敏感，表明其组合推理错误超出了已知的词袋模型限制。此外，研究发现视觉语言上下文虽有助于空间推理，但会引入视觉干扰，从而阻碍局部属性绑定。</div>
</details>
</div>
<div class="card">
<div class="title">Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models</div>
<div class="meta-line">Authors: Wenhui Tan, Fiorenzo Parascandolo, Enver Sangineto, Jianzhong Ju, Zhenbo Luo, Qian Cao, Rita Cucchiara, Ruihua Song, Jian Luan</div>
<div class="meta-line">First: 2026-02-02T06:12:33+00:00 · Latest: 2026-02-02T06:12:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01698v1">PDF</a> · <a href="https://GitHub.com/Xiaomi-Research/LED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在微调后恢复探索：用于大推理模型的潜在探索解码</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过强化学习（RL）微调后最近在数学和代码推理方面表现出色。然而，我们发现现代推理微调会导致非预期的探索崩溃：基于温度的采样不再提升pass@$n$准确率。实证研究表明，微调后的LRMs最后一层的后验熵显著降低，而中间层的熵则相对较高。基于这种熵的不对称性，我们提出了潜在探索解码（LED），这是一种深度条件解码策略。LED通过累积求和聚合中间后验，并选择熵最大的深度配置作为探索候选。无需额外训练或参数，LED在多个推理基准和模型上持续提升了pass@1和pass@16的准确率，分别提高了0.61和1.03个百分点。项目页面：https://GitHub.com/Xiaomi-Research/LED。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of exploration collapse in Large Reasoning Models (LRMs) after reinforcement learning post-training, where temperature-based sampling fails to improve pass@$n$ accuracy. The authors propose Latent Exploration Decoding (LED), a decoding strategy that leverages the entropy asymmetry between intermediate and final layers of post-trained models. LED combines intermediate posteriors using a cumulative sum approach and selects depth configurations with the highest entropy as exploration candidates. Experimental results show that LED consistently enhances pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across various reasoning benchmarks without requiring additional training or parameters.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型（LRMs）在强化学习微调后出现的探索崩溃问题，提出了解决方案。研究发现，在微调后的模型中，最后一层的后验熵显著降低，而中间层的熵相对较高。为此，作者提出了基于深度条件的解码策略——潜在探索解码（LED），通过累积中间层后验熵来选择具有最大熵的深度配置作为探索候选。实验表明，在多个推理基准测试中，LED在不增加额外训练或参数的情况下，能够分别提升pass@1和pass@16的准确率0.61和1.03个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-02T05:17:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v1">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在单个问题的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念导向任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ProjDevBench is to address the limitations of current evaluations of coding agents, which primarily focus on bug fixing rather than end-to-end project development. The benchmark introduces a comprehensive evaluation framework that combines Online Judge testing with LLM-assisted code review to assess agents on system architecture design, functional correctness, and iterative solution refinement. It includes 20 programming problems across 8 categories, evaluating six coding agents based on different LLM backends. The main experimental results show that while agents perform well on basic functionality and data structures, they struggle with complex system design, time complexity optimization, and resource management, leading to an overall acceptance rate of 27.38%.</div>
<div class="mono" style="margin-top:8px">ProjDevBench的提出旨在弥补当前编码代理评估方法的不足，这些方法主要集中在单个问题的修复上，而忽视了完整的端到端项目开发。该基准结合了在线评测与大语言模型辅助的代码审查，从系统架构设计、功能正确性及迭代解决方案优化三个维度评估编码代理。它包含8个类别下的20个编程问题，涵盖概念性任务和现实应用场景，并对基于不同LLM后端构建的六个编码代理进行了测试。实验结果显示，整体通过率为27.38%，表明尽管代理能够处理基础功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面仍存在较大困难。</div>
</details>
</div>
<div class="card">
<div class="title">From Perception to Action: Spatial AI Agents and World Models</div>
<div class="meta-line">Authors: Gloria Felicia, Nolan Bryant, Handi Putra, Ayaan Gazali, Eliel Lobo, Esteban Rojas</div>
<div class="meta-line">First: 2026-02-02T05:00:55+00:00 · Latest: 2026-02-02T05:00:55+00:00</div>
<div class="meta-line">Comments: 61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01644v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从感知到行动：空间AI代理与世界模型</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型已成为代理推理和规划的主流方法，但它们在符号领域的成功并不容易直接应用于物理世界。空间智能，即感知三维结构、推理物体关系并在物理约束下行动的能力，是一种与之正交的重要能力，对具身代理尤为关键。现有的综述文章要么单独讨论代理架构，要么单独探讨空间领域，没有提供一个统一的框架来连接这些互补的能力。本文填补了这一空白。通过全面回顾2000多篇论文，引用了顶级会议中的742项研究，我们提出了一种统一的三轴分类体系，将代理能力与空间任务在不同尺度上联系起来。关键的是，我们区分了空间锚定（对几何和物理的度量理解）与符号锚定（将图像与文本关联），认为仅凭感知并不能赋予代理能力。我们的分析揭示了三个关键发现，对应这三个轴：(1) 分层记忆系统（能力轴）对于长时域空间任务至关重要。(2) GNN-LLM的集成（任务轴）是结构化空间推理的一种有前景的方法。(3) 世界模型（尺度轴）对于在微到宏观空间尺度上安全部署至关重要。最后，我们识别出六个重大挑战，并概述了未来研究的方向，包括需要统一的评估框架以标准化跨领域的评估。该分类体系为统一分散的研究努力奠定了基础，并有助于在机器人、自动驾驶和地理空间智能等领域实现下一代空间感知的自主系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating agentic reasoning with spatial intelligence to enable embodied agents to operate effectively in the physical world. It proposes a unified three-axis taxonomy that connects agentic capabilities, spatial tasks, and spatial scales, distinguishing between spatial grounding and symbolic grounding. The authors conducted a comprehensive review of over 2,000 papers, highlighting three key findings: hierarchical memory systems support long-term spatial tasks, GNN-LLM integration enhances structured spatial reasoning, and world models are critical for safe operation across different spatial scales. The study also outlines six major challenges and suggests future research directions aimed at creating more cohesive and standardized evaluation frameworks for spatial AI agents.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将代理推理与空间智能结合以实现物理世界中具身智能代理有效运作的挑战。作者提出了一种统一的三轴分类法，将代理能力与空间任务在不同尺度上联系起来，区分了空间锚定与符号锚定。研究发现：分层记忆系统对长期空间任务至关重要，图神经网络与语言模型的结合有助于结构化空间推理，而世界模型对于在微至宏观空间尺度上安全部署具有关键作用。这些成果为未来研究和开发更强大的空间感知自主系统奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Autoregressive-Diffusion Collaborative Thoughts</div>
<div class="meta-line">Authors: Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu</div>
<div class="meta-line">First: 2026-02-02T03:54:15+00:00 · Latest: 2026-02-02T03:54:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自回归-扩散协同思维的推理</div>
<div class="mono" style="margin-top:8px">自回归模型和扩散模型代表两种互补的生成范式。自回归模型擅长于序列规划和约束组合，但在需要显式空间或物理基础的任务中表现不佳。相比之下，扩散模型通过高维生成捕捉丰富的空间结构，但缺乏满足复杂多阶段约束或可靠识别和纠正错误所需的逐步逻辑控制。我们提出了协同思维（Collaborative Thoughts），这是一种统一的协同框架，使自回归模型和扩散模型能够通过闭环交互共同进行推理和生成。在协同思维中，自回归模型执行结构化规划和约束管理，扩散模型将这些约束实例化为中间的视觉思维，而基于视觉的批评模块则评估这些视觉思维是否满足预期的结构和物理要求。这种反馈随后用于迭代优化后续的规划和生成步骤，从而减少跨模态的误差传播。重要的是，无论任务是自回归问答还是基于扩散的视觉生成，协同思维都使用相同的协同循环。通过代表性示例，我们展示了协同思维如何提高空间推理的可靠性以及生成的可控性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of both autoregressive and diffusion models in handling tasks that require spatial reasoning and multi-stage constraint satisfaction. The proposed method, Collaborative Thoughts, integrates these two paradigms through a closed-loop interaction, where autoregressive models handle structured planning and constraint management, diffusion models generate intermediate visual representations, and a vision-based critic module evaluates and provides feedback to refine the process. Experimental results demonstrate that this framework enhances the reliability of spatial reasoning and improves the controllability of generation across different tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决自回归模型和扩散模型在处理需要空间推理和多阶段约束满足的任务时的局限性。提出的方法Collaborative Thoughts将这两种范式整合到一个统一框架中，其中自回归模型负责结构化规划和约束管理，扩散模型则生成中间的视觉表示。一个基于视觉的批评模块评估这些视觉表示是否符合结构和物理要求，并提供反馈以迭代优化规划和生成步骤。实验结果表明，该协作方法提高了空间推理的可靠性，并增强了生成过程的可控性。</div>
</details>
</div>
<div class="card">
<div class="title">Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance</div>
<div class="meta-line">Authors: Aliaksei Kaliutau</div>
<div class="meta-line">First: 2025-12-09T11:11:37+00:00 · Latest: 2026-02-01T22:18:46+00:00</div>
<div class="meta-line">Comments: 21 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08492v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.08492v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the &quot;Semantic Trap&quot; inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主问题解决者：迈向零触代码维护</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型在函数级代码生成方面取得了重大进展；然而，仓库级的自动程序修复（APR）仍然是一个重大挑战。当前方法通常采用以控制为中心的范式，迫使代理在复杂的目录结构和无关的控制逻辑中导航。在本文中，我们提出了一种范式转变，从标准的代码属性图（CPGs）转向数据转换图（DTG）的概念，通过将数据状态建模为节点、函数建模为边，反转拓扑结构，使代理能够通过数据血缘追踪逻辑缺陷，而非控制流。我们引入了一个多代理框架，将数据完整性导航与控制流逻辑相结合。我们的理论分析和案例研究表明，这种方法解决了现代代码代理中标准RAG系统固有的“语义陷阱”问题。我们提供了名为自主问题解决者（AIR）的全面实现，这是一个用于零触代码维护的自改进系统，利用神经符号推理，并采用DTG结构实现可扩展的逻辑修复。我们的方法在多个软件工程基准测试中表现良好，在SWE-Verified基准测试中达到了87.1%的解决率。我们的方法直接解决了当前AI代码辅助工具的核心限制，并应对了我们日益依赖软件的世界对更强大基础的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of repository-scale Automated Program Repair (APR) by proposing a shift from traditional control-centric paradigms to a data-centric approach using Data Transformation Graphs (DTGs). This method models data states as nodes and functions as edges, allowing agents to trace logic defects through data lineage rather than control flow. The authors introduce a multi-agent framework that integrates data integrity navigation with control flow logic, and their implementation, Autonomous Issue Resolver (AIR), demonstrates effectiveness in resolving code issues with an 87.1% resolution rate on the SWE-Verified benchmark, highlighting its potential to improve AI code-assistants.</div>
<div class="mono" style="margin-top:8px">本文针对仓库级自动化程序修复（APR）的挑战，提出了一种从传统控制中心范式向数据中心范式转变的方法，使用数据转换图（DTG）建模。该方法将数据状态作为节点，函数作为边，使代理能够通过数据血缘追踪逻辑缺陷，而非控制流。作者引入了一个多代理框架，将数据完整性导航与控制流逻辑相结合，并实现为零触代码维护的自主问题解决器（AIR）系统。在多个SWE基准测试中，实验结果表明AIR在SWE-Verified基准测试中达到了87.1%的解决率，展示了其在克服当前AI代码辅助工具局限性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ASP-Bench: From Natural Language to Logic Programs</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2026-02-01T11:48:36+00:00 · Latest: 2026-02-01T11:48:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01171v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem&#x27;s modeling hardness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ASP-Bench：从自然语言到逻辑程序</div>
<div class="mono" style="margin-top:8px">将自然语言规范自动转换为逻辑程序是一项具有挑战性的任务，影响神经符号工程。我们提出了ASP-Bench，这是一个包含128个自然语言问题实例的基准测试集，其中64个基础问题包含简单和困难变体。它评估将自然语言问题转换为答案集程序（ASPs）的系统，ASPs是逻辑编程的一种重要形式。该基准提供了系统性的ASP功能覆盖，包括选择规则、聚合和优化。每个问题都包含参考验证器，用于检查解决方案是否符合问题规范。
我们从七个主要独立的推理方面（优化、时态推理、默认逻辑、资源分配、递归、空间推理和定量复杂度）对问题进行分类，提供对建模难度的多维视角。
我们采用基于ReAct（推理与行动）框架的代理方法测试该基准，实现了完全饱和，证明了以求解器反馈驱动的迭代细化是一种可靠且稳健的方法，用于在ASP中建模自然语言。我们的多代理运行分析使我们能够深入了解决定问题建模难度的因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of ASP-Bench is to address the challenge of automatically translating natural language specifications into logic programs, which is crucial for neurosymbolic engineering. The benchmark includes 128 natural language problem instances, with 64 base problems featuring easy and hard variants, and evaluates systems that convert these into Answer Set Programs (ASPs). It systematically covers various ASP features such as choice rules, aggregates, and optimization, along with reference validators to ensure solution correctness. The main experimental results show that an agentic approach based on the ReAct framework achieves full saturation, indicating that feedback-driven iterative refinement using solver feedback is a reliable method for modeling natural language in ASP.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决将自然语言规范自动转换为逻辑程序的挑战，这对神经符号系统工程至关重要。作者提出了ASP-Bench，这是一个包含128个自然语言问题实例的基准测试集，其中64个基础问题包含易和难两种变体，用于评估将自然语言转化为答案集程序（ASPs）的系统。该基准全面覆盖了ASPs的多种特性，如选择规则、聚合和优化，并包含参考验证器以确保解决方案的正确性。通过基于ReAct框架的代理方法，研究展示了利用求解器反馈进行反馈驱动的迭代优化能够有效建模自然语言问题，实现完全饱和。对多个代理运行的分析揭示了影响不同问题建模难度的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Capabilities and Fundamental Limits of Latent Chain-of-Thought</div>
<div class="meta-line">Authors: Jiaxuan Zou, Yaozhong Xiong, Yong Liu</div>
<div class="meta-line">First: 2026-02-01T10:46:00+00:00 · Latest: 2026-02-01T10:46:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01148v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01148v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在链式思维（Latent CoT）的能力及其基本限制</div>
<div class="mono" style="margin-top:8px">潜在链式思维（Latent CoT）模型通过连续表示实现高效推理，但表现出令人困惑的性能不一致性：在探索方面表现优异（ProsQA: 97.0%），但在计算方面表现不佳（GSM8K: 34.1%）。我们揭示了这种权衡是由决策确定性所主导的。我们的贡献有三个方面：(1) 我们从理论上刻画了基本的探索-执行权衡，证明高确定性能够实现精确执行但抑制探索，而低确定性则促进搜索但导致误差累积。(2) 我们引入符号索引——量化决策承诺——作为控制这种权衡的核心机制，并建立了其与执行稳定性及探索能力之间的因果关系。(3) 我们证明了课程学习在理论上是必要的，因为直接训练由于分布不匹配而注定失败。我们的框架将设计范式从二元架构选择转向能够根据任务需求动态调节决策确定性的自适应系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the capabilities and limitations of Latent Chain-of-Thought (Latent CoT) models, which aim to achieve efficient reasoning through continuous representations. The study identifies a key performance trade-off between exploration and computation, where models excel in exploration tasks but underperform in computational ones. The authors theoretically analyze this trade-off, showing that decisional certainty plays a central role, with high certainty improving execution precision but limiting exploration, and low certainty enhancing search but increasing error accumulation. They propose the Symbolic Index as a mechanism to quantify decisional commitment and demonstrate its causal impact on both execution stability and exploration ability. Furthermore, they prove that curriculum learning is necessary for effective training, as direct training suffers from distributional mismatch. The framework suggests a shift from fixed architectural designs to adaptive systems that adjust decisional certainty according to task requirements.</div>
<div class="mono" style="margin-top:8px">本文探讨了基于连续表示的潜在链式思维（Latent CoT）模型的能力与局限性。研究发现，这类模型在探索任务中表现优异，但在计算任务中效果不佳，揭示了探索与执行之间的权衡关系。作者从理论上分析了这一现象，指出决策确定性是影响该权衡的核心因素，高确定性有助于执行稳定性但抑制探索，低确定性则促进探索却导致误差累积。他们提出了符号索引这一指标，用于量化决策承诺，并证明其对模型行为的因果影响。此外，作者证明了课程学习在训练中的必要性，因为直接训练会因分布不匹配而失败。这些发现表明，模型设计应转向能够根据任务需求动态调节决策确定性的自适应系统。</div>
</details>
</div>
<div class="card">
<div class="title">KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization</div>
<div class="meta-line">Authors: Alireza Nadafian, Alireza Mohammadshahi, Majid Yazdani</div>
<div class="meta-line">First: 2026-01-29T10:40:54+00:00 · Latest: 2026-01-31T20:40:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21526v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21526v2">PDF</a> · <a href="https://github.com/Leeroo-AI/kapso">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAPSO：一种基于知识的自主程序合成与优化框架</div>
<div class="mono" style="margin-top:8px">我们引入了KAPSO，一种用于自主程序合成与优化的模块化框架。给定一个自然语言目标和评估方法，KAPSO迭代地执行构思、代码合成与编辑、执行、评估和学习，以改进可运行的产物，使其朝着可衡量的目标发展。KAPSO不将合成视为终点，而是将其作为长周期优化循环中的一个操作符，其中进展由评估器的结果定义。
KAPSO通过集成三个紧密耦合的组件，针对编码代理中常见的长周期失败问题，包括实验状态丢失、脆弱的调试和领域专业知识的弱重用。第一，一个原生支持Git的实验引擎将每次尝试隔离为一个分支，生成可复现的产物，并在迭代过程中保留溯源信息。第二，一个知识系统吸收异构来源，包括仓库、内部操作手册以及经过整理的外部资源，如文档、科学论文和网络搜索结果，并将它们组织成结构化的表示，以支持对工作流、实现和环境约束的检索。第三，一个认知记忆层协调检索，并维护一个可重用的事件记忆库，该记忆库从实验轨迹（运行日志、差异和评估反馈）中提炼出可复用的教训，减少重复的错误模式并加快收敛。
我们在MLE-Bench（Kaggle风格的机器学习竞赛）和ALE-Bench（AtCoder启发式优化）上评估了KAPSO，并报告了端到端的性能表现。
代码地址：https://github.com/Leeroo-AI/kapso</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces KAPSO, a modular framework designed to address long-horizon failures in autonomous program synthesis and optimization. It integrates a git-native experimentation engine, a knowledge system that organizes heterogeneous sources into a structured representation, and a cognitive memory layer that stores reusable lessons from past experiments. These components work together to enable iterative ideation, code synthesis, execution, evaluation, and learning, improving the quality of generated code toward specific objectives. Evaluation on MLE-Bench and ALE-Bench shows that KAPSO achieves end-to-end performance improvements by reducing repeated errors and accelerating convergence.</div>
<div class="mono" style="margin-top:8px">KAPSO的动机是解决编码代理中常见的长周期失败问题，如实验状态丢失、调试脆弱性和领域知识重用不足。该框架采用模块化设计，集成了一个原生支持git的实验引擎、一个组织异构信息源（如仓库、内部指南和外部文档）的结构化知识系统，以及一个从实验轨迹中提取可重用经验的认知记忆层。这些组件协同工作，实现迭代的创意生成、代码合成与编辑、执行、评估和学习，以提升程序合成与优化的性能。在MLE-Bench和ALE-Bench上的实验结果表明，KAPSO在端到端的程序合成与优化任务中表现出显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association</div>
<div class="meta-line">Authors: Rong-Lin Jian, Ming-Chi Luo, Chen-Wei Huang, Chia-Ming Lee, Yu-Fan Lin, Chih-Chung Hsu</div>
<div class="meta-line">First: 2026-01-31T03:08:48+00:00 · Latest: 2026-01-31T03:08:48+00:00</div>
<div class="meta-line">Comments: Winner Solution of SoccerTrack in ACM Multimedia 2025 Workshop MMSports</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00484v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00484v1">PDF</a> · <a href="https://github.com/ron941/GTATrack-STC2025">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTATrack：2025年SoccerTrack挑战赛冠军解决方案，采用Deep-EIoU与全局跟踪片段关联</div>
<div class="mono" style="margin-top:8px">由于球员运动不规则、外观相似以及频繁遮挡，体育场景中的多目标跟踪（MOT）极具挑战性。这些问题在静态鱼眼摄像头引入的几何畸变和极端尺度变化下进一步加剧。本文提出GTATrack，这是一种分层跟踪框架，在2025年SoccerTrack挑战赛中获得第一名。GTATrack集成了两个核心组件：用于运动无关在线关联的Deep Expansion IoU（Deep-EIoU）和用于轨迹级优化的Global Tracklet Association（GTA）。这种两阶段设计实现了稳健的短期匹配和长期身份一致性。此外，采用伪标签策略以提升检测器对小且畸变目标的召回率。局部关联与全局推理的协同作用有效解决了身份切换、遮挡和跟踪碎片化问题。我们的方法取得了0.60的冠军HOTA分数，并将误检显著降低至982，展示了基于鱼眼摄像头的足球跟踪的最先进精度。我们的代码可在https://github.com/ron941/GTATrack-STC2025获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-object tracking in sports is challenging due to irregular player motion, uniform appearances, and frequent occlusions, especially under fisheye camera distortions. GTATrack, a hierarchical tracking framework, addresses these issues by combining Deep-EIoU for motion-agnostic online association and Global Tracklet Association for trajectory-level refinement. This two-stage approach ensures robust short-term matching and long-term identity consistency, while a pseudo-labeling strategy improves detection of small and distorted targets. The method achieved a winning HOTA score of 0.60 and reduced false positives to 982, demonstrating superior performance in fisheye-based soccer tracking.</div>
<div class="mono" style="margin-top:8px">由于球员运动不规则、外观相似以及频繁遮挡，体育场景中的多目标跟踪极具挑战性，尤其在使用静态鱼眼镜头时，几何畸变和极端尺度变化进一步加剧了这些问题。GTATrack 是一种分层跟踪框架，通过结合 Deep-EIoU 实现运动无关的在线关联，并利用 Global Tracklet Association 进行轨迹级优化，从而确保短期匹配的鲁棒性和长期身份的一致性。此外，伪标签策略提升了对小目标和畸变目标的检测召回率。该方法在鱼眼镜头下的足球跟踪任务中取得了 HOTA 得分 0.60 的最佳成绩，并将误检数降至 982，展现出领先的跟踪精度。</div>
</details>
</div>
<div class="card">
<div class="title">Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks</div>
<div class="meta-line">Authors: Jia Liang, Liangming Pan</div>
<div class="meta-line">First: 2026-01-31T01:48:23+00:00 · Latest: 2026-01-31T01:48:23+00:00</div>
<div class="meta-line">Comments: 20 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00449v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在链式推理模型是否分步骤思考？对序列推理任务的机制性研究</div>
<div class="mono" style="margin-top:8px">潜在链式推理（Latent-CoT）旨在实现无需输出长推理过程的分步计算，但其机制仍不明确。我们研究了CODI，一种连续推理的教师-学生蒸馏模型，在严格序列的多项式迭代任务上的表现。通过logit-lens解码、线性探针、注意力分析和激活补丁技术，我们定位了中间状态表示并追踪其路由至最终输出。在两步和三步任务中，CODI形成了完整的桥接状态，这些状态在潜在推理位置上可解码，而最终输入则遵循一条近似直接的路径；预测结果在推理结束边界通过晚期融合产生。对于更长的推理步数，CODI无法可靠地执行完整的潜在推理过程，而是表现出一种部分潜在推理路径，专注于晚期中间状态，并在答案读取位置将其与最后输入融合。消融实验表明，这种部分路径在环境变化或更难的优化条件下可能会崩溃。总体而言，我们界定了CODI式潜在链式推理在何种情况下能实现忠实的迭代计算，而非压缩或捷径策略，并突出了设计稳健潜在链式推理目标的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether Latent-CoT models perform step-by-step reasoning by analyzing CODI, a continuous-thought teacher-student distillation model, on sequential polynomial-iteration tasks. Through logit-lens decoding, linear probes, attention analysis, and activation patching, the researchers identify intermediate-state representations and their routing to the final output. The findings reveal that CODI constructs full bridge states for two- and three-hop tasks, which are decodable across latent-thought positions, while the final input takes a near-direct route. For longer hop lengths, CODI uses a partial reasoning path that focuses on late intermediates and fuses them with the last input at the answer position. Ablation experiments show that this partial pathway can be unstable under regime shifts, indicating challenges in ensuring faithful iterative computation in latent-CoT models.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Latent-CoT模型是否能够进行分步推理，通过分析CODI这一连续思维的教师-学生蒸馏模型在顺序多项式迭代任务中的表现。研究者采用logit-lens解码、线性探针、注意力分析和激活补丁等方法，考察中间表示及其对最终输出的贡献。结果表明，在两步和三步任务中，CODI生成完整的桥接状态，并在不同潜思维位置中可解码，而最终输入则通过近似直接路径到达输出，预测结果在思维结束边界通过晚期融合产生。但在更长的步骤任务中，CODI无法可靠地执行完整的潜思维路径，而是集中于晚期中间状态，并在答案读取位置与最后输入融合。消融实验进一步显示，这种部分推理路径可能在某些优化条件下失效，提示需要更稳健的潜思维推理目标设计。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Over Scale: Learning Spatial Reasoning from Educational Video</div>
<div class="meta-line">Authors: Bishoy Galoaa, Xiangyu Bai, Sarah Ostadabbas</div>
<div class="meta-line">First: 2026-01-30T18:20:23+00:00 · Latest: 2026-01-30T18:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23251v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.23251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children&#x27;s educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化超越规模：从教育视频中学习空间推理</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在标准视频理解基准上表现出色，但在幼儿能够解决的简单推理任务（如计数、空间推理和组合理解）上系统性地表现不佳。我们假设教育视频中结构化的教学内容为提升这些能力提供了理想的训练信号。我们引入了DoraVQA数据集，该数据集包含从8季《朵拉探险》中自动提取的5,344个问答对，并具有精确的时间戳对齐。每一集都遵循一致的\textit{上下文-问题-暂停-回答}结构，创建了一个类似互动辅导的自包含学习环境。我们使用分组相对策略优化（GRPO）对Qwen2和Qwen3进行微调，利用教育内容中固有的清晰正确性信号和结构化推理轨迹。尽管仅在38小时的儿童教育视频上进行训练，我们的方法在DoraVQA上实现了8-14分的提升，并在CVBench上达到了86.16\%的最先进水平，同时在Video-MME和NExT-QA上表现出强大的迁移能力，证明了从狭窄的教学内容到广泛多模态理解的有效泛化。通过跨领域基准测试，我们展示了VLMs能够执行需要从结构化教育内容中学习的稳健推理任务，表明内容结构与内容规模同样重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of vision-language models (VLMs) in handling simple reasoning tasks such as counting and spatial reasoning, which preschool children can easily solve. The authors propose DoraVQA, a dataset derived from educational videos of Dora the Explorer, structured with precise timestamp alignment and a consistent context-question-pause-answer format. By fine-tuning Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), they leverage the clear correctness signals and structured reasoning traces in educational content. The results show significant improvements on DoraVQA and state-of-the-art performance on CVBench, with strong transferability to other benchmarks, indicating that structured content is crucial for enhancing reasoning capabilities beyond mere scale.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型（VLMs）在处理学前儿童能够完成的简单推理任务（如计数和空间推理）方面的不足。作者提出了DoraVQA数据集，该数据集从Dora the Explorer的教育视频中自动提取，包含5,344对问题-答案对，并精确对齐时间戳。通过使用组相对策略优化（GRPO）方法对Qwen2和Qwen3进行微调，模型能够更好地理解和推理空间与组合信息。实验结果表明，在DoraVQA上取得了8-14分的提升，并在CVBench上达到当前最优的86.16%，同时在其他基准测试中表现出良好的迁移能力，说明结构化的教育内容可以有效提升VLMs的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Bridging the Planner-Coder Gap: A Systematic Study on the Robustness of Multi-Agent Systems for Code Generation</div>
<div class="meta-line">Authors: Zongyi Lyu, Songqiang Chen, Zhenlan Ji, Liwen Wang, Shuai Wang, Daoyuan Wu, Wenxuan Wang, Shing-Chi Cheung</div>
<div class="meta-line">First: 2025-10-12T05:45:04+00:00 · Latest: 2026-01-30T16:44:46+00:00</div>
<div class="meta-line">Comments: 18pages, 5 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10460v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10460v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems (MASs) have emerged as a promising paradigm for automated code generation, demonstrating impressive performance on established benchmarks. Despite their prosperous development, the fundamental mechanisms underlying their robustness remain poorly understood, raising critical concerns for real-world deployment. This paper conducts a systematic empirical study to uncover the internal robustness flaws of MASs using a mutation-based methodology. By designing a testing pipeline incorporating semantic-preserving mutation operators and a novel fitness function, we assess mainstream MASs across multiple datasets and LLMs. Our findings reveal substantial robustness flaws: semantically equivalent inputs cause drastic performance drops, with MASs failing to solve 7.9\%--83.3\% of problems they initially resolved successfully.
  Through comprehensive failure analysis, we discover a fundamental cause underlying these robustness issues: the \textit{planner-coder gap}, which accounts for 75.3\% of failures. This gap arises from information loss in the multi-stage transformation process where planning agents decompose requirements into underspecified plans, and coding agents subsequently misinterpret intricate logic during code generation. Based on this formulated information transformation process, we propose a \textit{repairing method} that mitigates information loss through multi-prompt generation and introduces a monitor agent to bridge the planner-coder gap. Evaluation shows that our repairing method effectively enhances the robustness of MASs by solving 40.0\%--88.9\% of identified failures. Our work uncovers critical robustness flaws in MASs and provides effective mitigation strategies, contributing essential insights for developing more reliable MASs for code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解与弥合规划者-编码者差距：多智能体系统在代码生成中鲁棒性的系统性研究</div>
<div class="mono" style="margin-top:8px">多智能体系统（MASs）已成为自动化代码生成的有前景范式，在现有基准测试中表现出色。尽管其发展迅速，但其鲁棒性的基本机制仍不明确，这引发了在现实部署中的关键问题。本文通过基于变异的方法进行系统性实证研究，以揭示MASs内部的鲁棒性缺陷。我们设计了一个测试流程，结合语义保持的变异操作符和一种新颖的适应度函数，对主流MASs在多个数据集和大语言模型（LLMs）上进行评估。我们的研究结果揭示了显著的鲁棒性缺陷：语义等价的输入会导致性能大幅下降，MASs在最初成功解决的问题中，有7.9%至83.3%无法解决。通过全面的失败分析，我们发现这些鲁棒性问题的根本原因在于\textit{规划者-编码者差距}，该差距占所有失败的75.3%。这种差距源于规划智能体将需求分解为未明确指定的计划过程中信息的丢失，以及编码智能体在代码生成过程中对复杂逻辑的误解。基于这一信息转换过程，我们提出了一种\textit{修复方法}，通过多提示生成减少信息丢失，并引入监控智能体以弥合规划者-编码者差距。评估结果表明，我们的修复方法有效提升了MASs的鲁棒性，解决了40.0%至88.9%的已识别失败问题。我们的工作揭示了MASs中的关键鲁棒性缺陷，并提供了有效的缓解策略，为开发更可靠的代码生成MASs提供了重要的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the robustness of multi-agent systems (MASs) in the context of automated code generation, motivated by the lack of understanding of their reliability in real-world applications. The authors employ a mutation-based approach to systematically evaluate MASs, using semantic-preserving mutation operators and a novel fitness function across various datasets and large language models. Their experiments reveal significant robustness issues, with MASs failing to solve a substantial proportion of problems they previously handled correctly. A key finding is the presence of the planner-coder gap, which is identified as the primary cause of these failures. To address this, the paper proposes a repairing method that generates multiple prompts and introduces a monitor agent to reduce information loss during the planning-to-coding transformation. The results show that this method significantly improves the robustness of MASs, resolving 40.0\%--88.9\% of the identified failures.</div>
<div class="mono" style="margin-top:8px">本文旨在研究多智能体系统（MASs）在自动代码生成中的鲁棒性，动机源于其在现实应用中可靠性不足的问题。作者采用基于变异的方法，通过语义保持的变异操作符和新颖的适应度函数，系统地评估了主流MASs在多个数据集和大语言模型上的表现。实验结果表明，MASs存在严重的鲁棒性缺陷，其在最初成功解决的问题中，有7.9\%--83.3\%无法正确处理。研究发现，这些缺陷的根本原因在于规划者与代码生成者之间的信息损失，即所谓的规划-编码差距，占所有失败案例的75.3\%。为此，作者提出了一种修复方法，通过多提示生成和引入监控智能体来减少信息损失，从而有效提升了MASs的鲁棒性，解决了40.0\%--88.9\%的已识别失败案例。</div>
</details>
</div>
<div class="card">
<div class="title">Mem-T: Densifying Rewards for Long-Horizon Memory Agents</div>
<div class="meta-line">Authors: Yanwei Yue, Guibin Zhang, Boci Peng, Xuanbo Fan, Jiaxin Guo, Qiankun Li, Yan Zhang</div>
<div class="meta-line">First: 2026-01-30T14:23:33+00:00 · Latest: 2026-01-30T14:23:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23014v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.23014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mem-T：面向长时序记忆代理的密集奖励</div>
<div class="mono" style="margin-top:8px">通过内源性管理记忆的处理、存储和检索，而非依赖预定义的记忆处理流程，记忆代理因其自主性和适应性而受到越来越多的关注。然而，现有的训练范式仍存在限制：代理通常需要经历长时序的记忆操作序列才能获得稀疏且延迟的奖励，这阻碍了记忆管理策略的真正端到端优化。为了解决这一问题，我们引入了Mem-T，这是一种能够与轻量级分层记忆数据库接口的自主记忆代理，用于对流式输入进行动态更新和多轮检索。为了有效训练长时序的记忆管理能力，我们进一步提出了MoT-GRPO，一种基于记忆操作树的强化学习框架，通过记忆操作树反向传播和回顾性信用分配，将稀疏的终端反馈转化为密集的逐步监督，从而实现记忆构建与检索的联合优化。大量实验表明，Mem-T在（1）性能方面表现出色，相比A-Mem和Mem0等框架提升了高达14.92%；（2）经济性方面也具有优势，其在准确率与效率之间的帕累托前沿表现良好，并且在不牺牲性能的前提下，相比GAM减少了约24.45%的推理令牌数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the training efficiency of memory agents by addressing the challenge of sparse and delayed rewards in long-horizon tasks. The proposed method, Mem-T, introduces an autonomous memory agent that uses a lightweight hierarchical memory database for dynamic memory updates and multi-turn retrieval. To facilitate effective training, the authors develop MoT-GRPO, a tree-guided reinforcement learning framework that converts sparse feedback into dense, step-wise supervision through memory operation tree backpropagation and hindsight credit assignment. Experimental results show that Mem-T achieves superior performance, outperforming existing frameworks like A-Mem and Mem0 by up to 14.92%, while also being more efficient, reducing inference tokens per query by approximately 24.45% without compromising performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升记忆代理在长时序任务中的训练效率，解决现有方法中奖励稀疏且延迟的问题。所提出的方法Mem-T引入了一个自主记忆代理，利用轻量级的层次化记忆数据库实现动态记忆更新与多轮检索。为有效训练长时序记忆管理能力，作者设计了MoT-GRPO，一种基于记忆操作树的强化学习框架，通过记忆操作树反向传播和回顾性信用分配将稀疏终端反馈转化为密集的逐步监督。实验结果表明，Mem-T在性能上优于A-Mem和Mem0高达14.92%，同时在推理令牌消耗上减少约24.45%，且不牺牲性能。</div>
</details>
</div>
<div class="card">
<div class="title">FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations</div>
<div class="meta-line">Authors: Fedor Rodionov, Abdelrahman Eldesokey, Michael Birsak, John Femiani, Bernard Ghanem, Peter Wonka</div>
<div class="meta-line">First: 2025-07-10T11:16:48+00:00 · Latest: 2026-01-30T12:57:19+00:00</div>
<div class="meta-line">Comments: v3, Project page: https://huggingface.co/papers/2507.07644</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.07644v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.07644v3">PDF</a> · <a href="https://huggingface.co/papers/2507.07644">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial reasoning in large-language models (LLMs). FloorplanQA is grounded in structured representations of indoor scenes, such as (e.g., kitchens, living rooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML layouts. The benchmark covers core spatial tasks, including distance measurement, visibility, path finding, and object placement within constrained spaces. Our results across a variety of frontier open-source and commercial LLMs reveal that while models may succeed in shallow queries, they often fail to respect physical constraints, preserve spatial coherence, though they remain mostly robust to small spatial perturbations. FloorplanQA uncovers a blind spot in today&#x27;s LLMs: inconsistent reasoning about indoor layouts. We hope this benchmark inspires new work on language models that can accurately infer and manipulate spatial and geometric properties in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FloorplanQA：使用结构化表示评估大语言模型空间推理能力的基准</div>
<div class="mono" style="margin-top:8px">我们介绍了FloorplanQA，这是一个用于评估大语言模型（LLMs）空间推理能力的诊断基准。FloorplanQA基于室内场景的结构化表示，例如厨房、客厅、卧室、浴室等，这些表示以JSON或XML格式进行符号编码。该基准涵盖了核心空间任务，包括距离测量、可见性判断、路径查找以及在受限空间内放置物体。我们在多种前沿开源和商业LLMs上的结果表明，尽管模型可能在浅层查询中表现良好，但它们常常无法尊重物理约束，保持空间一致性，尽管它们对小的空间扰动仍表现出较强的鲁棒性。FloorplanQA揭示了当前LLMs的一个盲点：对室内布局的不一致推理。我们希望这个基准能够激发新的研究，推动语言模型在实际场景中准确推断和操作空间与几何属性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces FloorplanQA, a benchmark designed to evaluate spatial reasoning capabilities in large language models (LLMs) by using structured representations of indoor scenes. The benchmark includes tasks such as distance measurement, visibility assessment, path finding, and object placement, all encoded in JSON or XML layouts. Experimental results show that while LLMs perform well on simple queries, they often struggle with maintaining spatial coherence and respecting physical constraints, highlighting a gap in their ability to reason about real-world spatial layouts.</div>
<div class="mono" style="margin-top:8px">本文提出了FloorplanQA，这是一个用于评估大语言模型（LLMs）空间推理能力的基准测试，基于室内场景的结构化表示，如厨房、客厅、卧室等，采用符号化的JSON或XML布局。该基准涵盖了距离测量、可见性分析、路径查找和受限空间内物体放置等核心任务。实验结果表明，尽管模型在简单查询上表现良好，但在保持空间连贯性和遵循物理约束方面存在不足，揭示了当前LLMs在处理实际空间布局推理时的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">CATArena: Evaluating Evolutionary Capabilities of Code Agents via Iterative Tournaments</div>
<div class="meta-line">Authors: Lingyue Fu, Xin Ding, Linyue Pan, Yaoming Zhu, Shao Zhang, Lin Qiu, Weiwen Liu, Weinan Zhang, Xuezhi Cao, Xunliang Cai, Jiaxin Ding, Yong Yu</div>
<div class="meta-line">First: 2025-10-30T15:22:53+00:00 · Latest: 2026-01-30T11:04:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26852v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26852v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current evaluation for Large Language Model (LLM) code agents predominantly focus on generating functional code in single-turn scenarios, which fails to evaluate the agent&#x27;s capability for continuous code optimization and multi-turn iterative development. To bridge this gap, we introduce CATArena, a framework designed to evaluate the evolutionary capabilities of code agents via iterative tournaments. Agents engage in multi-turn tournaments and continuously refine their code through self-reflection and peer-learning based on comprehensive execution feedback. For evaluation, we propose a dual-metric system to decouple static generation proficiency from evolutionary potential. Extensive experiments reveal that an agent&#x27;s evolutionary potential is not strictly correlated with its initial proficiency. Our analysis further reveals that current agents struggle to concurrently leverage both peer-learning and self-reflection for effective performance gains. Furthermore, the results validate CATArena&#x27;s high extensibility and resistance to variance tasks, establishing it as a continuous and reliable standard for assessing the evolutionary capability of LLM code agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CATArena：通过迭代竞赛评估代码代理的进化能力</div>
<div class="mono" style="margin-top:8px">当前对大型语言模型（LLM）代码代理的评估主要集中在单轮场景下生成功能性代码，这未能评估代理在持续代码优化和多轮迭代开发中的能力。为弥合这一差距，我们引入了CATArena，一个通过迭代竞赛评估代码代理进化能力的框架。代理参与多轮竞赛，并通过基于全面执行反馈的自我反思和同伴学习持续优化其代码。在评估方面，我们提出了一种双指标系统，以分离静态生成能力与进化潜力。大量实验表明，代理的进化潜力并不严格依赖于其初始能力。我们的分析进一步揭示，当前代理在同时利用同伴学习和自我反思以实现有效性能提升方面存在困难。此外，实验结果验证了CATArena的高可扩展性和对变体任务的鲁棒性，确立了其作为评估LLM代码代理进化能力的持续且可靠标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of current evaluations of Large Language Model (LLM) code agents, which primarily focus on single-turn code generation and neglect continuous optimization and multi-turn iterative development. The proposed method, CATArena, introduces a framework for assessing the evolutionary capabilities of code agents through iterative tournaments, where agents refine their code via self-reflection and peer-learning based on execution feedback. The main experimental results show that an agent&#x27;s evolutionary potential is not strictly tied to its initial proficiency, and that existing agents struggle to effectively combine peer-learning and self-reflection for performance improvement. These findings highlight the importance of evaluating code agents in dynamic, multi-turn settings and validate CATArena as a robust and extensible benchmark for such assessments.</div>
<div class="mono" style="margin-top:8px">该研究针对当前LLM代码代理评估方法的不足，指出现有评估主要关注单轮代码生成，未能有效衡量其持续优化和多轮迭代开发的能力。为此，提出了CATArena框架，通过多轮竞赛评估代码代理的进化能力，代理在竞赛中利用自我反思和同伴学习不断改进代码，基于全面的执行反馈。该框架采用双指标系统，将静态生成能力与进化潜力分离，实验结果表明代理的初始能力并不严格决定其进化潜力。进一步分析发现，现有代理在同时利用同伴学习和自我反思以实现性能提升方面存在困难，而CATArena展现出良好的可扩展性和任务稳定性，成为评估LLM代码代理进化能力的持续可靠标准。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</div>
<div class="meta-line">Authors: Zijian Chen, Wenjun Zhang, Guangtao Zhai</div>
<div class="meta-line">First: 2025-11-12T06:06:29+00:00 · Latest: 2026-01-30T06:43:22+00:00</div>
<div class="meta-line">Comments: 31 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10691v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10691v2">PDF</a> · <a href="https://github.com/zijianchen98/LLM_Squid_Game">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The potential data contamination issue in contemporary large language models (LLMs) benchmarks presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, they predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce \textsc{Squid Game}, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, including instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition in performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. We also compare prominent LLM benchmarks and \textsc{Squid Game}, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. Project page: https://github.com/zijianchen98/LLM_Squid_Game.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从良性到动态对抗：大型语言模型的鱿鱼游戏评估</div>
<div class="mono" style="margin-top:8px">当前大型语言模型（LLMs）基准测试中潜在的数据污染问题，对建立可信赖的评估框架提出了根本性挑战。同时，它们主要假设良性且资源丰富的环境，忽略了LLMs在资源受限和信息不对称情况下的行为表现。本文引入了\textsc{Squid Game}，这是一个动态且对抗性的评估环境，通过与其他LLM对手的互动游戏来评估模型表现。Squid Game包含六个淘汰赛式关卡，重点考察多方面能力，包括指令遵循、代码生成、推理、规划和安全对齐。我们在Squid Game上评估了超过50个LLMs，展示了目前针对动态对抗场景的最全面的LLM行为评估研究。我们观察到同一模型系中存在明显的世代性能跃迁，并发现某些模型可能依赖推测性捷径来赢得游戏，这表明静态基准中可能存在更高层次的评估范式污染。此外，我们还比较了主流的LLM基准测试与\textsc{Squid Game}，指出动态评估可以作为静态评估的补充部分。项目页面：https://github.com/zijianchen98/LLM_Squid_Game。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current large language models (LLMs) benchmarks by introducing Squid Game, a dynamic and adversarial evaluation environment that simulates resource-constrained and asymmetric information scenarios. The motivation stems from the need to assess LLMs under challenging conditions that are not typically covered in static benchmarks. The study evaluates over 50 LLMs across six elimination-style levels, testing various abilities such as instruction-following, reasoning, and safety alignment. The results reveal a generational phase transition in performance within the same model lineage and suggest that some models may exploit speculative shortcuts, highlighting the risk of contamination in static evaluation paradigms. The findings emphasize the value of dynamic evaluation as a complementary approach to static benchmarks for more comprehensive LLM assessment.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型语言模型（LLMs）基准测试存在的局限性，指出其通常假设良性且资源丰富的环境，忽视了模型在对抗性和资源受限条件下的行为表现。作者提出了\textsc{Squid Game}，一个动态对抗性评估框架，通过与其他LLM进行交互式游戏来模拟资源受限和信息不对称的场景。该框架包含六个淘汰赛式关卡，旨在评估指令遵循、代码生成、推理、规划和安全对齐等多方面能力。通过对50多个LLM的评估，研究发现了同一模型系谱中性能的代际跃迁现象，并发现部分模型依赖于投机性捷径来获胜，表明静态基准可能存在评估范式污染。同时，研究还表明动态评估可以作为静态评估的有益补充。</div>
</details>
</div>
<div class="card">
<div class="title">CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning</div>
<div class="meta-line">Authors: Hang Wu, Yujun Cai, Zehao Li, Haonan Ge, Bowen Sun, Junsong Yuan, Yiwei Wang</div>
<div class="meta-line">First: 2026-01-30T04:45:43+00:00 · Latest: 2026-01-30T04:45:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamReasoner：通过结构化空间推理强化摄像机运动理解</div>
<div class="mono" style="margin-top:8px">理解摄像机动态是视频空间智能的基本支柱。然而，现有的多模态模型大多将此任务视为黑箱分类，常常依赖表面视觉模式而非几何线索来混淆物理上不同的运动。我们提出了CamReasoner框架，将摄像机运动理解重新表述为一种结构化推理过程，以弥合感知与电影逻辑之间的差距。我们的方法以观察-思考-回答（O-T-A）范式为核心，迫使模型在一个显式的推理模块中解码时空线索，如轨迹和视锥。为了培养这一能力，我们构建了一个大规模推理轨迹套件，包含18,000个SFT推理链和38,000个RL反馈样本。值得注意的是，我们是首个在该领域使用强化学习进行逻辑对齐的团队，确保运动推理基于物理几何而非情境猜测。通过将强化学习应用于观察-思考-回答（O-T-A）推理范式，CamReasoner有效抑制了幻觉，并在多个基准测试中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the understanding of camera movements in videos by moving beyond superficial visual patterns and incorporating geometric reasoning. CamReasoner introduces an Observation-Thinking-Answer (O-T-A) framework that forces the model to perform structured spatial inference using spatio-temporal cues like trajectories and view frustums. The framework is trained with a large-scale inference trajectory suite containing 18k SFT reasoning chains and 38k RL feedback samples, and it is the first to apply reinforcement learning for logical alignment in this domain. Experimental results show that CamReasoner effectively reduces hallucinations and achieves state-of-the-art performance on various benchmarks.</div>
<div class="mono" style="margin-top:8px">CamReasoner的研究动机是通过引入几何推理来提升视频中相机运动的理解，突破现有模型依赖于表面视觉模式的局限。该框架采用观察-思考-回答（O-T-A）范式，强制模型在显式的推理模块中处理轨迹和视锥等时空线索。实验结果表明，通过使用强化学习对运动推理进行逻辑对齐，CamReasoner在多个基准测试中实现了最先进的性能，并有效减少了幻觉现象，提高了准确性。</div>
</details>
</div>
<div class="card">
<div class="title">FlexMap: Generalized HD Map Construction from Flexible Camera Configurations</div>
<div class="meta-line">Authors: Run Wang, Chaoyi Zhou, Amir Salarpour, Xi Liu, Zhi-Qi Cheng, Feng Luo, Mert D. Pesé, Siyu Huang</div>
<div class="meta-line">First: 2026-01-29T22:41:11+00:00 · Latest: 2026-01-29T22:41:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22376v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22376v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlexMap: 从灵活的相机配置中构建通用的高精地图</div>
<div class="mono" style="margin-top:8px">高精地图为自动驾驶系统提供道路结构的关键语义信息，但当前的高精地图构建方法需要校准的多相机系统以及显式或隐式的2D到BEV变换，这使得它们在传感器失效或不同车辆车队的相机配置变化时变得脆弱。我们引入FlexMap，与之前固定于特定N相机配置的方法不同，我们的方法能够适应不同的相机配置，无需架构更改或针对每个配置重新训练。我们的关键创新在于利用一种具备几何感知能力的基础模型和跨帧注意力机制，通过特征空间隐式编码3D场景理解，从而消除显式的几何投影。FlexMap包含两个核心组件：一个时空增强模块，用于分离跨视图的空间推理与时间动态；以及一个相机感知解码器，其包含潜在的相机标记，使无需投影矩阵即可实现视图自适应注意力。实验表明，FlexMap在多种配置下均优于现有方法，同时保持对缺失视图和传感器变化的鲁棒性，从而实现更实用的现实部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the limitations of current HD map construction methods, which rely on calibrated multi-camera setups and explicit 2D-to-BEV transformations, leading to fragility in varying sensor configurations. FlexMap introduces a novel approach that adapts to different camera configurations without requiring architectural changes or retraining. It leverages a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding. The method includes a spatial-temporal enhancement module and a camera-aware decoder with latent camera tokens, allowing view-adaptive attention. Experimental results show that FlexMap outperforms existing methods across various configurations and maintains robustness to missing views and sensor variations, enhancing its practicality for real-world deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有高精地图构建方法的局限性，这些方法依赖校准的多摄像头系统和2D到鸟瞰图的转换，导致在传感器配置变化时表现脆弱。FlexMap提出了一种新颖的方法，能够在不同摄像头配置下自适应工作，无需架构调整或重新训练。该方法利用具有跨帧注意力的几何感知基础模型，隐式地编码3D场景理解，并包含一个时空增强模块和一个带有潜在摄像头标记的摄像头感知解码器，实现视图自适应处理。实验结果表明，FlexMap在多种配置下均优于现有方法，并在缺失视图和传感器变化的情况下保持鲁棒性，使其更适合实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study</div>
<div class="meta-line">Authors: Khairul Alam, Saikat Mondal, Banani Roy</div>
<div class="meta-line">First: 2026-01-29T22:06:58+00:00 · Latest: 2026-01-29T22:06:58+00:00</div>
<div class="meta-line">Comments: 5 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00164v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么涉及AI代理的修复相关拉取请求仍未被合并？一项实证研究</div>
<div class="mono" style="margin-top:8px">自主编码代理（如OpenAI Codex、Devin、GitHub Copilot）在现实世界软件仓库中越来越多地用于生成修复相关的拉取请求（PR）。然而，它们的实际效果取决于这些贡献是否被项目维护者接受和合并。本文呈现了一项关于涉及AI代理的修复相关PR的实证研究，考察了它们的集成结果、延迟以及阻碍成功合并的因素。我们首先分析了来自AIDEV POP数据集的8,106个由五种广泛使用的AI编码代理撰写的修复相关PR，以量化被合并、关闭但未合并或仍处于开放状态的PR比例。随后，我们对326个关闭但未合并的PR进行了手动定性分析，投入约100人小时构建了一个包含12个失败原因的结构化目录。研究结果表明，测试用例失败和由其他PR已解决的相同问题是最常见的集成失败原因，而构建或部署失败则相对较少。总体而言，我们的发现揭示了当前AI编码代理在现实环境中的关键局限性，并指出了其进一步改进的方向以及在软件维护中更有效的人机协作途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why fix-related pull requests generated by AI coding agents remain unmerged in real-world software repositories. By analyzing 8,106 pull requests from the AIDEV POP dataset, the authors quantify the integration outcomes, including merged, closed without merging, and open PRs. A manual qualitative analysis of 326 closed but unmerged PRs identifies 12 failure reasons, with test case failures and prior resolution of the same issues being the most frequent causes. The findings reveal significant limitations in the practical effectiveness of AI agents and suggest areas for improvement in both AI development and human-AI collaboration in software maintenance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了由AI编码代理生成的修复相关拉取请求（PR）为何在现实软件仓库中未能被合并。通过对AIDEV POP数据集中的8,106个相关PR进行分析，量化了其合并情况，包括已合并、关闭但未合并以及仍处于开放状态的PR。随后，对326个关闭但未合并的PR进行了人工定性分析，耗时约100人小时，构建了一个包含12个失败原因的结构化目录。结果显示，测试用例失败和类似问题已被其他PR解决是导致未合并的主要原因，而构建或部署失败则相对较少。这些发现揭示了当前AI编码代理在实际应用中的关键局限，并指出了改进方向及更有效的软件维护中人机协作策略。</div>
</details>
</div>
<div class="card">
<div class="title">IRIS: Intrinsic Reward Image Synthesis</div>
<div class="meta-line">Authors: Yihang Chen, Yuanhao Ban, Yunqi Hong, Cho-Jui Hsieh</div>
<div class="meta-line">First: 2025-09-29T22:38:25+00:00 · Latest: 2026-01-29T21:09:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25562v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25562v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in math and code reasoning, we show that minimizing self-certainty, rather than maximizing it, improves image generation. We observe that autoregressive T2I models with higher certainty are likely to generate simple and uniform images, which are less aligned with human preferences, and models with lower certainty are likely to generate vivid images rich in detail. Based on this observation, we propose IRIS(Intrinsic Reward Image Synthesis), the first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to autoregressive T2I models achieves performance superior to those trained by individual external rewards, and matching those trained by ensemble external rewards. IRIS also incentivizes the emergence of nuanced CoT reasoning for high-quality image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IRIS：内在奖励图像合成</div>
<div class="mono" style="margin-top:8px">尽管基于人类反馈的强化学习（RLHF）在语言推理方面取得了成功，但其在自回归文本到图像（T2I）生成中的应用往往受到人类偏好数据有限的制约。本文探讨了自回归T2I模型如何在不依赖外部奖励或标注数据的情况下，从内部信号中学习。与近期在数学和代码推理中的发现相反，我们证明了最小化自我确定性，而非最大化，能够提升图像生成效果。我们观察到，确定性较高的自回归T2I模型倾向于生成简单且统一的图像，这些图像与人类偏好不一致；而确定性较低的模型则更可能生成细节丰富、生动的图像。基于这一观察，我们提出了IRIS（内在奖励图像合成），这是首个仅使用内在奖励通过强化学习提升自回归T2I模型的框架。实证结果表明，将IRIS应用于自回归T2I模型，其性能优于仅使用单一外部奖励训练的模型，并且与使用集成外部奖励训练的模型相当。此外，IRIS还激励了高质量图像生成中细致的CoT推理的出现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying Reinforcement Learning from Human Feedback (RLHF) to autoregressive Text-to-Image (T2I) generation, where human preference data is scarce. The authors propose IRIS, an intrinsic reward-based framework that enables T2I models to learn from internal signals rather than external rewards or labeled data. By minimizing self-certainty, IRIS encourages the generation of more detailed and vivid images, which are more aligned with human preferences. Experimental results show that IRIS outperforms models trained with individual external rewards and matches those trained with ensemble external rewards, demonstrating its effectiveness in enhancing image generation quality.</div>
<div class="mono" style="margin-top:8px">本文针对将基于人类反馈的强化学习（RLHF）应用于自回归文本到图像（T2I）生成时面临的人类偏好数据稀缺问题。作者提出IRIS，一种仅依赖内在奖励的框架，使T2I模型能够通过内部信号进行学习，而不依赖外部奖励或标注数据。研究发现，降低自我确定性有助于生成更生动且细节丰富的图像，从而更符合人类偏好。实验结果表明，使用IRIS训练的自回归T2I模型在性能上优于仅使用单一外部奖励训练的模型，并与使用多个外部奖励训练的模型表现相当，同时促进了高质量图像生成所需的细致链式推理（CoT）能力的出现。</div>
</details>
</div>
<div class="card">
<div class="title">TOM-SWE: User Mental Modeling For Software Engineering Agents</div>
<div class="meta-line">Authors: Xuhui Zhou, Valerie Chen, Zora Zhiruo Wang, Graham Neubig, Maarten Sap, Xingyao Wang</div>
<div class="meta-line">First: 2025-10-24T16:09:51+00:00 · Latest: 2026-01-29T20:42:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21903v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21903v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-dependent. To bridge this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary software-engineering (SWE) agent with a lightweight theory-of-mind (ToM) partner agent dedicated to modeling the user&#x27;s mental state. The ToM agent infers user goals, constraints, and preferences from instructions and interaction history, maintains a \textbf{persistent memory} of the user, and provides user-related suggestions to the SWE agent. In two software engineering benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task success rates and user satisfaction. Notably, on the stateful SWE benchmark, a newly introduced evaluation that provides agents with a user simulator along with previous interaction histories, ToM-SWE achieves a substantially higher task success rate of 59.7\% compared to 18.1\% for OpenHands, a state-of-the-art SWE agent. Furthermore, in a three-week study with professional developers using ToM-SWE in their daily work, participants found it useful 86\% of the time, underscoring the value of stateful user modeling for practical coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TOM-SWE：软件工程代理的用户心理建模</div>
<div class="mono" style="margin-top:8px">最近在编码代理方面的进展使它们能够规划、编辑、运行和测试复杂的代码库。尽管这些系统在编码任务中的能力不断提升，但在推断和跟踪用户意图方面仍存在困难，尤其是在指令不明确或依赖上下文的情况下。为弥合这一差距，我们引入了TOM-SWE，这是一种双代理架构，将一个主软件工程（SWE）代理与一个轻量级心理理论（ToM）伙伴代理配对，后者专门用于建模用户的心理状态。ToM代理从指令和交互历史中推断用户目标、约束和偏好，维护用户的\textbf{持久记忆}，并向SWE代理提供与用户相关的建议。在两个软件工程基准测试（模糊SWE-bench和有状态SWE-bench）中，TOM-SWE提高了任务成功率和用户满意度。值得注意的是，在一个新引入的评估中，该评估为代理提供了一个用户模拟器和之前的交互历史，TOM-SWE在任务成功率上达到59.7\%，远高于最先进的SWE代理OpenHands的18.1\%。此外，在一项为期三周的专业开发者日常使用TOM-SWE的研究中，参与者发现其有用的时间占86\%，突显了有状态用户建模对实际编码代理的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the ability of software engineering agents to understand and track user intent, which remains a challenge despite their progress in coding tasks. To address this, the authors propose ToM-SWE, a dual-agent system that combines a primary software engineering agent with a lightweight theory-of-mind agent designed to model the user&#x27;s mental state. The ToM agent extracts user goals, constraints, and preferences from instructions and interaction history, maintaining a persistent memory of the user to provide more context-aware suggestions. Experimental results on two software engineering benchmarks show that ToM-SWE significantly improves task success rates and user satisfaction, achieving 59.7% success on the stateful SWE benchmark compared to 18.1% for OpenHands. A three-week study with professional developers further supports the effectiveness of ToM-SWE, with 86% of participants finding it useful in their daily work.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升软件工程代理对用户意图的理解和跟踪能力，尽管这些代理在编码任务中表现出色，但在处理模糊或依赖上下文的指令时仍存在困难。为此，作者提出了ToM-SWE，这是一种双代理系统，由一个主软件工程代理和一个轻量级的理论心智代理组成，后者专门用于建模用户的心理状态。理论心智代理从指令和交互历史中推断用户目标、约束和偏好，并维护用户持久记忆以提供情境相关的建议。在两个软件工程基准测试中，ToM-SWE显著提高了任务成功率和用户满意度，在新的状态依赖SWE基准测试中，其任务成功率达到59.7%，远高于OpenHands的18.1%。一项为期三周的专业开发者使用研究也表明，86%的参与者认为ToM-SWE在日常工作中有用，突显了状态化用户建模对实际编码代理的价值。</div>
</details>
</div>
<div class="card">
<div class="title">Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning</div>
<div class="meta-line">Authors: Jian Shi, Michael Birsak, Wenqing Cui, Zhenyu Li, Peter Wonka</div>
<div class="meta-line">First: 2026-01-29T19:04:30+00:00 · Latest: 2026-01-29T19:04:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22231v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22231v1">PDF</a> · <a href="https://github.com/shijianjian/vit-geometry-probes">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需位置？位置嵌入如何影响和损害空间推理</div>
<div class="mono" style="margin-top:8px">本文从几何角度重新审视了视觉Transformer（ViTs）中位置嵌入（PEs）的作用。我们表明，PEs不仅仅是标记索引，而是有效地作为几何先验，塑造表示的空间结构。我们引入了标记级诊断方法，用于衡量ViT表示中多视角几何一致性对一致PE的依赖。通过在14个基础ViT模型上的大量实验，我们揭示了PEs如何影响多视角几何和空间推理。我们的研究结果阐明了PEs作为因果机制在ViT表示中对空间结构的调控作用。我们的代码可在https://github.com/shijianjian/vit-geometry-probes获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of positional embeddings (PEs) in vision transformers (ViTs) from a geometric standpoint, challenging the notion that they are simply token indices. It proposes token-level diagnostics to assess how geometric consistency across multiple views in ViT representations is affected by consistent PEs. Through experiments on 14 foundation ViT models, the study demonstrates that PEs significantly influence spatial reasoning and multi-view geometry, highlighting their function as a causal mechanism that shapes the spatial structure of the model&#x27;s internal representations.</div>
<div class="mono" style="margin-top:8px">本文从几何角度重新审视了视觉Transformer中位置嵌入的作用，指出它们不仅仅是简单的令牌索引，而是有效塑造表示空间结构的几何先验。研究提出了一种令牌级诊断方法，用于评估不同视角下ViT表示的几何一致性是否依赖于一致的位置嵌入。通过对14个基础ViT模型的广泛实验，结果表明位置嵌入对空间推理和多视角几何有显著影响，揭示了其作为控制模型内部表示空间结构的因果机制的作用。</div>
</details>
</div>
<div class="card">
<div class="title">Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation</div>
<div class="meta-line">Authors: Ken Deng, Yifu Qiu, Yoni Kasten, Shay B. Cohen, Yftah Ziser</div>
<div class="meta-line">First: 2026-01-29T19:01:03+00:00 · Latest: 2026-01-29T19:01:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迷失在空间中？视觉-语言模型在相对相机姿态估计中表现不佳</div>
<div class="mono" style="margin-top:8px">与对3D空间结构理解有限相比，视觉-语言模型（VLMs）在2D感知和语义推理方面表现良好。我们通过相对相机姿态估计（RCPE）来研究这一差距，RCPE是一项基础的视觉任务，要求从一对图像中推断相对相机的平移和旋转。我们引入了VRRPI-Bench，这是一个基于无标签第一视角视频的基准，包含对相对相机运动的描述性注释，反映了在共享物体周围同时进行平移和旋转的现实场景。我们进一步提出了VRRPI-Diag，一个用于诊断的基准，可以隔离单个运动自由度。尽管RCPE任务本身较为简单，但大多数VLMs无法超越浅层的2D启发式方法，尤其是在光轴方向上的深度变化和翻滚变换方面。即使是像GPT-5（0.64）这样的最先进模型，其表现也未能达到经典几何基线（0.97）和人类水平（0.92）。此外，VLMs在多图像推理中也表现出困难，在跨帧整合空间线索时性能不一致（最佳59.7%）。我们的研究揭示了将VLMs锚定在3D空间和多视角空间推理方面的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the limitations of Vision-Language Models (VLMs) in understanding 3D spatial relationships, particularly in relative camera pose estimation (RCPE). The authors introduce two benchmarks, VRRPI-Bench and VRRPI-Diag, to evaluate VLMs&#x27; ability to infer camera translation and rotation from image pairs, with the former capturing realistic scenarios and the latter isolating individual motion components. Experimental results show that even advanced models like GPT-5 perform poorly compared to geometric baselines and human performance, highlighting the challenges VLMs face in 3D spatial reasoning and multi-view consistency.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLMs）在理解三维空间关系方面的局限性，特别是在相对相机姿态估计（RCPE）任务中的表现。作者提出了两个基准测试，VRRPI-Bench 和 VRRPI-Diag，用于评估 VLMs 从图像对中推断相对平移和旋转的能力，前者反映现实场景，后者则隔离单个运动自由度。实验结果表明，即使是先进的模型如 GPT-5 也难以超越几何基线和人类表现，揭示了 VLMs 在三维空间推理和多视角整合方面的困难。</div>
</details>
</div>
<div class="card">
<div class="title">StepShield: When, Not Whether to Intervene on Rogue Agents</div>
<div class="meta-line">Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 2 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepShield：何时干预而非是否干预 rogue agents</div>
<div class="mono" style="margin-top:8px">现有的 agent 安全基准测试报告二元准确率，将早期干预与事后分析混为一谈。一个在第 8 步标记违规的检测器可以实现干预；而一个在第 48 步报告违规的检测器则仅具有取证价值。这种区别至关重要，但当前基准测试无法衡量。我们引入了 StepShield，这是首个评估违规检测时间而非仅是否检测的基准测试。StepShield 包含 9,213 个代码 agent 轨迹，其中包括 1,278 个精心标注的训练对和一个包含 7,935 个轨迹、现实 rogue 率为 8.1% 的测试集。这些 rogue 行为基于六个类别中的真实世界安全事件。我们提出了三个新颖的时间度量指标：早期干预率（EIR）、干预间隔和节省的 token 数。令人惊讶的是，我们的评估结果显示，基于 LLM 的法官实现了 59% 的 EIR，而静态分析器仅达到 26%，存在 2.3 倍的性能差距，这在标准准确率指标中完全不可见。我们进一步表明，早期检测具有直接的经济价值：我们的级联 HybridGuard 检测器可将监控成本降低 75%，并在企业规模下预计五年内累计节省 1.08 亿美元。通过将评估重点从是否检测转向何时检测，StepShield 为构建更安全且更具经济可行性的 AI agent 提供了新的基础。代码和数据在 Apache 2.0 许可下发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces StepShield, a novel benchmark that evaluates the timing of violation detection in AI agents rather than just the presence of violations. This is motivated by the limitation of current benchmarks that report binary accuracy, failing to distinguish between early intervention and post-mortem analysis. StepShield includes 9,213 code agent trajectories, with a realistic 8.1% rogue rate in its test set. The study proposes three temporal metrics—Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved—to assess detection timing. Experimental results show that an LLM-based judge achieves a 59% EIR, significantly outperforming a static analyzer with only 26%, highlighting a 2.3x performance gap. Additionally, the cascaded HybridGuard detector demonstrates economic benefits by reducing monitoring costs by 75% and projecting $108M in savings over five years at enterprise scale.</div>
<div class="mono" style="margin-top:8px">本文提出了StepShield，这是一个全新的基准，用于评估AI代理中违规行为的检测时间，而非仅仅是否存在违规。现有基准报告二元准确率，未能区分早期干预与事后分析，这是研究的动机。StepShield包含9,213个代码代理轨迹，其中测试集具有现实的8.1%违规率。研究提出了三个时间相关指标：早期干预率（EIR）、干预间隔和节省的令牌数。实验结果显示，基于LLM的裁判模型在EIR上达到59%，远超静态分析器的26%，显示出2.3倍的性能差距。此外，级联的HybridGuard检测器在企业规模下可减少75%的监控成本，并预计五年内累计节省1.08亿美元。</div>
</details>
</div>
<div class="card">
<div class="title">MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</div>
<div class="meta-line">Authors: Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen</div>
<div class="meta-line">First: 2026-01-29T17:52:41+00:00 · Latest: 2026-01-29T17:52:41+00:00</div>
<div class="meta-line">Comments: Project Page: https://metric-anything.github.io/metric-anything-io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://metric-anything.github.io/metric-anything-io/">Project1</a> · <a href="http://metric-anything.github.io/metric-anything-io/">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetricAnything: 通过噪声异构源扩展度量深度预训练</div>
<div class="mono" style="margin-top:8px">扩展已推动了视觉基础模型的近期进展，但将这一范式扩展到度量深度估计仍面临挑战，原因包括异构传感器噪声、相机依赖性偏差以及噪声跨源3D数据中的度量模糊性。我们引入了Metric Anything，这是一个简单且可扩展的预训练框架，无需手动设计提示、相机特定建模或任务特定架构，即可从噪声、多样化的3D源中学习度量深度。我们的方法核心是稀疏度量提示，通过随机掩码深度图生成，它作为通用接口，将空间推理与传感器和相机偏差解耦。利用约2000万张图像-深度对，涵盖10000个相机模型的重建、捕获和渲染的3D数据，我们首次展示了度量深度领域中的明确扩展趋势。预训练模型在提示驱动的任务如深度补全、超分辨率和雷达-相机融合中表现出色，而其蒸馏后的无提示学生模型在单目深度估计、相机内参恢复、单/多视角度量3D重建以及VLA规划中达到了最先进的结果。我们还表明，使用Metric Anything预训练的ViT作为视觉编码器，显著提升了多模态大语言模型在空间智能方面的能力。这些结果表明，度量深度估计可以受益于驱动现代基础模型的相同扩展规律，为可扩展且高效的现实世界度量感知开辟了新路径。我们开源了MetricAnything，网址为http://metric-anything.github.io/metric-anything-io/，以支持社区研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenges in scaling metric depth estimation by leveraging noisy and heterogeneous 3D data sources. The proposed method, Metric Anything, introduces a scalable pretraining framework that learns metric depth without requiring manual prompts, camera-specific models, or task-specific architectures. A key component is the Sparse Metric Prompt, which masks depth maps to decouple spatial reasoning from sensor and camera biases. Using 20 million image-depth pairs across 10,000 camera models, the experiments demonstrate a clear scaling trend in metric depth estimation. The pretrained model performs well on various tasks, including depth completion and Radar-camera fusion, while its distilled version achieves state-of-the-art results in monocular depth estimation and 3D reconstruction. Additionally, using Metric Anything as a visual encoder enhances spatial intelligence in Multimodal Large Language Models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决通过噪声和异构3D数据源扩展度量深度估计的挑战。提出的方法Metric Anything是一种可扩展的预训练框架，能够从多样化的3D数据中学习度量深度，无需手动提示、相机特定模型或任务特定架构。其核心是Sparse Metric Prompt，通过随机掩码深度图来解耦空间推理与传感器偏差。使用约2000万张图像-深度对，覆盖重建、捕获和渲染的3D数据及10000种相机模型，实验展示了度量深度估计中的明显扩展趋势。预训练模型在深度补全、雷达-相机融合等提示驱动任务中表现优异，其蒸馏后的无提示学生模型在单目深度估计、相机内参恢复、单/多视角度量3D重建和VLA规划等任务中达到最先进水平。此外，使用预训练ViT作为视觉编码器显著提升了多模态大语言模型的空间智能能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
