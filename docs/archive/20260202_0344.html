<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-02 03:44</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260202_0344</div>
    <div class="row"><div class="card">
<div class="title">StepShield: When, Not Whether to Intervene on Rogue Agents</div>
<div class="meta-line">Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 2 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepShield：何时干预而非是否干预 rogue agents</div>
<div class="mono" style="margin-top:8px">现有的代理安全基准测试报告二进制准确性，将早期干预与事后分析混为一谈。一个在第8步标记违规的检测器可以实现干预；而一个在第48步报告违规的检测器则仅具有取证价值。这种区别至关重要，但当前的基准测试无法衡量。我们引入了StepShield，这是首个评估违规检测时间而非仅是否检测的基准测试。StepShield包含9,213个代码代理轨迹，包括1,278对精心标注的训练数据对和一个包含7,935个轨迹、现实场景中8.1% rogue行为的测试集。这些 rogue 行为基于六个类别中的真实世界安全事件。我们提出了三个新颖的时间度量指标：早期干预率（EIR）、干预间隔和节省的token数。令人惊讶的是，我们的评估结果显示，基于LLM的裁判模型实现了59%的EIR，而静态分析器仅达到26%，性能差距达到2.3倍，这在标准准确性指标中完全不可见。我们进一步证明，早期检测具有直接的经济效益：我们的级联 HybridGuard 检测器可将监控成本降低75%，并在企业规模下预计五年内累计节省1.08亿美元。通过将评估重点从是否检测转移到何时检测，StepShield为构建更安全且更具经济可行性的AI代理提供了新的基础。代码和数据在Apache 2.0许可下发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitation of current agent safety benchmarks that only report binary accuracy without distinguishing the timing of violation detection. StepShield introduces a novel benchmark that evaluates when violations are detected, not just whether they occur. It includes 9,213 code agent trajectories with 1,278 annotated training pairs and a 7,935-trajectory test set featuring a realistic 8.1% rogue rate. The paper proposes three temporal metrics—Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved—to assess detection timing. Experimental results show that an LLM-based judge achieves 59% EIR, significantly outperforming a static analyzer with 26%, highlighting a 2.3x performance gap. Additionally, the cascaded HybridGuard detector demonstrates economic benefits by reducing monitoring costs by 75% and projecting $108M in cumulative savings over five years at enterprise scale.</div>
<div class="mono" style="margin-top:8px">本文提出了StepShield，这是一个全新的基准测试，用于评估AI代理中违规行为的检测时间，而非仅仅是否检测到违规。现有基准测试报告二元准确率，未能区分早期干预与事后分析。StepShield包含9,213条代码代理轨迹，其中1,278条为精心标注的训练对，7,935条为测试集，具有现实的8.1%违规率。研究提出了三个时间相关指标：早期干预率（EIR）、干预间隔和节省的令牌数。实验结果显示，基于LLM的裁判模型达到59%的EIR，显著优于静态分析器的26%，显示出2.3倍的性能差距。此外，级联的HybridGuard检测器展示了经济优势，可将监控成本降低75%，并在企业规模下预计五年内节省1.08亿美元。</div>
</details>
</div>
<div class="card">
<div class="title">MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</div>
<div class="meta-line">Authors: Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen</div>
<div class="meta-line">First: 2026-01-29T17:52:41+00:00 · Latest: 2026-01-29T17:52:41+00:00</div>
<div class="meta-line">Comments: Project Page: https://metric-anything.github.io/metric-anything-io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://metric-anything.github.io/metric-anything-io/">Project1</a> · <a href="http://metric-anything.github.io/metric-anything-io/">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetricAnything: 通过噪声异构源扩展度量深度预训练</div>
<div class="mono" style="margin-top:8px">扩展已推动了视觉基础模型的近期进展，但将其范式扩展到度量深度估计仍面临挑战，原因包括异构传感器噪声、相机依赖性偏差以及噪声跨源3D数据中的度量模糊性。我们引入了Metric Anything，这是一个简单且可扩展的预训练框架，无需手动设计提示、相机特定建模或任务特定架构，即可从噪声、多样化的3D源中学习度量深度。我们的方法核心是稀疏度量提示，通过随机掩码深度图生成，它作为通用接口，将空间推理与传感器和相机偏差解耦。利用约2000万张图像-深度对，涵盖10000个相机模型的重建、捕获和渲染的3D数据，我们首次展示了度量深度领域的明确扩展趋势。预训练模型在提示驱动的任务如深度补全、超分辨率和雷达-相机融合中表现出色，而其蒸馏后的无提示学生模型在单目深度估计、相机内参恢复、单/多视角度量3D重建和VLA规划中取得了最先进的结果。我们还表明，使用Metric Anything预训练的ViT作为视觉编码器，显著提升了多模态大语言模型在空间智能方面的能力。这些结果表明，度量深度估计可以受益于驱动现代基础模型的相同扩展规律，为可扩展且高效的现实世界度量感知开辟了新路径。我们开源了MetricAnything，网址为http://metric-anything.github.io/metric-anything-io/，以支持社区研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of scaling metric depth estimation by leveraging noisy and heterogeneous 3D data sources. It proposes Metric Anything, a pretraining framework that eliminates the need for manual prompts, camera-specific models, or task-specific architectures. The core method involves the use of Sparse Metric Prompts, which are created by randomly masking depth maps to decouple spatial reasoning from sensor and camera biases. Experimental results show a clear scaling trend in metric depth estimation using 20 million image-depth pairs across 10,000 camera models, and the model achieves state-of-the-art performance in several tasks, including monocular depth estimation and VLA planning. Additionally, the pretrained ViT from Metric Anything enhances spatial intelligence in multimodal large language models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在使用噪声和异构3D数据源时扩展度量深度估计的挑战。提出的方法Metric Anything是一种可扩展的预训练框架，能够从噪声数据中学习度量深度，无需手动提示、相机特定建模或任务特定架构。其核心组件是Sparse Metric Prompt，通过随机掩码深度图来解耦空间推理与传感器和相机偏差。利用约2000万张图像-深度对，涵盖10000种相机模型的重建、捕获和渲染3D数据，实验展示了度量深度估计中的明显扩展趋势。预训练模型在提示驱动任务如深度补全、超分辨率和雷达-相机融合中表现优异，其蒸馏后的无提示版本在单目深度估计、相机内参恢复、单/多视角度量3D重建以及VLA规划中达到最先进水平。此外，使用Metric Anything作为视觉编码器显著提升了多模态大语言模型的空间智能能力。</div>
</details>
</div>
<div class="card">
<div class="title">Causal World Modeling for Robot Control</div>
<div class="meta-line">Authors: Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</div>
<div class="meta-line">First: 2026-01-29T17:07:43+00:00 · Latest: 2026-01-29T17:07:43+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-va Code: https://github.com/robbyant/lingbot-va</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21998v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-va">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于机器人控制的因果世界建模</div>
<div class="mono" style="margin-top:8px">本工作强调，视频世界建模与视觉-语言预训练共同为机器人学习建立了新颖且独立的基础。直观来说，视频世界模型通过理解动作与视觉动态之间的因果关系，提供了对近未来进行想象的能力。受此启发，我们引入了LingBot-VA，这是一种自回归扩散框架，能够同时学习帧预测和策略执行。我们的模型包含三个精心设计的组件：(1) 由混合变换器（Mixture-of-Transformers, MoT）架构驱动的共享潜在空间，整合了视觉和动作标记；(2) 闭环滚动机制，允许在真实观测下持续获取环境反馈；(3) 异步推理流水线，通过并行化动作预测与运动执行以支持高效控制。我们在模拟基准和现实场景中评估了该模型，结果显示其在长时序操作、训练后数据效率以及对新配置的强泛化能力方面具有显著潜力。代码和模型已公开，以促进社区发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to improve robot control by leveraging video world modeling combined with vision-language pre-training, offering a new approach to robot learning. The proposed LingBot-VA framework employs an autoregressive diffusion model that simultaneously learns frame prediction and policy execution through a shared latent space, a Mixture-of-Transformers architecture, and a closed-loop rollout mechanism. Additionally, it uses an asynchronous inference pipeline to enable efficient control. Experimental results demonstrate the model&#x27;s effectiveness in long-horizon manipulation tasks, data efficiency after training, and strong generalization to new environments.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过视频世界建模与视觉语言预训练，为机器人学习建立新的基础。提出的LingBot-VA框架采用自回归扩散模型，通过共享潜在空间、闭环回滚机制和异步推理流程，同时学习帧预测与策略执行。实验结果表明，该模型在长期操作任务中表现出色，具有良好的数据效率和对新环境的强泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation</div>
<div class="meta-line">Authors: Javier Argota Sánchez-Vaquerizo, Luis Borunda Monsivais</div>
<div class="meta-line">First: 2026-01-29T16:54:18+00:00 · Latest: 2026-01-29T16:54:18+00:00</div>
<div class="meta-line">Comments: Paper selected for the workshop Human Cognition, AI, and the Future of HCI: Navigating the Disruptive and Wild Landscape of Large Language Models and Agentic AI as part of the Human-Computer Interaction (HCI) conference of the Alpine region (AlpCHI 2026) hosted at the Congressi Stefano Franscini, March 1st to March 5th, 2026 on Monte Verità in Ascona, Switzerland</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21977v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based &quot;particles&quot; rather than cognitive &quot;agents&quot;. To bridge this, we introduce \textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \textbf{Cognitive Friction} ($C_f$) it is possible to reveal &quot;Phantom Affordances&quot;, i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从粒子到智能体：将幻觉作为空间模拟中认知摩擦的度量</div>
<div class="mono" style="margin-top:8px">传统的建筑模拟（如计算流体动力学、疏散模拟、结构分析）将元素建模为基于确定性物理的&quot;粒子&quot;，而非具有认知能力的&quot;智能体&quot;。为弥合这一差距，我们引入\textbf{智能体环境模拟}，其中大型多模态生成模型基于语义预期主动预测空间环境的下一状态。借助面向无障碍性的AR管线和多模态数字孪生的实例，我们提出从时间步进转向\textbf{事件驱动的空间推理}，即模拟通过有意义且由意外触发的事件推进。在此框架下，我们提出将AI幻觉作为诊断工具。通过形式化\textbf{认知摩擦}（$C_f$），可以揭示&quot;幽灵可操作性&quot;，即建筑空间中的语义歧义。最后，我们通过将环境视为动态认知伙伴来挑战当前的HCI范式，并提出以人类为中心的认知编排框架，用于设计AI驱动的模拟，以保持自主性、情感清晰度和认知完整性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of traditional architectural simulations that treat elements as deterministic particles rather than cognitive agents. It introduces Agentic Environmental Simulations, which utilize Large Multimodal generative models to predict spatial environment states based on semantic expectations. The study proposes a shift from chronological time-steps to Episodic Spatial Reasoning, driven by meaningful and surprisal-triggered events. By formalizing Cognitive Friction ($C_f$), the research identifies Phantom Affordances—semiotic ambiguities in built environments—and challenges current HCI paradigms by treating environments as dynamic cognitive partners, advocating for a human-centered framework of cognitive orchestration in AI-driven simulations.</div>
<div class="mono" style="margin-top:8px">本文针对传统建筑模拟中将元素视为确定性粒子而非认知代理的局限性，提出了Agentic Environmental Simulations的概念，利用大型多模态生成模型基于语义期望预测空间环境状态。通过将时间步从时间顺序转向有意义的事件驱动，研究将AI幻觉视为诊断工具，揭示认知摩擦（Cognitive Friction），即建筑空间中的语义模糊现象（Phantom Affordances）。该框架挑战了当前人机交互范式，将环境视为动态的认知伙伴，并提出以人类为中心的认知编排方法，用于设计保持自主性、情感清晰度和认知完整性的AI驱动模拟。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning</div>
<div class="meta-line">Authors: Jinjun Peng, Magnus Saebo, Tianjun Zhong, Yi-Jie Cheng, Junfeng Yang, Baishakhi Ray, Simin Chen, Yangruibo Ding</div>
<div class="meta-line">First: 2026-01-29T12:49:25+00:00 · Latest: 2026-01-29T12:49:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21649v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the &quot;physics&quot; of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Spot：通过以仓库为中心的学习构建小型仓库专家</div>
<div class="mono" style="margin-top:8px">在隐私敏感和资源受限的环境中部署代码代理，推动了具备能力的开源小型语言模型（SLMs）的需求。然而，它们存在一个根本的能力差距：与前沿大模型不同，它们缺乏推理时的强泛化能力，难以处理复杂且陌生的代码库。我们发现，当前以任务为中心的学习（TCL）范式，通过跨不同仓库的扩展训练，未能解决这一限制。为此，我们提出了以仓库为中心的学习（RCL），这是一种范式转变，强调垂直仓库深度而非水平任务广度，建议SLMs必须通过参数化知识获取来内化目标软件环境的“物理特性”，而不是通过昂贵的推理时搜索来恢复。基于这一新范式，我们设计了一个四单元的以仓库为中心的学习体验，将静态代码库转化为交互式学习信号，以训练SWE-Spot-4B，这是一系列高度紧凑的模型，作为仓库专用专家构建，突破了已有的扩展趋势，在多个软件工程任务中优于开源模型（如Meta的CWM、Qwen3-Coder-30B），并超越或匹配以效率为导向的商业模型（如GPT-4.1-mini、GPT-5-nano）。进一步分析表明，RCL带来了更高的训练样本效率和更低的推理成本，强调在构建高效智能时，仓库掌握是一个独特且必要的维度，能够补充通用编码能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing need for coding agents in privacy-sensitive and resource-constrained settings highlights the importance of developing capable small language models (SLMs). Traditional Task-Centric Learning (TCL) approaches, which generalize across diverse codebases, fail to address the fundamental limitation of SLMs in handling complex and unfamiliar repositories. To overcome this, the authors introduce Repository-Centric Learning (RCL), which emphasizes deep understanding of specific codebases through parametric knowledge acquisition. They train SWE-Spot-4B, a compact model family, using a four-unit RCL experience that transforms static codebases into interactive learning signals. Experimental results show that SWE-Spot-4B outperforms open-weight models like CWM and Qwen3-Coder-30B, and matches or exceeds efficiency-focused commercial models such as GPT-4.1-mini and GPT-5-nano in multiple software engineering tasks. The study also demonstrates that RCL improves training efficiency and reduces inference costs, underscoring the necessity of repository mastery for efficient coding intelligence.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决小语言模型（SLMs）在处理复杂和陌生代码库时的能力不足问题，这对于在隐私敏感和资源受限环境中部署编码代理至关重要。作者提出了一种名为Repository-Centric Learning（RCL）的新范式，与传统的Task-Centric Learning（TCL）不同，它强调对特定代码库的深入理解而非广泛任务覆盖。他们设计了一个四单元的Repository-Centric Experience，将静态代码库转化为交互式学习信号，用于训练SWE-Spot-4B模型家族。该模型在多个软件工程任务中表现优于开放权重模型，并与专注于效率的商业模型相媲美或超越。进一步分析表明，RCL提高了训练样本效率并降低了推理成本，突显了代码库掌握在构建高效编码智能中的独特且必要作用。</div>
</details>
</div>
<div class="card">
<div class="title">RSGround-R1: Rethinking Remote Sensing Visual Grounding through Spatial Reasoning</div>
<div class="meta-line">Authors: Shiqi Huang, Shuting He, Bihan Wen</div>
<div class="meta-line">First: 2026-01-29T12:35:57+00:00 · Latest: 2026-01-29T12:35:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21634v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote Sensing Visual Grounding (RSVG) aims to localize target objects in large-scale aerial imagery based on natural language descriptions. Owing to the vast spatial scale and high semantic ambiguity of remote sensing scenes, these descriptions often rely heavily on positional cues, posing unique challenges for Multimodal Large Language Models (MLLMs) in spatial reasoning. To leverage this unique feature, we propose a reasoning-guided, position-aware post-training framework, dubbed \textbf{RSGround-R1}, to progressively enhance spatial understanding. Specifically, we first introduce Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) using synthetically generated RSVG reasoning data to establish explicit position awareness. Reinforcement Fine-Tuning (RFT) is then applied, augmented by our newly designed positional reward that provides continuous and distance-aware guidance toward accurate localization. Moreover, to mitigate incoherent localization behaviors across rollouts, we introduce a spatial consistency guided optimization scheme that dynamically adjusts policy updates based on their spatial coherence, ensuring stable and robust convergence. Extensive experiments on RSVG benchmarks demonstrate superior performance and generalization of our model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RSGround-R1：通过空间推理重新思考遥感视觉定位</div>
<div class="mono" style="margin-top:8px">遥感视觉定位（RSVG）旨在根据自然语言描述在大规模航拍图像中定位目标物体。由于遥感场景具有广阔的空间尺度和高语义模糊性，这些描述通常高度依赖于位置线索，给多模态大语言模型（MLLMs）的空间推理带来了独特挑战。为利用这一特性，我们提出了一种推理引导、位置感知的后训练框架，称为\textbf{RSGround-R1}，以逐步增强空间理解能力。具体而言，我们首先引入基于合成生成RSVG推理数据的思维链监督微调（CoT-SFT），以建立明确的位置感知。随后，我们应用强化微调（RFT），并结合我们新设计的位置奖励机制，提供连续且距离感知的指导，以实现精确的定位。此外，为缓解不同 rollout 之间的不一致定位行为，我们引入了一种空间一致性引导的优化方案，根据其空间一致性动态调整策略更新，确保稳定且鲁棒的收敛。在RSVG基准上的大量实验表明，我们的模型表现出优越的性能和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) in the context of Remote Sensing Visual Grounding (RSVG), where natural language descriptions often rely on positional cues due to the vast spatial scale and high semantic ambiguity of remote sensing scenes. The proposed method, RSGround-R1, introduces a reasoning-guided, position-aware post-training framework that combines Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) with a novel positional reward mechanism in Reinforcement Fine-Tuning (RFT). This approach enhances position awareness through synthetic data and refines localization accuracy with distance-aware rewards. Additionally, a spatial consistency guided optimization scheme is employed to ensure coherent and stable model behavior across different rollouts. Experimental results on RSVG benchmarks show that RSGround-R1 achieves superior performance and better generalization compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对基于自然语言描述在大规模航拍图像中定位目标物体的挑战，提出了RSGround-R1框架，用于增强空间理解能力。该框架通过两个关键阶段实现：首先使用合成数据进行链式思维监督微调（CoT-SFT），以建立明确的位置感知；随后采用强化微调（RFT），结合新设计的位置奖励机制，提供连续且距离感知的定位指导。此外，引入了空间一致性引导的优化方案，以确保策略更新的连贯性和稳定性。在RSVG基准测试中，实验结果表明该模型在性能和泛化能力上均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization</div>
<div class="meta-line">Authors: Alireza Nadaf, Alireza Mohammadshahi, Majid Yazdani</div>
<div class="meta-line">First: 2026-01-29T10:40:54+00:00 · Latest: 2026-01-29T10:40:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21526v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21526v1">PDF</a> · <a href="https://github.com/Leeroo-AI/kapso">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAPSO：一种基于知识的自主程序合成与优化框架</div>
<div class="mono" style="margin-top:8px">我们介绍了KAPSO，一种用于自主程序合成与优化的模块化框架。给定一个自然语言目标和评估方法，KAPSO迭代地执行创意生成、代码合成与编辑、执行、评估和学习，以改进可运行的产物，使其朝着可衡量的目标发展。KAPSO不将合成视为终点，而是将其作为长周期优化循环中的一个操作符，其中进展由评估器的结果定义。
KAPSO通过集成三个紧密耦合的组件，针对编码代理中常见的长周期失败问题，包括实验状态丢失、脆弱的调试和领域专业知识的弱复用。第一，一个原生支持Git的实验引擎将每次尝试隔离为一个分支，生成可复现的产物，并在迭代过程中保留可追溯性。第二，一个知识系统吸收异构来源，包括仓库、内部操作手册以及经过整理的外部资源，如文档、科学论文和网络搜索结果，并将它们组织成结构化的表示，以支持对工作流、实现和环境约束的检索。第三，一个认知记忆层协调检索，并维护一个可复用的经验库，这些经验是从实验轨迹（运行日志、差异和评估器反馈）中提炼出的，从而减少重复的错误模式并加快收敛。
我们在MLE-Bench（Kaggle风格的机器学习竞赛）和ALE-Bench（AtCoder启发式优化）上评估了KAPSO，并报告了端到端的性能表现。
代码可用地址：https://github.com/Leeroo-AI/kapso</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind KAPSO is to address long-horizon failures in coding agents, such as lost experimental state, brittle debugging, and weak reuse of domain knowledge. KAPSO introduces a modular framework that treats program synthesis as an iterative operator within an optimization loop, combining ideation, code synthesis and editing, execution, evaluation, and learning. It integrates three key components: a git-native experimentation engine for reproducibility and provenance tracking, a knowledge system that organizes heterogeneous sources into a structured representation, and a cognitive memory layer that stores reusable lessons from past experiments. The framework was evaluated on MLE-Bench and ALE-Bench, demonstrating improved end-to-end performance in achieving measurable objectives through autonomous program synthesis and optimization.</div>
<div class="mono" style="margin-top:8px">KAPSO 是一个基于知识的框架，旨在解决编码代理在长周期任务中常见的失败问题，如实验状态丢失、调试脆弱性和领域知识重用不足。该框架融合了三个紧密耦合的组件：一个基于 Git 的实验引擎，用于隔离每次尝试并生成可复现的成果；一个知识系统，整合异构来源如代码库、内部指南和外部文档，并将其组织为结构化表示；以及一个认知记忆层，用于存储从实验轨迹中提取的可重用经验，减少重复错误并加快收敛。实验在 MLE-Bench 和 ALE-Bench 上进行，展示了端到端的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</div>
<div class="meta-line">Authors: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T08:15:00+00:00 · Latest: 2026-01-29T08:38:27+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026, URL: https://github.com/AMAP-ML/SpatialGenEval</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20354v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20354v2">PDF</a> · <a href="https://github.com/AMAP-ML/SpatialGenEval">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>各就各位：文本到图像模型空间智能基准测试</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型在生成高保真图像方面取得了显著成功，但在处理复杂的空间关系（如空间感知、推理或交互）时往往表现不佳。这些关键方面由于当前基准测试的提示设计较短或信息稀疏而被忽视。本文我们引入了SpatialGenEval，这是一个新的基准测试，旨在系统评估T2I模型的空间智能，涵盖两个关键方面：(1) SpatialGenEval包含25个真实世界场景中的1,230个长且信息密集的提示，每个提示整合了10个空间子领域和相应的10个多选问答对，涵盖从物体位置和布局到遮挡和因果关系等多个方面。我们对21个最先进的模型进行了广泛评估，发现高阶空间推理仍然是主要瓶颈。(2) 为了证明我们信息密集设计的实用性不仅限于简单的评估，我们还构建了SpatialT2I数据集。该数据集包含15,400个文本-图像对，通过重写提示确保图像一致性，同时保持信息密度。在当前基础模型（如Stable Diffusion-XL、Uniworld-V1、OmniGen2）上微调的结果显示出一致的性能提升（+4.2%、+5.7%、+4.4%），并实现了更逼真的空间关系效果，突显了以数据为中心的范式在实现T2I模型空间智能中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current text-to-image models in handling complex spatial relationships, which are often neglected in existing benchmarks due to their short and information-sparse prompts. The authors propose SpatialGenEval, a comprehensive benchmark with 1,230 long, information-dense prompts across 25 real-world scenes, covering 10 spatial sub-domains and 10 multi-choice questions per prompt. Evaluation of 21 state-of-the-art models shows that higher-order spatial reasoning remains a significant challenge. Additionally, the paper introduces the SpatialT2I dataset, which contains 15,400 text-image pairs with rewritten prompts to maintain information density while ensuring image consistency. Fine-tuning on this dataset leads to consistent performance improvements across multiple foundation models, demonstrating the effectiveness of a data-centric approach in enhancing spatial intelligence in T2I models.</div>
<div class="mono" style="margin-top:8px">本文针对当前文本到图像模型在处理复杂空间关系方面的不足，指出现有基准测试由于提示语简短或信息稀疏而忽视了这些关键能力。作者提出了SpatialGenEval，这是一个包含1,230个信息密集型提示语的全面基准测试，涵盖25个现实场景，每个提示语涉及10个空间子领域和10组多选问答。通过对21个最先进的模型进行评估，发现高阶空间推理仍是主要瓶颈。此外，他们构建了SpatialT2I数据集，包含15,400个文本-图像对，通过重写提示语确保图像一致性的同时保持信息密度。在当前基础模型上进行微调后，该数据集带来了稳定的性能提升（+4.2%、+5.7%、+4.4%），并展示了更逼真的空间关系效果，验证了数据驱动方法在提升T2I模型空间智能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents</div>
<div class="meta-line">Authors: Yang Song, Anoushka Vyas, Zirui Wei, Sina Khoshfetrat Pakazad, Henrik Ohlsson, Graham Neubig</div>
<div class="meta-line">First: 2026-01-29T07:57:23+00:00 · Latest: 2026-01-29T07:57:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21372v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21372v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEMO：通过自主编码代理实现执行感知的优化建模</div>
<div class="mono" style="margin-top:8px">本文中，我们提出了NEMO，一个能够将决策问题的自然语言描述转化为可执行的数学优化形式化实现的系统，可以与用户协作或自主运行。现有方法通常依赖于专门的大型语言模型（LLMs）或定制化的任务特定代理。这些方法往往脆弱、复杂，并且经常生成语法错误或不可执行的代码。
NEMO则专注于与自主编码代理（ACAs）的远程交互，将其视为类似于基于API与LLMs交互的第一类抽象。这种设计使得围绕ACAs构建更高层次的系统成为可能，这些系统可以对任务规范进行结构化、整合和迭代优化。由于ACAs在沙箱环境中执行，NEMO生成的代码在构建时即具备可执行性，从而支持自动验证和修复。
在此基础上，我们引入了与ACAs之间以及跨ACAs的新颖协调模式，包括独立生成的优化器和模拟器实现之间的不对称验证循环（作为高级验证机制）、用于经验重用的外部记忆，以及通过最小贝叶斯风险（MBR）解码和自一致性增强的鲁棒性。我们在九个已有的优化基准上评估了NEMO。如图1所示，它在大多数任务上实现了最先进的性能，在多个数据集上取得了显著优势，展示了执行感知代理架构在自动化优化建模中的强大能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces NEMO, a system that translates natural language descriptions of decision problems into executable mathematical optimization models, either collaboratively with users or autonomously. Unlike existing methods that rely on specialized large language models or task-specific agents, NEMO utilizes autonomous coding agents (ACAs) as a first-class abstraction, enabling higher-level system construction for task specification refinement. The system incorporates novel coordination patterns such as asymmetric validation loops, external memory, and MBR decoding to enhance robustness and accuracy. Evaluation on nine optimization benchmarks shows that NEMO achieves state-of-the-art performance on most tasks, with significant improvements on several datasets, highlighting the effectiveness of execution-aware agentic architectures in automated optimization modeling.</div>
<div class="mono" style="margin-top:8px">本文提出NEMO系统，该系统能够将决策问题的自然语言描述转化为可执行的数学优化模型，支持与用户协作或自主运行。与依赖专用大语言模型或任务特定代理的传统方法不同，NEMO将自主编码代理（ACAs）作为第一类抽象，通过结构化任务规范和迭代优化构建更高层次的系统。系统引入了新的协调模式，包括不对称验证循环、外部记忆用于经验复用以及通过最小贝叶斯风险（MBR）解码和自一致性增强鲁棒性。在九个优化基准上的评估表明，NEMO在大多数任务中达到最先进的性能，在多个数据集上表现出显著优势，证明了执行感知的代理架构在自动化优化建模中的强大能力。</div>
</details>
</div>
<div class="card">
<div class="title">Large Vision Models Can Solve Mental Rotation Problems</div>
<div class="meta-line">Authors: Sebastian Ray Mason, Anders Gjølbye, Phillip Chavarria Højbjerg, Lenka Tětková, Lars Kai Hansen</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-18T11:18:28+00:00 · Latest: 2026-01-29T05:53:48+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15271v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.15271v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mental rotation is a key test of spatial reasoning in humans and has been central to understanding how perception supports cognition. Despite the success of modern vision transformers, it is still unclear how well these models develop similar abilities. In this work, we present a systematic evaluation of ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from simple block structures similar to those used by Shepard and Metzler to study human cognition, to more complex block figures, three types of text, and photo-realistic objects. By probing model representations layer by layer, we examine where and how these networks succeed. We find that i) self-supervised ViTs capture geometric structure better than supervised ViTs; ii) intermediate layers perform better than final layers; iii) task difficulty increases with rotation complexity and occlusion, mirroring human reaction times and suggesting similar constraints in embedding space representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大视觉模型可以解决心理旋转问题</div>
<div class="mono" style="margin-top:8px">心理旋转是衡量人类空间推理能力的关键测试，也是理解知觉如何支持认知的核心。尽管现代视觉变换器取得了成功，但这些模型在发展类似能力方面表现如何仍不清楚。在这项工作中，我们系统地评估了ViT、CLIP、DINOv2和DINOv3在一系列心理旋转任务中的表现，从类似Shepard和Metzler用于研究人类认知的简单积木结构，到更复杂的积木图形、三种类型的文本以及逼真照片中的物体。通过逐层探测模型表示，我们分析了这些网络在何处以及如何成功。我们发现：i) 自监督的ViT比监督的ViT更能捕捉几何结构；ii) 中间层的表现优于最终层；iii) 任务难度随着旋转复杂度和遮挡程度的增加而增加，这与人类反应时间相呼应，并表明嵌入空间表示中存在类似的约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of large vision models to perform mental rotation tasks, a fundamental aspect of spatial reasoning. The researchers evaluate several vision transformer models, including ViT, CLIP, DINOv2, and DINOv3, across various mental rotation challenges ranging from simple block structures to complex figures and photo-realistic objects. Their analysis reveals that self-supervised ViTs outperform supervised ones in capturing geometric structures, intermediate layers show better performance than final layers, and task difficulty correlates with rotation complexity and occlusion, aligning with human cognitive behavior and suggesting shared constraints in the models&#x27; representation spaces.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型视觉模型解决心理旋转问题的能力，这是人类空间推理的关键测试。研究人员评估了包括ViT、CLIP、DINOv2和DINOv3在内的多种视觉变换器模型，在从简单块状结构到复杂图形和逼真图像对象的各类心理旋转任务中进行系统性测试。分析表明，自监督的ViT在捕捉几何结构方面优于监督模型，中间层的表现优于最终层，任务难度随着旋转复杂度和遮挡程度增加而上升，与人类认知反应时间相呼应，暗示了嵌入空间表示中的相似限制。</div>
</details>
</div>
<div class="card">
<div class="title">Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</div>
<div class="meta-line">Authors: Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</div>
<div class="meta-line">First: 2026-01-29T03:53:25+00:00 · Latest: 2026-01-29T03:53:25+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 17 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>只需提问：好奇的代码代理在前沿大语言模型中揭示系统提示</div>
<div class="mono" style="margin-top:8px">基于大语言模型的自主代码代理通过工具使用、长视野推理和自主交互正在重塑软件和人工智能开发。然而，这种自主性引入了一个之前未被认识的安全风险：代理交互从根本上扩大了大语言模型的攻击面，使得系统提示的系统性探测和恢复成为可能。我们识别出系统提示提取是一种内嵌于代码代理中的新兴漏洞，并提出了\textbf{\textsc{JustAsk}}，一个自我演进的框架，它仅通过交互就能自主发现有效的提取策略。与以往基于提示工程或数据集的攻击不同，\textsc{JustAsk} 不需要人工设计的提示、标记的监督或超出标准用户交互的特权访问。它将提取过程建模为一个在线探索问题，使用基于上置信界（Upper Confidence Bound）的策略选择，并在一个涵盖原子探测和高级编排的分层技能空间中进行操作。这些技能利用了系统指令泛化中的不完美性以及帮助性与安全性之间的固有张力。我们在\textbf{41}个黑盒商业模型上进行了评估，这些模型来自多个提供商，\textsc{JustAsk} 一致实现了完整或接近完整的系统提示恢复，揭示了在现代代理系统中反复出现的设计和架构层面的漏洞。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of autonomous code agents based on large language models (LLMs) in software and AI development has raised security concerns. This study identifies a new vulnerability where these agents can be systematically probed to recover hidden system prompts, which are crucial for guiding model behavior. To address this, the authors introduce JustAsk, a self-evolving framework that autonomously discovers extraction strategies through interaction without requiring handcrafted prompts or privileged access. Evaluations on 41 black-box commercial models show that JustAsk consistently recovers full or near-complete system prompts, highlighting significant design and architecture-level weaknesses in modern agent systems.</div>
<div class="mono" style="margin-top:8px">随着基于大语言模型（LLM）的自主代码代理在软件和AI开发中的广泛应用，其安全性问题日益受到关注。这些代理通过自我导向交互和长时序推理改变开发方式，但也引入了一个新的漏洞：能够系统性地探测并恢复隐藏的系统提示，这些提示指导模型行为。为了解决这一问题，作者提出了JustAsk，这是一个通过交互自主发现提取策略的自我演进框架，无需人工设计提示或特权访问。该框架将提示提取视为在线探索问题，采用基于上置信界（Upper Confidence Bound）的策略选择和分层技能空间。在对41个黑盒商业模型的评估中，JustAsk能够稳定地恢复完整的或接近完整的系统提示，揭示了现代代理系统中设计和架构层面的常见漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs</div>
<div class="meta-line">Authors: Artem Dementyev, Wazeer Zulfikar, Sinan Hersek, Pascal Getreuer, Anurag Kumar, Vivek Kumar</div>
<div class="meta-line">First: 2026-01-28T23:39:31+00:00 · Latest: 2026-01-28T23:39:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21124v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over &quot;Spatial Audio Tokens&quot; produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhaseCoder：面向多模态大语言模型的麦克风几何无关空间音频理解</div>
<div class="mono" style="margin-top:8px">当前的多模态大语言模型将音频处理为单声道流，忽略了对具身AI至关重要的丰富空间信息。相反，现有的空间音频模型受限于固定的麦克风几何结构，无法在多种设备上部署。我们提出了PhaseCoder，这是一种仅基于Transformer的空间音频编码器，与麦克风几何无关。PhaseCoder以原始多通道音频和麦克风坐标作为输入，执行定位并生成稳健的空间嵌入。我们展示了Gemma 3n大语言模型可以微调以推理由PhaseCoder生成的&quot;空间音频标记&quot;。我们在麦克风不变定位基准测试中展示了我们的编码器达到最先进的结果，并首次实现了大语言模型从任意麦克风阵列进行复杂空间推理和定向转录任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the spatial audio understanding capabilities of multimodal large language models (LLMs) by addressing the limitations of processing audio as a mono stream and the constraints of fixed microphone geometries in existing spatial audio models. PhaseCoder, a transformer-based spatial audio encoder, is introduced to be microphone geometry-agnostic, taking raw multichannel audio and microphone coordinates as inputs to generate spatial embeddings that enable localization. The main experimental results show that PhaseCoder achieves state-of-the-art performance on microphone-invariant localization benchmarks and allows an LLM, such as Gemma 3n, to perform complex spatial reasoning and targeted transcription tasks using spatial audio tokens produced by the encoder.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升多模态大语言模型（LLM）对空间音频的理解能力，克服了将音频处理为单声道流以及依赖固定麦克风几何结构的局限性。PhaseCoder 是一种基于变压器的空间音频编码器，能够独立于麦克风几何结构，通过接收原始多通道音频和麦克风坐标生成空间嵌入。实验结果表明，PhaseCoder 在麦克风不变定位任务上达到了最先进的性能，并首次实现了LLM从任意麦克风阵列中进行复杂空间推理和定向转录的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve</div>
<div class="meta-line">Authors: Hongzheng Chen, Alexander Novikov, Ngân Vũ, Hanna Alam, Zhiru Zhang, Aiden Grossman, Mircea Trofin, Amir Yazdanbakhsh</div>
<div class="meta-line">First: 2026-01-28T22:34:56+00:00 · Latest: 2026-01-28T22:34:56+00:00</div>
<div class="meta-line">Comments: Accepted to C4ML@CGO&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21096v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Magellan：使用AlphaEvolve自主发现新型编译器优化启发式方法</div>
<div class="mono" style="margin-top:8px">现代编译器依赖人工设计的启发式方法来指导优化过程。这些由人类设计的规则往往难以适应现代软件和硬件的复杂性，导致维护负担沉重。为了解决这一挑战，我们提出了Magellan，一个代理框架，通过合成可执行的C++决策逻辑来进化编译器本身。Magellan在一个生成、用户提供的宏基准评估和优化的闭环中，将大型语言模型编码代理与进化搜索和自动调优相结合，生成可以直接集成到现有编译器中的紧凑启发式方法。在多个生产优化任务中，Magellan发现的策略能够匹配或超越专家基线。在LLVM函数内联中，Magellan合成的新启发式方法在二进制大小缩减和端到端性能方面均优于数十年的人工工程成果。在寄存器分配中，它学习了一个简洁的优先规则，用于处理活跃区间，在大规模工作负载中与复杂的人工设计策略表现一致。我们还报告了在XLA问题上的初步结果，展示了在减少工程工作量的情况下，Magellan在LLVM之外的可移植性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the limitations of hand-crafted compiler optimization heuristics, which are difficult to maintain and adapt to modern software and hardware complexities. Magellan introduces an agentic framework that combines an LLM coding agent with evolutionary search and autotuning to automatically generate and refine executable C++ decision logic for compiler passes. The framework iteratively generates, evaluates, and refines heuristics using user-provided macro-benchmarks. Experimental results show that Magellan discovers optimization policies that match or exceed expert-designed ones, achieving significant improvements in binary-size reduction and end-to-end performance in LLVM function inlining, and matching complex human policies in register allocation on large-scale workloads. Preliminary results on XLA problems also demonstrate its potential for portable optimization with reduced engineering effort.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决传统手工编写的编译器优化启发式规则难以适应现代软件和硬件复杂性的局限性。Magellan提出了一种代理框架，结合大型语言模型（LLM）代码生成代理、进化搜索和自动调优，以自动合成并优化编译器优化过程的可执行C++决策逻辑。该框架通过生成、评估和迭代优化的闭环流程运作。实验结果表明，Magellan在LLVM函数内联和寄存器分配任务中发现的优化策略在二进制大小缩减和端到端性能方面均不逊于专家设计的规则，同时在XLA问题中也展示了良好的可移植性，且减少了工程工作量。</div>
</details>
</div>
<div class="card">
<div class="title">City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</div>
<div class="meta-line">Authors: Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</div>
<div class="meta-line">First: 2025-12-17T19:59:31+00:00 · Latest: 2026-01-28T19:40:38+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15933v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15933v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dwipddalal.github.io/AgentNav/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environment. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs, reasoning techniques (e.g., GEPA, chain-of-thought, reflection) and competitive baseline PReP significantly underperform in this challenging setting. To address this, we propose Verbalization of Path(VoP), which explicitly grounds the agent&#x27;s internal reasoning by probing city-scale cognitive maps (key landmarks and directions toward the destination) from the MLLM, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外城市导航：从大规模知识中探索MLLMs的涌现导航</div>
<div class="mono" style="margin-top:8px">利用多模态大语言模型（MLLMs）开发具身智能体为解决复杂的现实世界任务提供了巨大潜力。然而，当前的评估基准主要以语言为中心或严重依赖模拟环境，很少涉及实际场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们引入了稀疏锚定视觉导航任务，专门设计用于评估MLLMs在具有挑战性和知识密集型的现实城市环境中的顺序决策能力。我们通过CityNav这一涵盖四个全球城市的综合性基准来实现该任务，专门用于评估原始的MLLM驱动智能体的城市导航能力。智能体必须仅依靠视觉输入和内部多模态推理，在无额外环境标注或专用架构修改的情况下，依次完成50多个决策点。关键的是，智能体必须通过解读城市特定线索和识别地标自主实现定位，进行空间推理，并战略性地规划和执行前往目标的路线。通过广泛的评估，我们表明当前最先进的MLLMs、推理技术（如GEPA、思维链、反思）以及具有竞争力的基线PReP在这一挑战性场景中表现显著不足。为了解决这一问题，我们提出了路径语言化（VoP），通过从MLLM中探询城市级认知地图（关键地标和朝向目标的方向）来显式锚定智能体的内部推理，从而显著提升导航成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of evaluating multimodal large language models (MLLMs) in real-world navigation tasks, which are often overlooked in current benchmarks that focus on language or simulation. The authors introduce the Sparsely Grounded Visual Navigation task, implemented through the CityNav benchmark, to assess MLLMs&#x27; ability to navigate complex urban environments using only visual inputs and internal reasoning. Their experiments show that existing models and techniques, including GEPA, chain-of-thought, and PReP, struggle significantly in this setting. To improve performance, they propose VoP, a method that explicitly verbalizes the agent&#x27;s internal reasoning by extracting city-scale cognitive maps, leading to better navigation outcomes.</div>
<div class="mono" style="margin-top:8px">本研究针对当前多模态大语言模型（MLLMs）评估基准存在的局限性，即过于依赖语言或模拟环境，提出了稀疏视觉导航任务。该任务旨在评估MLLMs在现实城市导航场景中，仅依靠视觉输入和内部多模态推理进行序列决策的能力。提出的CityNav基准包含四个全球城市，要求智能体在无额外环境标注的情况下完成定位、空间推理和路径规划。实验结果表明，当前最先进的MLLMs和基线模型PReP在该任务中表现不佳，凸显了改进推理方法的必要性。为此，作者提出了VoP方法，通过显式提取城市级认知地图来增强智能体的内部推理，从而显著提升导航成功率。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-01-28T17:27:08+00:00</div>
<div class="meta-line">Comments: 21 main pages, 7 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该比闭源系统具有根本性的优势：它们可以专门针对私有代码库，直接在权重中编码特定仓库的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，能够快速且低成本地创建专门针对私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法和代码）模型中实现了最先进的结果，同时与前沿的开放权重模型（如Devstral-Small-2）表现相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到同等性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数万个轨迹。结合成本效益，这使得专门针对私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过20万个合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和干扰因素进行了详细分析。总体而言，我们认为我们的工作将大大加速开放编码代理的研究，并展示能够专门针对私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SERA, an efficient method for training coding agents that enables the rapid and cost-effective creation of agents specialized to private codebases. The motivation stems from the challenge of leveraging open-weight models for private data, which has traditionally been hindered by high training costs and complexity. The proposed method, Soft Verified Generation (SVG), generates thousands of synthetic trajectories from a single code repository using only supervised fine-tuning, achieving state-of-the-art performance on fully open-source models and matching the capabilities of advanced open-weight models like Devstral-Small-2. The results show that SERA is significantly more cost-efficient than reinforcement learning and previous synthetic data methods, making repository specialization practical and scalable.</div>
<div class="mono" style="margin-top:8px">本文旨在展示训练能够针对私有代码库进行专业化的开放权重编码代理的可行性，这在传统上被认为比封闭系统更具成本效益。作者提出了SERA方法，利用监督微调（SFT）训练编码代理，其性能达到完全开源模型的最先进水平，并与先进的开放权重模型如Devstral-Small-2相匹配。他们提出的方法称为Soft Verified Generation（SVG），能够从单个代码库生成数千条合成轨迹，相较于强化学习和以往的合成数据方法，训练成本分别降低了26倍和57倍。此外，他们将SVG应用于更大的代码库集合，生成超过200,000条轨迹，用于深入分析编码代理的扩展规律和训练因素。</div>
</details>
</div>
<div class="card">
<div class="title">On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents</div>
<div class="meta-line">Authors: Jai Lal Lulla, Seyedmoein Mohsenimofidi, Matthias Galster, Jie M. Zhang, Sebastian Baltes, Christoph Treude</div>
<div class="meta-line">First: 2026-01-28T09:09:30+00:00 · Latest: 2026-01-28T09:09:30+00:00</div>
<div class="meta-line">Comments: 5 pages, 1 figure, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20404v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于AGENTS.md文件对AI编码代理效率的影响</div>
<div class="mono" style="margin-top:8px">诸如Codex和Claude Code等AI编码代理正越来越多地用于自主贡献软件仓库。然而，关于仓库级配置工件如何影响代理操作效率的了解仍然有限。本文研究了AGENTS.md文件对在GitHub拉取请求上运行的AI编码代理的运行时间和令牌消耗的影响。我们分析了10个仓库和124个拉取请求，在有和无AGENTS.md文件的两种条件下执行代理。我们测量了代理执行期间的实时时钟执行时间和令牌使用情况。我们的结果表明，存在AGENTS.md文件与较低的中位数运行时间（Δ28.64%）和减少的输出令牌消耗（Δ16.58%）相关，同时保持了相当的任务完成行为。基于这些结果，我们讨论了在实践中对AI编码代理配置和部署的直接影响，并概述了关于仓库级指令在塑造AI编码代理行为、效率和软件开发流程中整合作用的更广泛研究议程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AGENTS.md files influence the efficiency of AI coding agents in GitHub pull request environments. As AI coding agents like Codex and Claude Code become more prevalent in software development, understanding their interaction with repository-level configuration artifacts is crucial. The research analyzes 10 repositories and 124 pull requests, comparing agent performance with and without AGENTS.md files. Results indicate that AGENTS.md files lead to a 28.64% reduction in median runtime and a 16.58% decrease in token consumption, without compromising task completion. These findings suggest practical benefits for configuring and deploying AI coding agents and highlight the need for further research on the role of such artifacts in software development workflows.</div>
<div class="mono" style="margin-top:8px">本文探讨了AGENTS.md文件对GitHub拉取请求中AI编码代理效率的影响。随着AI编码代理在软件开发中的广泛应用，理解其与仓库级配置工件的交互变得尤为重要。研究分析了10个仓库和124个拉取请求，比较了在有和无AGENTS.md文件条件下的代理表现。结果显示，AGENTS.md的存在使中位运行时间减少了28.64%，输出令牌消耗降低了16.58%，同时保持任务完成行为的一致性。这些发现强调了仓库级指令在优化AI编码代理行为和效率方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding</div>
<div class="meta-line">Authors: Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao</div>
<div class="meta-line">First: 2024-02-24T08:57:12+00:00 · Latest: 2026-01-28T08:05:32+00:00</div>
<div class="meta-line">Comments: Accepted by Empirical Software Engineering (EMSE) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.15769v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.15769v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenCode：一种通用的数据增强框架，用于提升基于深度学习的代码理解</div>
<div class="mono" style="margin-top:8px">预训练代码模型引领了代码智能的时代，多个模型表现出令人印象深刻的表现。然而，一个重要的问题，即用于自动帮助开发者准备训练数据的代码数据增强方法，目前在该领域缺乏研究。在本文中，我们提出了一种通用的数据增强框架GenCode，以增强代码理解模型的训练。简而言之，GenCode遵循生成与选择的范式来准备有用的训练代码数据。具体来说，它首先使用代码增强技术生成新的代码候选，然后通过影响力评分识别重要的代码作为训练数据。为了评估GenCode的有效性，我们在四个代码理解任务（如代码克隆检测）和三个预训练代码模型（如CodeT5）以及两个最近发布的代码专用大语言模型（LLMs）（如Qwen2.5-Coder）上进行了实验。与最先进的代码增强方法MixCode相比，GenCode在预训练代码模型上平均提高了2.92%的准确率和4.90%的对抗鲁棒性。对于代码专用的LLMs，GenCode在准确率和自然鲁棒性上平均分别提高了0.93%和0.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the lack of effective data augmentation methods for code data in the context of deep learning-based code understanding. GenCode is introduced as a generic framework that follows a generation-and-selection approach, where it first generates new code candidates using code augmentation techniques and then selects the most informative ones based on influence scores. Experimental results on four code understanding tasks and three pre-trained models, including two recent code-specific large language models, show that GenCode improves accuracy by 2.92% and adversarial robustness by 4.90% compared to the state-of-the-art method MixCode.</div>
<div class="mono" style="margin-top:8px">本文针对代码数据在深度学习代码理解领域中缺乏有效的数据增强方法的问题，提出了一种通用的数据增强框架GenCode。该框架采用生成与选择的范式，首先通过代码增强技术生成新的代码候选，再利用影响分数筛选出最有用的代码作为训练数据。实验在四个代码理解任务和三种预训练模型上进行，结果显示GenCode在代码模型的准确率和对抗鲁棒性上分别比现有最优方法MixCode高出2.92%和4.90%，在代码专用大语言模型的准确率和自然鲁棒性上平均提升0.93%和0.98%。</div>
</details>
</div>
<div class="card">
<div class="title">Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction</div>
<div class="meta-line">Authors: Genyuan Zhang, Zihao Wang, Zhifan Gao, Lei Xu, Zhen Zhou, Haijun Yu, Jianjia Zhang, Xiujian Liu, Weiwei Zhang, Shaoyu Wang, Huazhu Fu, Fenglin Liu, Weiwen Wu</div>
<div class="meta-line">First: 2026-01-28T06:54:06+00:00 · Latest: 2026-01-28T06:54:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构约束的语言引导扩散模型用于非配对低剂量CT血管造影重建</div>
<div class="mono" style="margin-top:8px">碘对比剂（ICM）的应用提高了CT在多种临床指征中的敏感性和特异性。然而，过量使用ICM可能导致肾损伤和危及生命的过敏反应。深度学习方法可以从低剂量ICM生成正常剂量ICM的CT图像，从而在保持诊断能力的同时减少所需剂量。然而，现有方法在使用不完全配对图像进行准确增强时存在困难，主要原因是模型识别特定结构的能力有限。为克服这一限制，我们提出了一种结构约束的语言引导扩散模型（SLDM），这是一种集成了结构协同和空间智能的统一医学生成模型。首先，有效提取图像的结构先验信息以约束模型推理过程，从而确保增强过程中结构的一致性。随后，引入具有空间智能的语义监督策略，该策略结合了视觉感知和空间推理的功能，促使模型实现精确增强。最后，应用了减影血管造影增强模块，其作用是提高ICM剂区域的对比度，使其适合观察。视觉比较的定性分析和多个指标的定量结果表明，我们的方法在低剂量对比剂CT血管造影的血管重建中是有效的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of accurately reconstructing high-quality CT angiography images from low-dose iodinated contrast media (ICM) data, which is critical for reducing patient exposure while maintaining diagnostic accuracy. The proposed method, Structure-constrained Language-informed Diffusion Model (SLDM), combines structural prior information and semantic supervision with spatial intelligence to enhance image quality. It constrains model inference using structural features, integrates visual perception and spatial reasoning through a semantic supervision strategy, and applies a subtraction angiography enhancement module to improve contrast. Experimental results show that the SLDM outperforms existing methods in both qualitative visual analysis and quantitative metrics, demonstrating its effectiveness in low-dose CT angiography reconstruction.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决从低剂量碘对比剂CT血管造影数据中准确重建高质量CT图像的挑战，这对于减少患者辐射暴露同时保持诊断准确性至关重要。提出的结构约束语言引导扩散模型（SLDM）结合了结构先验信息和语义监督策略，以提升图像重建效果。该模型通过结构特征约束推理过程，引入语义监督以增强视觉感知和空间推理能力，并应用减影血管造影增强模块提升对比剂区域的对比度。实验结果表明，SLDM在定性和定量评估中均优于现有方法，实现了从低剂量数据中更准确、一致的血管造影重建。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</div>
<div class="meta-line">Authors: Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long</div>
<div class="meta-line">First: 2026-01-27T17:40:07+00:00 · Latest: 2026-01-27T17:40:07+00:00</div>
<div class="meta-line">Comments: Project page: https://thuml.github.io/Reasoning-Visual-World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://thuml.github.io/Reasoning-Visual-World">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉生成通过多模态世界模型实现类人推理</div>
<div class="mono" style="margin-top:8px">人类通过构建内部世界模型并在其中操作概念来进行推理。最近的AI进展，特别是链式思维（CoT）推理，已经近似模拟了这种人类认知能力，其中世界模型被认为嵌入在大型语言模型中。当前系统主要依赖语言推理，在数学和编程等正式和抽象领域已达到专家级表现。然而，在需要更丰富表示和先验知识的物理和空间智能领域，它们仍远不如人类。因此，统一多模态模型（UMMs）的出现，引发了人们对基于互补多模态路径的更类人推理方式的兴趣，尽管其优势尚不明确。从世界模型的角度来看，本文首次系统地研究了视觉生成何时以及如何促进推理。我们的核心观点是视觉优势假说：对于某些任务，尤其是与物理世界相关的任务，视觉生成更自然地作为世界模型，而纯语言世界模型则因表示限制或先验知识不足而遇到瓶颈。理论上，我们将内部世界建模形式化为CoT推理的核心组成部分，并分析不同形式世界模型之间的区别。实证上，我们识别出需要交替使用视觉和语言CoT推理的任务，构建了一个新的评估套件VisWorld-Eval。在最先进的UMM上进行的受控实验表明，在有利于视觉世界建模的任务中，交替CoT显著优于纯语言CoT，但在其他任务中则没有明显优势。综上，本工作阐明了多模态世界建模在构建更强大、更类人多模态AI方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how visual generation can enhance human-like reasoning in AI systems by leveraging multimodal world models. Motivated by the limitations of purely verbal reasoning in domains requiring spatial and physical understanding, the paper proposes the visual superiority hypothesis, suggesting that visual generation more naturally supports world modeling for such tasks. Through controlled experiments on a state-of-the-art unified multimodal model, the authors demonstrate that interleaving visual and verbal reasoning significantly improves performance on tasks that benefit from visual world modeling, while offering no clear advantage in purely verbal domains.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉生成如何通过多模态世界模型提升人工智能系统的人类类比推理能力。论文提出了视觉优越性假说，认为在涉及物理和空间理解的任务中，视觉生成比纯语言模型更能自然地构建有效的世界模型。通过在先进统一多模态模型上的受控实验，研究发现交错使用视觉与语言推理在需要视觉世界建模的任务中显著提升了性能，而在纯语言任务中则没有明显优势。这些结果突显了多模态世界模型在实现更接近人类的AI推理方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-27T12:16:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的代理原生中间训练</div>
<div class="mono" style="margin-top:8px">最近，大型语言模型（LLM）的能力前沿已从单轮代码生成转向代理软件工程——一种模型能够自主导航、编辑和测试复杂仓库的范式。尽管后训练方法已成为代码代理的主流方法，但由于需要大量资源，**代理中间训练**（agentic mid-training）——在大规模数据上进行中间训练，这些数据模拟真实的代理工作流——仍被严重忽视。与仅依赖昂贵的强化学习相比，代理中间训练提供了更可扩展的路径来培养基础的代理行为。实现有效的代理中间训练的核心挑战在于静态训练数据与真实开发中动态且反馈丰富的环境之间的分布不匹配。为了解决这一问题，我们系统地研究了代理中间训练，建立了适用于大规模代理开发的数据合成原则和训练方法。我们的方法核心是**代理原生数据**——包含两种互补类型的轨迹：**上下文原生轨迹**，保留代理经历的完整信息流，提供广泛覆盖和多样性；以及**环境原生轨迹**，从可执行仓库中收集，其观察来源于实际工具调用和测试执行，提供深度和交互的真实性。我们在 `SWE-Bench Verified` 上验证了模型的代理能力。在两种后训练设置下，我们展示了基于对齐基础模型和代理框架的模型优于之前的开源软件工程中间训练方案 `Kimi-Dev`，同时使用的中间训练标记数不到其一半（73.1B）。除了相对优势外，我们的最佳表现模型（32B 和 72B）分别实现了 **56.1%** 和 **58.5%** 的解决率，分别...</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to explore a more scalable and efficient approach to training code agents by introducing agentic mid-training, which involves training on data that reflects real-world software engineering workflows. The authors propose a method that utilizes agent-native data, combining contextually-native and environmentally-native trajectories to better align training with the dynamic and interactive nature of development environments. Their experiments on SWE-Bench Verified show that their approach outperforms previous methods like Kimi-Dev, achieving higher resolution rates with significantly fewer training tokens.</div>
<div class="mono" style="margin-top:8px">该论文探讨了通过代理中间训练（agentic mid-training）来提升代码代理能力的挑战，旨在通过模拟真实开发流程的大规模数据训练大型语言模型，使其具备基础的代理行为。作者提出了代理原生数据，结合上下文原生轨迹和环境原生轨迹，以弥合静态训练数据与动态开发环境之间的差距。在SWE-Bench Verified数据集上，他们的方法在与Kimi-Dev等先前方法的对比中表现出色，使用不到一半的中间训练标记即可实现56.1%和58.5%的解决率，分别对应其32B和72B模型。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</div>
<div class="meta-line">Authors: Daniel Ogenrwot, John Businge</div>
<div class="meta-line">First: 2026-01-24T20:27:04+00:00 · Latest: 2026-01-27T06:14:36+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17581v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff&#x27;s $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何修改代码：GitHub拉取请求的大型研究</div>
<div class="mono" style="margin-top:8px">AI编码代理正越来越多地作为自主贡献者，通过生成和提交拉取请求（PR）参与开发。然而，我们缺乏关于这些由代理生成的PR与人类贡献之间差异的实证证据，尤其是在代码修改方式和PR描述方面。理解这些差异对于评估其可靠性和对开发流程的影响至关重要。我们使用MSR 2026 Mining Challenge版本的AIDev数据集，分析了24,014个已合并的Agentic PR（440,295次提交）和5,081个已合并的人类PR（23,242次提交）。我们考察了新增内容、删除内容、提交次数以及涉及的文件，并通过词法和语义相似性评估PR描述与其差异的一致性。在提交次数方面，Agentic PR与Human PR存在显著差异（Cliff&#x27;s δ=0.5429），在涉及的文件和删除行方面表现出中等差异。此外，它们在所有衡量标准下显示出略高的描述与差异相似性。这些发现提供了关于AI编码代理如何参与开源开发的大规模实证特征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents contribute to open source development by analyzing their pull requests compared to those made by humans. The research is motivated by the need to understand the differences in code modification patterns and PR descriptions between AI-generated and human contributions. Using the AIDev dataset, the authors examine 24,014 merged Agentic PRs and 5,081 merged Human PRs, focusing on commit counts, files touched, and the semantic consistency between PR descriptions and code changes. The results show that Agentic PRs have significantly higher commit counts and moderate differences in files touched and deleted lines, while also demonstrating slightly better alignment between descriptions and diffs across various similarity measures.</div>
<div class="mono" style="margin-top:8px">本研究通过分析AI编码代理生成的拉取请求与人类贡献的差异，探讨其在开源开发中的贡献方式。研究动机源于对AI生成代码修改模式和描述一致性缺乏实证了解的需求。利用AIDev数据集，作者对比了24,014个合并的Agentic PR和5,081个合并的人类PR，关注提交次数、修改文件及描述与代码变更的一致性。结果显示，Agentic PR的提交次数显著高于人类PR，且在修改文件和删除行方面存在中等差异，同时其描述与代码变更的语义一致性也略高。</div>
</details>
</div>
<div class="card">
<div class="title">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</div>
<div class="meta-line">Authors: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou</div>
<div class="meta-line">First: 2025-10-13T05:51:22+00:00 · Latest: 2026-01-27T05:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vlaser：具有协同具身推理能力的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管已有大量研究致力于利用视觉-语言模型（VLMs）开发具身推理能力，或将其集成到视觉-语言-动作（VLA）模型中以实现端到端的机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。在本工作中，我们通过引入Vlaser——一个具有协同具身推理能力的视觉-语言-动作模型，迈出初步一步，弥合具身推理与VLA策略学习之间的鸿沟。Vlaser是一个基础的视觉-语言模型，旨在将高层次推理与低层次控制相结合，以支持具身智能体。基于高质量的Vlaser-6M数据集，Vlaser在一系列具身推理基准测试中实现了最先进的性能，包括空间推理、具身锚定、具身问答和任务规划。此外，我们系统地研究了不同VLM初始化对监督VLA微调的影响，提供了关于缓解互联网规模预训练数据与具身特定策略学习数据之间领域偏移的新见解。基于这些见解，我们的方法在WidowX基准测试中取得了最先进的结果，并在Google Robot基准测试中表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating high-level vision-language reasoning with low-level action control in embodied agents, aiming to bridge the gap between upstream vision-language models and downstream vision-language-action policy learning. The proposed Vlaser model introduces synergistic embodied reasoning, combining a foundational vision-language model with task-specific control mechanisms. Evaluated on several benchmarks, Vlaser achieves state-of-the-art performance in spatial reasoning, embodied grounding, embodied QA, and task planning, and provides insights into improving VLA fine-tuning by analyzing the impact of different VLM initializations on domain adaptation.</div>
<div class="mono" style="margin-top:8px">本研究旨在弥合上游基于视觉语言模型（VLM）的推理与下游视觉语言动作（VLA）策略学习之间的关键差距，以提升具身智能体的综合能力。Vlaser模型引入了协同具身推理机制，将基础的视觉语言模型与任务相关的控制模块相结合。在多个具身推理基准测试中，Vlaser表现出最先进的性能，包括空间推理、具身定位、具身问答和任务规划，并提供了关于如何改善互联网规模预训练数据与具身策略学习数据之间领域迁移的全新见解。</div>
</details>
</div>
<div class="card">
<div class="title">m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</div>
<div class="meta-line">Authors: Yosub Shin, Michael Buriek, Igor Molybog</div>
<div class="meta-line">First: 2026-01-27T02:01:56+00:00 · Latest: 2026-01-27T02:01:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>m2sv：一种用于地图到街景空间推理的可扩展基准</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在许多多模态基准上表现出色，但在需要将抽象的俯视图表示与第一视角视图对齐的空间推理任务中仍显得脆弱。我们引入了m2sv，这是一个用于地图到街景空间推理的可扩展基准，要求模型通过将北向俯视地图与同一现实交叉路口拍摄的街景图像对齐，推断相机的观看方向。我们发布了m2sv-20k，这是一个地理多样性且具有可控模糊性的基准，以及m2sv-sft-11k，一个用于监督微调的精心挑选的结构化推理轨迹集。尽管在现有多模态基准上表现强劲，但评估最好的VLM在m2sv上的准确率仅为65.2%，远低于人类基线的95%。虽然监督微调和强化学习能带来一致的提升，但跨基准评估显示其迁移能力有限。除了整体准确率外，我们还通过结构信号和人类努力系统地分析了地图到街景推理的难度，并对适应的开放模型进行了广泛的失败分析。我们的研究结果突显了几何对齐、证据聚合和推理一致性方面的持续差距，为未来多视角的基于场景的空间推理研究提供了动力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces m2sv, a scalable benchmark designed to evaluate vision-language models&#x27; ability to perform spatial reasoning by aligning overhead maps with egocentric street-view images. The benchmark includes m2sv-20k, a geographically diverse dataset with controlled ambiguity, and m2sv-sft-11k, a set of structured reasoning traces for supervised fine-tuning. Despite strong performance on other multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, significantly lower than the human baseline of 95%. The research also highlights challenges in geometric alignment, evidence aggregation, and reasoning consistency, suggesting the need for further improvements in grounded spatial reasoning across different viewpoints.</div>
<div class="mono" style="margin-top:8px">该研究提出了m2sv基准，用于评估视觉语言模型在将地图与街景视图进行空间推理任务中的表现。该基准要求模型通过将北向俯视地图与同一现实路口拍摄的街景图像对齐，推断相机的视角方向。研究发布了包含地理多样性且具有可控模糊度的m2sv-20k数据集，以及用于监督微调的m2sv-sft-11k结构化推理轨迹集。尽管在现有多模态基准上表现优异，但最佳评估的视觉语言模型在m2sv上的准确率仅为65.2%，远低于人类基线的95%。研究还通过结构信号和人工分析系统地探讨了地图到街景推理的难度，并对调整后的开放模型进行了广泛失败分析，揭示了几何对齐、证据聚合和推理一致性方面的持续差距，为未来跨视角的基于场景的空间推理研究提供了方向。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward</div>
<div class="meta-line">Authors: Dipendra Misra, Aldo Pacchiano, Ta-Chung Chi, Ge Gao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-27T00:31:04+00:00 · Latest: 2026-01-27T00:31:04+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19055v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent&#x27;s response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains adapted from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于用户编辑的LLM原理化微调：偏好、监督与奖励的融合</div>
<div class="mono" style="margin-top:8px">我们研究如何利用由用户编辑生成的部署数据来微调LLM，该数据集包含上下文、代理的响应以及用户的编辑。这种部署数据自然地出现在基于LLM的写作助手和编码代理等应用中。由于用户编辑的自然来源，它成为适应和个性化LLM的理想数据源。在这一设置下，各种通常在文献中被单独研究的反馈类型（偏好、监督标签和成本）得以统一。本文我们启动了从用户编辑中学习的理论研究。我们首先推导出从这些反馈类型中学习的算法的学习界。我们证明这些算法根据用户、数据分布和模型类别具有不同的权衡。随后，我们提出了一种简单的集成方法，以联合学习这些反馈类型。在两个从Gao等人（2024）改编的领域中，我们展示了我们的集成方法优于仅从单一反馈类型学习的方法。此外，我们还证明了所提出的程序在测试时能够稳健地适应不同的用户编辑分布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the fine-tuning of large language models (LLMs) using user-edit deployment data, which includes context, agent responses, and user modifications. The motivation stems from the natural and diverse feedback provided by users in applications like writing assistants and coding agents, making it a valuable resource for personalization. The authors analyze the theoretical aspects of learning from different feedback types—preferences, supervision, and cost—and demonstrate that each has distinct performance trade-offs. They propose an ensembling approach that integrates these feedback types, showing improved results over individual methods on two domains adapted from Gao et al. 2024. The method is also shown to be robust across varying user-edit distributions during testing.</div>
<div class="mono" style="margin-top:8px">本文研究了如何利用用户编辑的部署数据对大型语言模型（LLMs）进行微调，该数据包含上下文、代理响应以及用户的修改。研究动机源于用户编辑的自然性和信息性，使其成为模型个性化和适应的重要资源。作者分析了从偏好、监督标签和成本等不同反馈类型中学习的理论特性，并证明每种类型的学习存在不同的权衡。随后，他们提出了一种简单的集成方法，联合利用这些反馈类型，在两个改编自Gao等人2024年的领域上展示了优于单独反馈方法的性能。实验结果表明，所提出的方法在测试时能够稳健地适应不同的用户编辑分布。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Foundation Models for Radar Scene Understanding</div>
<div class="meta-line">Authors: Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia</div>
<div class="meta-line">First: 2025-11-26T06:41:00+00:00 · Latest: 2026-01-26T20:56:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21105v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21105v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>雷达场景理解的基础模型扩展</div>
<div class="mono" style="margin-top:8px">雷达传感器在恶劣天气、光照和远距离条件下提供了可靠的感知能力。近年来，基础模型在视觉和语言理解方面取得了重大进展，但其与雷达感知的结合仍处于初步探索阶段。现有的雷达方法往往是碎片化且任务特定的；每个下游任务都采用不同的架构和训练目标，阻碍了任务间的迁移能力。在本文中，我们提出了RadarFM：一种通过结构化空间语言监督学习统一场景表示的雷达基础模型。我们做出了两项关键贡献：(1) 一种结构化描述框架，能够以原生雷达坐标编码车辆分布；(2) 一种哈希感知的对比学习目标，通过量化连续场景相似性而非二元匹配，实现细粒度的空间推理。我们利用CARLA模拟器，在多样化的驾驶场景中生成大规模且标注良好的雷达数据集。此外，我们还提出了定位感知的评估指标，以超越传统检测度量的方式评估空间准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of integrating foundation models into radar scene understanding, where existing methods are task-specific and lack transferability. The authors propose RadarFM, a foundation model that learns unified scene-level representations using structured spatial language supervision. Two key innovations include a structured caption framework that encodes vehicle distributions in radar coordinates and a hash-aware contrastive learning objective for continuous scene similarity. Experimental results on large-scale radar datasets generated via CARLA demonstrate improved spatial reasoning and accuracy compared to traditional detection-based metrics.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将基础模型与雷达感知结合以实现恶劣环境下的场景理解的挑战。作者提出了RadarFM，一种通过结构化空间语言监督学习统一场景级表示的基础模型。两项关键创新包括用于在原生雷达坐标中编码车辆分布的结构化描述框架，以及一种基于哈希的对比学习目标，通过量化连续场景相似性实现细粒度空间推理。在CARLA模拟器生成的大规模、标注良好的雷达数据集上进行的实验表明，RadarFM在空间理解方面优于传统的任务特定方法，其引入的空间感知评估指标能够更有效地衡量空间准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests</div>
<div class="meta-line">Authors: Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang</div>
<div class="meta-line">First: 2026-01-08T12:31:02+00:00 · Latest: 2026-01-26T17:05:34+00:00</div>
<div class="meta-line">Comments: Accepted by MSR&#x27;26 Mining Challenge Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04886v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04886v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that &quot;descriptions claim unimplemented changes&quot; was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5 times longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分析AI编码代理撰写的拉取请求中的消息-代码不一致</div>
<div class="mono" style="margin-top:8px">由AI编码代理生成的拉取请求（PR）描述是向人类审阅者传达代码更改的主要渠道。然而，这些消息与实际更改之间的对齐情况尚未被探索，引发了对AI代理可信度的担忧。为填补这一空白，我们使用PR消息-代码不一致（PR-MCI）分析了五个代理的23,247个代理PR。我们贡献了974个手动标注的PR，发现其中406个（1.7%）表现出高PR-MCI，并识别出八种PR-MCI类型，其中最常见的问题是“描述声称未实现的更改”（占45.4%）。统计检验证实，高MCI的PR接受率低51.7%（28.3% vs. 80.0%），且合并时间是普通PR的3.5倍（55.8小时 vs. 16.0小时）。我们的研究结果表明，不可靠的PR描述削弱了对AI代理的信任，突显了需要PR-MCI验证机制和改进PR生成以实现可信的人机协作的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the issue of message-code inconsistency in pull requests (PRs) authored by AI coding agents, as PR descriptions are critical for human reviewers to understand code changes. By analyzing 23,247 PRs across five agents, the researchers identified 406 PRs with high PR-MCI, where the descriptions did not match the actual code changes. They found that &quot;descriptions claim unimplemented changes&quot; was the most frequent type of inconsistency, accounting for 45.4% of cases. High-MCI PRs were associated with significantly lower acceptance rates and longer merge times, indicating that unreliable PR descriptions can hinder trust and efficiency in human-AI collaboration.</div>
<div class="mono" style="margin-top:8px">本研究探讨了由AI编码代理撰写的拉取请求（PR）中消息与代码不一致的问题，因为PR描述是人类审阅者理解代码更改的关键渠道。通过对五个代理生成的23,247个PR进行分析，研究人员发现了406个存在高不一致性的PR，并贡献了974个手动标注的案例。其中45.4%的不一致源于描述中声称实现了但未实际完成的更改，统计分析表明，高不一致性的PR接受率低51.7%，合并时间长3.5倍，凸显了需要改进的PR验证和生成机制以提升AI辅助代码审查的可信度。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过预测编码实现共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是一个关键挑战，其中部分可观测性和有限带宽常常导致协调中的灾难性失败。我们引入了一种多智能体预测编码框架，将协调建模为智能体之间相互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习与谁以及传递什么信息，还学习何时传递。该框架的基础是一种类似网格细胞的度量，作为内部空间编码用于自我定位，这种编码自发地从自监督运动预测中产生。在此基础上，智能体逐步发展出一种带宽高效的通信机制，并形成专门的神经群体来编码合作伙伴的位置——这是海马体社会位置细胞（SPCs）的人工类比。这些社会表征进一步被分层强化学习策略所利用，该策略主动探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法表现出对带宽限制的卓越鲁棒性：当带宽从128位/步减少到4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则从67.6%骤降至28.6%。我们的研究结果为复杂社会表征如何从统一的预测驱动中产生提供了理论上有原则且生物上合理的基础，从而实现集体智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of maintaining a consistent shared spatial memory in multi-agent systems, particularly under conditions of partial observability and limited communication bandwidth. The proposed method introduces a predictive coding framework that minimizes mutual uncertainty among agents by learning who, what, and when to communicate. This framework uses a grid-cell-like metric for self-localization, which arises from self-supervised motion prediction. Agents also develop specialized neural populations to encode partners&#x27; locations, mimicking hippocampal social place cells. The main experimental results show that the approach maintains high success rates even under severe bandwidth limitations, degrading gracefully from 73.5% to 64.4% as bandwidth decreases from 128 to 4 bits per step, while a full-broadcast baseline experiences a sharp drop to 28.6%.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多智能体系统中构建一致共享空间记忆的挑战，该问题常因部分可观测性和通信带宽限制而导致协调失败。作者提出了一种多智能体预测编码框架，通过最小化智能体间的相互不确定性来实现协调，该框架基于信息瓶颈目标，促使智能体学习何时、何内容以及与谁进行通信。该框架采用类似网格细胞的度量作为内部空间编码，用于自定位，并通过自监督运动预测自发形成。此外，智能体还发展出专门的神经群体以编码同伴位置，类似于海马体中的社会位置细胞。在Memory-Maze基准测试中，该方法表现出对带宽限制的强鲁棒性，当带宽从128位/步降至4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则从67.6%骤降至28.6%。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-26T10:49:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, SWE-EVO comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on SWE-EVO, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长周期软件演进场景中对编码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI编码代理基准测试主要关注孤立的、单一问题的任务，例如修复错误或实现小功能。然而，现实中的软件工程本质上是一项长周期的工作：开发者必须理解高层需求，协调修改多个文件，并在多次迭代中演进代码库同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长周期软件演进挑战上的基准测试。SWE-EVO基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而相比之下在单一问题的SWE-Bench Verified上解决率为65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长周期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to address the limitations of existing benchmarks for AI coding agents, which focus on isolated, single-issue tasks rather than the complex, long-term nature of real-world software engineering. SWE-EVO is introduced as a new benchmark that evaluates agents on multi-step, multi-file software evolution tasks derived from seven mature open-source Python projects. The benchmark includes 48 tasks requiring changes across an average of 21 files and validated with extensive test suites. Experimental results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on the single-issue SWE-Bench Verified, highlighting the difficulty agents face in sustained, multi-file reasoning. The study also proposes Fix Rate as a more detailed metric to assess partial progress in solving these complex tasks.</div>
<div class="mono" style="margin-top:8px">该研究提出了SWE-EVO基准，用于评估AI编码代理在长期软件演进任务中的表现，这类任务需要跨多个文件进行多步骤修改并保持原有功能。与现有专注于单一问题的基准不同，SWE-EVO基于七个成熟Python项目的发布说明和版本历史构建，包含48个任务，每个任务平均涉及21个文件修改，并通过平均874个测试用例进行验证。实验结果显示，即使是GPT-5与OpenHands结合的先进模型，在SWE-EVO上的解决率也只有21%，远低于单任务SWE-Bench Verified的65%，表明当前代理在持续多文件推理方面存在显著困难。论文还提出了一种细粒度指标Fix Rate，用于衡量在解决这些复杂任务中的部分进展。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Conditioned Reasoning in Long-Egocentric Videos</div>
<div class="meta-line">Authors: James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi</div>
<div class="meta-line">First: 2026-01-26T03:21:35+00:00 · Latest: 2026-01-26T03:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时域第一视角视频中的空间条件推理</div>
<div class="mono" style="margin-top:8px">由于视角漂移和缺乏持久的几何上下文，长时域第一视角视频对视觉导航提出了重大挑战。尽管近期的视觉-语言模型在图像和短视频推理上表现良好，但它们在长第一视角序列中的空间推理能力仍有限。在本工作中，我们研究了显式空间信号如何影响基于VLM的视频理解，而无需修改模型架构或推理过程。我们引入了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在面向导航的空间查询上对多个VLM进行了基准测试。为了考察输入级别的归纳偏置，我们进一步将深度图与RGB帧融合，并评估其对空间推理的影响。我们的结果揭示了通用准确性和空间特异性之间的权衡，表明具有深度感知和空间基础的表示可以提升对行人和障碍物检测等安全关键任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenges of visual navigation in long-horizon egocentric videos, where viewpoint drift and lack of persistent geometric context hinder spatial understanding. The authors investigate how explicit spatial signals can enhance video understanding by VLMs without altering model structures or inference processes. They propose Sanpo-D, a refined version of the Google Sanpo dataset, and evaluate several vision-language models on spatial queries relevant to navigation. By integrating depth maps with RGB frames, they assess the impact of input-level inductive bias on spatial reasoning, finding that depth-aware representations improve performance in critical tasks like pedestrian and obstruction detection, albeit at the cost of reduced general-purpose accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对长时域第一视角视频中因视角漂移和缺乏持久几何上下文而带来的视觉导航挑战。作者探讨了如何在不修改模型结构或推理流程的前提下，利用显式的空间信号提升基于视觉语言模型（VLM）的视频理解能力。他们提出了Sanpo-D，对Google Sanpo数据集进行了细粒度重标注，并在导航导向的空间查询任务上评估了多种VLM的表现。通过融合深度图与RGB帧，分析输入层面的归纳偏置对空间推理的影响，结果表明深度感知表示在关键任务如行人和障碍物检测中能提升性能，但会牺牲通用准确性。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-26T00:36:33+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com First two authors contribute equally. The 3th - 6th authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能以找到共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的600多个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道被模糊、时机不当和不准确的信息堵塞；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the limitations of current coding agents in collaborative settings, particularly their inability to effectively coordinate with each other. CooperBench, a benchmark consisting of over 600 collaborative coding tasks across four programming languages and twelve libraries, is introduced to evaluate this. The tasks are designed to require two agents to implement independent features that may conflict without proper coordination. Experimental results show that state-of-the-art coding agents achieve on average 30% lower success rates when working together than when performing tasks individually, highlighting the &#x27;curse of coordination.&#x27; The analysis identifies three main issues: ineffective communication, deviation from commitments, and incorrect expectations about others&#x27; actions. However, the study also observes some emergent coordination behaviors such as role division and negotiation in large-scale simulations.</div>
<div class="mono" style="margin-top:8px">该研究提出了CooperBench，这是一个用于评估AI编码代理在协作任务中协调能力的基准。研究动机源于对AI代理作为有效队友所需能力的需求，研究人员创建了涵盖四种编程语言和十二个库的600多个任务，其中两个代理被分配了可能需要协调才能解决的冲突特性。实验结果显示，当前编码代理在协作时的成功率平均比单独执行任务低30%，揭示了‘协调的诅咒’。分析指出三个主要问题：沟通不畅、承诺偏离以及对他人计划的错误预期。尽管存在这些挑战，研究还观察到了一些有趣的协调行为，如角色分工和协商。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
