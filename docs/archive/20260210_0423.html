<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-10 04:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260210_0423</div>
    <div class="row"><div class="card">
<div class="title">MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</div>
<div class="meta-line">Authors: Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</div>
<div class="meta-line">First: 2026-02-06T18:59:59+00:00 · Latest: 2026-02-06T18:59:59+00:00</div>
<div class="meta-line">Comments: 21 pages, 6 figures and 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://genmilab.github.io/MedMO-Page">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO&#x27;s broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedMO：面向医学图像的多模态大语言模型的定位与理解</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）迅速发展，但其在医学领域的应用仍受限于领域覆盖不足、模态对齐和基于上下文的推理能力的差距。本文提出MedMO，这是一个基于通用MLLM架构并仅在大规模、领域特定数据上训练的医学基础模型。MedMO采用多阶段训练方案：(i) 跨模态预训练，将异构视觉编码器与医学语言主干对齐；(ii) 在涵盖图像描述、视觉问答（VQA）、报告生成、检索和基于边界框的疾病定位等多任务监督下进行指令调优；(iii) 采用可验证奖励的强化学习，结合事实性检查和框级GIoU奖励，以增强复杂临床场景中的空间定位和逐步推理能力。MedMO在多个模态和任务上均优于其他强大的开源医学MLLMs。在VQA基准测试中，MedMO在基线基础上平均准确率提升了13.7%，且性能接近当前最优模型Fleming-VL（仅差1.9%）。在基于文本的问答任务中，其准确率分别比基线提升6.9%和比Fleming-VL提升14.5%。在医学报告生成任务中，MedMO在语义和临床准确性方面均取得显著提升。此外，它展现出强大的定位能力，IoU指标比基线提升40.4%，比Fleming-VL提升37.0%，突显了其在空间推理和定位方面的稳健性能。在放射学、眼科和病理学显微镜领域的评估验证了MedMO在跨模态任务中的广泛泛化能力。我们发布了两个版本的MedMO：4B和8B。项目地址：https://genmilab.github.io/MedMO-Page</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to address the limitations of existing multimodal large language models (MLLMs) in medical domains, such as insufficient domain coverage, poor modality alignment, and weak grounded reasoning. MedMO is introduced as a medical foundation model, built on a generalized MLLM architecture and trained solely on large-scale, domain-specific medical data. It employs a three-stage training approach: cross-modal pretraining to align visual and language modalities, instruction tuning on diverse medical tasks including captioning, VQA, report generation, and disease localization, and reinforcement learning with a reward combining factuality checks and box-level GIoU metrics. The model demonstrates significant improvements across multiple tasks, achieving +13.7% average accuracy on VQA benchmarks, +6.9% on text-based QA, and +40.4% IoU improvement in spatial grounding tasks, outperforming existing medical MLLMs and showing strong cross-modality generalization.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有多模态大语言模型（MLLMs）在医学领域中的不足，特别是在领域覆盖、模态对齐和基于上下文的推理方面。MedMO 是一个基于通用 MLLM 架构的医学基础模型，专门使用大规模、领域相关的数据进行训练。该模型采用多阶段训练方法，包括跨模态预训练、多任务指令调优以及结合事实性检查和框级 GIoU 奖励的强化学习。实验结果显示，MedMO 在多个模态和任务上均优于其他医学 MLLMs，尤其在 VQA 任务、基于文本的问答和医学报告生成中表现出显著的准确率提升，同时在空间定位能力上也取得了明显进步。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Beyond Redundancy: Task Complexity&#x27;s Role in Vision Token Specialization in VLLMs</div>
<div class="meta-line">Authors: Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</div>
<div class="meta-line">First: 2026-02-06T18:13:01+00:00 · Latest: 2026-02-06T18:13:01+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06914v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越冗余：VLLMs中视觉标记专业化与任务复杂度的关系</div>
<div class="mono" style="margin-top:8px">视觉大语言模型（VLLMs）的视觉能力一直落后于其语言能力。特别是，许多基准研究已经表明，当需要细粒度视觉信息或空间推理时，VLLMs表现不佳。然而，我们尚未完全理解为何VLLMs在这些任务上相较于其他任务表现如此薄弱。一些研究将视觉冗余作为解释，认为高层视觉信息均匀分布在多个标记中，而具体的细粒度视觉信息则被舍弃。在本研究中，我们更深入地探讨这一前提，旨在更好地理解模型如何处理不同类型视觉信息以及哪些信息被舍弃。为此，我们引入了一个简单的人工合成基准数据集，专门用于探测各种视觉特征，并设计了一组衡量视觉冗余的指标，使我们能够更深入地理解它们之间的关系。随后，我们探索了在多个复杂视觉任务上微调VLLMs，以更好地理解冗余和压缩如何随着训练数据复杂度的变化而变化。我们发现任务复杂度与视觉压缩之间存在联系，这意味着拥有足够比例的高复杂度视觉数据对于改变VLLMs的视觉表示分布方式至关重要，从而提高其在复杂视觉任务上的表现。我们希望这项研究能为下一代VLLMs的训练提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why vision large language models (VLLMs) perform poorly on complex visual tasks compared to linguistic ones. It challenges the common assumption that visual redundancy is the main cause by introducing a synthetic benchmark dataset and metrics to measure visual redundancy. The results show that task complexity influences visual compression, suggesting that incorporating high-complexity visual data during training can improve how VLLMs allocate their visual representations and enhance performance on challenging visual tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了为何视觉大语言模型（VLLMs）在处理复杂视觉任务时表现不如其语言能力。作者提出了一种合成基准数据集和视觉冗余度量指标，以分析模型如何处理和压缩不同类型视觉信息。实验结果表明，任务复杂度影响视觉压缩的程度，提示在训练中引入高复杂度的视觉数据对于提升模型处理复杂视觉任务的能力至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCode: A Benchmark for Evaluating Software Engineering Agents</div>
<div class="meta-line">Authors: Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Simon Alford, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta</div>
<div class="meta-line">First: 2026-02-02T16:04:10+00:00 · Latest: 2026-02-06T15:49:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02262v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02262v2">PDF</a> · <a href="https://github.com/seal-research/OmniCode">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCode：评估软件工程代理的基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型的编码代理正在重新定义现实世界软件的开发方式。为了推动对更优编码代理的研究，我们需要能够严格评估此类代理执行各种软件工程任务能力的挑战性基准。然而，像HumanEval和SWE-Bench这样的流行编码基准主要关注狭窄范围的任务，如编程竞赛和补丁生成。实际上，软件工程师在现实世界软件开发中需要处理更广泛的任务。为了解决这一差距，我们提出了OmniCode，一个包含超越代码或补丁生成的更广泛和多样化任务类别的新型软件工程基准。总体而言，OmniCode包含1794个任务，涵盖三种编程语言（Python、Java和C++）以及四个关键类别：错误修复、测试生成、代码审查修复和风格修复。与之前的软件工程基准相比，OmniCode的任务（1）经过人工验证以消除定义不清的问题，（2）通过合成生成或最近整理以避免数据泄露问题，提供了一种从有限现实数据中合成生成多样化软件任务的新框架。我们使用流行的代理框架（如SWE-Agent）对OmniCode进行了评估，结果显示虽然它们在Python错误修复方面表现良好，但在测试生成任务以及C++和Java等语言中表现不足。例如，SWE-Agent在Java测试生成任务中使用DeepSeek-V3.1仅达到20.9%的准确率。OmniCode旨在成为一种稳健的基准，推动开发能够在软件开发不同方面表现优异的代理。代码和数据可在https://github.com/seal-research/OmniCode获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind OmniCode is to provide a comprehensive benchmark for evaluating the capabilities of LLM-powered coding agents in real-world software engineering tasks. The proposed benchmark includes tasks beyond code or patch generation, such as bug fixing, test generation, code review fixing, and style fixing, covering three programming languages (Python, Java, and C++) with 1794 total tasks. These tasks are manually validated and synthetically crafted to avoid data leakage, ensuring a rigorous evaluation. Experimental results show that existing agent frameworks like SWE-Agent perform well on Python bug fixing but struggle with tasks like Java test generation, achieving only 20.9% accuracy with DeepSeek-V3.1.</div>
<div class="mono" style="margin-top:8px">LLM驱动的编码代理正在改变现实世界的软件开发方式，但现有的基准如HumanEval和SWE-Bench在任务范围上较为狭窄，主要集中在编程竞赛和补丁生成。为弥补这一不足，提出了OmniCode这一新的软件工程基准，涵盖四个关键类别：错误修复、测试生成、代码审查修复和风格调整，共包含1794个任务，覆盖Python、Java和C++三种编程语言。这些任务经过人工验证并合成生成，以确保其准确性和避免数据泄露问题，为评估编码代理提供了更全面的框架。评估结果显示，当前框架如SWE-Agent在Python错误修复方面表现良好，但在Java和C++的测试生成等任务上存在明显不足。</div>
</details>
</div>
<div class="card">
<div class="title">PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks</div>
<div class="meta-line">Authors: Junxian Li, Kai Liu, Leyang Chen, Weida Wang, Zhixin Wang, Jiaqi Xu, Fan Li, Renjing Pei, Linghe Kong, Yulun Zhang</div>
<div class="meta-line">First: 2026-02-06T12:47:16+00:00 · Latest: 2026-02-06T12:47:16+00:00</div>
<div class="meta-line">Comments: The main part of our paper: PlanViz Code is at: https://github.com/lijunxian111/PlanViz Supplementary material is at: https://github.com/lijunxian111/PlanViz/releases/tag/v1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06663v1">PDF</a> · <a href="https://github.com/lijunxian111/PlanViz">Code1</a> · <a href="https://github.com/lijunxian111/PlanViz/releases/tag/v1">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&amp;UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PlanViz：评估面向计算机使用任务的规划导向图像生成与编辑</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在生成自然图像和多模态推理方面展现出令人印象深刻的能力。然而，它们在支持与我们的生活密切相关的计算机使用规划任务方面的潜力仍待探索。图像生成和编辑在计算机使用任务中需要空间推理和程序理解等能力，目前尚不清楚UMMs是否具备完成这些任务的能力。因此，我们提出了PlanViz，一个专门用于评估计算机使用任务中图像生成与编辑的新基准。为了实现我们的评估目标，我们关注那些频繁出现在日常生活中并需要规划步骤的子任务。具体来说，设计了三个新的子任务：路线规划、工作流程图绘制和网页与用户界面展示。我们通过整理人工标注的问题和参考图像，并实施质量控制流程，来应对数据质量方面的挑战。针对全面且精确的评估问题，我们提出了一个任务自适应评分系统PlanScore，该评分系统有助于理解生成图像的正确性、视觉质量和效率。通过实验，我们突出了该领域研究的关键局限性与未来研究的机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces PlanViz, a new benchmark designed to evaluate the performance of unified multimodal models (UMMs) in planning-oriented image generation and editing for computer-use tasks. The motivation stems from the underexplored potential of UMMs in tasks requiring spatial reasoning and procedural understanding, which are essential for daily life. The proposed method includes three sub-tasks: route planning, work diagramming, and web&amp;UI displaying, along with human-annotated questions and reference images to ensure data quality. A task-adaptive evaluation score, PlanScore, is introduced to assess correctness, visual quality, and efficiency of generated images. Experimental results highlight the current limitations of UMMs in these planning tasks and suggest directions for future research.</div>
<div class="mono" style="margin-top:8px">本文提出了PlanViz，这是一个用于评估统一多模态模型（UMMs）在计算机使用任务中进行规划导向图像生成与编辑能力的新基准。研究动机源于UMMs在需要空间推理和程序理解的日常任务（如路线规划、工作流程图绘制和网页与用户界面展示）中的潜力尚未被充分探索。作者通过整理人工标注的问题和参考图像，并提出一种任务自适应的评估指标PlanScore，来衡量生成图像的正确性、视觉质量和效率。实验结果揭示了当前UMMs在这些任务中的主要局限性，并指出了未来研究的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</div>
<div class="meta-line">Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</div>
<div class="meta-line">First: 2026-02-05T18:59:55+00:00 · Latest: 2026-02-06T06:50:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06041v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06041v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视角描述预测相机姿态以实现空间推理</div>
<div class="mono" style="margin-top:8px">多图像空间推理对于当前的多模态大语言模型（MLLMs）仍然是一个挑战。虽然单视角感知本质上是2D的，但跨多个视角进行推理需要在不同视角之间建立一致的场景理解。特别是，我们研究视角转换（perspective taking），其中模型必须从多视角观察中构建一致的3D理解，并据此在新的语言指定视角下进行推理。我们引入了CAMCUE，这是一个姿态感知的多图像框架，它将相机姿态作为跨视角融合和新视角推理的显式几何锚点。CAMCUE将每个视角的姿态注入到视觉标记中，将自然语言视角描述锚定到目标相机姿态，并合成基于姿态的想象目标视角以支持回答。为了支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例配对了多视角图像和姿态，并与多样化的目标视角描述和视角转换问题相关联。我们还在测试集中包含了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确率提高了9.06%，并且能够从自然语言视角描述中预测目标姿态，其旋转准确率超过90%（在20°以内），平移准确率在0.5误差阈值内。这种直接的锚定避免了昂贵的测试时搜索和匹配过程，将推理时间从每个示例256.6秒减少到1.45秒，从而实现了快速、交互式的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance multi-image spatial reasoning capabilities in multimodal large language models (MLLMs) by addressing the challenge of coherent scene understanding across different viewpoints. The proposed method, CAMCUE, introduces a pose-aware framework that explicitly uses camera pose as a geometric anchor for cross-view fusion and novel-view reasoning. It integrates per-view camera poses into visual tokens, maps natural-language viewpoint descriptions to target poses, and synthesizes imagined views to support reasoning. The main experimental results show that CAMCUE improves overall accuracy by 9.06% and achieves over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold, significantly reducing inference time from 256.6s to 1.45s per example.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升多模态大语言模型（MLLMs）在多图像空间推理中的能力，解决其在多视角理解上的局限性。提出的方法CAMCUE引入了一个基于相机姿态的框架，将相机姿态作为跨视角融合和新视角推理的几何锚点。该方法将每个视角的相机姿态注入到视觉标记中，将自然语言视角描述映射到目标姿态，并合成基于姿态的想象目标视角以支持推理。实验结果显示，CAMCUE整体准确率提升了9.06%，在旋转误差小于20°和位移误差小于0.5的条件下，旋转准确率超过90%。这种方法将推理时间从256.6秒大幅降低至1.45秒，使得模型在现实场景中能够高效、交互式地应用。</div>
</details>
</div>
<div class="card">
<div class="title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</div>
<div class="meta-line">Authors: Zhuofan Shi, Hubao A, Yufei Shao, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing</div>
<div class="meta-line">First: 2026-01-05T12:56:51+00:00 · Latest: 2026-02-06T05:47:09+00:00</div>
<div class="meta-line">Comments: 24 pages,4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02075v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.02075v4">PDF</a> · <a href="https://github.com/FredericVAN/PKU_MDAgent2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MDAgent2：用于分子动力学领域代码生成和知识问答的大语言模型</div>
<div class="mono" style="margin-top:8px">分子动力学（MD）模拟对于理解材料科学中的原子尺度行为至关重要，但编写LAMMPS脚本仍然是高度专业化且耗时的任务。尽管大语言模型（LLMs）在代码生成和领域特定问答方面展现出潜力，但其在MD场景中的表现受限于领域数据稀缺、先进LLMs的高部署成本以及生成代码的可执行性较低。基于我们之前提出的MDAgent，我们提出了MDAgent2，这是首个能够在MD领域内同时执行知识问答和代码生成的端到端框架。我们构建了一个领域特定的数据构建流水线，生成了三个高质量的数据集，涵盖MD知识、问答和代码生成。基于这些数据集，我们采用三阶段后训练策略——持续预训练（CPT）、监督微调（SFT）和强化学习（RL）——来训练两个领域适应模型：MD-Instruct和MD-Code。此外，我们引入了MD-GRPO，一种闭环强化学习方法，利用模拟结果作为奖励信号，并重用低奖励轨迹以实现持续优化。我们还构建了MDAgent2-RUNTIME，一个可部署的多智能体系统，集成了代码生成、执行、评估和自我修正功能。结合本文提出的MD-EvalBench，这是首个针对LAMMPS代码生成和问答的基准测试，我们的模型和系统在性能上超越了多个强大的基线模型。本工作系统地展示了大语言模型在工业模拟任务中的适应性和泛化能力，为AI for Science和大规模工业模拟中的自动代码生成奠定了方法论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Molecular dynamics (MD) simulations are crucial in materials science, but creating LAMMPS scripts is a specialized and time-consuming process. To address this, MDAgent2 introduces an end-to-end framework that combines knowledge Q&amp;A and code generation. The framework is trained using a three-stage strategy—continued pre-training, supervised fine-tuning, and reinforcement learning—on three domain-specific datasets. The resulting models, MD-Instruct and MD-Code, along with the MD-GRPO method and MDAgent2-RUNTIME system, outperform existing baselines in code generation and question answering tasks, demonstrating the effectiveness of large language models in industrial simulation contexts.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决分子动力学（MD）模拟中代码生成和知识问答的挑战，现有大语言模型（LLMs）在该领域面临数据稀缺、部署成本高和代码可执行性低等问题。MDAgent2提出了一种端到端框架，整合了这两个任务，并通过领域专用的数据构建流程生成了三个高质量数据集。该框架采用三阶段后训练策略——持续预训练、监督微调和强化学习——训练出两个模型MD-Instruct和MD-Code，并引入了基于模拟结果的闭环强化学习方法MD-GRPO。系统还包含MDAgent2-RUNTIME，一个可部署的多智能体系统，支持代码生成、执行、评估和自我修正。实验结果表明，所提出的模型和系统在MD-EvalBench基准测试中表现优于多个强基线模型，展示了大语言模型在工业模拟任务中的适应性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the effects of data contamination in Code Intelligence</div>
<div class="meta-line">Authors: Zhen Yang, Hongyi Lin, Yifan He, Junqi Wang, Zeyu Sun, Shuo Liu, Jie Xu, Pengpeng Wang, Zhongxing Yu, Qingyuan Liang</div>
<div class="meta-line">First: 2025-06-03T12:15:44+00:00 · Latest: 2026-02-06T05:44:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.02791v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.02791v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation. Previous studies mainly focused on sample-level contamination, ignoring partial contamination scenarios that are pervasive in code intelligence. This paper fills this gap and presents a systematic empirical study to investigate the fine-grained data contamination on mainstream code tasks. Our study involves diverse representative PLMs: RoBERTa and GPT-2, and LLMs: LLaMA and StarCoder, covering three major tasks: code translation, code generation, and code summarization, across two Programming Languages (PLs): Java and Python. We categorize contamination scenarios into four types according to the code intelligence practice, namely input-only, output-only, unpaired, and paired contamination settings, and construct corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination. Apart from the above, other contamination scenarios have no impact on both PLMs and LLMs. Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考数据污染对代码智能的影响</div>
<div class="mono" style="margin-top:8px">近年来，代码智能在自动化软件工程领域的重要性日益增加。同时，预训练语言模型（PLMs）和大语言模型（LLMs）的广泛应用引发了对数据污染及其对模型性能评估潜在影响的关注。以往的研究主要关注样本级别的污染，忽略了在代码智能中普遍存在部分污染场景。本文填补了这一空白，提出一项系统性的实证研究，探讨主流代码任务中细粒度的数据污染。我们的研究涵盖了多种具有代表性的PLMs：RoBERTa和GPT-2，以及LLMs：LLaMA和StarCoder，涉及三个主要任务：代码翻译、代码生成和代码摘要，覆盖两种编程语言（Java和Python）。我们根据代码智能实践将污染场景分为四类：仅输入污染、仅输出污染、非配对污染和配对污染，并构建了相应的实验组和对照组进行探索。实验结果表明，在PLMs采用的预训练、微调和推理范式下，即使故意注入配对污染，也不会导致显著的性能高估。但直接推理或小规模微调则会揭示污染的影响。相比之下，采用预训练和推理范式的LLMs则显著受到配对污染的影响。除此之外，其他污染场景对PLMs和LLMs均无影响。我们的发现挑战了传统观点，即污染必然导致性能高估，为代码智能模型的评估和部署提供了新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper reexamines the impact of data contamination on code intelligence models, addressing the gap in previous studies that primarily focused on sample-level contamination while neglecting more common partial contamination scenarios. The authors conduct a systematic empirical analysis on mainstream code tasks—code translation, code generation, and code summarization—using representative models such as RoBERTa, GPT-2, LLaMA, and StarCoder across Java and Python. They categorize contamination into four types and design corresponding experiments. Results indicate that under the standard pre-training, fine-tuning, and inference paradigm, performance overestimation due to contamination is not significant, but direct inference or small-scale fine-tuning reveals contamination effects. LLMs are more vulnerable to paired contamination than PLMs, challenging the assumption that contamination always leads to performance overestimation.</div>
<div class="mono" style="margin-top:8px">本文探讨了数据污染对代码智能模型的影响，挑战了数据污染必然导致性能高估的普遍观点。研究通过系统性实证分析，考察了代码翻译、代码生成和代码摘要三大任务中细粒度的数据污染场景，涵盖Java和Python两种编程语言。实验将污染分为四种类型，并评估了RoBERTa、GPT-2等预训练语言模型（PLMs）以及LLaMA、StarCoder等大语言模型（LLMs）在不同污染情况下的表现。结果显示，在标准的预训练和推理范式下，PLMs对配对污染的敏感性较低，而LLMs则表现出显著的性能下降，这为代码智能模型的评估与部署提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning</div>
<div class="meta-line">Authors: Bangji Yang, Ruihan Guo, Jiajun Fan, Chaoran Cheng, Ge Liu</div>
<div class="meta-line">First: 2026-02-05T20:10:27+00:00 · Latest: 2026-02-05T20:10:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06166v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>M3：通过多模态、多智能体和多轮次视觉推理实现高保真文本到图像生成</div>
<div class="mono" style="margin-top:8px">生成模型在文本到图像合成中已实现了令人印象深刻的保真度，但在涉及多个约束的复杂组合提示中仍存在困难。我们引入了\textbf{M3（多模态、多智能体、多轮次）}，这是一种无需训练的框架，通过迭代推理时的细化系统性地解决这些问题。M3在稳健的多智能体循环中协调现成的基础模型：一个规划器将提示分解为可验证的检查清单，而专门的检查器、细化器和编辑器智能体则依次精确修正约束，验证器确保持续改进。在开源模型上应用M3，在具有挑战性的OneIG-EN基准测试中取得了显著成果，我们的Qwen-Image+M3超越了包括Imagen4（0.515）和Seedream 3.0（0.530）在内的商业旗舰系统，达到最先进的性能（总体0.532）。这表明智能多智能体推理可以将开源模型提升至超越专有模型的水平。此外，M3还显著提升了GenEval组合指标，在强化测试集上有效将空间推理性能翻倍。作为一个即插即用模块，兼容任何预训练的文本到图像模型，M3建立了一种无需昂贵再训练的组合生成新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of generating high-fidelity images from complex text prompts that involve multiple constraints. M3, a training-free framework, employs a multi-modal, multi-agent, and multi-round approach to iteratively refine image generation during inference. It uses a Planner to break down prompts into checklists, and specialized Checker, Refiner, and Editor agents to correct constraints sequentially, with a Verifier ensuring consistent improvement. When applied to open-source models, M3 significantly enhances performance on the OneIG-EN benchmark, achieving state-of-the-art results with Qwen-Image+M3 outperforming commercial systems like Imagen4 and Seedream 3.0. It also doubles spatial reasoning performance on compositional metrics, demonstrating the potential of multi-agent reasoning to improve open-source text-to-image models without retraining.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前文本到图像生成模型在处理包含多个约束的复杂组合提示时的不足。提出的M3框架采用无训练方法，通过多模态、多智能体和多轮次的视觉推理，在推理过程中迭代优化图像生成。它包含一个规划器，将提示分解为可验证的清单，并结合专门的检查器、优化器和编辑器智能体，逐步修正每个约束，同时通过验证器确保生成质量的持续提升。实验结果表明，M3在OneIG-EN基准测试中表现出色，Qwen-Image+M3的得分为0.532，超越了包括Imagen4和Seedream 3.0在内的商业旗舰模型。此外，M3在GenEval组合指标上也有显著提升，使空间推理性能在强化测试集上翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</div>
<div class="meta-line">Authors: Shravan Chaudhari, Rahul Thomas Jacob, Mononito Goswami, Jiajun Cao, Shihab Rashid, Christian Bock</div>
<div class="meta-line">First: 2025-12-18T01:32:25+00:00 · Latest: 2026-02-05T19:34:42+00:00</div>
<div class="meta-line">Comments: Initial preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16956v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving code functions, classes or files that are relevant in order to solve a given user query, bug report or feature request from large codebases is a fundamental challenge for Large Language Model (LLM)-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify semantically relevant units. While embedding-based approaches can outperform BM25 by large margins, they often don&#x27;t take into consideration the underlying graph-structured characteristics of the codebase. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that integrates LLM-based reasoning along with auxiliary information obtained from graph-based exploration of the codebase. We further introduce SpIDER-Bench, a graph-structured evaluation benchmark curated from SWE-PolyBench, SWEBench-Verified and Multi-SWEBench, spanning codebases from Python, Java, JavaScript and TypeScript programming languages. Empirical results show that SpIDER consistently improves dense retrieval performance by at least 13% across programming languages and benchmarks in SpIDER-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpIDER：基于空间信息的密集嵌入检索用于软件问题定位</div>
<div class="mono" style="margin-top:8px">从大型代码库中检索与给定用户查询、错误报告或功能请求相关的代码函数、类或文件，是基于大型语言模型（LLM）的编码代理的基本挑战。代理方法通常采用BM25等稀疏检索方法或密集嵌入策略来识别语义相关的单元。虽然基于嵌入的方法在性能上远超BM25，但它们通常未考虑代码库的底层图结构特性。为了解决这一问题，我们提出了SpIDER（空间信息密集嵌入检索），这是一种增强的密集检索方法，结合了基于LLM的推理以及通过图结构探索代码库获得的辅助信息。我们进一步引入了SpIDER-Bench，这是一个从SWE-PolyBench、SWEBench-Verified和Multi-SWEBench中整理的图结构评估基准，涵盖Python、Java、JavaScript和TypeScript等编程语言的代码库。实证结果表明，SpIDER在SpIDER-Bench中的各种编程语言和基准测试中，密集检索性能至少提升了13%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The challenge of retrieving relevant code units from large codebases for software issue localization motivates the development of more effective retrieval methods. SpIDER introduces a dense retrieval approach that incorporates LLM-based reasoning and graph-structured information from the codebase to enhance semantic understanding. Experimental results on SpIDER-Bench, a benchmark derived from multiple software engineering datasets, demonstrate that SpIDER improves retrieval performance by at least 13% across different programming languages and benchmarks.</div>
<div class="mono" style="margin-top:8px">在大型代码库中检索与用户查询、错误报告或功能请求相关的代码单元是软件问题定位的核心挑战。SpIDER提出了一种结合LLM推理和代码库图结构信息的密集检索方法，以提升语义理解能力。在SpIDER-Bench基准测试中，该方法在多种编程语言和数据集上均表现出至少13%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-05T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v1">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以几何思维：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型（MLLMs）在空间推理方面取得了显著进展，越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这常常导致语义与几何的不匹配以及冗余信号。我们提出GeoThinker框架，将融合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控进行校准，该门控机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面达到了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力和显著提升的空间感知能力，包括具身指称和自动驾驶。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), which often leads to semantic-geometry misalignment and redundant signals. GeoThinker introduces an active perception framework that enables the model to selectively retrieve geometric evidence based on its internal reasoning needs. This is achieved through Spatial-Grounded Fusion at specific VLM layers, where semantic visual priors query and integrate task-relevant geometry using frame-strict cross-attention, further refined by Importance Gating. The framework achieves state-of-the-art performance on the VSI-Bench with a peak score of 72.6, and shows strong generalization and improved spatial perception in complex downstream tasks such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多模态大语言模型（MLLMs）在空间推理任务中被动几何融合的局限性，这种做法常导致语义与几何信息的不匹配以及冗余信号。GeoThinker提出了一种主动感知框架，使模型能够根据内部推理需求选择性地检索几何证据。该方法通过在特定VLM层应用空间基础融合，利用帧严格交叉注意力机制让语义视觉先验查询并整合任务相关的几何信息，进一步通过重要性门控机制优化每帧的注意力权重。实验结果显示，GeoThinker在VSI-Bench上达到了72.6的峰值得分，成为空间智能领域的最新技术，并在复杂场景如具身指称和自动驾驶中展现出强大的泛化能力和显著提升的空间感知能力。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-05T17:10:26+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cioutn.github.io/context-bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：面向编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估基准。ContextBench包含来自66个仓库、涵盖八种编程语言的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四种前沿LLM和五个编码代理。我们的结果表明，复杂的代理框架在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有的端到端基准，揭示了问题解决过程。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。数据和代码可在 https://cioutn.github.io/context-bench/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces ContextBench, a benchmark designed to evaluate how coding agents retrieve and utilize code context during issue resolution. Existing benchmarks primarily focus on final task success, offering limited understanding of the underlying retrieval process. ContextBench includes 1,136 tasks from 66 repositories across eight programming languages, each with human-annotated gold contexts. The authors implement an automated framework to track agent behavior and measure context recall, precision, and efficiency. Evaluations of four LLMs and five coding agents reveal that advanced agent structures provide only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between explored and used contexts. These findings highlight the importance of intermediate context metrics in improving the reasoning capabilities of coding agents in software tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有编码代理评估的不足，这些评估主要关注最终任务的成功率，而忽视了上下文检索的过程。ContextBench 提出一个面向过程的基准测试，包含 1,136 个跨八种编程语言的代码问题解决任务，每个任务都附有人工标注的黄金上下文。作者构建了一个自动化评估框架，用于追踪代理行为并评估问题解决过程中上下文的召回率、精确率和效率。实验结果表明，先进的代理结构在上下文检索方面仅带来微小提升，LLMs 更倾向于召回而非精确，且存在显著的探索上下文与使用上下文之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution</div>
<div class="meta-line">Authors: Philipp D. Siedler</div>
<div class="meta-line">First: 2025-05-21T22:00:20+00:00 · Latest: 2026-02-05T16:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16048v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.16048v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPhyR：基于拓扑优化的材料分布空间-物理推理基准数据集</div>
<div class="mono" style="margin-top:8px">我们引入了一个新颖的数据集，用于基于拓扑优化方法评估大型语言模型（LLM）的物理和空间推理能力。该方法用于在给定载荷和支撑条件下计算设计空间内的最优材料分布。在该数据集中，LLM会接收到诸如2D边界、施加的力和支撑等条件，并需推理出相应的最优材料分布。数据集包含多种任务，从填充部分结构中的掩码区域到预测完整的材料分布。解决这些任务需要理解力的流动以及在给定约束下的所需材料分布，而无需访问仿真工具或显式的物理模型，从而挑战模型对结构稳定性和空间组织的推理能力。我们的数据集旨在评估二维环境下的空间和物理推理能力，为传统的语言和逻辑基准提供补充视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SPhyR, a novel dataset aimed at evaluating the spatial and physical reasoning capabilities of Large Language Models (LLMs) in material distribution tasks. The dataset is based on topology optimization, where LLMs are given 2D boundary conditions, applied forces, and supports, and must infer the optimal material layout without access to simulation tools or explicit physical models. The tasks include predicting complete material distributions and filling in masked regions of partial structures, requiring an understanding of force flow and structural stability. Experimental results demonstrate that LLMs can perform these tasks with varying degrees of accuracy, highlighting the potential and limitations of their reasoning abilities in physical and spatial domains.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为SPhyR的新数据集，旨在评估大型语言模型（LLM）在材料分布任务中的空间与物理推理能力。该数据集基于拓扑优化方法，为LLM提供二维边界条件、施加的力和支撑信息，要求其在不依赖仿真工具或显式物理模型的情况下推断出最优的材料分布。实验结果表明，LLM能够进行力流分析和结构稳定性推理，但其表现因任务复杂度和输入信息的详细程度而有所不同。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非自我中心感知者：通过帧实例化将非自我中心推理与自我中心视觉先验分离</div>
<div class="mono" style="margin-top:8px">随着对空间基础任务（如视觉-语言导航/动作）的需求增加，视觉-语言模型（VLMs）中的非自我中心感知能力正受到越来越多的关注。然而，VLMs在需要显式视角转换的非自我中心空间查询上仍表现脆弱，答案依赖于目标中心的推理框架，而非观察到的相机视角。因此，我们引入了Allocentric Perceiver，这是一种无需训练的策略，通过使用现成的几何专家从一张或多张图像中恢复度量3D状态，然后实例化一个与指令语义意图对齐的查询条件非自我中心参考框架。通过确定性地将重建的几何结构转换到目标框架，并使用结构化的、基于几何的表示提示主干VLM，Allocentric Perceiver将心理旋转从隐式推理转移到显式计算。我们在多个主干家族上评估Allocentric Perceiver在空间推理基准上的表现，观察到在非自我中心任务上取得了稳定且显著的提升（约10%），同时保持了强大的自我中心性能，并超越了空间感知微调模型以及最先进的开源和专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing demand for spatially grounded tasks like Vision-Language Navigation/Action has highlighted the limitations of Vision-Language Models (VLMs) in handling allocentric spatial queries that require perspective shifts. To address this, the Allocentric Perceiver is introduced as a training-free method that recovers metric 3D states from images using geometric experts and constructs a query-conditioned allocentric reference frame aligned with the instruction&#x27;s intent. This approach transforms reconstructed geometry into the target frame and prompts the VLM with structured, geometry-based representations, enabling explicit computation of spatial reasoning rather than relying on implicit mental rotation. Experimental results show consistent and significant improvements ($\sim$10%) on allocentric tasks while preserving strong egocentric performance, outperforming both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div>
<div class="mono" style="margin-top:8px">随着对空间感知任务如视觉-语言导航/动作的需求增加，视觉语言模型（VLMs）在处理需要视角转换的分配中心空间查询时存在局限性。为此，提出了Allocentric Perceiver，这是一种无需训练的方法，通过几何专家从图像中重建度量3D状态，并根据指令语义意图实例化查询条件下的分配中心参考系。该方法通过显式计算将重建的几何信息转换为目标视角，从而减少对隐式心理旋转的依赖。在空间推理基准测试中，该方法在分配中心任务上表现出显著提升（约10%），同时保持了良好的自我中心性能，优于经过空间感知微调的模型以及最先进的开源和专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Coding Agents with Environment Interaction: A Theoretical Perspective</div>
<div class="meta-line">Authors: Nicolas Menet, Michael Hersche, Andreas Krause, Abbas Rahimi</div>
<div class="meta-line">First: 2026-02-05T13:49:42+00:00 · Latest: 2026-02-05T13:49:42+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过环境交互构建编码代理：理论视角</div>
<div class="mono" style="margin-top:8px">编码代理在测试驱动的软件开发中被越来越多地使用，但其环境交互策略背后的理论机制仍被较少探索。我们为两种主流范式提供了概率框架：使用执行环境进行生成后的代码选择，以及基于环境反馈的代码生成。首先，我们将几种已建立的代码选择启发式方法形式化为环境感知的代码正确性估计器。我们从理论上证明，基于模糊功能相似性的估计器引入了归纳偏差，并在信噪比方面严格优于基于功能等价性的估计器。其次，我们将回提示（backprompting）框架视为上下文中的汤普森采样近似。我们推导出一种新的遗憾界（regret bound），用于具有不可观测成分的奖励函数，从理论上解释了为什么回提示的有效性受到非正式任务描述模糊性（不可约遗憾）的限制。通过三个最先进的开源权重模型，我们在BigCodeBenchHard、LeetCodeDataset和QiskitHumanEvalSim数据集上验证了这些发现。我们的形式化还提出了如何有效改进任务描述的方法，从而创建了一个新的基准QiskitHumanEvalSimX。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the lack of theoretical understanding of environment interaction strategies in coding agents used for test-driven software development. It introduces a probabilistic framework to analyze two key paradigms: code selection based on execution environment feedback and code generation conditioned on environment interaction. The research formally represents established selection heuristics as environment-aware correctness estimators and demonstrates that fuzzy functional similarity-based estimators provide a stronger inductive bias and better signal-to-noise ratio than equivalence-based ones. Additionally, it frames backprompting as an in-context approximation of Thompson sampling and derives a regret bound for reward functions with unobservable components, explaining the limitations of backprompting due to task description ambiguity. Experimental results on three state-of-the-art models across multiple benchmarks support these theoretical insights and suggest improvements for task descriptions, resulting in a new benchmark, QiskitHumanEvalSimX.</div>
<div class="mono" style="margin-top:8px">该研究旨在弥补编码代理在测试驱动软件开发中环境交互策略的理论理解不足。它提出了一种概率框架，用于分析两种主要范式：基于执行环境的代码选择和基于环境反馈的代码生成。研究将已有的选择启发式方法形式化为环境感知的代码正确性估计器，并证明基于模糊功能相似性的估计器具有更强的归纳偏差，在信噪比方面优于基于功能等价性的估计器。此外，将回提示（backprompting）视为上下文中的汤普森采样近似，并推导了具有不可观测成分的奖励函数的新型遗憾界，解释了回提示效果受限于任务描述模糊性（不可消除的遗憾）的原因。实验结果使用三种最先进的模型在BigCodeBenchHard、LeetCodeDataset和QiskitHumanEvalSim数据集上验证了这些理论发现，并提出了改进任务描述的方法，从而创建了一个新的基准QiskitHumanEvalSimX。</div>
</details>
</div>
<div class="card">
<div class="title">TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?</div>
<div class="meta-line">Authors: Yikun Zong, Cheston Tan</div>
<div class="meta-line">First: 2026-02-05T11:49:30+00:00 · Latest: 2026-02-05T11:49:30+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05570v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramSR：视觉-语言模型能否在连续几何空间中进行推理？</div>
<div class="mono" style="margin-top:8px">人类通过心理旋转、迭代优化和视觉反馈等认知过程，在空间推理任务（如七巧板拼图）中表现出色。受人类通过试错、观察和修正来解决七巧板拼图的启发，我们设计了一个框架，以建模这些人类认知机制。然而，对五种代表性视觉-语言模型（VLMs）的全面实验揭示了在连续几何推理中的系统性失败：单块任务的平均IoU仅为0.41，两块组合任务则降至0.23，远低于人类表现，儿童能够成功完成七巧板任务。本文解决了一个自改进AI的基本挑战：模型能否在测试时通过迭代优化预测结果，而无需参数更新？我们引入了一种测试时自优化框架，结合上下文学习（ICL）与奖励引导的反馈循环，灵感来源于人类认知过程。我们的无训练验证-优化智能体应用递归优化循环，基于几何一致性反馈迭代优化预测结果，在中等三角形案例中实现了IoU从0.63到0.932的显著提升。这表明，通过ICL和奖励循环引入人类启发的迭代优化机制，可以显著增强视觉-语言模型的几何推理能力，推动自改进AI在连续空间领域从理论走向实践。我们的工作可通过此匿名链接获取：https://anonymous.4open.science/r/TangramVLM-F582/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the ability of Vision-Language Models (VLMs) to perform continuous geometric reasoning, inspired by human spatial reasoning skills demonstrated in tasks like Tangram puzzle assembly. The authors propose a test-time self-refinement framework that integrates in-context learning with reward-guided feedback loops to mimic human cognitive processes such as mental rotation and iterative refinement. Experimental results show that their training-free verifier-refiner agent significantly improves geometric reasoning performance, achieving an IoU of 0.932 on medium-triangle cases from an initial 0.63, without retraining the model.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在连续几何推理中的表现，受人类空间推理过程（如心理旋转和迭代优化）的启发。作者提出了一种测试时自我优化框架，结合上下文学习与奖励引导的反馈循环，使模型能够在不进行再训练的情况下迭代改进预测结果。实验结果显示，该方法在中等三角形任务中将平均IoU从0.63提升至0.932，证明了引入人类启发的优化机制能够显著增强VLMs的几何推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Imagine a City: CityGenAgent for Procedural 3D City Generation</div>
<div class="meta-line">Authors: Zishan Liu, Zecong Tang, RuoCheng Wu, Xinzhe Zheng, Jingyu Hu, Ka-Hei Hui, Haoran Xie, Bo Dai, Zhengzhe Liu</div>
<div class="meta-line">First: 2026-02-05T06:36:03+00:00 · Latest: 2026-02-05T06:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05362v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05362v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models&#x27; generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>想象一座城市：用于程序化3D城市生成的CityGenAgent</div>
<div class="mono" style="margin-top:8px">交互式3D城市的自动化生成是自动驾驶、虚拟现实和具身智能等广泛应用中的关键挑战。尽管生成模型和程序化技术的最新进展提高了城市生成的逼真度，但现有方法在高保真资产创建、可控性和操作性方面仍存在困难。在本工作中，我们引入了CityGenAgent，这是一个由自然语言驱动的框架，用于高质量3D城市的分层程序化生成。我们的方法将城市生成分解为两个可解释的组件：Block Program和Building Program。为确保结构正确性和语义对齐，我们采用了一种两阶段学习策略：(1) 监督微调（SFT）。我们训练BlockGen和BuildingGen生成符合模式约束的有效程序，包括无自相交的多边形和完整的字段；(2) 强化学习（RL）。我们设计了空间对齐奖励（Spatial Alignment Reward）以增强空间推理能力，并设计了视觉一致性奖励（Visual Consistency Reward）以弥合文本描述与视觉模态之间的差距。得益于程序和模型的泛化能力，CityGenAgent支持自然语言编辑和操作。全面的评估表明，与现有方法相比，CityGenAgent在语义对齐、视觉质量和可控性方面具有显著优势，为可扩展的3D城市生成奠定了坚实的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to address the limitations of current methods in generating high-fidelity, controllable, and semantically aligned 3D cities. The authors propose CityGenAgent, a natural language-driven framework that enables hierarchical procedural generation of 3D urban environments. The method divides city generation into two components: Block Program and Building Program, and employs a two-stage learning strategy, starting with supervised fine-tuning to ensure schema compliance and then using reinforcement learning with spatial alignment and visual consistency rewards to improve reasoning and alignment. Experimental results show that CityGenAgent achieves better semantic alignment, visual quality, and controllability than existing approaches, making it a promising solution for scalable 3D city generation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决高保真、可控且语义对齐的3D城市生成问题，以支持自动驾驶和虚拟现实等应用。CityGenAgent提出了一种基于自然语言驱动的框架，将城市生成分解为两个可解释的层级组件：Block Program和Building Program。该框架采用两阶段学习策略，首先通过监督微调确保生成程序符合结构约束，随后利用强化学习结合自定义奖励机制提升空间推理能力和视觉一致性。实验结果表明，CityGenAgent在语义对齐、视觉质量和可控性方面优于现有方法，为可扩展的3D城市生成提供了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">SVRepair: Structured Visual Reasoning for Automated Program Repair</div>
<div class="meta-line">Authors: Xiaoxuan Tang, Jincheng Wang, Liwei Luo, Jingxuan Xu, Sheng Zhou, Dajun Chen, Wei Jiang, Yong Li</div>
<div class="meta-line">First: 2026-02-05T06:26:46+00:00 · Latest: 2026-02-05T06:26:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06090v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVRepair：用于自动化程序修复的结构化视觉推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）最近在自动化程序修复（APR）中展现出强大的潜力，但大多数现有方法仍为单模态，未能有效利用截图、控制流图等视觉产物中包含的丰富诊断信号。在实践中，许多 bug 报告通过视觉方式传达关键信息（例如布局破坏或缺失控件），但直接使用这些密集的视觉输入常常导致上下文丢失和噪声，使得 MLLMs 难以将视觉观察映射到精确的故障定位和可执行补丁。为弥合这一语义鸿沟，我们提出了 \textbf{SVRepair}，一个具有结构化视觉表示的多模态 APR 框架。SVRepair 首先对视觉-语言模型 \textbf{结构化视觉表示（SVR）} 进行微调，将其异构的视觉产物统一转换为一个 \emph{语义场景图}，捕捉 GUI 元素及其结构关系（如层次结构），为后续修复提供标准化、与代码相关的上下文。基于该图，SVRepair 驱动编码代理进行故障定位和补丁合成，并进一步引入一种迭代的视觉产物分割策略，逐步将输入聚焦到与 bug 相关的区域，以抑制无关上下文并减少幻觉。在多个基准测试上的广泛实验表明，SVRepair 表现出最先进的性能：在 SWE-Bench M 上达到 \textbf{36.47\%} 的准确率，在 MMCode 上达到 \textbf{38.02\%}，在 CodeVision 上达到 \textbf{95.12\%}，验证了 SVRepair 在多模态程序修复中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SVRepair addresses the limitations of existing unimodal approaches in Automated Program Repair (APR) by integrating visual information into the repair process. It introduces a structured visual representation method that transforms diverse visual artifacts into a semantic scene graph, capturing GUI elements and their hierarchical relationships. This graph provides a normalized and code-relevant context for fault localization and patch synthesis. The framework also employs an iterative visual-artifact segmentation strategy to focus on bug-related regions, reducing noise and hallucinations. Experimental results on multiple benchmarks show that SVRepair achieves state-of-the-art performance with 36.47% accuracy on SWE-Bench M, 38.02% on MMCode, and 95.12% on CodeVision.</div>
<div class="mono" style="margin-top:8px">SVRepair 解决了现有单模态方法在自动程序修复中的不足，通过整合截图和控制流图等视觉信息来提升修复效果。它提出了一种结构化视觉表示方法，利用视觉语言模型将异构的视觉输入转换为语义场景图，该图捕捉了 GUI 元素及其层次关系，为后续修复任务提供标准化且与代码相关的上下文。该框架进一步引入了迭代的视觉工件分割策略，以聚焦于与 bug 相关的区域，减少无关信息和幻觉。在多个基准测试中，SVRepair 表现出最先进的性能，分别在 SWE-Bench M、MMCode 和 CodeVision 上达到 36.47%、38.02% 和 95.12% 的准确率，验证了其在多模态程序修复中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking</div>
<div class="meta-line">Authors: Bishoy Galoaa, Xiangyu Bai, Utsav Nandi, Sai Siddhartha Vivek Dhir Rangoju, Somaieh Amraee, Sarah Ostadabbas</div>
<div class="meta-line">First: 2026-02-04T20:44:16+00:00 · Latest: 2026-02-04T20:44:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniTrack：用于多目标跟踪的可微分图表示学习</div>
<div class="mono" style="margin-top:8px">我们提出了UniTrack，这是一种即插即用的图论损失函数，旨在通过统一的可微分学习直接优化跟踪特定目标，从而显著提升多目标跟踪（MOT）性能。与之前需要重新设计跟踪架构的图基MOT方法不同，UniTrack提供了一个通用的训练目标，将检测精度、身份保持和时空一致性整合到一个端到端可训练的损失函数中，无需架构修改即可无缝集成到现有MOT系统中。通过可微分图表示学习，UniTrack使网络能够学习跨帧的运动连续性和身份关系的整体表示。我们在多种跟踪模型和多个具有挑战性的基准上验证了UniTrack，展示了在所有测试架构和数据集（包括Trackformer、MOTR、FairMOT、ByteTrack、GTR和MOTE）中的一致性能提升。广泛评估表明，在具有挑战性的基准上，身份切换减少了高达53%，IDF1提高了12%，其中GTR在SportsMOT上实现了9.7%的MOTA性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniTrack introduces a plug-and-play graph-theoretic loss function aimed at improving multi-object tracking performance by directly optimizing tracking-specific objectives through unified differentiable learning. It integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss, allowing for seamless integration with existing MOT systems without altering their architecture. The method leverages differentiable graph representation learning to enable networks to capture motion continuity and identity relationships across frames. Experimental results across various tracking models and benchmarks show significant improvements, including up to 53% reduction in identity switches and 12% IDF1 gains, with GTR achieving a 9.7% increase in MOTA on SportsMOT.</div>
<div class="mono" style="margin-top:8px">UniTrack的动机是通过统一的可微学习方法直接优化多目标跟踪中的特定目标，从而提升跟踪性能。该方法提出了一种即插即用的图论损失函数，将检测精度、身份保持和时空一致性整合为一个端到端可训练的目标。这使得UniTrack能够无缝集成到现有多目标跟踪系统中，无需修改其架构。通过可微图表示学习，网络可以学习跨帧的运动连续性和身份关系的整体表示。实验结果表明，UniTrack在多种跟踪模型和基准测试中均表现出色，实现了高达53%的身份切换减少和12%的IDF1提升，其中在SportsMOT数据集上，GTR模型的MOTA指标提升了9.7%。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：通过视觉语言模型将任务规划接地为空间感知的自中心动作以实现人形机器人应用</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人本质上具有挑战性，因为这需要在部分信息观察和动态变化环境中紧密集成感知、移动和操作。此外，还需要在不同类型子任务之间稳健地进行转换。为了解决这些挑战，我们提出了一种新的任务——EgoActing，该任务要求将高层指令直接接地为各种精确且具有空间感知的人形机器人动作。我们进一步通过引入EgoActor，一个统一且可扩展的视觉语言模型（VLM），来实例化这一任务。EgoActor能够预测移动原语（如行走、转向、侧移、高度变化）、头部运动、操作指令和人机交互，从而实时协调感知与执行。我们利用来自现实世界演示的广泛监督，结合空间推理问答和模拟环境演示，使EgoActor能够做出稳健且上下文感知的决策，并在1秒内进行流畅的动作推理，适用于8B和4B参数模型。在模拟和现实环境中的大量评估表明，EgoActor能够有效连接抽象的任务规划与具体的运动执行，并在多样任务和未见过的环境中进行泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The deployment of humanoid robots in real-world environments is challenging due to the need for integrating perception, locomotion, and manipulation under partial observations and dynamic conditions. To address this, the paper introduces EgoActing, a task that directly grounds high-level instructions into precise, spatially aware actions. The proposed EgoActor is a unified vision-language model capable of predicting various locomotion primitives, head movements, manipulation commands, and human-robot interactions. It is trained using real-world demonstrations, spatial reasoning tasks, and simulated data, allowing it to make context-aware decisions and perform fast action inference. Experimental results show that EgoActor effectively connects abstract task planning with concrete motor execution and generalizes well across different tasks and environments.</div>
<div class="mono" style="margin-top:8px">在真实环境中部署人形机器人具有挑战性，因为需要在部分信息观测和动态变化的环境下整合感知、移动和操作。为此，本文提出了EgoActing任务，该任务直接将高层指令映射到具有空间感知的人形动作。论文引入了EgoActor，这是一个统一的视觉-语言模型，能够预测多种移动原语、头部运动、操作指令以及人机交互。通过使用真实世界演示、空间推理问答和模拟环境数据进行广泛监督训练，EgoActor能够在不同任务和未知环境中做出上下文感知的决策并快速推理动作。实验结果表明，EgoActor有效连接了任务规划与运动执行，展现出良好的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-02-04T12:38:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v2">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即超越二维感知，解释几何、视角和空间关系的能力。尽管近期的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入Think3D，一个使VLM代理能够以三维空间进行思考的框架。通过利用3D重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换，主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使模型能够选择信息量大的视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的、工具增强的空间探索是实现多模态代理中更灵活且类似人类的三维推理的可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of Think3D is to enhance spatial reasoning capabilities in vision large models (VLMs), which are inherently limited to 2D perception. The framework introduces a 3D thinking mechanism by integrating 3D reconstruction models to generate point clouds and camera poses from images or videos, enabling agents to interactively manipulate 3D space through camera-based operations and view switching. Experimental results show that Think3D significantly improves spatial reasoning performance without additional training, achieving average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. Furthermore, reinforcement learning policies enhance smaller models&#x27; ability to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">Think3D的研究动机是提升视觉大模型（VLMs）的空间推理能力，因为这些模型受限于2D感知，在真正的3D理解上存在困难。该框架通过集成3D重建模型，从图像或视频中生成点云和相机姿态，使VLM代理能够通过基于相机的操作和视角切换来交互式地操控3D空间。实验结果显示，在不进行额外训练的情况下，Think3D使GPT-4.1和Gemini 2.5 Pro等先进模型在BLINK Multi-view和MindCube上的空间推理性能分别提升了7.8%，在VSI-Bench上提升了4.7%。此外，对于空间探索能力较弱的小模型，结合强化学习策略可显著提升其性能，使工具使用带来的收益从0.7%提升至6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</div>
<div class="meta-line">Authors: Mingcong Lei, Honghao Cai, Yuyuan Yang, Yimou Wu, Jinke Ren, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Shaohan Jiang, Ge Wang, Junyuan Tan, Zhenglin Wan, Zheng Li, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han</div>
<div class="meta-line">First: 2025-08-02T15:39:42+00:00 · Latest: 2026-02-04T12:10:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01415v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.01415v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied intelligence aims to enable robots to learn, reason, and generalize robustly across complex real-world environments. However, existing approaches often struggle with partial observability, fragmented spatial reasoning, and inefficient integration of heterogeneous memories, limiting their capacity for long-horizon adaptation. To address this, we introduce RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory within a parallelized architecture for efficient long-horizon planning and interactive learning. Its core innovations are a dynamic spatial knowledge graph for scalable, consistent memory updates and a closed-loop planner with a critic module for adaptive decision-making. Extensive experiments on EmbodiedBench show that RoboMemory, instantiated with Qwen2.5-VL-72B-Ins, improves the average success rate by 26.5% over its strong baseline and even surpasses the closed-source SOTA, Claude-3.5-Sonnet. Real-world trials further confirm its capability for cumulative learning, with performance consistently improving over repeated tasks. Our results position RoboMemory as a scalable foundation for memory-augmented embodied agents, bridging insights from cognitive neuroscience with practical robotic autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMemory：一种受大脑启发的多记忆智能体框架，用于物理具身系统中的交互式环境学习</div>
<div class="mono" style="margin-top:8px">具身智能旨在使机器人能够在复杂的真实环境中稳健地学习、推理和泛化。然而，现有方法常面临部分可观测性、空间推理碎片化以及异质记忆整合效率低的问题，限制了其长期适应能力。为解决这些问题，我们引入了RoboMemory，这是一种受大脑启发的框架，通过并行化架构将空间、时间、事件和语义记忆统一起来，以实现高效的长期规划和交互式学习。其核心创新包括用于可扩展且一致记忆更新的动态空间知识图谱，以及带有批评模块的闭环规划器，以实现自适应决策。在EmbodiedBench上的大量实验表明，RoboMemory在Qwen2.5-VL-72B-Ins实例化后，其平均成功率比其强基线提高了26.5%，甚至超过了闭源的SOTA模型Claude-3.5-Sonnet。现实世界测试进一步验证了其累积学习能力，性能在重复任务中持续提升。我们的结果表明，RoboMemory为记忆增强的具身智能体提供了一个可扩展的基础，将认知神经科学的见解与实际的机器人自主性相结合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboMemory is designed to enhance embodied intelligence by addressing challenges in partial observability, spatial reasoning, and memory integration in robots. The framework integrates spatial, temporal, episodic, and semantic memory into a parallelized architecture, featuring a dynamic spatial knowledge graph for scalable memory updates and a closed-loop planner with a critic module for adaptive decision-making. Experimental results on EmbodiedBench demonstrate that RoboMemory, when implemented with Qwen2.5-VL-72B-Ins, achieves a 26.5% improvement in average success rate compared to a strong baseline and outperforms the closed-source state-of-the-art model, Claude-3.5-Sonnet. Real-world trials further validate its ability to support cumulative learning through repeated task execution.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改进记忆整合和空间推理能力，提升机器人在复杂现实环境中的适应性和学习能力。RoboMemory是一个受大脑启发的多记忆代理框架，将空间、时间、事件和语义记忆统一在一个并行架构中，以支持高效的长时规划和交互式学习。该框架引入了动态空间知识图，用于可扩展且一致的记忆更新，并采用带有批评模块的闭环规划器实现自适应决策。在EmbodiedBench上的实验结果显示，使用Qwen2.5-VL-72B-Ins实现的RoboMemory相比强基线模型平均成功率提高了26.5%，并超越了闭源的最先进模型Claude-3.5-Sonnet。实际环境测试进一步验证了其累积学习的能力，性能在重复任务中持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">MapCoder-Lite: Distilling Multi-Agent Coding into a Single Small LLM</div>
<div class="meta-line">Authors: Woongkyu Lee, Junhee Cho, Jungwook Choi</div>
<div class="meta-line">First: 2025-09-22T08:19:11+00:00 · Latest: 2026-02-04T07:25:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17489v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17489v2">PDF</a> · <a href="https://github.com/aiha-lab/MapCoder-Lite">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale (&gt;30B) models or collapse when downsized to small open-source models. We present MapCoder-Lite, a framework for distilling the complex reasoning of large, multi-agent coding systems into a single 7B model. Our contribution is a novel, three-pillar methodology that synergistically generates, refines, and encodes multi-agent knowledge: (i) pass-based trajectory distillation from strong LLMs fixes format fragility in retrieval and reduces failures in debugging, (ii) supervisor-guided correction with global feedback strengthens planning and coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests shows that MapCoder-Lite more than doubles xCodeEval accuracy (from 13.2% to 28.3%), eliminates all format failures, while reducing GPU memory and token-generation time by 4x compared to a 32B model. It also achieves over 10% gains on simpler coding benchmarks, demonstrating broad improvements beyond competitive programming. These results demonstrate that careful agent-wise fine-tuning unleashes high-quality multi-agent coding on a small language model. Our code is publicly available at https://github.com/aiha-lab/MapCoder-Lite.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapCoder-Lite：将多智能体编程蒸馏到单一小型大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）已从单功能任务扩展到具有竞争力的编程问题，但现有的多智能体解决方案要么依赖昂贵的大型（&gt;30B）模型，要么在缩小到小型开源模型时失效。我们提出了MapCoder-Lite，一个将大型多智能体编程系统的复杂推理蒸馏到单一7B模型的框架。我们的贡献是一种新颖的三支柱方法，协同生成、优化和编码多智能体知识：(i) 基于传递的轨迹蒸馏从强LLM中修复检索中的格式脆弱性并减少调试失败；(ii) 带有全局反馈的监督引导修正增强了规划和编码智能体；(iii) 智能体级LoRA微调实现了内存高效的专门化。在xCodeEval、APPS和CodeContests上的全面评估表明，MapCoder-Lite将xCodeEval的准确率提升超过一倍（从13.2%提升至28.3%），消除了所有格式错误，同时相比32B模型减少了4倍的GPU内存和生成标记的时间。它还在更简单的编程基准测试中实现了超过10%的提升，展示了在编程竞赛之外的广泛改进。这些结果表明，仔细的智能体级微调可以在小型语言模型上释放高质量的多智能体编程能力。我们的代码可在https://github.com/aiha-lab/MapCoder-Lite上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing multi-agent coding systems, which either require expensive large-scale models or fail when scaled down. MapCoder-Lite proposes a framework to distill the complex reasoning of such systems into a single 7B parameter model. It employs a three-pillar methodology: pass-based trajectory distillation to enhance retrieval and debugging, supervisor-guided correction with global feedback to improve planning and coding, and agent-wise LoRA fine-tuning for memory-efficient specialization. Experimental results on xCodeEval, APPS, and CodeContests show that MapCoder-Lite significantly improves accuracy, eliminating format failures and reducing GPU memory and token generation time by 4x compared to a 32B model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有多智能体编程系统存在的问题，这些系统要么需要昂贵的大规模模型，要么在缩小规模后无法正常运行。MapCoder-Lite提出了一种框架，将这些系统的复杂推理过程蒸馏到一个7B参数的模型中。方法包括三个关键部分：基于传递的轨迹蒸馏以提升检索和调试能力，监督指导下的全局反馈修正以增强规划和编码智能体，以及基于智能体的LoRA微调以实现内存高效的专门化。在xCodeEval、APPS和CodeContests等数据集上的实验结果显示，MapCoder-Lite在xCodeEval上准确率超过28%，消除了所有格式错误，并将GPU内存和生成令牌时间分别减少了4倍，相较于32B模型表现显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization</div>
<div class="meta-line">Authors: Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing</div>
<div class="meta-line">First: 2025-09-15T20:36:19+00:00 · Latest: 2026-02-04T05:16:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12434v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.12434v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues. A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs. While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS. Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents. To bridge this gap, we introduce EntroPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting. EntroPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses. We validate EntroPO by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches. On the SWEBENCH leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with EntroPO ranks 1st on SWEBENCH-LITE and 4th on SWEBENCH-VERIFIED on the open-weight leaderboard, surpassed only by models with over 10x more parameters(e.g., &gt;$350B).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过熵增强的多轮偏好优化构建编码代理</div>
<div class="mono" style="margin-top:8px">软件工程为大型语言模型（LLMs）提出了复杂且多步骤的挑战，需要在大型代码库上进行推理并协调使用工具。这些任务的难度在诸如SWE-bench等基准测试中得到了体现，其中当前的LLMs仍然难以解决现实问题。一种有前景的方法是测试时扩展（TTS），但其效果高度依赖于模型输出的多样性。虽然标准对齐方法如直接偏好优化（DPO）和卡尼曼-特弗斯基优化（KTO）在将模型输出与人类偏好对齐方面有效，但这一过程可能会降低多样性，从而限制TTS的效果。此外，现有的偏好优化算法通常设计用于单轮任务，无法充分应对交互式编码代理所需的多轮推理和工具集成的复杂性。为弥合这一差距，我们引入了EntroPO，这是一种熵增强框架，将现有的偏好优化算法适应到多轮、工具辅助的环境中。EntroPO通过显式保留策略熵来增强偏好目标，并将学习过程推广到多轮交互的优化，而非单轮响应。我们通过微调来自不同家族和规模（最高达106B参数）的多样化模型来验证EntroPO。为了最大化TTS带来的性能提升，我们进一步提出了一种混合最佳轨迹选择方案，结合了学习的验证器模型和无模型方法。在SWEBENCH排行榜上，我们的方法在开放权重模型中建立了新的最先进成果。使用EntroPO训练的30B参数模型在开放权重排行榜上分别在SWEBENCH-LITE和SWEBENCH-VERIFIED中排名第一和第四，仅被参数量超过其10倍的模型（例如，&gt;350B参数）超越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of improving Large Language Models&#x27; (LLMs) performance in complex software engineering tasks that require multi-step reasoning and tool coordination. It introduces EntroPO, an entropy-enhanced framework that modifies existing preference optimization methods to support multi-turn interactions and maintain output diversity, which is crucial for effective test-time scaling. Experimental results show that EntroPO achieves state-of-the-art performance on the SWEBENCH leaderboard, with a 30B parameter model ranking 1st on SWEBENCH-LITE and 4th on SWEBENCH-VERIFIED among open-weight models, outperformed only by much larger models.</div>
<div class="mono" style="margin-top:8px">本文针对当前大语言模型（LLMs）在处理复杂多步骤编码任务时的不足，提出了一种基于熵增强的偏好优化框架EntroPO。研究动机源于LLMs在现实软件工程场景（如SWE-bench）中面对大规模代码库推理和工具协调时的困难。EntroPO通过保留策略熵并优化多轮交互，改进了传统方法如DPO和KTO在多轮推理和工具使用中的表现。实验结果表明，使用EntroPO训练的30B参数模型在SWEBENCH-LITE中排名第一，在SWEBENCH-VERIFIED中排名第四，优于许多较小模型，仅被参数量超过其10倍以上的模型超越。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-02-04T04:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot;-a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal audio scene analysis (ASA). To bridge this gap, we first introduce a hierarchical framework for audio scene analysis. Guided by this framework, we introduce a system that enables large audio-language models (LALMs) to understand and reason about the complex acoustic world.
  Our system endows LALMs with universal spatial understanding through four key innovations: (1) A scalable simulation pipeline that synthesizes high-quality First-Order-Ambisonics(FOA) data; (2) A unified model framework that integrates universal spatial encoding with a dense hybrid projection mechanism to bridge the modality gap; (3) A progressive training curriculum that evolves from representation alignment to reinforcement learning-based reasoning; and (4) A comprehensive benchmark for audio scene analysis (ASA) designed to rigorously evaluate atomic perception, relational integration, and cognitive reasoning capabilities, on which our model demonstrates comparatively strong capability for spatial understanding. Our work provides a clear pathway for leveraging the powerful reasoning abilities of LALMs towards holistic ASA, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：在大型音频语言模型中实现空间理解</div>
<div class="mono" style="margin-top:8px">现有的大型音频语言模型将世界视为&quot;单声道&quot;，即忽略关键的空间维度（&quot;在哪里&quot;）的单一音频流，无法进行通用音频场景分析（ASA）。为弥合这一差距，我们首先提出了一种用于音频场景分析的分层框架。基于该框架，我们引入了一种系统，使大型音频语言模型（LALMs）能够理解和推理复杂的声学世界。
我们的系统通过四项关键创新赋予LALMs普遍的空间理解能力：(1) 一种可扩展的模拟流水线，合成高质量的首阶全向声学（FOA）数据；(2) 一种统一的模型框架，将普遍空间编码与密集混合投影机制结合，以弥合模态差距；(3) 一种渐进式的训练课程，从表示对齐逐步发展到基于强化学习的推理；(4) 一个全面的音频场景分析（ASA）基准测试，旨在严格评估基本感知、关系整合和认知推理能力，我们的模型在该基准上表现出较强的空间理解能力。我们的工作为利用LALMs强大的推理能力实现全面的ASA提供了清晰的路径，从&quot;单声道&quot;语义识别推进到空间智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing large audio-language models (LALMs) that perceive the world as a single audio stream, neglecting the spatial dimension essential for universal audio scene analysis. The authors propose a hierarchical framework and a system that enhances spatial understanding through four innovations: a scalable FOA data simulation pipeline, a unified model integrating spatial encoding with a dense hybrid projection mechanism, a progressive training curriculum from representation alignment to reinforcement learning, and a comprehensive ASA benchmark. The model demonstrates strong performance in spatial perception, relational integration, and cognitive reasoning, marking a significant step toward spatial intelligence in audio-language models.</div>
<div class="mono" style="margin-top:8px">本文针对现有大型音频语言模型（LALMs）将世界视为单一音频流、忽略空间维度这一局限，提出了一种分层框架和系统，以增强LALMs对音频场景的全面理解。该系统通过四项创新实现：可扩展的FOA数据模拟流水线、整合空间编码与混合投影机制的统一模型框架、从表示对齐到基于强化学习的推理的渐进式训练课程，以及一个全面的音频场景分析（ASA）基准测试。实验结果表明，该模型在空间感知、关系整合和认知推理方面表现出较强的性能，标志着从单一语义识别向空间智能的迈进。</div>
</details>
</div>
<div class="card">
<div class="title">CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning</div>
<div class="meta-line">Authors: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le</div>
<div class="meta-line">First: 2025-05-31T23:32:01+00:00 · Latest: 2026-02-03T23:34:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00750v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00750v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codesense-bench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about code semantics is essential for enhancing code LLMs&#x27; abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limits models&#x27; capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CodeSense：面向代码语义推理的现实基准和数据集</div>
<div class="mono" style="margin-top:8px">理解并推理代码语义对于增强代码大语言模型（LLMs）解决现实软件工程（SE）任务的能力至关重要。尽管已有多个代码推理基准，但大多数依赖合成数据集或教育编程问题，且主要关注输入/输出预测等粗粒度推理任务，这限制了它们在实际软件工程场景中评估LLMs的有效性。为弥合这一差距，我们提出了CodeSense，这是首个提供一系列与现实代码软件工程相关的细粒度代码推理任务的基准。我们从现实仓库中收集了Python、C和Java软件项目，执行了这些仓库中的测试，收集了其执行轨迹，并构建了一个用于细粒度语义推理任务的基准数据集。随后，我们在最先进的LLMs上进行了全面评估。我们的结果表明，模型在处理细粒度推理任务时存在明显的性能差距。尽管链式思维和上下文学习等提示技术有所帮助，但LLMs中缺乏代码语义从根本上限制了其代码推理能力。除了数据集、基准和评估，我们的工作还产生了一个执行追踪框架和工具集，使得收集细粒度软件工程推理任务的基准数据变得容易，为未来基准构建和模型微调提供了坚实的基础。我们的代码和数据位于https://codesense-bench.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the ability of code LLMs to perform real-world software engineering tasks by addressing the limitations of existing benchmarks that focus on coarse-grained reasoning. CodeSense introduces a new benchmark with fine-grained code reasoning tasks based on real-world code repositories, including Python, C, and Java projects. The authors collected execution traces from these projects to create a ground truth dataset, which was then used to evaluate state-of-the-art LLMs. The results indicate a significant performance gap in handling fine-grained reasoning tasks, even with prompting techniques like chain-of-thought and in-context learning, highlighting the fundamental lack of code semantics in current models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升代码大语言模型（LLMs）在实际软件工程任务中的表现，因为现有基准主要集中在粗粒度的代码推理任务上，无法有效评估模型在现实场景中的能力。CodeSense提出了首个包含真实世界代码细粒度推理任务的基准和数据集，涵盖Python、C和Java项目。该数据集通过执行真实代码仓库中的代码并收集执行轨迹构建而成，以提供细粒度语义推理的基准。对当前最先进的LLMs进行全面评估后发现，模型在处理细粒度推理任务时存在明显性能差距，表明其缺乏对代码语义的深入理解。尽管链式推理和上下文学习等提示技术有所改善，但模型本身的代码语义理解能力仍是关键限制因素。此外，本研究还提供了一个执行轨迹追踪框架和工具集，便于未来构建基准和进行模型训练。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents</div>
<div class="meta-line">Authors: Bhanu Prakash Vangala, Ali Adibifar, Ashish Gehani, Tanu Malik</div>
<div class="meta-line">First: 2025-12-26T21:17:22+00:00 · Latest: 2026-02-03T22:46:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22387v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI生成的代码不可复现（尚且如此）：基于大语言模型编码代理的依赖缺口实证研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为编码代理的兴起承诺加速软件开发，但其对生成代码可复现性的影响仍鲜有研究。本文通过实证研究探讨LLM生成的代码是否可以在仅使用操作系统包和模型指定依赖的干净环境中成功执行。我们评估了三个最先进的LLM编码代理（Claude Code、OpenAI Codex和Gemini）在Python、JavaScript和Java三种语言中，由100个标准化提示生成的300个项目。我们引入了一个三层依赖框架（区分声明依赖、工作依赖和运行时依赖），以量化代码执行的可复现性。研究结果表明，仅有68.3%的项目能够直接运行，不同语言之间存在显著差异（Python为89.2%，Java为44.0%）。我们还发现，从声明依赖到实际运行时依赖的平均扩展率为13.5倍，揭示了大量隐藏依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reproducibility of code generated by large language models (LLMs) when used as coding agents. The research is motivated by the growing use of LLMs in software development and the need to understand their impact on code execution reliability. The authors evaluate three advanced LLM-based coding agents—Claude Code, OpenAI Codex, and Gemini—on 300 projects created from 100 standardized prompts in Python, JavaScript, and Java. They introduce a three-layer dependency framework to categorize claimed, working, and runtime dependencies. The main findings reveal that only 68.3% of the generated projects can be executed successfully in a clean environment, with notable differences across programming languages. Additionally, the average number of runtime dependencies is 13.5 times higher than the declared ones, highlighting the presence of significant hidden dependencies.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）作为编码代理时生成代码的可重复性问题。研究动机源于LLMs在软件开发中日益广泛的应用，以及对其生成代码执行可靠性的影响缺乏了解。研究人员评估了三种LLM编码代理——Claude Code、OpenAI Codex和Gemini——在Python、JavaScript和Java三种语言中，基于100个标准化提示生成的300个项目。他们引入了一个三层依赖框架，用于衡量声明依赖与实际运行依赖之间的差异。结果显示，仅有68.3%的项目能够在干净环境中成功执行，不同编程语言之间存在显著差异。此外，实际运行依赖的数量平均比声明依赖多出13.5倍，揭示了大量隐藏依赖的存在。</div>
</details>
</div>
<div class="card">
<div class="title">FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation</div>
<div class="meta-line">Authors: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Mingjie Zhan, Hongsheng Li</div>
<div class="meta-line">First: 2026-02-03T18:01:34+00:00 · Latest: 2026-02-03T18:01:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03798v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03798v1">PDF</a> · <a href="https://github.com/mnluzimu/FullStack-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FullStack-Agent：通过面向开发的测试和仓库反向翻译增强代理式全栈网页编码</div>
<div class="mono" style="margin-top:8px">帮助非专家用户开发复杂的交互式网站已成为LLM驱动代码代理的热门任务。然而，现有的代码代理往往仅生成前端网页，用华丽的视觉效果掩盖了实际全栈数据处理和存储能力的不足。值得注意的是，构建生产级的全栈网页应用远比仅生成前端网页更具挑战性，需要对数据流进行细致控制，全面理解不断更新的包和依赖关系，并准确定位代码库中模糊的错误。为了解决这些困难，我们引入了FullStack-Agent，这是一个统一的代理系统，用于全栈代理式编码，包含三个部分：(1) FullStack-Dev，一个具备强大规划、代码编辑、代码库导航和错误定位能力的多代理框架；(2) FullStack-Learn，一种创新的数据扩展和自我提升方法，通过爬取和合成的网站仓库进行反向翻译，以提升FullStack-Dev的核心LLM；(3) FullStack-Bench，一个全面的基准测试系统，系统性地测试生成网站的前端、后端和数据库功能。我们的FullStack-Dev在前端、后端和数据库测试用例上分别优于先前最先进的方法8.7%、38.2%和15.9%。此外，FullStack-Learn通过自我提升，使一个30B模型在三个测试用例集上的性能分别提升了9.7%、9.5%和2.8%，证明了我们方法的有效性。代码已发布在https://github.com/mnluzimu/FullStack-Agent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing LLM-powered code agents that focus only on frontend development, neglecting the complexities of full-stack web applications. The proposed FullStack-Agent system integrates three components: FullStack-Dev, a multi-agent framework with advanced planning, code editing, and bug localization capabilities; FullStack-Learn, a self-improving method that enhances the backbone LLM through repository back-translation; and FullStack-Bench, a benchmark for evaluating frontend, backend, and database functionalities. The results show that FullStack-Dev improves performance by 8.7%, 38.2%, and 15.9% on frontend, backend, and database test cases respectively, while FullStack-Learn boosts a 30B model&#x27;s performance by 9.7%, 9.5%, and 2.8% on the same test sets, highlighting the system&#x27;s effectiveness in full-stack agentic coding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有基于大语言模型的代码代理仅关注前端开发、忽略全栈应用复杂性的局限。FullStack-Agent提出一个包含三个模块的统一代理系统：FullStack-Dev是一个具备强大规划、代码编辑和代码库导航能力的多代理框架；FullStack-Learn是一种创新的数据扩展与自我提升方法，通过爬取和合成的网站仓库进行反向翻译以提升主干大语言模型；FullStack-Bench是一个全面的基准测试平台，用于系统评估生成网站的前端、后端和数据库功能。实验结果显示，FullStack-Dev在前端、后端和数据库测试用例上的表现分别优于先前最佳方法8.7%、38.2%和15.9%，而FullStack-Learn使30B模型的性能分别提升9.7%、9.5%和2.8%。</div>
</details>
</div>
<div class="card">
<div class="title">SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?</div>
<div class="meta-line">Authors: Azmine Toushik Wasi, Wahid Faisal, Abdur Rahman, Mahfuz Ahmed Anik, Munem Shahriar, Mohsin Mahmud Topu, Sadia Tasnim Meem, Rahatun Nesa Priti, Sabrina Afroz Mitu, Md. Iqramul Hoque, Shahriyar Zaman Ridoy, Mohammed Eunus Ali, Majd Hawasly, Mohammad Raza, Md Rizwan Parvez</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:52:02+00:00 · Latest: 2026-02-03T17:52:02+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. 92 Pages. 42 Figures and 29 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03916v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spatialab-reasoning.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs&#x27; spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth &amp; Occlusion, Orientation, Size &amp; Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs&#x27; spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatiaLab：视觉语言模型能否在真实环境中进行空间推理？</div>
<div class="mono" style="margin-top:8px">空间推理是人类认知的基本组成部分，但仍然是当前视觉语言模型（VLMs）面临的主要挑战。以往的工作主要依赖于合成或LLM生成的环境，任务设计和类似谜题的设置有限，未能捕捉到VLMs在现实世界中遇到的复杂性、视觉噪声和多样的空间关系。为了解决这一问题，我们引入了SpatiaLab，这是一个用于评估VLMs在现实、无约束情境下空间推理能力的全面基准。SpatiaLab包含六个主要类别：相对位置、深度与遮挡、方向、大小与比例、空间导航和3D几何，每个类别下有五个子类别，共计30种不同的任务类型。每个子类别至少包含25个问题，每个主要类别至少包含200个问题，支持多项选择和开放式评估。在多种最先进的VLMs上进行的实验表明，其空间推理能力与人类存在显著差距。在多项选择设置中，InternVL3.5-72B的准确率为54.93%，而人类为87.57%。在开放式的设置中，所有模型的性能下降约10-25%，其中GPT-5-mini得分最高，为40.93%，而人类为64.93%。这些结果突显了处理复杂空间关系、深度感知、导航和3D几何方面的关键局限性。通过提供一个多样化的现实评估框架，SpatiaLab揭示了推进VLMs空间推理能力的关键挑战和机遇，为未来研究提供了指导基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of spatial reasoning in vision-language models (VLMs) by introducing SpatiaLab, a new benchmark designed to evaluate their performance in realistic, unconstrained environments. Unlike previous approaches that relied on synthetic or LLM-generated data, SpatiaLab includes 1,400 visual question-answer pairs across six categories and 30 task types, covering aspects such as relative positioning, depth perception, and 3D geometry. Experimental results show that even state-of-the-art models like InternVL3.5-72B and GPT-5-mini significantly underperform humans, achieving 54.93% and 40.93% accuracy in multiple-choice settings, respectively, compared to 87.57% for humans. The findings underscore the limitations of current VLMs in handling complex spatial relationships and highlight the need for more robust spatial reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">该研究探讨了视觉语言模型（VLMs）在现实场景中的空间推理能力，揭示了其与人类表现之间的显著差距。作者提出了SpatiaLab，这是一个包含1400个视觉问答对的基准测试，涵盖六个主要类别和30种任务类型，旨在评估VLMs在复杂、无约束环境中的表现。实验结果显示，即使是最先进的模型，如InternVL3.5-72B，在多项选择设置中也仅达到54.93%的准确率，远低于人类的87.57%。在开放式任务中，所有模型的表现均下降了10-25%，其中GPT-5-mini表现最佳，得分为40.93%，而人类得分为64.93%。这些结果突显了提升VLMs空间推理能力的必要性，并为未来研究提供了重要的评估基准。</div>
</details>
</div>
<div class="card">
<div class="title">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</div>
<div class="meta-line">Authors: Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao</div>
<div class="meta-line">First: 2025-03-17T09:24:26+00:00 · Latest: 2026-02-03T13:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12968v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiPMB：基于优化泊松多伯努利滤波的3D多目标跟踪增强方法</div>
<div class="mono" style="margin-top:8px">准确的3D多目标跟踪（MOT）对自动驾驶至关重要，因为它能够实现复杂环境下的鲁棒感知、导航和规划。尽管基于深度学习的解决方案在3D MOT任务中表现出色，但基于模型的方法因其简单性、可解释性和数据效率仍然具有吸引力。传统的基于模型的跟踪器通常依赖于随机向量贝叶斯滤波器，在基于检测的跟踪（TBD）框架中存在局限，主要由于启发式的数据关联和轨迹管理方案。相比之下，基于随机有限集（RFS）的贝叶斯滤波方法在理论上能够有效处理目标的生成、存活和消失，从而提升可解释性和参数调整能力。本文提出了一种新的基于RFS的3D MOT方法OptiPMB，该方法在基于检测的跟踪框架中采用优化的泊松多伯努利（PMB）滤波器，并引入了多项关键创新设计。具体而言，我们提出了一种测量驱动的混合自适应生成模型以提升轨迹初始化效果，采用自适应检测概率参数以有效维护被遮挡目标的轨迹，并优化密度剪枝和轨迹提取模块以进一步提升整体跟踪性能。在nuScenes和KITTI数据集上的大量实验表明，OptiPMB在跟踪精度上优于现有最先进的方法，从而为基于模型的3D MOT建立了新的基准，并为未来基于RFS的跟踪器研究提供了有价值的参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the accuracy of 3D multi-object tracking (MOT) in autonomous driving by leveraging the advantages of model-based approaches over deep learning-based ones, such as simplicity, interpretability, and data efficiency. OptiPMB introduces an optimized Poisson multi-Bernoulli (PMB) filter within the tracking-by-detection (TBD) framework, incorporating key innovations like a measurement-driven hybrid adaptive birth model for better track initialization, adaptive detection probability parameters to handle occluded objects, and optimized density pruning and track extraction modules. Experimental results on the nuScenes and KITTI datasets demonstrate that OptiPMB achieves superior tracking accuracy compared to existing state-of-the-art methods, setting a new benchmark for model-based 3D MOT.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用模型化方法的优势来提升自动驾驶中3D多目标跟踪（MOT）的准确性。OptiPMB提出了一种在跟踪-检测（TBD）框架下优化的泊松多伯努利（PMB）滤波器，结合了测量驱动的混合自适应出生模型、自适应检测概率参数以及优化的密度剪枝和轨迹提取模块。在nuScenes和KITTI数据集上的实验结果表明，OptiPMB在跟踪精度上优于现有最先进的方法，为模型化3D MOT设定了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">CP-Agent: Agentic Constraint Programming</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2025-08-10T19:59:01+00:00 · Latest: 2026-02-03T12:13:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver&#x27;s feedback, and refining constraint models based on execution results.
  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CP-Agent：代理约束编程</div>
<div class="mono" style="margin-top:8px">将自然语言翻译为形式化约束模型需要对问题领域和建模框架有专业知识。为探索代理工作流的有效性，我们提出了CP-Agent，这是一个使用ReAct框架并带有持久IPython内核的Python编码代理。我们将相关的领域知识作为项目提示提供，长度不超过50行。该算法通过迭代执行代码、观察求解器的反馈，并根据执行结果优化约束模型来工作。我们在CP-Bench的101个约束编程问题上评估了CP-Agent。我们对基准进行了小幅修改，以解决问题说明中的系统性歧义和真实模型中的错误。在澄清后的基准上，CP-Agent在所有101个问题上均达到完美准确率。我们的实验表明，最小的指导优于详细的程序结构。我们的实验还表明，显式的任务管理工具对集中建模任务可能产生正面或负面的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the effectiveness of agentic workflows in translating natural language into formal constraint models. CP-Agent, a Python coding agent, is proposed to utilize the ReAct framework with a persistent IPython kernel, incorporating domain knowledge through a concise project prompt. The agent iteratively executes code, observes solver feedback, and refines constraint models based on execution outcomes. Experimental results on 101 constraint programming problems from CP-Bench demonstrate that CP-Agent achieves perfect accuracy on all problems after minor adjustments to the benchmark. The study also highlights that minimal guidance can outperform detailed procedural scaffolding, while explicit task management tools may have mixed impacts on modeling tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨将自然语言转化为形式化约束模型过程中代理工作流的有效性。提出了一种名为CP-Agent的Python编码代理，其结合了ReAct框架和持久化的IPython内核，使用简短的领域知识作为项目提示。该代理通过迭代执行代码、观察求解器反馈并根据执行结果优化约束模型来工作。在对CP-Bench的101个约束编程问题进行评估后，经过对基准的微小调整以解决问题描述中的系统歧义和真实模型中的错误，CP-Agent在所有问题上均实现了完美准确率。实验结果表明，少量指导可能优于详细的程序框架，而显式的任务管理工具对专注建模任务的影响可能是正负参半的。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
