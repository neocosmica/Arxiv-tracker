<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-05 04:04</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260205_0404</div>
    <div class="row"><div class="card">
<div class="title">FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation</div>
<div class="meta-line">Authors: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Mingjie Zhan, Hongsheng Li</div>
<div class="meta-line">First: 2026-02-03T18:01:34+00:00 · Latest: 2026-02-03T18:01:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03798v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03798v1">PDF</a> · <a href="https://github.com/mnluzimu/FullStack-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FullStack-Agent: 通过面向开发的测试和仓库反向翻译增强代理式全栈网页编码</div>
<div class="mono" style="margin-top:8px">帮助非专家用户开发复杂的交互式网站已成为LLM驱动代码代理的热门任务。然而，现有的代码代理往往只生成前端网页，用华丽的视觉效果掩盖了实际全栈数据处理和存储的缺失。值得注意的是，构建生产级别的全栈网页应用远比仅生成前端网页更具挑战性，需要对数据流进行细致控制，全面理解不断更新的包和依赖关系，并准确定位代码库中的模糊错误。为了解决这些困难，我们引入了FullStack-Agent，这是一个统一的代理系统，用于全栈代理式编码，包含三个部分：(1) FullStack-Dev，一个具备强大规划、代码编辑、代码库导航和错误定位能力的多代理框架；(2) FullStack-Learn，一种创新的数据扩展和自我提升方法，通过反向翻译爬取和合成的网站仓库来提升FullStack-Dev的核心LLM；(3) FullStack-Bench，一个全面的基准测试系统，系统性地测试生成网站的前端、后端和数据库功能。我们的FullStack-Dev在前端、后端和数据库测试用例上分别优于之前最先进的方法8.7%、38.2%和15.9%。此外，FullStack-Learn通过自我提升，使一个30B模型在三个测试用例集上的性能分别提升了9.7%、9.5%和2.8%，证明了我们方法的有效性。代码已发布在https://github.com/mnluzimu/FullStack-Agent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing code agents that focus only on frontend development, neglecting the complexities of full-stack data processing and storage. The proposed FullStack-Agent system integrates three components: FullStack-Dev, a multi-agent framework with advanced planning, code editing, and bug localization capabilities; FullStack-Learn, a self-improving method that enhances the backbone LLM through repository back-translation; and FullStack-Bench, a benchmark for evaluating full-stack functionalities. Experimental results show that FullStack-Dev improves performance by 8.7%, 38.2%, and 15.9% on frontend, backend, and database tasks respectively, while FullStack-Learn boosts a 30B model&#x27;s performance by 9.7%, 9.5%, and 2.8% on the same test sets.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有代码代理仅关注前端开发而忽略全栈应用复杂性的不足。提出的FullStack-Agent系统包含三个部分：FullStack-Dev，一个具备强大规划、代码编辑和代码库导航能力的多代理框架；FullStack-Learn，一种通过爬取和合成网站仓库进行反向翻译的自改进方法，用于提升FullStack-Dev的核心大语言模型；以及FullStack-Bench，一个全面评估生成网站前端、后端和数据库功能的基准测试。实验结果表明，FullStack-Dev在前端、后端和数据库测试任务上的表现分别提升了8.7%、38.2%和15.9%，而FullStack-Learn使30B模型的性能在三个测试集上分别提高了9.7%、9.5%和2.8%，验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</div>
<div class="meta-line">Authors: Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao</div>
<div class="meta-line">First: 2025-03-17T09:24:26+00:00 · Latest: 2026-02-03T13:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12968v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiPMB：基于优化泊松多伯努利滤波的3D多目标跟踪增强</div>
<div class="mono" style="margin-top:8px">准确的3D多目标跟踪（MOT）对自动驾驶至关重要，因为它能够实现复杂环境下的鲁棒感知、导航和规划。尽管基于深度学习的解决方案在3D MOT任务中表现出色，但基于模型的方法因其简单性、可解释性和数据效率仍然具有吸引力。传统的基于模型的跟踪器通常依赖于随机向量贝叶斯滤波器，在基于检测的跟踪（TBD）框架中，但由于启发式的数据关联和跟踪管理方案而存在局限性。相比之下，基于随机有限集（RFS）的贝叶斯滤波方法在理论上能够处理目标的生成、存活和消失，从而提升可解释性和参数调整能力。本文提出OptiPMB，一种新颖的基于RFS的3D MOT方法，该方法在基于检测的跟踪框架中采用优化的泊松多伯努利（PMB）滤波器，并结合了多项关键创新设计。具体而言，我们提出了一种测量驱动的混合自适应生成模型以提升跟踪初始化效果，采用自适应检测概率参数以有效维护被遮挡目标的跟踪，同时优化密度剪枝和跟踪提取模块以进一步提升整体跟踪性能。在nuScenes和KITTI数据集上的大量评估表明，OptiPMB在跟踪精度上优于现有最先进的方法，从而为基于模型的3D MOT建立了新的基准，并为未来自动驾驶中基于RFS的跟踪器研究提供了有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the accuracy of 3D multi-object tracking (MOT) in autonomous driving by leveraging the advantages of model-based approaches over deep learning-based ones, such as simplicity, interpretability, and data efficiency. OptiPMB introduces an optimized Poisson multi-Bernoulli (PMB) filter within the tracking-by-detection (TBD) framework, incorporating innovations like a measurement-driven hybrid adaptive birth model, adaptive detection probability parameters, and optimized density pruning and track extraction modules. The method achieves superior tracking accuracy on the nuScenes and KITTI datasets, outperforming state-of-the-art methods and setting a new benchmark for model-based 3D MOT.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过模型化方法提升自动驾驶中3D多目标跟踪（MOT）的准确性和效率，以实现更可靠的环境感知、导航和规划。OptiPMB提出了一种基于优化的泊松多伯努利（PMB）滤波器的新型RFS（随机有限集）方法，在跟踪-检测（TBD）框架中引入了多项创新设计，包括测量驱动的混合自适应出生模型、自适应检测概率参数以及优化的密度剪枝和轨迹提取模块。在nuScenes和KITTI数据集上的大量实验表明，OptiPMB在跟踪精度上优于现有最先进的方法，为基于RFS的跟踪器研究提供了新的基准和有价值的参考。</div>
</details>
</div>
<div class="card">
<div class="title">CP-Agent: Agentic Constraint Programming</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2025-08-10T19:59:01+00:00 · Latest: 2026-02-03T12:13:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver&#x27;s feedback, and refining constraint models based on execution results.
  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CP-Agent：代理约束编程</div>
<div class="mono" style="margin-top:8px">将自然语言翻译为形式化约束模型需要对问题领域和建模框架有专业知识。为探索代理工作流的有效性，我们提出了CP-Agent，这是一个使用ReAct框架并带有持久IPython内核的Python编码代理。我们将相关的领域知识作为项目提示提供，长度不超过50行。该算法通过迭代执行代码、观察求解器的反馈，并根据执行结果改进约束模型来工作。我们在CP-Bench的101个约束编程问题上评估了CP-Agent。我们对基准进行了小幅修改，以解决问题说明中的系统性歧义和真实模型中的错误。在澄清后的基准上，CP-Agent在所有101个问题上均实现了完美准确率。我们的实验表明，最小的指导优于详细的程序框架。我们的实验还表明，显式的任务管理工具对集中建模任务可能有正面和负面的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the effectiveness of agentic workflows in translating natural language into formal constraint models. CP-Agent, a Python coding agent, employs the ReAct framework with a persistent IPython kernel and is guided by a concise project prompt containing domain knowledge. The agent iteratively executes code, observes solver feedback, and refines constraint models based on execution outcomes. Evaluation on 101 constraint programming problems from CP-Bench, with minor adjustments to address ambiguities and errors, shows that CP-Agent achieves perfect accuracy on all problems. The results indicate that minimal guidance can outperform detailed procedural scaffolding, while explicit task management tools may have mixed impacts on focused modeling tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨代理工作流在将自然语言转换为形式化约束模型中的有效性。CP-Agent是一个使用ReAct框架并配备持久IPython内核的Python编码代理，通过迭代执行代码、观察求解器反馈并根据执行结果优化约束模型来实现目标。在对CP-Bench基准进行微调以解决原始问题描述中的歧义和错误后，CP-Agent在101个约束编程问题上实现了完美准确率。实验结果表明，少量指导优于详细的程序框架，同时显式的任务管理工具对专注建模任务既有积极也有消极影响。</div>
</details>
</div>
<div class="card">
<div class="title">InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation</div>
<div class="meta-line">Authors: Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu, Yanyong Zhang</div>
<div class="meta-line">First: 2026-02-03T08:22:13+00:00 · Latest: 2026-02-03T08:22:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03242v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://shanpoyang654.github.io/InstaDrive/page.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#x27;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InstaDrive：面向实例感知的驾驶世界模型用于真实且一致的视频生成</div>
<div class="mono" style="margin-top:8px">自动驾驶依赖于在高质量、大规模多视角驾驶视频上训练的鲁棒模型。虽然世界模型为生成真实驾驶视频提供了一种成本效益高的解决方案，但它们在保持实例级时间一致性和空间几何保真度方面存在困难。为了解决这些挑战，我们提出了InstaDrive，一个通过两项关键进展提升驾驶视频真实性的新框架：(1) 实例流引导器，从帧中提取并传播实例特征以强制时间一致性，保持实例身份随时间不变。(2) 空间几何对齐器，提升空间推理能力，确保实例定位精确，并显式建模遮挡层次。通过引入这些实例感知机制，InstaDrive实现了最先进的视频生成质量，并在nuScenes数据集上增强了下游自动驾驶任务的性能。此外，我们利用CARLA的自动驾驶功能，在多样化的地图和区域上程序化和随机地模拟罕见但安全关键的驾驶场景，从而实现对自动驾驶系统的严格安全评估。我们的项目页面是https://shanpoyang654.github.io/InstaDrive/page.html。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InstaDrive was developed to improve the realism and consistency of driving videos generated by world models, which are essential for autonomous driving systems. The framework introduces two key components: the Instance Flow Guider, which maintains temporal consistency by propagating instance features across frames, and the Spatial Geometric Aligner, which enhances spatial accuracy and models occlusion hierarchies. These instance-aware mechanisms lead to state-of-the-art video generation quality and improve performance in downstream autonomous driving tasks on the nuScenes dataset. Additionally, InstaDrive leverages CARLA&#x27;s autopilot to simulate rare and safety-critical driving scenarios for comprehensive safety evaluation.</div>
<div class="mono" style="margin-top:8px">InstaDrive 是为提升驾驶视频生成的逼真度和一致性而设计，这对自动驾驶系统的训练至关重要。该框架引入了两个关键组件：实例流引导器，通过在帧间传播实例特征来保持时间一致性，并保留实例身份；以及空间几何对齐器，增强空间推理能力，确保实例定位精确并显式建模遮挡层次。这些实例感知机制使 InstaDrive 在视频生成质量上达到最先进水平，并提升了在 nuScenes 数据集上的下游自动驾驶任务表现。此外，利用 CARLA 的自动驾驶功能，可以模拟罕见但安全关键的驾驶场景，从而进行严格的系统安全性评估。</div>
</details>
</div>
<div class="card">
<div class="title">GeoResponder: Towards Building Geospatial LLMs for Time-Critical Disaster Response</div>
<div class="meta-line">Authors: Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</div>
<div class="meta-line">First: 2025-09-18T09:46:55+00:00 · Latest: 2026-02-03T07:50:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19354v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19354v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models excel at linguistic tasks but lack the inner geospatial capabilities needed for time-critical disaster response, where reasoning about road networks, continuous coordinates, and access to essential infrastructure such as hospitals, shelters, and pharmacies is vital. We introduce GeoResponder, a framework that instills robust spatial reasoning through a scaffolded instruction-tuning curriculum. By stratifying geospatial learning into different cognitive layers, we effectively anchor semantic knowledge to the continuous coordinate manifold and enforce the internalization of spatial axioms. Extensive evaluations across four topologically distinct cities and diverse tasks demonstrate that GeoResponder significantly outperforms both state-of-the-art foundation models and domain-specific baselines. These results suggest that LLMs can begin to internalize and generalize geospatial structures, pointing toward the future development of language models capable of supporting disaster response needs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoResponder：构建用于时效性灾害响应的地理空间大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型在语言任务上表现出色，但在时效性灾害响应中缺乏必要的地理空间能力，其中对道路网络、连续坐标以及医院、避难所和药店等关键基础设施的访问推理至关重要。我们引入了GeoResponder框架，通过结构化的指令微调课程来增强其强大的空间推理能力。通过将地理空间学习分层为不同的认知层次，我们有效地将语义知识锚定在连续坐标流形上，并强制模型内化空间公理。在四个拓扑结构不同的城市和多种任务上的广泛评估表明，GeoResponder在性能上显著优于最先进的基础模型和领域特定基线模型。这些结果表明，大语言模型开始能够内化和泛化地理空间结构，预示着未来能够支持灾害响应需求的语言模型的发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GeoResponder is to enhance the geospatial reasoning capabilities of large language models (LLMs) for time-critical disaster response scenarios. The framework employs a scaffolded instruction-tuning curriculum to embed spatial knowledge into LLMs, enabling them to understand and reason about road networks, continuous coordinates, and essential infrastructure. Experimental results across four topologically distinct cities show that GeoResponder outperforms both state-of-the-art foundation models and domain-specific baselines in various geospatial tasks, indicating that LLMs can effectively internalize and generalize spatial structures for real-world applications.</div>
<div class="mono" style="margin-top:8px">GeoResponder的研究动机是提升大型语言模型在时间敏感的灾害响应场景中的地理空间推理能力。该框架采用分层的指令微调课程，通过将地理空间学习划分为不同的认知层次，将语义知识锚定到连续坐标空间，并强制内化空间公理。在四个拓扑结构不同的城市和多种任务上的广泛评估表明，GeoResponder在性能上显著优于当前最先进的基础模型和领域专用基线，展示了语言模型内化和泛化地理空间结构的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-03T07:43:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展边际效益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，用于代码代理的性能扩展，通过多样性而非原始数据量来提升表现。在固定训练预算下，增加轨迹多样性所带来的性能提升比增加轨迹数量更大，从而优化代理训练的性能成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，用于捕捉真实服务中的逻辑依赖关系；（2）蓝图驱动的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙盒化的代码工具，以减轻内在编码能力的灾难性遗忘。我们在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得显著提升。我们计划在发表后公开完整的代码库和合成数据集（包含30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of code large language models (LLMs) when evolving into tool-interactive agents, particularly the issues of low-quality synthetic data and the diminishing returns of quantity-based scaling. The proposed TDScaling framework focuses on enhancing performance through trajectory diversity rather than increasing data volume. It incorporates four key innovations: a Business Cluster mechanism to capture logical dependencies, a blueprint-driven multi-agent paradigm for trajectory coherence, an adaptive evolution mechanism guided by entropy and action complexity metrics to avoid mode collapse, and a sandboxed code tool to preserve coding capabilities. Experimental results on multiple benchmarks and code agent tasks show that TDScaling significantly improves both tool-use generalization and coding proficiency, achieving better performance-cost trade-offs than traditional scaling approaches.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）作为工具交互代理时所面临的性能瓶颈问题，指出其受限于低质量合成数据和数据量增长带来的边际效益递减。作者提出TDScaling，一种基于轨迹多样性的数据合成框架，通过提升轨迹多样性而非单纯增加数据量来增强模型性能。TDScaling包含四项创新：业务聚类机制以捕捉真实服务的逻辑依赖关系，蓝图驱动的多代理范式确保轨迹一致性，利用领域熵、推理模式熵和累积动作复杂度的自适应演化机制防止模式坍缩，以及一个沙盒环境的代码工具以保留模型的内在编码能力。在多个基准测试中，TDScaling显著提升了工具使用泛化能力和编码熟练度，实现了性能与成本之间的更优平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</div>
<div class="meta-line">Authors: Sherman Wong, Zhenting Qi, Zhaodong Wang, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Wenlin Chen, Yilun Du, Minlan Yu, Ying Zhang</div>
<div class="meta-line">First: 2025-12-11T08:05:58+00:00 · Latest: 2026-02-03T05:01:47+00:00</div>
<div class="meta-line">Comments: The latest version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10398v6">Abs</a> · <a href="https://arxiv.org/pdf/2512.10398v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world software engineering tasks require coding agents that can operate on massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK supports a unified orchestrator with advanced context management for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the construction, evaluation, and refinement of agents through a build-test-improve cycle, enabling rapid agent development on new tasks and tool stacks. Instantiated on the Confucius SDK using the meta-agent, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a Resolve@1 of 59%, exceeding prior research baselines as well as commercial results, under identical repositories, model backends, and tool access.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>孔子代码代理：面向真实代码库的可扩展代理框架</div>
<div class="mono" style="margin-top:8px">现实中的软件工程任务需要能够处理大规模仓库、支持长时序会话并可靠协调复杂工具链的编码代理。现有的研究级编码代理虽然透明，但在扩展到更重的生产级工作负载时表现不佳，而生产级系统虽然具有强大的实际性能，但扩展性、可解释性和可控性有限。我们引入了孔子代码代理（CCA），这是一个能够在大规模代码库中运行的软件工程代理。CCA基于孔子SDK构建，这是一个围绕三个互补视角（代理体验AX、用户体验UX和开发者体验DX）设计的代理开发平台。SDK支持统一的编排器，具备高级上下文管理以实现长上下文推理，支持跨会话的持续学习的持久笔记系统，以及用于可靠工具使用的模块化扩展系统。此外，我们还引入了一个元代理，通过构建-测试-改进的循环自动完成代理的构建、评估和优化，从而实现对新任务和工具栈的快速代理开发。基于孔子SDK和元代理实例化的CCA在现实中的软件工程任务中表现出色。在SWE-Bench-Pro上，CCA在相同仓库、模型后端和工具访问条件下，实现了59%的Resolve@1，超过了先前的研究基准和商业结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the Confucius Code Agent (CCA) is to address the limitations of existing coding agents, which either lack scalability for real-world codebases or offer limited extensibility and interpretability. CCA is built using the Confucius SDK, which provides a structured platform for agent development through three perspectives: Agent Experience, User Experience, and Developer Experience. The SDK includes features such as a unified orchestrator with advanced context management, a persistent note-taking system for continual learning, and a modular extension system for reliable tool integration. The meta-agent framework automates the build-test-improve cycle, enabling efficient agent development. Experimental results show that CCA achieves a Resolve@1 score of 59% on SWE-Bench-Pro, outperforming previous research baselines and commercial systems in real-world software engineering tasks.</div>
<div class="mono" style="margin-top:8px">开发Confucius Code Agent（CCA）的动机是解决现有编码代理在实际代码库中的可扩展性不足以及灵活性和可解释性有限的问题。CCA基于Confucius SDK构建，该SDK通过三个视角（代理体验、用户体验和开发者体验）提供结构化的代理开发平台。SDK包含统一的编排器用于长上下文推理、持久的笔记系统用于跨会话持续学习，以及模块化的扩展系统用于可靠工具集成。在SWE-Bench-Pro上，CCA实现了59%的Resolve@1得分，优于之前的科研基准和商业系统，在相同代码库、模型后端和工具访问条件下表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</div>
<div class="meta-line">Authors: Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T03:39:31+00:00 · Latest: 2026-02-03T03:39:31+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03060v1">PDF</a> · <a href="https://github.com/FireRedTeam/IVC-Prune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVC-Prune: 揭示LVLMs中隐含的视觉坐标以实现视觉标记剪枝</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在多个任务中表现出色。然而，处理高分辨率视觉输入时，其推理成本却很高。尽管视觉标记剪枝已成为一种有前景的解决方案，但现有方法主要关注语义相关性，往往丢弃对空间推理至关重要的标记。我们通过一种新颖的洞察来解决这一问题，即\emph{LVLMs如何处理空间推理}。具体而言，我们发现LVLMs通过旋转位置嵌入（Rotary Position Embeddings, RoPE）隐式地建立视觉坐标系统，其中特定的标记位置作为\textbf{隐含视觉坐标}（IVC tokens），对空间推理至关重要。基于这一洞察，我们提出\textbf{IVC-Prune}，一种无需训练、基于提示的剪枝策略，保留IVC标记和语义相关的前景标记。IVC标记通过理论分析RoPE的数学性质来识别，目标是那些其旋转矩阵近似单位矩阵或$90^\circ$旋转矩阵的位置。前景标记则通过一个稳健的两阶段过程来识别：首先发现语义种子，然后通过值向量相似性进行上下文优化。在四个代表性LVLMs和二十个多样化基准上的广泛评估表明，IVC-Prune可将视觉标记减少约50\%，同时保持$\geq$ 99\%的原始性能，并在多个基准上实现性能提升。源代码可在https://github.com/FireRedTeam/IVC-Prune获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high inference costs in Large Vision-Language Models (LVLMs) when handling high-resolution visual inputs. The authors propose IVC-Prune, a training-free and prompt-aware token pruning method that identifies and retains tokens critical for spatial reasoning. They reveal that LVLMs implicitly use Rotary Position Embeddings (RoPE) to form visual coordinate systems, with specific token positions acting as implicit visual coordinates (IVC tokens). By theoretically analyzing RoPE&#x27;s mathematical properties and employing a two-stage process for foreground token selection, IVC-Prune achieves a 50% reduction in visual tokens while preserving over 99% of the original model performance and improving results on several benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉语言模型（LVLMs）在处理高分辨率视觉输入时的高昂推理成本问题。作者提出了一种无需训练且基于提示的视觉标记剪枝方法IVC-Prune，通过分析旋转位置嵌入（RoPE）的数学特性，识别出对空间推理至关重要的标记，即隐式视觉坐标（IVC tokens）。此外，通过两阶段流程（语义种子发现和上下文细化）识别语义相关的前景标记。实验结果表明，在四个代表性LVLMs和二十个不同基准测试中，IVC-Prune实现了约50%的视觉标记减少，同时保持了原性能的99%以上，并在多个基准测试中取得了性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</div>
<div class="meta-line">Authors: Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</div>
<div class="meta-line">First: 2026-02-03T03:10:27+00:00 · Latest: 2026-02-03T03:10:27+00:00</div>
<div class="meta-line">Comments: In Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03045v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03045v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>绘制前请澄清：面向稳健文本到CAD生成的主动代理</div>
<div class="mono" style="margin-top:8px">大型语言模型最近使文本到CAD系统成为可能，这些系统能够从自然语言提示中合成参数化CAD程序（例如CadQuery）。然而在实际应用中，几何描述可能不够具体或内部不一致：关键尺寸可能缺失，约束可能冲突。现有的微调模型倾向于根据用户指令反应性地执行，并在文本模糊时虚构尺寸。为了解决这一问题，我们提出了一种名为ProCAD的主动代理框架，用于文本到CadQuery的生成，该框架在代码合成之前解决规格问题。我们的框架结合了一个主动澄清代理，它仅在必要时审核提示并提出有针对性的澄清问题，以生成自洽的规格，以及一个CAD编码代理，它将规格转换为可执行的CadQuery程序。我们通过一个精心整理的高质量文本到CadQuery数据集对编码代理进行微调，并通过代理式监督微调（SFT）在澄清轨迹上训练澄清代理。实验表明，主动澄清显著提高了对模糊提示的鲁棒性，同时保持了较低的交互开销。ProCAD在包括Claude Sonnet 4.5在内的前沿闭源模型上表现更优，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。我们的代码和数据集将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the robustness of text-to-CAD systems by addressing the issues of under-specified and inconsistent geometric descriptions in natural language prompts. The proposed method, ProCAD, introduces a proactive agentic framework that includes a clarifying agent and a CAD coding agent. The clarifying agent audits the prompt and asks targeted questions only when necessary to ensure a self-consistent specification, while the coding agent translates this specification into an executable CadQuery program. The clarifying agent is trained using agentic SFT on clarification trajectories, and the coding agent is fine-tuned on a high-quality dataset. Experimental results demonstrate that ProCAD significantly improves robustness to ambiguous prompts, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent, outperforming closed-source models like Claude Sonnet 4.5.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升文本到CAD系统的鲁棒性，解决自然语言提示中几何描述不足或不一致的问题。提出的方法ProCAD采用了一种主动代理框架，包含一个澄清代理和一个CAD编码代理。澄清代理审计提示内容，并在必要时提出针对性问题以确保生成的规范自洽，而编码代理则将该规范转换为可执行的CadQuery程序。实验结果表明，ProCAD在处理模糊提示时表现显著优于现有模型，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。</div>
</details>
</div>
<div class="card">
<div class="title">CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability</div>
<div class="meta-line">Authors: Xianzhen Luo, Jingyuan Zhang, Shiqi Zhou, Rain Huang, Chuan Xiao, Qingfu Zhu, Zhiyuan Ma, Xing Yue, Yang Yue, Wencong Zeng, Wanxiang Che</div>
<div class="meta-line">First: 2026-02-03T02:27:16+00:00 · Latest: 2026-02-03T02:27:16+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03012v1">PDF</a> · <a href="https://github.com/livecvebench/CVE-Factory">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CVE-Factory：为代码安全漏洞实现专家级代理任务的扩展</div>
<div class="mono" style="margin-top:8px">评估和提升代码代理的安全能力需要高质量且可执行的漏洞任务。然而，现有工作依赖于成本高昂且难以扩展的手动复现，并且数据分布过时。为了解决这些问题，我们提出了CVE-Factory，这是首个在自动将稀疏的CVE元数据转换为完全可执行的代理任务方面达到专家级质量的多代理框架。与人类专家复现的交叉验证表明，CVE-Factory实现了95\%的解决方案正确性和96\%的环境保真度，确认了其专家级质量。它还被评估在最新的现实漏洞上，取得了66.2\%的验证成功率。这种自动化实现了两个下游贡献。首先，我们构建了LiveCVEBench，这是一个持续更新的基准，包含190个任务，涵盖14种语言和153个仓库，捕捉了包括AI工具漏洞在内的新兴威胁。其次，我们合成超过1,000个可执行的训练环境，这是代码安全领域代理任务的首次大规模扩展。经过微调的Qwen3-32B在LiveCVEBench上的表现从5.3\%提升至35.8\%，超越了Claude 4.5 Sonnet，其提升效果也推广到Terminal Bench（从12.5\%提升至31.3\%）。我们开源了CVE-Factory、LiveCVEBench、Abacus-cve（微调模型）、训练数据集和排行榜。所有资源均可在https://github.com/livecvebench/CVE-Factory 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the evaluation and improvement of code agents&#x27; security capabilities by providing high-quality, executable vulnerability tasks. To achieve this, the authors introduce CVE-Factory, a multi-agent framework that automatically transforms sparse CVE metadata into fully executable agentic tasks. The framework is validated against human expert reproductions, achieving 95% solution correctness and 96% environment fidelity, and is tested on recent vulnerabilities with a 66.2% verified success rate. Two downstream contributions include the creation of LiveCVEBench, a continuously updated benchmark with 190 tasks across 14 languages and 153 repositories, and the synthesis of over 1,000 training environments. Fine-tuned Qwen3-32B significantly improves performance on LiveCVEBench, surpassing Claude 4.5 Sonnet, with results generalizing to Terminal Bench.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提供高质量、可执行的漏洞任务来提升代码代理的安全评估能力。CVE-Factory提出了一种多代理框架，能够自动将稀疏的CVE元数据转换为完整的可执行代理任务，解决了手动复现成本高、数据过时的问题。实验结果表明，CVE-Factory在与人类专家任务的交叉验证中达到了95%的解决方案正确性和96%的环境保真度，并在最新漏洞上实现了66.2%的成功率。该框架还构建了LiveCVEBench，包含14种语言和153个仓库的190个任务，涵盖了新兴威胁，如AI工具漏洞，并合成超过1000个可执行训练环境。经过微调的Qwen3-32B在LiveCVEBench上的表现从5.3%提升至35.8%，超越了Claude 4.5 Sonnet，并在Terminal Bench上也表现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing</div>
<div class="meta-line">Authors: Saeid Sheikhi</div>
<div class="meta-line">First: 2026-02-02T21:44:01+00:00 · Latest: 2026-02-02T21:44:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02842v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟链：一种面向大语言模型的双模式推理框架，具有动态问题路由</div>
<div class="mono" style="margin-top:8px">我们提出了模拟链（CoS），一种新颖的双模式推理框架，该框架通过动态路由将问题引导至大语言模型（LLMs）中的专用推理策略。与现有的统一提示方法不同，CoS采用三种不同的推理模式：(1) 用于数学问题的计算流程与自洽性，(2) 用于空间推理的符号状态跟踪与JSON表示，(3) 用于多跳推理的混合事实提取。通过在GSM8K、StrategyQA和bAbI基准上对四个最先进的模型（Gemma-3 27B、LLaMA-3.1 8B、Mistral 7B和Qwen-2.5 14B）进行综合评估，我们证明CoS在GSM8K上达到71.5%的准确率（绝对提升1.0%），在StrategyQA上达到90.0%（提升2.5%），在bAbI上达到19.0%（相对提升65.2%），优于最强基线。分析表明，针对特定问题选择合适的推理模式至关重要，当正确应用于数学问题时，计算模式可达到81.2%的准确率，而错误路由则导致0%的准确率。我们提供了模式选择、状态跟踪和答案提取的详细算法，确立了CoS作为一种无需额外训练即可提升LLM推理能力的有效方法。与Self-Consistency相比，该框架在准确率和效率之间提供了更优的权衡，计算成本降低了54%，同时实现了相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the reasoning capabilities of Large Language Models (LLMs) by addressing their limitations in handling diverse problem types through a structured approach. Chain of Simulation (CoS) introduces a dual-mode reasoning framework that dynamically routes problems to specialized strategies, including computational flow for math problems, symbolic state tracking for spatial reasoning, and hybrid fact-extraction for multi-hop inference. Experimental results on GSM8K, StrategyQA, and bAbI benchmarks show significant improvements: 71.5% accuracy on GSM8K (1.0% absolute gain), 90.0% on StrategyQA (2.5% gain), and 19.0% on bAbI (65.2% relative gain) compared to existing methods. The framework also demonstrates better efficiency, achieving similar performance at 54% lower computational cost.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Chain of Simulation（CoS）的双模式推理框架，旨在通过动态路由问题到专门的推理策略来提升大型语言模型（LLM）的推理能力。该框架包含三种不同的推理模式：用于数学问题的计算流程模式、用于空间推理的符号状态跟踪模式以及用于多跳推理的混合事实提取模式。在GSM8K、StrategyQA和bAbI基准测试中，使用四个最先进的模型进行评估，结果显示CoS在GSM8K上达到71.5%的准确率（绝对提升1.0%），在StrategyQA上达到90.0%（提升2.5%），在bAbI上达到19.0%（相对提升65.2%），优于现有最佳基线方法。分析表明，准确的模式选择至关重要，当计算模式正确应用于数学问题时，准确率可达81.2%，而错误路由则导致准确率为0%。此外，CoS在准确率与效率之间提供了更优的平衡，相较于Self-Consistency方法，在计算成本降低54%的情况下实现了相近的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-02-02T19:55:32+00:00</div>
<div class="meta-line">Comments: 21 main pages, 6 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该比闭源系统具有根本性的优势：它们可以专门化于私有代码库，直接在权重中编码特定仓库的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，能够快速且低成本地创建专门化于私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法和代码）模型中实现了最先进的结果，同时与前沿的开放权重模型（如Devstral-Small-2）表现相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到同等性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数以千计的轨迹。结合成本效益，这使得专门化于私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过20万个合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和混淆因素进行了详细分析。总体而言，我们相信我们的工作将大大加速开放编码代理的研究，并展示能够专门化于私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to enable open-weight coding agents to effectively specialize to private codebases, leveraging their open-source nature for practical deployment. The authors propose SERA, an efficient training method that uses only supervised fine-tuning (SFT) to achieve state-of-the-art performance on fully open-source models while matching the capabilities of advanced open-weight models like Devstral-Small-2. Key results show that SERA is significantly more cost-effective than reinforcement learning and previous synthetic data methods, achieving equivalent performance at 26x and 57x lower cost respectively. Additionally, the method Soft Verified Generation (SVG) generates thousands of trajectories from a single code repository, and when applied to a larger corpus, produces over 200,000 synthetic trajectories for analyzing scaling laws and training factors.</div>
<div class="mono" style="margin-top:8px">本研究的动机是使开放权重的编码代理能够有效适应私有代码库，利用其开源特性实现实际应用。作者提出了SERA方法，通过仅使用监督微调（SFT）训练，实现了完全开源模型的最先进性能，同时达到如Devstral-Small-2等前沿开放权重模型的水平。关键实验结果表明，SERA在达到相同性能时，比强化学习便宜26倍，比之前合成数据方法便宜57倍。此外，Soft Verified Generation（SVG）技术可以从单个代码库生成数千条合成轨迹，支持对编码代理训练中扩展规律和混淆因素的深入分析。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将现实世界可验证环境扩展到百万级</div>
<div class="mono" style="margin-top:8px">我们提出了SWE-Universe，这是一个可扩展且高效的框架，用于从GitHub拉取请求（PRs）自动构建现实世界软件工程（SWE）可验证环境。为了解决自动构建中普遍存在的挑战，如低生产率、弱验证器和高昂成本，我们的框架采用了一个由高效自定义训练模型驱动的构建代理。该代理通过迭代自验证和循环内黑客检测，确保高保真、可验证任务的可靠生成。使用该方法，我们将现实世界多语言SWE环境的数量扩展到百万级（807,693）。我们通过大规模代理中间训练和强化学习展示了这些环境的深远价值。最后，我们将该技术应用于Qwen3-Max-Thinking，并在SWE-Bench Verified上取得了75.3%的得分。我们的工作为推进下一代代码代理提供了关键资源和稳健的方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the limitations of existing methods in automatically generating high-quality, verifiable software engineering environments. The framework introduces a building agent based on a custom-trained model to efficiently construct these environments from GitHub pull requests. This agent performs iterative self-verification and in-loop hacking detection to ensure reliability and fidelity. The method successfully scales the creation of real-world multilingual SWE environments to over 800,000 instances, and the results show its effectiveness through large-scale agentic mid-training and reinforcement learning. When applied to Qwen3-Max-Thinking, it achieved a 75.3% score on the SWE-Bench Verified benchmark, demonstrating its practical value.</div>
<div class="mono" style="margin-top:8px">SWE-Universe的动机是解决自动构建高保真、可验证的现实软件工程环境所面临的挑战。该框架引入了一个由自定义模型训练的构建代理，通过迭代自验证和循环中的漏洞检测来确保任务生成的可靠性。利用这种方法，框架成功构建了超过80万个多语言软件工程环境。该方法通过在大规模代理中期训练和强化学习中展示其有效性得到验证，并在应用于Qwen3-Max-Thinking时取得了75.3%的SWE-Bench Verified评分。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image</div>
<div class="meta-line">Authors: Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik</div>
<div class="meta-line">First: 2025-09-02T13:17:11+00:00 · Latest: 2026-02-02T16:22:10+00:00</div>
<div class="meta-line">Comments: Project page is at https://learn-to-steer-paper.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02295v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://learn-to-steer-paper.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model&#x27;s internal representations.
  We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model&#x27;s cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces in the cross-attention maps, rather than learning true spatial patterns. We solve this by augmenting our training data with samples generated using prompts with incorrect relation words, which encourages the classifier to avoid linguistic shortcuts and learn spatial patterns from the attention maps. Our method dramatically improves spatial accuracy: from 20% to 61% on FLUX.1-dev and from 7% to 54% on SD2.1 across standard benchmarks. It also generalizes to multiple relations with significantly improved accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像推理时优化的数据驱动损失函数</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型可以生成令人惊叹的视觉效果，但在儿童看来微不足道的任务上却常常失败，例如将狗放在泰迪熊的右侧而不是左侧。当组合变得更加不寻常时，例如将长颈鹿放在飞机上方，这些失败会更加明显。现有方法通过模型微调或使用手工设计的次优损失函数在推理时进行优化，以解决这些空间推理问题。我们提出的方法是直接从模型的内部表示中学习这些目标，而不是强加我们对空间编码的假设。我们引入了Learn-to-Steer，这是一个新颖的框架，用于学习数据驱动的推理时优化目标，而非手工设计。我们的关键洞察是训练一个轻量级分类器，从扩散模型的交叉注意力图中解码空间关系，然后在推理过程中将该分类器作为学习得到的损失函数进行部署。训练此类分类器面临一个令人惊讶的挑战：它们可能通过检测交叉注意力图中的语言痕迹来走捷径，而不是学习真正的空间模式。我们通过在训练数据中加入使用错误关系词提示生成的样本，来解决这一问题，这促使分类器避免语言捷径，并从注意力图中学习空间模式。我们的方法显著提高了空间准确性：在FLUX.1-dev上从20%提升至61%，在SD2.1上从7%提升至54%。它还能够显著提高准确率地推广到多种关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of spatial reasoning failures in text-to-image diffusion models, which struggle with simple tasks such as placing objects in specific relative positions. The authors propose Learn-to-Steer, a framework that learns data-driven loss functions for inference-time optimization by training a lightweight classifier to decode spatial relationships from cross-attention maps. To prevent the classifier from relying on linguistic cues, the training data is augmented with prompts containing incorrect relation words. The method significantly improves spatial accuracy, achieving 61% on FLUX.1-dev and 54% on SD2.1 across standard benchmarks, demonstrating strong generalization to multiple spatial relations.</div>
<div class="mono" style="margin-top:8px">本文旨在解决文本到图像扩散模型在空间推理任务中的失败问题，例如无法正确放置物体的相对位置。作者提出Learn-to-Steer框架，通过训练一个轻量级分类器来从扩散模型的交叉注意力图中解码空间关系，从而学习数据驱动的推理时间优化目标。为防止分类器依赖语言线索，训练数据通过包含错误关系词的提示进行增强。该方法显著提升了空间准确性，在FLUX.1-dev和SD2.1标准基准上分别达到61%和54%，并表现出对多种空间关系的良好泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCode: A Benchmark for Evaluating Software Engineering Agents</div>
<div class="meta-line">Authors: Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta</div>
<div class="meta-line">First: 2026-02-02T16:04:10+00:00 · Latest: 2026-02-02T16:04:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02262v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02262v1">PDF</a> · <a href="https://github.com/seal-research/OmniCode">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCode：评估软件工程代理的基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型的编码代理正在重新定义现实世界软件的开发方式。为了推动对更优编码代理的研究，我们需要能够严格评估此类代理执行各种软件工程任务能力的挑战性基准。然而，像HumanEval和SWE-Bench这样的流行编码基准主要关注狭窄范围的任务，如编程竞赛和补丁生成。实际上，软件工程师在现实世界软件开发中需要处理更广泛的任务。为了解决这一差距，我们提出了OmniCode，一个包含超越代码或补丁生成的更广泛和多样化任务类别的新型软件工程基准。总体而言，OmniCode包含1794个任务，涵盖三种编程语言（Python、Java和C++）以及四个关键类别：错误修复、测试生成、代码审查修复和风格修复。与以往的软件工程基准相比，OmniCode的任务（1）经过人工验证以消除定义不清的问题，（2）合成构建或最近整理以避免数据泄露问题，提供了一种从有限现实数据中合成生成多样化软件任务的新框架。我们使用流行的代理框架如SWE-Agent对OmniCode进行了评估，结果显示虽然它们在Python错误修复方面表现良好，但在测试生成等任务以及C++和Java等语言中表现不足。例如，在Java测试生成任务中，SWE-Agent使用DeepSeek-V3.1最多仅达到20.9%。OmniCode旨在作为稳健的基准，推动开发能够在软件开发不同方面表现良好的代理。代码和数据可在https://github.com/seal-research/OmniCode获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of LLM-powered coding agents is transforming real-world software engineering practices, but existing benchmarks like HumanEval and SWE-Bench are limited in scope, focusing mainly on competition programming and patch generation. To address this, OmniCode is introduced as a comprehensive benchmark covering four key categories: bug fixing, test generation, code review fixing, and style fixing, with 1794 tasks across Python, Java, and C++. The tasks are manually validated and synthetically generated to avoid data leakage, ensuring a rigorous evaluation of agents&#x27; capabilities. Evaluation on popular frameworks such as SWE-Agent shows that while they perform reasonably on Python bug fixing, they struggle with test generation and other tasks in Java and C++.</div>
<div class="mono" style="margin-top:8px">OmniCode 的研究动机是为了解决现有 LLM 编码代理在真实软件工程任务中评估不足的问题。该基准包含 1794 个任务，涵盖 Python、Java 和 C++ 三种编程语言，涉及四个类别：错误修复、测试生成、代码审查修复和风格调整。与以往专注于狭窄任务的基准不同，OmniCode 通过人工验证和合成生成任务来避免数据泄露并确保任务多样性。评估结果显示，尽管某些框架在 Python 错误修复上表现良好，但在 Java 和 C++ 的测试生成等任务上仍存在明显不足，例如 SWE-Agent 在 Java 测试生成任务中仅达到 20.9% 的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</div>
<div class="meta-line">Authors: Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</div>
<div class="meta-line">First: 2025-05-23T11:48:48+00:00 · Latest: 2026-02-02T13:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17779v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.17779v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U2-BENCH：在超声理解上评估大型视觉-语言模型的基准</div>
<div class="mono" style="margin-top:8px">超声是一种广泛使用的成像方式，对全球医疗至关重要，但由于其图像质量在操作者、噪声和解剖结构上的差异，其解读仍具挑战性。尽管大型视觉-语言模型（LVLMs）在自然和医学领域展示了令人印象深刻的多模态能力，但其在超声方面的表现仍鲜有研究。我们引入了U2-BENCH，这是首个全面评估LVLMs在超声理解上的基准，涵盖分类、检测、回归和文本生成等任务。U2-BENCH汇集了7,241个案例，涉及15个解剖区域，并在50个超声应用场景中定义了8个临床启发式任务，如诊断、视图识别、病灶定位、临床价值评估和报告生成。我们评估了23个最先进的LVLMs，包括开源和闭源、通用型和医学专用型。我们的结果表明，在图像分类任务上表现强劲，但在空间推理和临床语言生成方面仍存在持续挑战。U2-BENCH建立了一个严格且统一的测试平台，用于评估和加速医学超声成像这一独特多模态领域的LVLM研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the lack of evaluation frameworks for large vision-language models (LVLMs) in ultrasound understanding, a critical yet challenging modality in medical imaging. U2-BENCH is introduced as the first comprehensive benchmark that assesses LVLMs across multiple tasks including classification, detection, regression, and text generation. It includes 7,241 cases from 15 anatomical regions and covers 50 ultrasound application scenarios with 8 clinically relevant tasks. The evaluation of 23 state-of-the-art LVLMs shows strong performance in image-level classification but highlights persistent difficulties in spatial reasoning and clinical language generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在医学超声成像领域缺乏对大型视觉语言模型（LVLMs）的评估框架的问题，该领域对全球医疗至关重要，但由于图像质量的差异和噪声，其解读具有挑战性。U2-BENCH被引入作为首个全面评估LVLMs在超声理解方面能力的基准，涵盖分类、检测、回归和文本生成等任务。该基准包含来自15个解剖区域的7241个案例，并涉及8个临床相关的任务和50种超声应用场景。对23个最先进的LVLMs进行评估结果显示，模型在图像分类任务上表现良好，但在空间推理和临床语言生成方面仍面临持续挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models</div>
<div class="meta-line">Authors: Cristian Sbrolli, Matteo Matteucci, Toshihiko Yamasaki</div>
<div class="meta-line">First: 2026-02-02T12:39:39+00:00 · Latest: 2026-02-02T12:39:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02043v1">PDF</a> · <a href="https://huggingface.co/AutoComp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing &quot;a red cube and a blue sphere&quot; with &quot;a blue cube and a red sphere&quot;. Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., &quot;a monitor to the left of a bicycle on a white background&quot;) and LLM-generated Contextual captions (e.g., &quot;In a brightly lit photography studio, a monitor is positioned to the left of a bicycle&quot;), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel &quot;Confusion Benchmark&quot; reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Auto-Comp：用于对比视觉-语言模型可扩展组合探查的自动化流水线</div>
<div class="mono" style="margin-top:8px">现代视觉-语言模型（VLMs）在组合推理方面存在关键缺陷，常常将 &quot;一个红色立方体和一个蓝色球体&quot; 与 &quot;一个蓝色立方体和一个红色球体&quot; 混淆。分离这些失败的视觉和语言根源是实现稳健评估的基本挑战。为实现细粒度、可控的分析，我们引入了 Auto-Comp，这是一个完全自动化且合成的流水线，用于生成可扩展的基准测试。其可控性是解构和隔离不同推理能力的关键。Auto-Comp 从最小化描述（例如：&quot;一个显示器位于白色背景上的自行车左侧&quot;）和大语言模型生成的上下文描述（例如：&quot;在一个明亮的摄影工作室中，显示器位于自行车左侧&quot;）中生成配对图像，从而允许进行受控的 A/B 测试，以区分核心绑定能力与视觉-语言复杂性。我们在新的基准测试上评估了 20 个 VLMs，发现 CLIP 和 SigLIP 模型家族在颜色绑定和空间关系方面均存在普遍的组合失败。关键的是，我们提出的新型 &quot;混淆基准&quot; 揭示了一个比简单属性交换更深层次的缺陷：模型对低熵值的干扰项（例如重复的物体或颜色）高度敏感，表明其组合失败不仅限于已知的词袋模型限制。我们发现了一个令人惊讶的权衡：视觉-语言上下文虽然提供了全局场景线索，有助于空间推理，但同时通过引入视觉杂乱而阻碍了局部属性绑定。我们发布了 Auto-Comp 流水线以促进未来基准测试的创建，并提供了所有生成的基准测试（https://huggingface.co/AutoComp）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the critical issue of compositional reasoning in Vision-Language Models (VLMs), where models often fail to distinguish between attribute swaps such as &#x27;a red cube and a blue sphere&#x27; versus &#x27;a blue cube and a red sphere&#x27;. To enable detailed and controlled analysis, the authors propose Auto-Comp, an automated pipeline that generates synthetic benchmarks by pairing minimal images with contextual captions. The evaluation of 20 VLMs on these benchmarks highlights widespread compositional failures, particularly in color and spatial relation understanding. A key finding is that models are vulnerable to low-entropy distractors, indicating that their issues go beyond simple bag-of-words limitations. The study also identifies a trade-off between global visio-linguistic context aiding spatial reasoning and hindering local attribute binding due to visual clutter.</div>
<div class="mono" style="margin-top:8px">本文针对视觉语言模型（VLMs）在组合推理中的关键问题，即模型常混淆属性交换如&quot;一个红色立方体和一个蓝色球体&quot;与&quot;一个蓝色立方体和一个红色球体&quot;，提出了解决方案。作者开发了Auto-Comp，一个自动化且合成的管道，用于生成可扩展的基准测试数据。通过在这些基准上进行A/B测试，他们评估了20个VLMs，并发现了CLIP和SigLIP模型家族中普遍存在的组合推理缺陷。其提出的&quot;混淆基准&quot;表明，模型对低熵干扰项（如重复的物体或颜色）高度敏感，说明其组合推理错误超出了已知的词袋模型限制。研究还揭示了视觉语言上下文在促进空间推理的同时，可能妨碍局部属性绑定的权衡关系。</div>
</details>
</div>
<div class="card">
<div class="title">Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models</div>
<div class="meta-line">Authors: Wenhui Tan, Fiorenzo Parascandolo, Enver Sangineto, Jianzhong Ju, Zhenbo Luo, Qian Cao, Rita Cucchiara, Ruihua Song, Jian Luan</div>
<div class="meta-line">First: 2026-02-02T06:12:33+00:00 · Latest: 2026-02-02T06:12:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01698v1">PDF</a> · <a href="https://GitHub.com/Xiaomi-Research/LED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在微调后恢复探索：用于大推理模型的潜在探索解码</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过强化学习（RL）微调后最近在数学和代码推理方面表现出色。然而，我们发现现代推理微调会导致意外的探索崩溃：基于温度的采样不再提升pass@$n$准确率。实证研究表明，微调后的LRMs最后一层的后验熵显著降低，而中间层的熵则相对较高。基于这种熵的不对称性，我们提出了潜在探索解码（LED），这是一种基于深度的解码策略。LED通过累积求和聚合中间后验，并选择熵最大的深度配置作为探索候选。无需额外训练或参数，LED在多个推理基准和模型上持续提升了pass@1和pass@16的准确率，分别提高了0.61和1.03个百分点。项目页面：https://GitHub.com/Xiaomi-Research/LED。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of exploration collapse in Large Reasoning Models (LRMs) after reinforcement learning post-training, where temperature-based sampling fails to improve pass@$n$ accuracy. The authors observe that post-trained LRMs show a significant reduction in entropy in the final layer, while intermediate layers retain higher entropy. To counteract this, they introduce Latent Exploration Decoding (LED), a decoding strategy that leverages cumulative sum of intermediate posteriors to identify high-entropy depth configurations as exploration candidates. Experimental results across multiple reasoning benchmarks demonstrate that LED improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points respectively, without requiring additional training or parameters.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型（LRMs）在强化学习微调后出现的探索坍塌问题，指出基于温度的采样无法提升pass@$n$准确率。作者发现微调后的LRMs在最后一层表现出显著的熵值下降，而中间层的熵值相对较高。为此，他们提出了一种名为Latent Exploration Decoding（LED）的解码策略，通过累积中间层后验分布并选择熵值最高的深度配置作为探索候选。实验结果表明，LED在多个推理基准测试中提升了pass@1和pass@16的准确率，分别提高了0.61和1.03个百分点，且无需额外训练或参数调整。</div>
</details>
</div>
<div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-02T05:17:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v1">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在单个问题的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念导向任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of current coding agent evaluations, which primarily focus on bug fixing rather than comprehensive end-to-end project development. ProjDevBench introduces a benchmark that provides coding agents with project requirements and evaluates the resulting code repositories based on system architecture design, functional correctness, and iterative solution refinement. The benchmark includes 20 programming problems across 8 categories, and evaluates six coding agents using different LLM backends. The main experimental results show that while agents perform well on basic functionality and data structures, they struggle with complex system design, time complexity optimization, and resource management, leading to an overall acceptance rate of 27.38%.</div>
<div class="mono" style="margin-top:8px">ProjDevBench的提出旨在弥补当前编码代理评估的不足，现有评估主要集中在单个问题的修复上，而忽视了完整的端到端项目开发。该基准结合了在线判题系统与大语言模型辅助的代码评审，从系统架构设计、功能正确性及迭代解决方案优化三个维度对编码代理进行评估。它包含8个类别下的20个编程问题，涵盖概念性任务和现实应用场景。实验结果显示，尽管代理在基础功能和数据结构方面表现良好，但在复杂系统设计、时间复杂度优化和资源管理方面存在明显困难，整体通过率为27.38%。</div>
</details>
</div>
<div class="card">
<div class="title">From Perception to Action: Spatial AI Agents and World Models</div>
<div class="meta-line">Authors: Gloria Felicia, Nolan Bryant, Handi Putra, Ayaan Gazali, Eliel Lobo, Esteban Rojas</div>
<div class="meta-line">First: 2026-02-02T05:00:55+00:00 · Latest: 2026-02-02T05:00:55+00:00</div>
<div class="meta-line">Comments: 61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01644v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从感知到行动：空间AI代理与世界模型</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型已成为代理推理和规划的主流方法，但它们在符号领域的成功并不容易直接应用于物理世界。空间智能，即感知三维结构、推理物体关系并在物理约束下行动的能力，是与代理能力正交的重要能力，对具身代理至关重要。现有的综述文章要么单独讨论代理架构，要么单独探讨空间领域，没有提供一个统一的框架来连接这些互补的能力。本文填补了这一空白。通过全面回顾2000多篇论文，引用了顶级会议中的742项研究，我们提出了一种统一的三轴分类法，将代理能力与跨尺度的空间任务联系起来。关键的是，我们区分了空间锚定（对几何和物理的度量理解）与符号锚定（将图像与文本关联），认为仅凭感知并不能赋予代理能力。我们的分析揭示了三个关键发现，对应这三个轴：(1) 分层记忆系统（能力轴）对于长视野空间任务至关重要。(2) GNN-LLM的集成（任务轴）是结构化空间推理的一种有前景的方法。(3) 世界模型（尺度轴）对于在微到宏观空间尺度上安全部署至关重要。最后，我们识别出六个重大挑战，并概述了未来研究的方向，包括需要统一的评估框架以标准化跨领域评估。该分类法为统一分散的研究努力奠定了基础，并有助于在机器人、自动驾驶和地理空间智能等领域实现下一代空间感知的自主系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating agentic reasoning with spatial intelligence to enable embodied agents to operate effectively in the physical world. It proposes a unified three-axis taxonomy that connects agentic capabilities with spatial tasks across different scales, distinguishing between spatial grounding and symbolic grounding. The key findings include the importance of hierarchical memory systems for long-term spatial tasks, the potential of GNN-LLM integration for structured spatial reasoning, and the necessity of world models for safe deployment across various spatial scales. The authors highlight six major challenges and suggest future research directions aimed at creating standardized evaluation frameworks for cross-domain assessment.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将代理推理与空间智能结合以实现物理世界中具身智能体有效操作的挑战。它提出了一种统一的三轴分类法，将代理能力与空间任务在不同尺度上联系起来，区分了空间接地与符号接地。主要发现包括：分层记忆系统对长期空间任务的重要性，GNN-LLM融合在结构化空间推理中的潜力，以及世界模型在微至宏观空间尺度安全部署中的必要性。文章还指出了六个重大挑战，并提出了未来研究方向，包括建立统一的评估框架以实现跨领域评估的标准化。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Autoregressive-Diffusion Collaborative Thoughts</div>
<div class="meta-line">Authors: Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu</div>
<div class="meta-line">First: 2026-02-02T03:54:15+00:00 · Latest: 2026-02-02T03:54:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自回归-扩散协同思维的推理</div>
<div class="mono" style="margin-top:8px">自回归模型和扩散模型代表两种互补的生成范式。自回归模型擅长于序列规划和约束组合，但在需要显式空间或物理基础的任务中表现不佳。相比之下，扩散模型通过高维生成捕捉丰富的空间结构，但缺乏满足复杂多阶段约束或可靠识别和纠正错误所需的逐步逻辑控制。我们引入了协同思维（Collaborative Thoughts），这是一种统一的协同框架，使自回归模型和扩散模型能够通过闭环交互共同进行推理和生成。在协同思维中，自回归模型执行结构化规划和约束管理，扩散模型将这些约束实例化为中间的视觉思维，而基于视觉的批评模块则评估这些视觉思维是否满足预期的结构和物理要求。这种反馈随后用于迭代优化后续的规划和生成步骤，从而减少跨模态的误差传播。重要的是，无论任务是自回归问答还是基于扩散的视觉生成，协同思维都使用相同的协同循环。通过代表性示例，我们展示了协同思维如何提高空间推理的可靠性以及生成的可控性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of both autoregressive and diffusion models in handling tasks that require spatial reasoning and multi-stage constraint satisfaction. The proposed method, Collaborative Thoughts, integrates these two paradigms into a unified framework where autoregressive models handle structured planning and constraint management, while diffusion models generate intermediate visual representations. A vision-based critic module evaluates these visual outputs for compliance with structural and physical requirements, providing feedback to refine the planning and generation process iteratively. The framework is effective across both question-answering and visual generation tasks, enhancing the reliability of spatial reasoning and the controllability of generation outcomes.</div>
<div class="mono" style="margin-top:8px">本文旨在解决自回归模型和扩散模型在处理需要空间推理和多阶段约束满足的任务时的局限性。作者提出了Collaborative Thoughts框架，将自回归模型用于结构化规划和约束管理，扩散模型用于生成中间视觉表示。一个基于视觉的批评模块评估这些视觉输出是否符合结构和物理要求，并提供反馈以迭代优化规划和生成过程。实验结果表明，该协作方法提高了空间推理的可靠性，并增强了生成的可控性。</div>
</details>
</div>
<div class="card">
<div class="title">Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance</div>
<div class="meta-line">Authors: Aliaksei Kaliutau</div>
<div class="meta-line">First: 2025-12-09T11:11:37+00:00 · Latest: 2026-02-01T22:18:46+00:00</div>
<div class="meta-line">Comments: 21 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08492v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.08492v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the &quot;Semantic Trap&quot; inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主问题解决者：迈向零触代码维护</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型在函数级代码生成方面取得了重大进展；然而，仓库级的自动程序修复（APR）仍然是一个重大挑战。当前方法通常采用以控制为中心的范式，迫使代理在复杂的目录结构和无关的控制逻辑中导航。在本文中，我们提出了一种范式转变，从标准的代码属性图（CPGs）转向数据转换图（DTG）的概念，通过将数据状态建模为节点、函数建模为边，反转拓扑结构，使代理能够通过数据血缘追踪逻辑缺陷，而非控制流。我们引入了一个多代理框架，将数据完整性导航与控制流逻辑相结合。我们的理论分析和案例研究表明，这种方法解决了现代代码代理中标准RAG系统固有的“语义陷阱”问题。我们提供了名为自主问题解决者（AIR）的全面实现，这是一个用于零触代码维护的自改进系统，利用神经符号推理，并采用DTG结构实现可扩展的逻辑修复。我们的方法在多个软件工程基准测试中表现良好，在SWE-Verified基准测试中达到了87.1%的解决率。我们的方法直接针对当前AI代码辅助工具的核心限制，并应对我们日益依赖软件的世界中对更强大基础的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this paper is to address the limitations of existing automated program repair systems in handling repository-scale code maintenance. The authors propose a shift from traditional control-centric paradigms to a data-centric approach using Data Transformation Graphs (DTGs), which model data states as nodes and functions as edges. This allows agents to trace logic defects through data lineage rather than control flow. The multi-agent framework introduced in the paper integrates data integrity navigation with control flow logic, and the experimental results show that it achieves an 87.1% resolution rate on the SWE-Verified benchmark, outperforming existing methods and resolving the &quot;Semantic Trap&quot; issue in standard RAG systems.</div>
<div class="mono" style="margin-top:8px">本文针对仓库级自动化程序修复（APR）的挑战，提出了一种从传统控制中心范式向数据中心范式转变的方法，采用数据转换图（DTG）建模，将数据状态作为节点，函数作为边，使代理能够通过数据血缘追踪逻辑缺陷，而非控制流。作者引入了一个多代理框架，将数据完整性导航与控制流逻辑相结合，并提出了自主问题解决器（AIR）这一自改进系统，利用神经符号推理和DTG结构实现可扩展的逻辑修复。实验结果表明，该方法在多个软件工程基准测试中表现良好，尤其在SWE-Verified基准测试中达到87.1%的修复率，有效克服了当前AI代码助手的核心局限，为日益依赖软件的世界提供了更稳固的基础。</div>
</details>
</div>
<div class="card">
<div class="title">ASP-Bench: From Natural Language to Logic Programs</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2026-02-01T11:48:36+00:00 · Latest: 2026-02-01T11:48:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01171v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem&#x27;s modeling hardness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ASP-Bench：从自然语言到逻辑程序</div>
<div class="mono" style="margin-top:8px">将自然语言规范自动转换为逻辑程序是一项具有挑战性的任务，影响神经符号工程。我们提出了ASP-Bench，这是一个包含128个自然语言问题实例的基准测试集，其中64个基础问题包含简单和困难变体。它评估将自然语言问题转换为答案集程序（ASPs）的系统，ASPs是逻辑编程的一种重要形式。它系统地覆盖了ASP特性，包括选择规则、聚合和优化。每个问题都包含参考验证器，用于检查解决方案是否符合问题规范。
我们从七个主要独立的推理方面（优化、时态推理、默认逻辑、资源分配、递归、空间推理和定量复杂度）对问题进行分类，提供了一个多维的建模难度视角。
我们采用基于ReAct（推理与行动）框架的代理方法测试该基准，实现了完全饱和，证明了利用求解器反馈进行反馈驱动的迭代优化是一种可靠且稳健的自然语言建模方法。我们的多代理运行分析使我们能够深入了解决定问题建模难度的因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of automatically translating natural language specifications into logic programs, which is crucial for neurosymbolic engineering. The authors introduce ASP-Bench, a benchmark consisting of 128 natural language problem instances, including 64 base problems with easy and hard variants, designed to evaluate systems that convert natural language into Answer Set Programs. The benchmark systematically covers various ASP features and includes reference validators to ensure correctness. Using an agentic approach based on the ReAct framework, the study demonstrates that feedback-driven iterative refinement with solver feedback can achieve full saturation, indicating a reliable method for modeling natural language in ASP. The analysis across multiple agent runs provides insights into the factors influencing modeling difficulty.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决将自然语言规范自动转换为逻辑程序的挑战，这对神经符号工程至关重要。ASP-Bench 是一个包含 128 个自然语言问题实例的基准，其中包括 64 个基础问题及其易和难的变体，用于评估将自然语言转换为答案集程序（ASPs）的系统。该基准系统地覆盖了 ASP 的关键特性，如选择规则、聚合和优化，并包含参考验证器以确保解决方案的正确性。通过基于 ReAct 框架的代理方法进行测试，研究展示了利用求解器反馈进行反馈驱动的迭代优化可以实现完全饱和，表明了在 ASP 中建模自然语言的可靠方法。对多个代理运行的分析揭示了影响问题建模难度的因素。</div>
</details>
</div>
<div class="card">
<div class="title">Capabilities and Fundamental Limits of Latent Chain-of-Thought</div>
<div class="meta-line">Authors: Jiaxuan Zou, Yaozhong Xiong, Yong Liu</div>
<div class="meta-line">First: 2026-02-01T10:46:00+00:00 · Latest: 2026-02-01T10:46:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01148v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01148v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在思维链（Latent CoT）的能力与基本限制</div>
<div class="mono" style="margin-top:8px">潜在思维链（Latent CoT）模型通过连续表示实现高效推理，但表现出令人困惑的性能不一致性：在探索方面表现优异（ProsQA: 97.0%），但在计算方面表现不佳（GSM8K: 34.1%）。我们揭示了这种权衡是由决策确定性所主导的。我们的贡献有三个方面：（1）我们从理论上刻画了基本的探索-执行权衡，证明高确定性能够实现精确执行但抑制探索，而低确定性则促进搜索但导致误差累积。（2）我们引入符号索引——量化决策承诺——作为控制这种权衡的核心机制，并建立了其与执行稳定性及探索能力之间的因果关系。（3）我们证明了课程学习在理论上是必要的，因为直接训练由于分布不匹配而必然失败。我们的框架将设计范式从二元架构选择转向能够根据任务需求动态调节决策确定性的自适应系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the capabilities and limitations of Latent Chain-of-Thought (Latent CoT) models, which aim to enable efficient reasoning through continuous representations. The authors highlight a puzzling performance trade-off where these models perform well in exploration tasks (ProsQA: 97.0%) but poorly in computation tasks (GSM8K: 34.1%). They attribute this discrepancy to decisional certainty, demonstrating that high certainty improves execution stability but restricts exploration, while low certainty enhances search but leads to error accumulation. The study introduces the Symbolic Index as a metric to quantify decisional commitment and establishes its causal role in balancing exploration and execution. Furthermore, it proves that curriculum learning is theoretically required for effective training, as direct training suffers from distributional mismatch. These findings suggest a shift in model design toward adaptive systems that can dynamically adjust decisional certainty based on task requirements.</div>
<div class="mono" style="margin-top:8px">该论文探讨了基于连续表示的潜在链式思维（Latent CoT）模型的能力与局限性。研究发现，这些模型在探索任务中表现优异，但在计算任务中效果不佳，揭示了探索与执行之间的权衡关系。作者从理论上分析了这一现象，指出决策确定性是关键因素，高确定性有助于执行精确性但抑制探索，低确定性则促进探索但导致误差累积。他们提出了符号索引这一指标，用于量化决策承诺，并证明其对执行稳定性和探索能力的因果影响。此外，作者证明了课程学习在训练这些模型中的必要性，因为直接训练会因分布不匹配而失败。该框架倡导一种基于任务需求动态调节决策确定性的自适应系统设计方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization</div>
<div class="meta-line">Authors: Alireza Nadafian, Alireza Mohammadshahi, Majid Yazdani</div>
<div class="meta-line">First: 2026-01-29T10:40:54+00:00 · Latest: 2026-01-31T20:40:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21526v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21526v2">PDF</a> · <a href="https://github.com/Leeroo-AI/kapso">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAPSO：一种基于知识的自主程序合成与优化框架</div>
<div class="mono" style="margin-top:8px">我们介绍了KAPSO，一种用于自主程序合成与优化的模块化框架。给定一个自然语言目标和一个评估方法，KAPSO迭代地执行创意生成、代码合成与编辑、执行、评估和学习，以改进可运行的产物，使其更接近可衡量的目标。KAPSO不将合成视为终点，而是将其作为长周期优化循环中的一个操作符，其中进展由评估器的结果定义。
KAPSO通过整合三个紧密耦合的组件，针对编码代理中常见的长周期失败问题，包括实验状态丢失、脆弱的调试和领域专业知识的弱复用。首先，一个原生支持Git的实验引擎将每次尝试隔离为一个分支，生成可复现的产物，并在迭代过程中保留溯源信息。其次，一个知识系统吸收异构来源，包括仓库、内部操作手册以及经过整理的外部资源，如文档、科学论文和网络搜索结果，并将它们组织成结构化的表示，以支持对工作流、实现和环境约束的检索。第三，一个认知记忆层协调检索，并维护一个可复用的课件存储，这些课件是从实验轨迹（运行日志、差异和评估反馈）中提炼出的，从而减少重复的错误模式并加快收敛。
我们在MLE-Bench（Kaggle风格的机器学习竞赛）和ALE-Bench（AtCoder启发式优化）上评估了KAPSO，并报告了端到端的性能表现。
代码地址：https://github.com/Leeroo-AI/kapso</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">KAPSO is designed to address long-horizon failures in coding agents by providing a modular framework for autonomous program synthesis and optimization. It integrates a git-native experimentation engine, a knowledge system that organizes heterogeneous sources into a structured representation, and a cognitive memory layer that stores reusable lessons from past experiments. The framework iteratively performs ideation, code synthesis, execution, evaluation, and learning to refine a runnable artifact toward measurable objectives. Evaluation on MLE-Bench and ALE-Bench shows improved end-to-end performance compared to existing methods.</div>
<div class="mono" style="margin-top:8px">KAPSO的动机是解决编码代理中常见的长周期失败问题，如实验状态丢失、调试脆弱性和领域知识复用不足。该框架将程序合成视为优化循环中的一个迭代操作，结合了创意生成、代码合成与编辑、执行、评估和学习等步骤。KAPSO集成了三个紧密耦合的组件：一个基于git的实验引擎以确保可重复性，一个知识系统用于组织异构来源的信息，以及一个认知记忆层，用于存储从过往实验中提取的可复用经验。在MLE-Bench和ALE-Bench上的实验结果表明，KAPSO能够有效提升可运行产物的性能，实现可衡量的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Affine-Equivariant Kernel Space Encoding for NeRF Editing</div>
<div class="meta-line">Authors: Mikołaj Zieliński, Krzysztof Byrski, Tomasz Szczepanik, Dominik Belter, Przemysław Spurek</div>
<div class="meta-line">First: 2025-08-04T18:59:23+00:00 · Latest: 2026-01-31T20:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.02831v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.02831v2">PDF</a> · <a href="https://github.com/MikolajZielinski/eks">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural scene representations achieve high-fidelity rendering by encoding 3D scenes as continuous functions, but their latent spaces are typically implicit and globally entangled, making localized editing and physically grounded manipulation difficult. While several works introduce explicit control structures or point-based latent representations to improve editability, these approaches often suffer from limited locality, sensitivity to deformations, or visual artifacts. In this paper, we introduce Affine-Equivariant Kernel Space Encoding (EKS), a spatial encoding for neural radiance fields that provides localized, deformation-aware feature representations. Instead of querying latent features directly at discrete points or grid vertices, our encoding aggregates features through a field of anisotropic Gaussian kernels, each defining a localized region of influence. This kernel-based formulation enables stable feature interpolation under spatial transformations while preserving continuity and high reconstruction quality. To preserve detail without sacrificing editability, we further propose a training-time feature distillation mechanism that transfers information from multi-resolution hash grid encodings into the kernel field, yielding a compact and fully grid-free representation at inference. This enables intuitive, localized scene editing directly via Gaussian kernels without retraining, while maintaining high-quality rendering. The code can be found under (https://github.com/MikolajZielinski/eks)</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于NeRF编辑的仿射等变核空间编码</div>
<div class="mono" style="margin-top:8px">神经场景表示通过将3D场景编码为连续函数实现高质量渲染，但其潜在空间通常是隐式的且全局纠缠的，使得局部编辑和物理基础的操控变得困难。尽管已有若干工作引入显式的控制结构或基于点的潜在表示以提高可编辑性，但这些方法往往存在局部性有限、对形变敏感或产生视觉伪影的问题。本文提出仿射等变核空间编码（EKS），这是一种用于神经辐射场的空间编码，提供局部的、形变感知的特征表示。与直接在离散点或网格顶点查询潜在特征不同，我们的编码通过各向异性高斯核场聚合特征，每个核定义一个局部影响区域。这种基于核的公式在空间变换下能够实现稳定的特征插值，同时保持连续性和高质量的重建。为了在不牺牲可编辑性的前提下保留细节，我们进一步提出了一种训练时的特征蒸馏机制，将多分辨率哈希网格编码的信息转移到核场中，从而在推理时得到一个紧凑且完全无网格的表示。这使得能够通过高斯核直接进行直观的局部场景编辑，而无需重新训练，同时保持高质量的渲染。代码可在（https://github.com/MikolajZielinski/eks）中找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of localized and physically grounded editing in neural radiance fields (NeRF) by proposing Affine-Equivariant Kernel Space Encoding (EKS). Traditional NeRF representations suffer from implicit and globally entangled latent spaces, which hinder effective scene manipulation. EKS introduces a spatial encoding method that uses anisotropic Gaussian kernels to create deformation-aware feature representations, allowing for stable feature interpolation under spatial transformations. The method also incorporates a feature distillation technique during training to retain detail while achieving a compact, grid-free representation. Experimental results demonstrate that EKS enables intuitive, localized scene editing without retraining, while maintaining high-quality rendering and continuity.</div>
<div class="mono" style="margin-top:8px">本文针对神经辐射场（NeRF）中局部化和物理基础的场景编辑难题，提出了一种仿射等变核空间编码（Affine-Equivariant Kernel Space Encoding, EKS）方法。传统NeRF表示方式存在隐式且全局纠缠的潜在空间，限制了有效的场景修改能力。EKS通过使用各向异性高斯核场对特征进行编码，实现了对形变敏感的局部特征表示。该方法通过核场聚合特征，而非直接在离散点或网格顶点上查询，从而支持在空间变换下的稳定特征插值。同时，提出了一种训练时的特征蒸馏机制，将多分辨率哈希网格编码的信息转移到核场中，得到紧凑且无网格的表示。这种方法支持无需重新训练即可进行直观的场景编辑，同时保持高质量的渲染效果。</div>
</details>
</div>
<div class="card">
<div class="title">GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association</div>
<div class="meta-line">Authors: Rong-Lin Jian, Ming-Chi Luo, Chen-Wei Huang, Chia-Ming Lee, Yu-Fan Lin, Chih-Chung Hsu</div>
<div class="meta-line">First: 2026-01-31T03:08:48+00:00 · Latest: 2026-01-31T03:08:48+00:00</div>
<div class="meta-line">Comments: Winner Solution of SoccerTrack in ACM Multimedia 2025 Workshop MMSports</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00484v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00484v1">PDF</a> · <a href="https://github.com/ron941/GTATrack-STC2025">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GTATrack：2025年SoccerTrack挑战赛的冠军方案，采用Deep-EIoU与全局跟踪片段关联</div>
<div class="mono" style="margin-top:8px">由于球员运动不规则、外观相似以及频繁遮挡，体育场景中的多目标跟踪（MOT）极具挑战性。这些困难在静态鱼眼摄像头引入的几何畸变和极端尺度变化下进一步加剧。本文提出GTATrack，这是一种分层跟踪框架，在2025年SoccerTrack挑战赛中获得第一名。GTATrack集成了两个核心组件：用于运动无关在线关联的Deep Expansion IoU（Deep-EIoU）和用于轨迹级优化的Global Tracklet Association（GTA）。这种两阶段设计实现了稳健的短期匹配和长期身份一致性。此外，采用伪标签策略以提升检测器对小目标和畸变目标的召回率。局部关联与全局推理的协同作用有效解决了身份切换、遮挡和跟踪碎片化问题。我们的方法取得了0.60的冠军HOTA分数，并将误检率显著降低至982，展示了基于鱼眼摄像头的足球跟踪的最先进精度。我们的代码可在https://github.com/ron941/GTATrack-STC2025获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-object tracking in sports is challenging due to irregular player motion, uniform appearances, and frequent occlusions, especially under fisheye camera distortions. GTATrack, a hierarchical tracking framework, addresses these issues by combining Deep-EIoU for motion-agnostic online association and Global Tracklet Association for trajectory-level refinement. The method also employs a pseudo-labeling strategy to improve detection of small and distorted targets. Experimental results show that GTATrack achieved a HOTA score of 0.60 and reduced false positives to 982, outperforming existing approaches in fisheye-based soccer tracking.</div>
<div class="mono" style="margin-top:8px">由于球员运动不规律、外观相似以及频繁遮挡，体育场景中的多目标跟踪极具挑战性，而静态鱼眼镜头进一步加剧了几何畸变和尺度变化的问题。GTATrack 是一种分层跟踪框架，通过结合 Deep-EIoU 实现运动无关的在线关联，以及 Global Tracklet Association 进行轨迹级优化，有效解决了身份切换、遮挡和跟踪碎片化问题。此外，采用伪标签策略提升对小目标和畸变目标的检测召回率。实验结果显示，GTATrack 在 SoccerTrack 2025 挑战赛中取得第一名，HOTA 分数达到 0.60，误检数降至 982，展现了鱼眼镜头下足球跟踪的最先进精度。</div>
</details>
</div>
<div class="card">
<div class="title">Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks</div>
<div class="meta-line">Authors: Jia Liang, Liangming Pan</div>
<div class="meta-line">First: 2026-01-31T01:48:23+00:00 · Latest: 2026-01-31T01:48:23+00:00</div>
<div class="meta-line">Comments: 20 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00449v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在链式思维模型是否分步骤思考？对顺序推理任务的机制研究</div>
<div class="mono" style="margin-top:8px">潜在链式思维（Latent-CoT）旨在在不输出长篇推理过程的情况下实现分步骤计算，但其机制仍不明确。我们研究了CODI，一个连续思维的教师-学生蒸馏模型，在严格顺序的多项式迭代任务上的表现。通过logit-lens解码、线性探针、注意力分析和激活补丁技术，我们定位了中间状态表示，并追踪其路由至最终输出。在两步和三步任务中，CODI形成了完整的桥接状态，这些状态在潜在思维位置上可被解码，而最终输入则遵循一条近似直接的路径；预测结果在思维结束边界通过晚期融合产生。对于更长的跳跃长度，CODI无法可靠地执行完整的潜在推理路径，而是表现出一种部分潜在推理路径，专注于晚期中间状态，并在答案读取位置将其与最后一个输入融合。消融实验表明，这种部分路径在系统状态变化时可能会崩溃，包括更困难的优化场景。总体而言，我们界定了CODI风格的潜在链式思维在何种情况下能实现忠实的迭代计算，而非压缩或捷径策略，并突出了设计稳健潜在链式思维目标以用于顺序推理任务的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether Latent-CoT models perform step-by-step reasoning by analyzing CODI, a continuous-thought teacher-student distillation model, on sequential polynomial-iteration tasks. Through logit-lens decoding, linear probes, attention analysis, and activation patching, the authors identify intermediate-state representations and their routing to the final output. The results show that for two- and three-hop tasks, CODI generates a full set of bridge states that are decodable across latent-thought positions, with final predictions emerging via late fusion at the end-of-thought boundary. However, for longer hop lengths, CODI relies on a partial reasoning path that focuses on late intermediates and fuses them with the last input at the answer position. Ablation experiments reveal that this partial pathway can be unstable under regime shifts and harder optimization conditions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Latent-CoT模型是否能够进行分步推理，通过分析CODI这一连续思维的教师-学生蒸馏模型在顺序多项式迭代任务中的表现。采用logit-lens解码、线性探针、注意力分析和激活补丁等方法，研究人员定位了中间状态表示及其向最终输出的路由路径。研究结果表明，在两步和三步任务中，CODI形成了完整的桥梁状态，最终预测在思维结束边界通过晚期融合产生；而在更长的步骤任务中，模型采用了一种部分推理路径，专注于晚期中间状态，并在答案读取位置与最后输入融合。消融实验显示，这种部分路径在环境变化或优化难度增加时可能失效，揭示了构建稳健的Latent-CoT目标在顺序推理中的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Over Scale: Learning Spatial Reasoning from Educational Video</div>
<div class="meta-line">Authors: Bishoy Galoaa, Xiangyu Bai, Sarah Ostadabbas</div>
<div class="meta-line">First: 2026-01-30T18:20:23+00:00 · Latest: 2026-01-30T18:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23251v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.23251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children&#x27;s educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化超越规模：从教育视频中学习空间推理</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在标准视频理解基准上表现出色，但在幼儿能够解决的简单推理任务（如计数、空间推理和组合理解）上系统性地表现不佳。我们假设教育视频中结构化的教学内容为提升这些能力提供了理想的训练信号。我们引入了DoraVQA数据集，该数据集包含从8季《朵拉探险》中自动提取的5,344个问答对，并具有精确的时间戳对齐。每一集都遵循一致的\textit{上下文-问题-暂停-回答}结构，创建了一个类似互动辅导的自包含学习环境。我们使用分组相对策略优化（GRPO）对Qwen2和Qwen3进行微调，利用教育内容中固有的清晰正确性信号和结构化推理轨迹。尽管仅在38小时的儿童教育视频上进行训练，我们的方法在DoraVQA上实现了8-14分的提升，并在CVBench上达到了86.16\%的最先进水平，同时在Video-MME和NExT-QA上表现出强大的迁移能力，证明了从狭窄的教学内容到广泛多模态理解的有效泛化。通过跨领域基准测试，我们展示了VLMs能够执行需要从结构化教育内容中学习的稳健推理任务，表明内容结构与内容规模同样重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of vision-language models (VLMs) in handling simple reasoning tasks that preschool children can solve, such as counting and spatial reasoning. The authors propose DoraVQA, a dataset derived from educational videos of Dora the Explorer, structured with precise timestamp alignment and a consistent context-question-pause-answer format. By fine-tuning Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), they demonstrate that training on structured educational content significantly improves performance on reasoning tasks, achieving an 8-14 point increase on DoraVQA and an 86.16% accuracy on CVBench, with strong transferability to other benchmarks like Video-MME and NExT-QA.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLMs）在处理学前儿童能够解决的简单推理任务（如计数和空间推理）方面的不足。作者提出了DoraVQA数据集，该数据集从Dora the Explorer的8季教育视频中自动提取，具有精确的时间戳对齐和一致的上下文-问题-暂停-回答结构。通过使用组相对策略优化（GRPO）对Qwen2和Qwen3进行微调，他们展示了教育内容对提升推理能力的重要性。实验结果表明，在仅使用38小时儿童教育视频进行训练的情况下，该方法在DoraVQA上提升了8-14个百分点，并在CVBench上达到了86.16%的最先进水平，同时在多个跨领域基准测试中表现出良好的迁移能力，说明结构化内容对VLMs的推理能力提升具有关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Bridging the Planner-Coder Gap: A Systematic Study on the Robustness of Multi-Agent Systems for Code Generation</div>
<div class="meta-line">Authors: Zongyi Lyu, Songqiang Chen, Zhenlan Ji, Liwen Wang, Shuai Wang, Daoyuan Wu, Wenxuan Wang, Shing-Chi Cheung</div>
<div class="meta-line">First: 2025-10-12T05:45:04+00:00 · Latest: 2026-01-30T16:44:46+00:00</div>
<div class="meta-line">Comments: 18pages, 5 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10460v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10460v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems (MASs) have emerged as a promising paradigm for automated code generation, demonstrating impressive performance on established benchmarks. Despite their prosperous development, the fundamental mechanisms underlying their robustness remain poorly understood, raising critical concerns for real-world deployment. This paper conducts a systematic empirical study to uncover the internal robustness flaws of MASs using a mutation-based methodology. By designing a testing pipeline incorporating semantic-preserving mutation operators and a novel fitness function, we assess mainstream MASs across multiple datasets and LLMs. Our findings reveal substantial robustness flaws: semantically equivalent inputs cause drastic performance drops, with MASs failing to solve 7.9\%--83.3\% of problems they initially resolved successfully.
  Through comprehensive failure analysis, we discover a fundamental cause underlying these robustness issues: the \textit{planner-coder gap}, which accounts for 75.3\% of failures. This gap arises from information loss in the multi-stage transformation process where planning agents decompose requirements into underspecified plans, and coding agents subsequently misinterpret intricate logic during code generation. Based on this formulated information transformation process, we propose a \textit{repairing method} that mitigates information loss through multi-prompt generation and introduces a monitor agent to bridge the planner-coder gap. Evaluation shows that our repairing method effectively enhances the robustness of MASs by solving 40.0\%--88.9\% of identified failures. Our work uncovers critical robustness flaws in MASs and provides effective mitigation strategies, contributing essential insights for developing more reliable MASs for code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解与弥合规划者-编码者差距：多智能体系统在代码生成中鲁棒性的系统性研究</div>
<div class="mono" style="margin-top:8px">多智能体系统（MASs）已成为自动化代码生成的有前景范式，在现有基准测试中表现出色。尽管其发展迅速，但其鲁棒性的基本机制仍不明确，这引发了在现实部署中的关键问题。本文通过基于变异的方法进行系统性实证研究，以揭示MASs内部的鲁棒性缺陷。我们设计了一个测试流水线，结合语义保持的变异操作符和一种新颖的适应度函数，对主流MASs在多个数据集和大语言模型（LLMs）上进行评估。研究结果揭示了显著的鲁棒性缺陷：语义等价的输入会导致性能大幅下降，MASs在最初成功解决的问题中，有7.9%--83.3%无法解决。通过全面的失败分析，我们发现这些鲁棒性问题的根本原因在于\textit{规划者-编码者差距}，该差距占所有失败的75.3%。这种差距源于规划智能体将需求分解为未明确指定的计划过程中信息的丢失，以及编码智能体在代码生成过程中对复杂逻辑的误解。基于这一信息转换过程，我们提出了一种\textit{修复方法}，通过多提示生成来减少信息丢失，并引入监控智能体以弥合规划者-编码者差距。评估结果表明，我们的修复方法有效提升了MASs的鲁棒性，解决了40.0%--88.9%的已识别失败问题。我们的工作揭示了MASs中的关键鲁棒性缺陷，并提供了有效的缓解策略，为开发更可靠的代码生成MASs提供了重要的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the robustness of multi-agent systems (MASs) in automated code generation, motivated by the lack of understanding of their reliability in real-world applications. The authors employ a mutation-based approach to systematically evaluate MASs, using semantic-preserving mutation operators and a novel fitness function across various datasets and large language models. Their experiments reveal significant robustness issues, with MASs failing to solve up to 83.3% of previously solved problems when faced with semantically equivalent inputs. The study identifies the planner-coder gap as the primary cause of these failures, stemming from information loss during the planning and coding stages. A repairing method is proposed, incorporating multi-prompt generation and a monitor agent, which successfully resolves 40.0%--88.9% of the identified failures, thereby improving the robustness of MASs for code generation.</div>
<div class="mono" style="margin-top:8px">本文旨在研究多智能体系统（MASs）在自动化代码生成中的鲁棒性问题，动机源于对其实用性可靠性缺乏深入理解。作者采用基于变异的方法，通过生成语义等价的输入来系统评估MASs的性能变化，发现其在多个数据集和大语言模型上存在显著的鲁棒性缺陷，失败率高达7.9%–83.3%。研究指出，规划与编码之间的信息损失是导致这些问题的根本原因，即所谓的规划者-编码者差距。为此，提出了一种修复方法，通过多提示生成和引入监控智能体来减少信息损失，实验结果表明该方法可解决40.0%–88.9%的已识别问题，有效提升了MASs的鲁棒性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
