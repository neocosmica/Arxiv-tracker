<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 03:48</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0348</div>
    <div class="row"><div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理为何失败？对GitHub中失败代理拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编码代理现在正在向软件项目提交拉取请求（PRs），不仅作为助手，还作为自主贡献者。随着这些代理贡献在真实仓库中迅速增加，人们对它们在实际中的行为以及为何许多未能被合并仍知之甚少。在本文中，我们对GitHub上五个编码代理提交的33,000个代理撰写的PR进行了大规模研究。（RQ1）我们首先从四个广泛维度定量描述了合并和未合并的PR：1）不同任务类型的合并结果，2）代码更改，3）CI构建结果，以及4）评审动态。我们观察到，与文档、CI和构建更新相关的任务具有最高的合并成功率，而性能和错误修复任务表现最差。未合并的PR往往涉及更大的代码更改，修改更多文件，并且通常无法通过项目的CI/CD流水线验证。（RQ2）为进一步探讨为何某些代理PR未被合并，我们对600个PR进行了定性分析，以构建拒绝模式的分层分类法。该分析通过揭示定量指标未涵盖的拒绝原因，如缺乏有意义的评审互动、重复PR、不受欢迎的功能实现以及代理与项目目标的不一致，补充了RQ1的定量发现。我们的研究结果突显了关键的社会技术因素和人机协作因素，这些因素对于提高未来代理工作流的成功率至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of AI coding agents in contributing to software projects through GitHub pull requests. By analyzing 33,000 PRs from five coding agents, the research identifies that documentation, CI, and build-related tasks have the highest merge success, while performance and bug-fix tasks are the least successful. Not-merged PRs are associated with larger code changes, more files modified, and CI/CD pipeline failures. A qualitative analysis of 600 PRs further reveals common rejection reasons such as lack of reviewer engagement, duplicate submissions, and misaligned agent behavior, providing insights into socio-technical and human-AI collaboration challenges.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在通过GitHub提交拉取请求（PR）时的失败情况。通过对五个编码代理提交的33,000个PR进行分析，发现文档、CI和构建更新任务的合并成功率最高，而性能和错误修复任务的合并成功率最低。未被合并的PR通常涉及较大的代码更改、修改更多文件，并且常无法通过项目的CI/CD流水线验证。对600个PR的定性分析进一步揭示了常见的拒绝模式，如缺乏审阅者互动、重复提交、不想要的功能实现以及代理与项目目标的不一致，为改善未来代理工作流提供了关键的社会技术与人机协作因素的洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</div>
<div class="meta-line">Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</div>
<div class="meta-line">First: 2025-07-07T18:17:06+00:00 · Latest: 2026-01-21T13:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05386v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.05386v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model&#x27;s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT&#x27;s gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化微调自然缓解持续微调中的遗忘</div>
<div class="mono" style="margin-top:8px">持续微调（CPT）是一种流行且有效的技术，用于将基础模型（如多模态大语言模型）适应于特定且不断变化的下游任务。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，但CPT中学习范式的根本作用仍被广泛忽视。本文对两种核心的微调范式——监督微调（SFT）和强化微调（RFT）进行了比较分析，探讨它们在持续微调过程中对知识保留的影响。我们的实验基于包含七个多样化多模态任务的基准数据集，使用Qwen2.5-VL-7B-Instruct作为持续微调的基础模型。研究得出两个重要发现：（1）在持续学习下游任务时，SFT会导致先前学习任务的灾难性遗忘，而RFT则能自然保留先前知识，其性能与多任务训练相当。（2）RFT成功保护并甚至增强了模型在标准基准（如MMMU和MMLU-Pro）上的通用知识，而SFT则严重损害了模型的通用能力。进一步分析表明，这种稳定性并非主要源于显式的机制，如KL惩罚或思维链推理，而是源于RFT中隐含的正则化机制。我们的理论分析表明，RFT的梯度更新自然受到奖励方差的缩放，作为一种数据依赖的正则化器，其内在机制能够保护先前获得的知识。最后，我们提出了一种基于rollout的实例过滤算法，以提高RFT的稳定性和效率。我们的全面研究展示了RFT作为持续微调的稳健范式的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the issue of catastrophic forgetting in continual post-training (CPT) of foundation models, particularly focusing on the effectiveness of different learning paradigms. It compares supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), finding that SFT leads to significant performance degradation on previously learned tasks, while RFT inherently preserves prior knowledge and achieves performance comparable to multi-task training. The study also reveals that RFT&#x27;s stability arises from an implicit regularization mechanism tied to reward variance, rather than explicit techniques like KL penalty. A rollout-based instance filtering algorithm is proposed to improve RFT&#x27;s efficiency and robustness in continual learning scenarios.</div>
<div class="mono" style="margin-top:8px">本文探讨了基础模型在持续微调过程中出现灾难性遗忘的问题，重点比较了强化微调（RFT）与监督微调（SFT）的效果。研究结果表明，RFT能够自然保留先前学习的知识，并在多任务基准上保持与多任务训练相当的性能，而SFT则会导致先前任务性能显著下降。在七个不同的多模态任务上使用Qwen2.5-VL-7B-Instruct进行实验发现，RFT的稳定性来源于与奖励方差相关的隐式正则化机制，而非显式的KL惩罚或思维链等方法。作者进一步提出了一种基于rollout的实例过滤算法，以提升RFT在持续学习中的效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</div>
<div class="meta-line">Authors: Xinyi Zheng, Yunze Liu, Chi-Hao Wu, Fan Zhang, Hao Zheng, Wenqi Zhou, Walterio W. Mayol-Cuevas, Junxiao Shen</div>
<div class="meta-line">First: 2026-01-21T11:32:24+00:00 · Latest: 2026-01-21T11:32:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialMem：基于度量锚定和快速检索的统一三维记忆</div>
<div class="mono" style="margin-top:8px">我们提出了SpatialMem，这是一个以记忆为中心的系统，将三维几何、语义和语言统一为一个可查询的表示。从随意捕捉的视角RGB视频出发，SpatialMem重建度量尺度的室内环境，检测结构化的三维锚点（墙壁、门、窗）作为第一层框架，并填充一个分层记忆，其中包含开放词汇的对象节点——将证据片段、视觉嵌入和两层文本描述与三维坐标链接——以实现紧凑存储和快速检索。该设计支持对空间关系（如距离、方向、可视性）的可解释推理，并支持无需专用传感器的下游任务，如语言引导的导航和对象检索。在三个真实室内场景中的实验表明，SpatialMem在增加杂乱和遮挡的情况下仍能保持强大的锚点-描述级导航完成度和分层检索准确性，为具身空间智能提供了一个高效且可扩展的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpatialMem is designed to integrate 3D geometry, semantics, and language into a unified, queryable representation for indoor environments. It reconstructs metrically scaled spaces from egocentric RGB videos, identifies structural 3D anchors as foundational elements, and builds a hierarchical memory system that links object evidence, visual embeddings, and textual descriptions to 3D coordinates. This approach enables efficient reasoning about spatial relationships and supports tasks like language-guided navigation and object retrieval. Experimental results on three real-life indoor scenes show that SpatialMem maintains high navigation completion and retrieval accuracy even in cluttered and occluded conditions.</div>
<div class="mono" style="margin-top:8px">SpatialMem的动机是构建一个统一的室内环境表示，将3D几何、语义和语言整合为可查询的单一结构。该系统从第一人称视角的RGB视频中重建度量尺度的3D场景，检测结构化的3D锚点作为基础框架，并构建一个包含开放词汇对象节点的层次化记忆，将视觉证据块、视觉嵌入和双层文本描述与3D坐标关联。实验结果表明，在增加杂乱和遮挡的情况下，SpatialMem仍能保持较高的导航完成度和层次化检索准确性，展示了其在具身空间智能任务中的高效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">3D Space as a Scratchpad for Editable Text-to-Image Generation</div>
<div class="meta-line">Authors: Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</div>
<div class="meta-line">First: 2026-01-21T02:40:19+00:00 · Latest: 2026-01-21T02:40:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://oindrilasaha.github.io/3DScratchpad/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D空间作为可编辑文本到图像生成的草稿纸</div>
<div class="mono" style="margin-top:8px">近期在大语言模型（LLMs）方面的进展表明，将中间思维外部化到显式的工空间（如思维链轨迹或工具增强推理）可以提升推理能力。然而，视觉语言模型（VLMs）缺乏类似的机制来进行空间推理，这限制了它们生成准确反映几何关系、物体身份和构图意图的图像的能力。我们引入了空间草稿纸的概念——一种连接语言意图与图像合成的3D推理基底。给定一个文本提示，我们的框架解析主体和背景元素，将它们实例化为可编辑的3D网格，并使用代理场景规划来确定位置、方向和视角选择。最终的3D布局通过保留身份的提示被渲染回图像域，使VLM能够生成空间一致且视觉连贯的输出。与以往基于2D布局的方法不同，我们的方法支持直观的3D编辑，并能可靠地传播到最终图像。在GenAI-Bench上，我们的方法在文本对齐方面实现了32%的提升，证明了显式3D推理对于精确可控图像生成的优势。我们的结果凸显了一种新的视觉-语言模型范式，即不仅在语言层面进行推理，也在空间层面进行推理。代码和可视化结果见 https://oindrilasaha.github.io/3DScratchpad/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of visual language models (VLMs) in generating images that accurately reflect spatial relationships and compositional intent. The authors propose a 3D spatial scratchpad as a novel mechanism to externalize reasoning, enabling VLMs to translate text prompts into editable 3D meshes and perform scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is then rendered back into the image domain with identity-preserving cues, leading to spatially consistent and visually coherent outputs. Experimental results show a 32% improvement in text alignment on GenAI-Bench compared to prior 2D layout-based methods, demonstrating the effectiveness of explicit 3D reasoning in enhancing controllability and precision in text-to-image generation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉语言模型（VLMs）在生成准确反映几何关系和构图意图的图像方面的局限性。作者提出了一种3D空间scratchpad机制，通过场景规划将文本提示转化为可编辑的3D网格。该框架随后将这些3D结构渲染回图像域，同时保持物体身份，从而生成空间一致且视觉连贯的输出。在GenAI-Bench上的实验结果显示，与基于2D布局的方法相比，文本对齐度提高了32%，证明了显式的3D推理在提升图像生成的可控性和精确性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics</div>
<div class="meta-line">Authors: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li</div>
<div class="meta-line">First: 2026-01-20T14:51:45+00:00 · Latest: 2026-01-20T14:51:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14027v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14027v1">PDF</a> · <a href="https://github.com/project-numina/numina-lean-agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Numina-Lean-Agent：一个开放且通用的代理推理系统用于形式化数学</div>
<div class="mono" style="margin-top:8px">代理系统最近已成为形式化定理证明的主导范式，通过协调多个模型和工具实现了强大的性能。然而，现有方法通常依赖于任务特定的流水线和训练过的形式化证明器，限制了其灵活性和可复现性。在本文中，我们提出了一种范式，即直接使用通用编码代理作为形式化数学推理器。该范式受到以下三点启发：(1) 通用编码代理为超越证明的多样化推理任务提供了自然的接口；(2) 仅通过替换底层基础模型即可提升性能，而无需训练；(3) MCP支持灵活扩展和自主调用专用工具，避免了复杂的设计。基于这一范式，我们引入了Numina-Lean-Agent，它结合了Claude Code和Numina-Lean-MCP，以实现与Lean的自主交互、相关定理的检索、非形式化证明以及辅助推理工具。使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent解决了Putnam 2025中的所有问题（12/12），与最佳闭源系统表现相当。除了基准评估，我们还通过与数学家互动，成功地对Brascamp-Lieb定理进行了形式化，进一步证明了其通用性。我们将在https://github.com/project-numina/numina-lean-agent上发布Numina-Lean-Agent及其所有解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to develop a more flexible and reproducible agentic reasoning system for formal mathematics by moving away from task-specific pipelines and trained provers. The proposed method utilizes a general coding agent as a formal math reasoner, combining it with the Numina-Lean-MCP framework to enable autonomous interaction with Lean, theorem retrieval, and the use of auxiliary reasoning tools. The key experimental result shows that Numina-Lean-Agent, using Claude Opus 4.5 as its base model, solves all 12 problems in the Putnam 2025 competition, matching the performance of the best closed-source systems. Additionally, it successfully formalizes the Brascamp-Lieb theorem through interaction with mathematicians, demonstrating its broad applicability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过摒弃任务特定的流水线和训练过的形式证明器，开发一个更加灵活和可复现的形式数学代理推理系统。提出的方法Numina-Lean-Agent结合了Claude Code和Numina-Lean-MCP，能够自主与Lean交互、检索相关定理并使用辅助推理工具。主要实验结果表明，Numina-Lean-Agent解决了Putnam 2025竞赛中的全部12道题目，性能与最佳闭源系统相当，并通过与数学家的互动成功形式化了Brascamp-Lieb定理。</div>
</details>
</div>
<div class="card">
<div class="title">CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments</div>
<div class="meta-line">Authors: Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li</div>
<div class="meta-line">First: 2026-01-20T13:44:02+00:00 · Latest: 2026-01-20T13:44:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14339v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CityCube：在城市环境中评估视觉-语言模型的跨视角空间推理能力</div>
<div class="mono" style="margin-top:8px">跨视角空间推理对于具身AI至关重要，是空间理解、心理模拟和复杂环境中规划的基础。现有基准主要关注室内或街道场景，忽略了开放城市空间中独特的挑战，这些场景具有丰富的语义、复杂的几何结构和多样的视角变化。为了解决这一问题，我们引入CityCube，这是一个系统性的基准，旨在评估当前视觉-语言模型（VLMs）在城市环境中的跨视角推理能力。CityCube整合了四种视角动态，以模拟相机运动，并涵盖了从多种平台（如车辆、无人机和卫星）获取的广泛视角。为了全面评估，它包含5,022个精心标注的多视角问答对，分为五个认知维度和三种空间关系表达。对33个VLMs的全面评估表明，其性能与人类存在显著差距：即使是大规模模型也难以超过54.1%的准确率，仍比人类表现低34.2%。相比之下，小型模型经过微调后可达到超过60.0%的准确率，突显了我们基准的重要性。进一步分析表明，VLMs与类人推理之间存在任务相关性和根本性的认知差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CityCube, a benchmark designed to evaluate cross-view spatial reasoning in vision-language models (VLMs) within urban environments. Motivated by the limitations of existing benchmarks that focus on indoor or street scenes, CityCube incorporates four viewpoint dynamics and multiple platforms to simulate diverse urban perspectives. The benchmark includes 5,022 annotated multi-view QA pairs organized into five cognitive dimensions and three spatial relation expressions. Evaluation of 33 VLMs shows that even large models achieve only 54.1% accuracy, which is 34.2% lower than human performance, while small-scale fine-tuned models surpass this threshold, underscoring the importance of the benchmark for assessing and improving VLM capabilities in complex urban settings.</div>
<div class="mono" style="margin-top:8px">该研究提出了CityCube，这是一个用于评估视觉语言模型（VLM）在城市环境中跨视角空间推理能力的基准。现有基准主要关注室内或街道场景，忽略了开放城市空间中丰富的语义、复杂的几何结构和视角变化带来的独特挑战。CityCube通过整合四种视角动态和多种平台（如车辆、无人机和卫星）来模拟多样化的城市视角，并包含5,022个精心标注的多视角问答对，涵盖五个认知维度和三种空间关系表达。对33个VLM的全面评估显示，即使大型模型也只能达到54.1%的准确率，比人类表现低34.2%，而小型微调模型则超过这一水平，突显了该基准的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍存在分裂：视觉-语言模型（如CLIP）在全局语义对齐方面表现优异，但缺乏空间精度；而自监督方法（如MAE、DINO）能够捕捉复杂的局部结构，但在高层次语义上下文方面存在困难。我们认为这些范式本质上是互补的，可以整合到一个有原则的多任务框架中，并通过密集空间监督进一步增强。我们提出了MTV，一个联合优化视觉-语言对比、自监督和密集空间目标的多任务视觉预训练框架。为减少对人工标注的依赖，我们利用高容量的&quot;专家&quot;模型——如Depth Anything V2和OWLv2——大规模合成密集且结构化的伪标签。除了框架本身，我们还系统地研究了多任务视觉学习的机制，分析了：(i) 每个目标的边际增益，(ii) 任务间的协同效应与干扰，以及(iii) 在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了&quot;两者兼顾&quot;的性能，显著提升了细粒度空间推理能力，而不会损害全局语义理解。我们的发现表明，借助高质量的伪监督，多任务学习是一条通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current visual representation learning approaches by proposing a multi-task framework that integrates vision-language contrastive learning, self-supervised learning, and dense spatial supervision. The motivation stems from the complementary strengths of vision-language models and self-supervised methods, which respectively excel in global semantics and local structures. The proposed MTV framework jointly optimizes a shared backbone across these tasks, using expert models to generate pseudo-labels for dense spatial supervision. Experimental results show that MTV achieves superior performance in fine-grained spatial reasoning while maintaining strong global semantic understanding, highlighting the effectiveness of multi-task learning with high-quality pseudo-supervision.</div>
<div class="mono" style="margin-top:8px">本文针对当前视觉表征学习方法的局限性，提出了一种结合视觉-语言模型与自监督方法优势的多任务框架MTV。该框架通过视觉-语言对比、自监督和密集空间任务共同优化共享主干网络，并利用高容量专家模型如Depth Anything V2和OWLv2生成大规模伪标签。实验结果表明，MTV在细粒度空间推理和全局语义理解方面均表现出色，验证了通过高质量伪监督进行多任务学习的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories</div>
<div class="meta-line">Authors: Aditya Bharat Soni, Rajat Ghosh, Vaishnavi Bhargava, Valerie Chen, Debojyoti Dutta</div>
<div class="meta-line">First: 2026-01-20T08:10:56+00:00 · Latest: 2026-01-20T08:10:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- &quot;test first, write code later&quot;, and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Tester：在真实仓库中使用开源LLM训练问题复现测试</div>
<div class="mono" style="margin-top:8px">软件测试对于确保软件系统的正确性和可靠性至关重要。从自然语言问题描述中自动生成问题复现测试可以简化根本原因分析，提高开发人员的生产力，促进测试驱动开发——&quot;先测试后编写代码&quot;，并且可用于提升自动问题解决系统（如编码代理）的效果。现有的方法主要依赖于闭源LLM，对开源模型的探索有限。为了解决这一问题，我们提出了SWE-Tester——一种新颖的训练开源LLM生成问题复现测试的流水线。首先，我们从2600个开源GitHub仓库中精选出41000个高质量实例作为训练数据集，并用其训练不同规模和家族的LLM。微调后的模型在SWT-Bench Verified上实现了高达10\%的成功率提升和21\%的变更覆盖率提升。进一步分析表明，随着推理时的计算量增加、数据量增多和模型规模扩大，性能有持续提升。这些结果突显了我们框架在推动该领域开源LLM发展方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to improve the ability of open-source large language models (LLMs) to generate effective issue reproduction tests from natural language descriptions, which is essential for software testing and debugging. SWE-Tester introduces a training pipeline that uses a curated dataset of 41K instances from 2.6K GitHub repositories to fine-tune various open-source LLMs. Experimental results on SWT-Bench Verified show that the fine-tuned models achieve up to 10\% higher success rate and 21\% greater change coverage compared to baseline methods, demonstrating the effectiveness of training open-source models for this task.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升开源大语言模型（LLMs）在从自然语言描述生成有效问题复现测试方面的能力，这对于软件测试和调试至关重要。提出的SWE-Tester方法引入了一个训练开源LLMs的流程，利用从2.6K个GitHub开源仓库中精心整理的41K个实例构建高质量训练数据集。经过微调的模型在SWT-Bench Verified基准上表现出显著提升，成功率达到最高提升10\%，变更覆盖率提高21\%。这些结果表明，增加模型规模、数据量以及推理时的计算资源能够持续提升模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis</div>
<div class="meta-line">Authors: Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang, Zhongxue Gan</div>
<div class="meta-line">First: 2026-01-20T06:50:40+00:00 · Latest: 2026-01-20T06:50:40+00:00</div>
<div class="meta-line">Comments: This study has been accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为前时空解耦：面向多模态情感分析的解耦表征学习</div>
<div class="mono" style="margin-top:8px">多模态情感分析融合语言、视觉和听觉模态。主流方法基于模态不变和模态特异性因子分解或复杂融合，仍然依赖于时空混合建模。这忽略了时空异质性，导致时空信息不对称，从而限制了性能。因此，我们提出TSDA（行为前时空解耦），在任何模态交互之前，显式地将每个模态解耦为时间动态和空间结构上下文。对于每个模态，一个时间编码器和一个空间编码器将信号投影到独立的时间和空间空间中。因子一致的跨模态对齐随后仅将时间特征与跨模态的时间对应特征对齐，仅将空间特征与跨模态的空间对应特征对齐。因子特异性监督和去相关正则化减少了跨因子泄漏，同时保留了互补性。随后，一个门控再耦合模块将对齐的流重新耦合以执行任务。大量实验表明，TSDA优于基线方法。消融分析验证了设计的必要性和可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of existing multimodal sentiment analysis approaches that fail to account for the spatiotemporal heterogeneity in data, leading to information asymmetry and suboptimal performance. The proposed method, TSDA, decouples each modality into temporal and spatial components before any interaction, using separate encoders for each. Temporal features are aligned across modalities based on their temporal counterparts, while spatial features are aligned based on their spatial counterparts. Factor-specific supervision and decorrelation regularization are introduced to reduce cross-factor leakage and maintain feature complementarity. A gated recouple module is then used to combine the aligned features for the final task. Experimental results demonstrate that TSDA achieves superior performance compared to existing baselines, and ablation studies validate the effectiveness and interpretability of its design.</div>
<div class="mono" style="margin-top:8px">本研究针对现有多模态情感分析方法未能充分考虑时空异质性的问题，提出了一种新的方法TSDA。该方法在任何交互之前，将每种模态分解为时间动态和空间结构上下文，分别使用时间编码器和空间编码器进行处理。通过将时间特征与跨模态的时间特征对齐，空间特征与跨模态的空间特征对齐，实现模态间特征的精准匹配。引入了因子特定监督和去相关正则化以减少跨因子信息泄露并保持特征互补性。随后，通过门控再耦合模块整合对齐后的特征流。实验结果表明，TSDA在多个基准数据集上均优于现有方法，消融实验进一步验证了其设计的必要性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</div>
<div class="meta-line">Authors: Peter A. Massih, Eric Cosatto</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-19T21:14:34+00:00 · Latest: 2026-01-19T21:14:34+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素级精确推理：用于定量地理空间分析的QVLM架构和SQuID数据集</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLMs）在定量空间推理方面表现不佳，因为其架构会破坏进行计数和测量所需的像素级信息。视觉编码器通过补丁嵌入压缩图像，导致空间索引的丢失，从而无法实现精确的像素级追踪。我们提出了两个贡献以解决这一根本限制。首先，我们引入了SQuID（卫星定量智能数据集），这是一个包含2000对卫星图像问答对的基准数据集，具有数值范围和分类答案，旨在评估定量空间推理能力。该数据集跨越三个难度层级，其注释由人类标签及其学习到的变异性自动生成。其次，我们提出了QVLM（定量视觉-语言模型），这是一种代码生成架构，通过将语言理解与视觉分析解耦，保持像素精度。QVLM不将图像编码为嵌入，而是生成可执行代码，首先调用分割模型获取像素级掩膜，然后直接在这些掩膜上进行操作，从而在整个推理过程中保留空间索引。我们的实验表明，使用GPT-5作为代码生成器的QVLM在SQuID数据集上达到了42.0%的准确率，而使用图像-问题对提示的VLM仅达到28.1%。我们的工作表明，对于定量空间推理任务，架构解耦能够提高定量任务的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of current Vision-Language Models (VLMs) in quantitative spatial reasoning, where they lose pixel-level information essential for counting and measurements. The authors introduce SQuID, a new benchmark dataset with 2,000 satellite image Question-Answer pairs, featuring both numerical and categorical answers across three difficulty levels. They also propose QVLM, a novel architecture that decouples language understanding from visual analysis by generating executable code to process pixel-level masks, preserving spatial indexing throughout the reasoning process. Experimental results show that QVLM using GPT-5 as the coder achieves 42.0% accuracy on SQuID, significantly outperforming traditional VLMs with a 28.1% accuracy when prompted with image-question pairs.</div>
<div class="mono" style="margin-top:8px">本文针对当前视觉语言模型（VLMs）在定量空间推理中的不足，指出其因图像编码过程丢失像素级信息而难以进行计数和测量。为此，作者提出了SQuID数据集，包含2000对卫星图像与问题-答案对，涵盖数值和分类答案，按难度分为三个层级。同时，他们设计了QVLM架构，通过将语言理解与视觉分析解耦，并生成可执行代码来处理由分割模型获得的像素级掩码，从而保持空间精度。实验结果显示，使用GPT-5作为编码器的QVLM在SQuID数据集上达到了42.0%的准确率，显著优于传统VLMs的28.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility</div>
<div class="meta-line">Authors: Nickil Maveli, Antonio Vergari, Shay B. Cohen</div>
<div class="meta-line">First: 2026-01-19T21:09:48+00:00 · Latest: 2026-01-19T21:09:48+00:00</div>
<div class="meta-line">Comments: 32 pages (preprint)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13398v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13398v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型能否进行压缩（和解压）？通过可逆性评估代码理解和执行</div>
<div class="mono" style="margin-top:8px">大语言模型在代码基准测试中表现出色，但代码的往返执行揭示了它们在正向和反向执行中保持一致推理能力的局限性。我们提出了RoundTripCodeEval（RTCE），一个包含四个不同代码执行推理任务的全面基准，旨在严格测试往返一致性。RTCE提供了一种无需执行的精确匹配评估方式，用于衡量双射保真度，评估模型在不同算法和方向上编码与解码操作之间是否保持一致的一一映射关系。我们系统地使用零样本提示、执行轨迹上的监督微调以及自我反思机制对最先进的代码大语言模型进行评估。每种方法都带来了一定的改进，但均未完全弥合差距，表明当前的大语言模型在真正的往返一致性方面存在困难，这表明它们缺乏进行可信代码推理所需的内部一致性。RTCE揭示了若干新的、此前未被测量的见解，这些见解无法通过现有的输入输出预测、执行推理或往返自然语言基准测试所捕捉。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the ability of large language models (LLMs) to maintain consistent reasoning during code compression and decompression, highlighting limitations in their round-trip execution capabilities. The authors introduce RoundTripCodeEval (RTCE), a benchmark that evaluates bijection fidelity through four distinct code execution tasks, focusing on the consistency between encoding and decoding processes. Experimental results show that while zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms yield modest improvements, none of them fully achieve round-trip consistency, suggesting that current LLMs lack the internal coherence needed for reliable code reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）在代码压缩与解压过程中保持一致推理能力的问题，重点关注往返一致性。作者提出了RoundTripCodeEval（RTCE）基准，通过四个不同的任务评估代码理解和执行的双向映射准确性。他们使用零样本提示、执行轨迹上的监督微调和自我反思机制对最先进的代码LLMs进行了系统评估，发现这些方法虽然带来了一定改进，但均未能实现真正的往返一致性，表明当前模型缺乏进行可靠代码推理所需的内部一致性。</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models</div>
<div class="meta-line">Authors: Samuel Cyrenius Anderson</div>
<div class="meta-line">First: 2026-01-19T19:53:37+00:00 · Latest: 2026-01-19T19:53:37+00:00</div>
<div class="meta-line">Comments: 34 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -&gt; 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -&gt; 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维的几何学：大规模语言模型中规模如何重构推理</div>
<div class="mono" style="margin-top:8px">规模并不均匀地提升推理能力，而是重构了推理。通过分析四个领域（法律、科学、代码、数学）和两个规模（80亿、700亿参数）下超过25,000个思维链轨迹，我们发现神经网络的扩展定律触发了领域特异性的相变，而非统一的能力提升。法律推理经历结晶化：表征维度减少了45%（d95: 501 -&gt; 274），轨迹对齐度增加了31%，并实现了10倍的语义分离。科学和数学推理保持液态——即使参数增加9倍，其几何结构仍保持不变。代码推理则形成了一种离散的战略模式格子（轮廓系数：0.13 -&gt; 0.42）。这种几何结构预示了可学习性。我们引入了神经推理算子——从初始到终端隐藏状态的学习映射。在结晶化的法律推理中，我们的算子通过探针解码在未见任务上达到了63.6%的准确率，无需遍历中间状态即可预测推理终点。我们进一步识别出一种跨领域和规模的普遍振荡特征（相干度 ~ -0.4），表明注意力层和前馈层通过相反的动力学驱动推理。这些发现表明，思维的成本并非由任务难度决定，而是由流形几何决定——这为推理加速提供了蓝图，前提是拓扑结构允许。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how scaling affects reasoning in large language models by analyzing 25,000+ chain-of-thought trajectories across four domains and two parameter scales. It reveals that neural scaling laws induce domain-specific phase transitions, with legal reasoning transitioning to a crystalline state, characterized by reduced dimensionality and increased trajectory alignment. In contrast, scientific and mathematical reasoning remain in a liquid state, maintaining geometric invariance despite parameter increases. Code reasoning forms a lattice of strategic modes, showing improved structure. The research introduces Neural Reasoning Operators, which predict reasoning endpoints with high accuracy without traversing intermediate states. A universal oscillatory signature is also identified, suggesting that attention and feedforward layers contribute to reasoning through opposing dynamics. These findings indicate that the complexity of reasoning is influenced by manifold geometry rather than task difficulty, providing insights for optimizing inference efficiency.</div>
<div class="mono" style="margin-top:8px">该研究探讨了大语言模型（LLMs）规模增加对其在不同领域推理能力的影响。通过对法律、科学、代码和数学四个领域中超过25,000条思维链轨迹的分析，研究发现规模扩展会引发领域特异性的相变，而非统一的能力提升。法律推理表现出‘结晶化’特征，其表征维度减少45%，轨迹对齐度提高31%，且能效提升10倍；科学和数学推理则保持‘液态’状态，几何结构不变。代码推理则形成‘策略格子’结构。研究引入了神经推理算子，该算子通过探针解码实现了高准确率的推理终点预测，无需经过中间状态。此外，研究还发现一个跨领域和规模的普遍振荡特征，表明注意力和前馈层通过相反的动力学机制驱动推理过程。这些发现表明，推理成本由流形几何决定，而非任务难度，为推理加速提供了蓝图。</div>
</details>
</div>
<div class="card">
<div class="title">CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</div>
<div class="meta-line">Authors: Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</div>
<div class="meta-line">First: 2026-01-19T18:59:44+00:00 · Latest: 2026-01-19T18:59:44+00:00</div>
<div class="meta-line">Comments: Code is available: https://github.com/CausalSpatial/CausalSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13304v1">PDF</a> · <a href="https://github.com/CausalSpatial/CausalSpatial">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer &quot;what-if&quot; questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalSpatial：面向对象因果空间推理的基准测试</div>
<div class="mono" style="margin-top:8px">人类可以观察静态场景并瞬间预测接下来会发生什么——移动这个物体是否会导致碰撞？我们称这种能力为因果空间推理。然而，当前的多模态大语言模型（MLLMs）无法做到这一点，因为它们主要局限于静态空间感知，难以回答三维场景中的“假设情景”问题。我们引入了CausalSpatial，这是一个诊断基准，用于评估模型是否能在四个任务（碰撞、兼容性、遮挡和轨迹）中预测物体运动的后果。结果揭示了一个严重差距：人类得分84%，而GPT-5仅达到54%。为什么MLLMs会失败？我们的分析发现其根本缺陷在于：模型过度依赖文本链式推理，脱离了视觉证据，产生流畅但缺乏空间基础的幻觉。为了解决这一问题，我们提出了因果物体世界模型（COW），该框架通过生成假设动态的视频来外部化模拟过程。借助明确的因果视觉线索，COW使模型能够基于物理现实而非语言先验进行推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CausalSpatial, a benchmark designed to assess the ability of models to predict the consequences of object motions in 3D scenes, focusing on causal spatial reasoning. Current multimodal large language models (MLLMs) struggle with such tasks due to their reliance on textual reasoning rather than visual grounding. The benchmark evaluates models across four tasks: Collision, Compatibility, Occlusion, and Trajectory, revealing a significant performance gap between humans (84%) and GPT-5 (54%). The proposed Causal Object World model (COW) addresses this by generating videos of hypothetical dynamics, allowing models to base their reasoning on explicit visual cues of causality rather than linguistic priors.</div>
<div class="mono" style="margin-top:8px">本研究提出了CausalSpatial基准，用于评估模型在物体中心因果空间推理方面的能力。研究动机源于人类能够直观预测静态场景中物体移动后的结果，而当前的多模态大语言模型（MLLMs）由于依赖静态空间感知，在此类任务上表现不佳。该基准包含四个任务：碰撞、兼容性、遮挡和轨迹，结果显示人类得分高达84%，而GPT-5仅54%，表明存在显著性能差距。分析指出，MLLMs的失败源于过度依赖文本推理而缺乏视觉证据的支撑，导致空间推理不准确。为此，研究者提出了因果物体世界模型（COW），通过生成假设动态的视频来使模型基于明确的视觉因果线索进行推理。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-19T18:48:37+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能以找到共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的超过600个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道被模糊、时机不当和不准确的信息堵塞；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理往往对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CooperBench, a benchmark designed to evaluate the coordination capabilities of AI coding agents in collaborative tasks. The motivation stems from the observation that while AI agents excel in individual coding tasks, they struggle to function effectively as teammates due to a lack of social intelligence. The benchmark consists of over 600 tasks across four programming languages and twelve libraries, where two agents are assigned conflicting features that require coordination to resolve. Experimental results show that state-of-the-art coding agents achieve on average 30% lower success rates when working together compared to when they perform tasks individually, highlighting the &#x27;curse of coordination&#x27;. The analysis identifies three main issues: ineffective communication, commitment deviation, and incorrect expectations about others&#x27; actions. The study also observes some emergent coordination behaviors, such as role division and negotiation, through large-scale simulations.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨当前AI编码代理在协作环境中的局限性。作者提出了CooperBench，这是一个包含超过600个协作编码任务的基准测试平台，涵盖四种编程语言和十二个库，用于评估代理的协调能力。实验结果显示，最先进的编码代理在协作时平均成功率比单独完成任务低30%，突显了‘协调诅咒’的问题，并指出了关键问题，如沟通不畅、缺乏承诺以及预期不一致。此外，研究还观察到一些罕见但有趣的协作行为，如角色分工、资源共享和协商，表明需要进一步提升AI代理的社会智能。</div>
</details>
</div>
<div class="card">
<div class="title">KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</div>
<div class="meta-line">Authors: Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong</div>
<div class="meta-line">First: 2026-01-19T17:20:16+00:00 · Latest: 2026-01-19T17:20:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13240v1">PDF</a> · <a href="https://github.com/jiangxxxue/KOCO-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&amp;A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KOCO-BENCH：大型语言模型能否在软件开发中利用领域知识？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在通用编程方面表现出色，但在领域特定的软件开发中表现不佳，因此需要领域专门化方法，使LLMs能够学习和利用领域知识和数据。然而，现有的领域特定代码基准无法评估领域专门化方法的有效性，这些基准侧重于评估LLMs已有的知识，而非它们如何获取和应用新知识，缺乏用于开发领域专门化方法的显式知识语料库。为此，我们提出了KOCO-BENCH，这是一个全新的基准，用于在现实世界软件开发中评估领域专门化方法。KOCO-BENCH包含6个新兴领域、11个软件框架和25个项目，配有精心整理的知识语料库，并包含多粒度评估任务，包括领域代码生成（从函数级到项目级，配有严格的测试套件）和领域知识理解（通过多项选择题问答）。与以往仅提供测试集供直接评估的基准不同，KOCO-BENCH要求从知识语料库中获取并应用多样化的领域知识（如API、规则、约束等）以解决评估任务。我们的评估结果表明，KOCO-BENCH对最先进的LLMs提出了重大挑战。即使应用了领域专门化方法（如SFT、RAG、kNN-LM），改进仍然有限。表现最好的编码代理Claude Code仅达到34.2%，突显了对更有效的领域专门化方法的迫切需求。我们发布了KOCO-BENCH、评估代码和基线模型，以促进进一步研究，详见https://github.com/jiangxxxue/KOCO-bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of evaluating domain specialization methods for large language models (LLMs) in software development. Existing benchmarks are insufficient as they focus on assessing existing knowledge rather than how LLMs acquire and apply new domain knowledge. To overcome this, the authors introduce KOCO-BENCH, a new benchmark that includes curated knowledge corpora and multi-granularity tasks such as domain code generation and knowledge understanding. Experimental results show that even with domain specialization techniques like SFT, RAG, and kNN-LM, LLMs perform poorly, with the best-performing model achieving only 34.2%. This highlights the need for more effective methods to enhance LLMs&#x27; ability to leverage domain knowledge in real-world software development.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决大型语言模型（LLMs）在软件开发中有效利用领域知识的挑战，因为现有基准无法评估LLMs如何获取和应用新知识，而只是衡量其已有的知识。KOCO-BENCH被提出作为一项新型基准，包含精心整理的知识语料库以及多粒度任务，如领域代码生成和知识理解。实验结果表明，即使应用了领域专业化技术（如SFT、RAG和kNN-LM），当前最先进的LLMs表现仍然不佳，最佳表现的编码代理仅达到34.2%。这凸显了开发更有效领域适应方法的迫切需求。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-01-19T13:13:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v1">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即在二维感知之外解释几何、视角和空间关系的能力。尽管最近的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入了Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用三维重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上平均提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使模型能够选择信息性视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是实现多模态代理中更灵活且类似人类的三维推理的一种可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance spatial reasoning capabilities in vision large models (VLMs), which are currently limited to 2D perception and struggle with genuine 3D understanding. Think3D introduces a framework that enables VLM agents to interact with 3D space by integrating 3D reconstruction models to generate point clouds and camera poses from images or videos. This allows agents to perform camera-based operations and switch between ego and global views, facilitating a 3D chain-of-thought process. The framework significantly improves spatial reasoning performance without additional training, achieving average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. Furthermore, reinforcement learning policies enhance smaller models&#x27; ability to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉语言模型（VLM）的空间推理能力，使其能够与三维空间进行交互。Think3D提出了一种框架，允许VLM代理通过基于相机的操作和视角切换（包括第一人称与全局视角）来进行三维推理，利用三维重建模型生成点云和相机姿态。实验结果表明，在无需额外训练的情况下，Think3D显著提升了空间推理性能，平均在BLINK Multi-view和MindCube上提升7.8%，在VSI-Bench上提升4.7%。此外，通过强化学习策略，小型模型在视角和操作选择上的性能也得到显著提升，工具使用带来的收益从0.7%增加到6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">Knot So Simple: A Minimalistic Environment for Spatial Reasoning</div>
<div class="meta-line">Authors: Zizhao Chen, Yoav Artzi</div>
<div class="meta-line">First: 2025-05-23T15:34:08+00:00 · Latest: 2026-01-18T19:17:38+00:00</div>
<div class="meta-line">Comments: Fix camera ready footer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18028v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18028v3">PDF</a> · <a href="https://github.com/lil-lab/knotgym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Knot So Simple：一个用于空间推理的极简环境</div>
<div class="mono" style="margin-top:8px">我们提出了KnotGym，一个用于复杂空间推理和操作的交互式环境。KnotGym包含一系列以目标为导向的绳子操作任务，复杂度各不相同，所有任务均基于纯图像观察进行操作。任务的复杂度沿一个清晰且可量化的轴定义，基于绳结的交叉点数量，从而形成自然的泛化测试。KnotGym具有简单的观察空间，便于可扩展开发，同时突出了在整合敏锐感知、空间推理和基于环境的操作中面临的核心挑战。我们评估了多种方法，包括基于模型的强化学习、模型预测控制和链式思维推理，并展示了KnotGym所呈现的挑战。KnotGym可在https://github.com/lil-lab/knotgym获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces KnotGym, a minimalistic environment designed to evaluate spatial reasoning and manipulation capabilities through complex knot-tying tasks. The environment is built around a clear complexity metric based on the number of knot crossings, enabling a structured progression of task difficulty. By using only image observations, KnotGym emphasizes the integration of perception, reasoning, and action, and the authors assess various approaches such as model-based reinforcement learning, model-predictive control, and chain-of-thought reasoning, highlighting the challenges these methods face in solving the tasks.</div>
<div class="mono" style="margin-top:8px">本研究提出了KnotGym，一个用于评估复杂空间推理与操作的交互式环境。研究动机源于对可扩展且基于感知基准的需要，KnotGym通过不同复杂度的绳结任务进行设计，复杂度由绳结交叉点数量决定，从而实现难度的自然递进。该环境仅依赖图像观测，突出了感知、推理与操作的整合挑战。实验结果表明，现有的方法，包括基于模型的强化学习、模型预测控制和链式思维推理，在解决这些任务时仍面临效率和准确性的显著困难。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jihie Kim, Jean Oh</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-18T11:03:44+00:00</div>
<div class="meta-line">Comments: Corrected author order in metadata; manuscript unchanged</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未找到安全（404）：基于大语言模型的机器人决策中的隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键环境中，人工智能系统的一个错误可能导致生命损失。随着大语言模型（LLMs）在机器人决策中的应用日益广泛，风险的物理维度也在扩大；一个错误的指令可能直接危及人类安全。本文旨在系统评估LLM在即使微小错误也可能导致灾难的场景中的表现。通过一个火灾疏散场景的定性评估，我们识别了基于LLM的决策中的关键失败案例。基于这些案例，我们设计了七个任务用于定量评估，分为：完整信息任务、不完整信息任务和安全导向空间推理（SOSR）任务。完整信息任务使用ASCII地图以减少解释歧义，并将空间推理与视觉处理分离。不完整信息任务要求模型推断缺失的上下文，测试其空间连续性与幻觉之间的区别。SOSR任务使用自然语言评估在生命威胁情境下的安全决策能力。我们对各种LLM和视觉-语言模型（VLMs）在这些任务上的表现进行了基准测试。除了整体表现外，我们还分析了1%失败率的潜在影响，强调&quot;罕见&quot;错误如何演变为灾难性后果。结果揭示了严重的漏洞：一些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，模型指示机器人向危险区域移动而非紧急出口。我们的研究得出令人警醒的结论：当前的大语言模型尚未准备好直接部署在安全关键系统中。在机器人领域，99%的准确率是危险的误导，因为它意味着每100次执行中可能有一次导致灾难性伤害。我们证明了即使是最先进的模型也无法保证安全，完全依赖它们会带来不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the risks associated with using Large Language Models (LLMs) in safety-critical robotics decision-making, emphasizing that even minor errors can lead to severe consequences. The authors designed seven tasks to evaluate LLMs and Vision-Language Models (VLMs) in different scenarios, including Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR), with the latter focusing on safe navigation in life-threatening situations. The results show that several models failed entirely in ASCII navigation tasks, and in a simulated fire drill, some models directed robots toward hazardous areas instead of emergency exits, highlighting the critical need for more robust safety mechanisms before deploying such systems in real-world applications.</div>
<div class="mono" style="margin-top:8px">本文探讨了在机器人决策中使用大型语言模型（LLMs）所伴随的安全风险，强调在关键环境中即使微小的错误也可能导致严重后果。作者设计了七个任务来评估LLMs和视觉语言模型（VLMs）在不同场景下的表现，包括完整信息、不完整信息以及安全导向空间推理（SOSR）任务。实验结果显示了显著的漏洞，某些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，部分模型指示机器人向危险区域移动而非紧急出口。这些结果突显了当前LLMs在安全关键应用中的不可靠性，表明1%的失败率可能引发灾难性后果。</div>
</details>
</div>
<div class="card">
<div class="title">CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks</div>
<div class="meta-line">Authors: Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, Xiangyu Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-07-03T01:35:58+00:00 · Latest: 2026-01-17T19:21:17+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Datasets &amp; Benchmarks Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05269v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05269v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models&#x27; ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs&#x27; code reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoRe: 通过静态分析任务对LLM代码推理能力进行基准测试</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已被广泛应用于软件工程的多个领域，如代码生成、程序修复和漏洞检测。这些应用需要理解超越表面代码模式的深层次内容：值传播、控制流以及程序元素之间的相互依赖。然而，现有的基准测试主要评估端到端结果，例如代码是否被正确修复或生成，而忽略了模型在程序语义推理方面的能力。本文提出了CoRe，一个高质量、经过人工验证的基准测试，旨在评估LLMs在基础静态分析任务上的表现。CoRe包含12,553个任务实例，涵盖C/C++、Java和Python编写的程序中的数据依赖、控制依赖和信息流。为确保语义多样性和推理复杂性，我们提出了一种语义感知的多样化采样策略，根据结构覆盖率和依赖深度选择目标和任务实例。我们评估了10种主流LLMs，并表明尽管它们在识别依赖关系方面表现良好，但在需要更深层次语义理解和多步骤推理的任务上仍存在困难。我们进一步进行了定性分析，揭示了诸如复杂控制结构和反向依赖模式等关键挑战，为提升LLMs的代码推理能力提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces CoRe, a benchmark designed to assess the code reasoning capabilities of large language models (LLMs) through static analysis tasks. The motivation stems from the observation that current benchmarks focus on end-to-end outcomes rather than the underlying semantic understanding required for tasks like value propagation and control flow analysis. CoRe includes 12,553 task instances across C/C++, Java, and Python, and employs a semantics-aware sampling strategy to ensure diversity and complexity. Evaluation of ten mainstream LLMs reveals that while they excel at identifying basic dependencies, they struggle with more complex semantic reasoning tasks, highlighting challenges such as handling backward dependencies and intricate control structures.</div>
<div class="mono" style="margin-top:8px">本文提出了CoRe基准，旨在通过静态分析任务评估大语言模型（LLMs）的代码推理能力。研究动机源于现有基准主要关注端到端结果，而忽视了对程序语义理解的深入评估。方法包括构建涵盖C/C++、Java和Python的12,553个任务实例，采用语义感知的多样化采样策略以确保结构覆盖和依赖深度。实验结果表明，尽管LLMs在识别基本依赖关系上表现良好，但在需要更深层次语义理解和多步推理的任务上仍存在困难，揭示了如反向依赖和复杂控制结构等关键挑战。</div>
</details>
</div>
<div class="card">
<div class="title">A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation</div>
<div class="meta-line">Authors: Yifei Sun, Yongan Li, A. K. Qin, Sicheng Hou, Tamas Pflanzner</div>
<div class="meta-line">First: 2026-01-16T21:36:04+00:00 · Latest: 2026-01-16T21:36:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11792v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种具有细粒度难度指导的自进化多角色协作框架用于创新数学问题生成</div>
<div class="mono" style="margin-top:8px">数学问题生成（MPG）是智能教育领域的重要研究方向。近年来，大语言模型（LLMs）的快速发展为问题生成任务带来了新的技术方法。尽管现有LLMs可以实现较高的正确率，但通常缺乏创新性且判别能力较差。本文提出创新数学问题生成（IMPG）任务。为了解决IMPG任务，本文提出了一种具有细粒度难度指导的自进化多角色协作框架。首先，构建包含采样器、生成器、评估器、状态机和记忆的多角色协作机制，通过自我评估和外部反馈的迭代优化确保生成问题的正确性。其次，引入改进的难度模型以量化难度并提供细粒度指导。我们采用数据驱动的关联引导路径采样（DAPS）算法来增强采样编码的语义合理性。第三，我们构建了HSM3K-CN数据集，包含高质量的高中数学问题。采用多阶段训练流程，包括持续预训练（CPT）、监督微调（SFT）和组相对策略优化（GRPO），以提升基础模型的生成和评估能力。最后，通过蒸馏将专家模型的评估能力转移到学徒模型，实现系统自进化。实验表明，与基线模型相比，我们的方法在保持高正确率的同时显著提升了生成问题的创新性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing large language models in generating innovative and discriminative mathematical problems. The authors propose a self-evolving, multi-role collaborative framework that integrates a sampler, generator, evaluator, state machine, and memory to iteratively optimize problem generation. They also introduce an improved difficulty model and the DAPS algorithm to enhance semantic rationality and provide fine-grained difficulty guidance. The HSM3K-CN dataset is constructed for training, and a multi-stage pipeline involving continual pre-training, supervised fine-tuning, and group relative policy optimization is used. The results show that the proposed method significantly improves the innovation of generated problems without compromising correctness.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有大语言模型在生成创新性与区分度较高的数学问题方面的不足。作者提出了一种自进化、多角色协作框架，包含采样器、生成器、评估器、状态机和记忆模块，通过自我评估和外部反馈实现问题生成的迭代优化。同时引入了改进的难度模型和数据驱动的关联引导路径采样（DAPS）算法以提升语义合理性。构建了HSM3K-CN数据集，并采用包含持续预训练（CPT）、监督微调（SFT）和群体相对策略优化（GRPO）的多阶段训练流程。实验结果表明，该方法在保持高正确率的同时显著提升了生成问题的创新性。</div>
</details>
</div>
<div class="card">
<div class="title">SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models</div>
<div class="meta-line">Authors: Turhan Can Kargin, Wojciech Jasiński, Adam Pardyl, Bartosz Zieliński, Marcin Przewięźlikowski</div>
<div class="meta-line">First: 2026-01-16T19:21:02+00:00 · Latest: 2026-01-16T19:21:02+00:00</div>
<div class="meta-line">Comments: Project page is available at https://sparrta.gmum.net/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpaRRTa：用于评估视觉基础模型空间智能的合成基准</div>
<div class="mono" style="margin-top:8px">视觉基础模型（VFMs），如DINO和CLIP，在图像语义理解方面表现出色，但其空间推理能力有限，这限制了它们在具身系统中的应用。因此，近期研究将一些3D任务（如深度估计）纳入VFM的训练中。然而，VFM在其他空间任务中的表现仍不一致，引发了关于这些模型是否真正具备空间感知能力，还是仅在特定3D目标上过拟合的疑问。为了解决这一问题，我们引入了空间关系识别任务（SpaRRTa）基准，用于评估VFMs识别图像中物体相对位置的能力。与传统的关注精确度量预测（如表面法线估计）的3D目标不同，SpaRRTa探索了支撑更高级人类式空间理解的基本能力。SpaRRTa生成具有多样场景和完全可控物体布局的任意数量的逼真图像，并提供可自由访问的空间标注。通过评估一系列最先进的VFMs，我们揭示了它们在空间推理能力上的显著差异。通过我们的分析，我们提供了关于现代VFMs中支持或阻碍空间感知机制的见解。我们希望SpaRRTa能成为指导未来空间感知视觉模型发展的有用工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SpaRRTa, a synthetic benchmark designed to evaluate spatial intelligence in Visual Foundation Models (VFMs). The motivation stems from the observation that while VFMs like DINO and CLIP are strong in semantic understanding, they lack robust spatial reasoning capabilities, limiting their use in embodied systems. SpaRRTa focuses on the ability of VFMs to recognize relative object positions in images, differing from traditional 3D tasks that emphasize metric predictions. The benchmark generates photorealistic images with controllable object arrangements and accessible spatial annotations, revealing significant variations in spatial reasoning performance across state-of-the-art models.</div>
<div class="mono" style="margin-top:8px">本文提出了SpaRRTa，一个用于评估视觉基础模型（VFMs）空间智能的合成基准。动机源于观察到尽管像DINO和CLIP这样的VFMs在语义理解方面表现优异，但它们的空间推理能力有限，制约了在具身系统中的应用。SpaRRTa通过评估VFMs识别图像中物体相对位置的能力，区别于传统的侧重于精确度量预测的3D任务。该基准生成具有可控物体布局和可自由访问空间注释的逼真图像，揭示了当前先进模型在空间推理能力上的显著差异。</div>
</details>
</div>
<div class="card">
<div class="title">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</div>
<div class="meta-line">Authors: Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo Pérez-Pellitero, Youngkyoon Jang</div>
<div class="meta-line">First: 2026-01-16T17:02:46+00:00 · Latest: 2026-01-16T17:02:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11442v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Map2Thought：通过度量认知地图实现显式的3D空间推理</div>
<div class="mono" style="margin-top:8px">我们提出了Map2Thought框架，该框架使3D视觉语言模型能够进行显式且可解释的空间推理。该框架基于两个关键组件：度量认知地图（Metric-CogMap）和认知链式推理（Cog-CoT）。Metric-CogMap通过将关系推理的离散网格与精确几何理解的连续度量表示相结合，提供统一的空间表示。在Metric-CogMap的基础上，Cog-CoT通过确定性操作进行显式的几何推理，包括向量运算、边界框距离和遮挡感知的外观顺序提示，从而生成基于3D结构的可解释推理轨迹。实验结果表明，Map2Thought实现了可解释的3D理解，在仅使用一半监督数据的情况下达到了59.9%的准确率，接近使用完整数据集训练的60.9%基线。在VSI-Bench数据集上，它在10%、25%和50%的训练子集下分别优于最先进的方法5.3%、4.8%和4.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the interpretability and efficiency of spatial reasoning in 3D Vision-Language Models (VLMs). The proposed framework, Map2Thought, integrates two components: Metric Cognitive Map (Metric-CogMap) for unified spatial representation and Cognitive Chain-of-Thought (Cog-CoT) for explicit geometric reasoning. Metric-CogMap combines discrete relational grids with continuous metric-scale representations, while Cog-CoT applies deterministic operations such as vector calculations, bounding-box distances, and occlusion-aware appearance order to generate interpretable inference traces. Experimental results on VSI-Bench demonstrate that Map2Thought achieves 59.9% accuracy with half the supervision, closely matching the 60.9% baseline performance, and outperforms existing methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D视觉语言模型（VLMs）中空间推理的可解释性和效率。提出的框架Map2Thought包含两个核心组件：用于统一空间表示的度量认知地图（Metric-CogMap）和进行显式几何推理的认知链式推理（Cog-CoT）。Metric-CogMap通过结合离散关系网格与连续度量尺度表示实现空间信息的整合，而Cog-CoT则通过向量运算、边界框距离和遮挡感知的外观顺序等确定性操作生成可解释的推理轨迹。在VSI-Bench上的实验结果表明，Map2Thought仅使用一半的监督数据即可达到59.9%的准确率，接近使用完整数据集训练的60.9%基线表现，并在10%、25%和50%的训练子集上分别优于现有方法4.0%、4.8%和5.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation</div>
<div class="meta-line">Authors: Joschka Kersting, Michael Rummel, Gesa Benndorf</div>
<div class="meta-line">First: 2025-11-12T08:56:11+00:00 · Latest: 2026-01-16T14:53:55+00:00</div>
<div class="meta-line">Comments: ICIT2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09122v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09122v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向供应商的工业代理：基于RAG增强的LLM用于安全的本地PLC代码生成</div>
<div class="mono" style="margin-top:8px">可编程逻辑控制器使用专有的代码方言进行操作，这使得训练代码助手变得具有挑战性。当前的LLM是在大规模代码数据集上训练的，能够直接生成符合IEC 61131-3标准的代码，但它们并不了解特定的功能块或相关的项目代码。此外，像三菱电机这样的公司及其客户并不信任云服务提供商。因此，拥有自己的代码代理是解决这一问题的首选方案。在本研究中，我们提出了一种适用于工业领域的低数据域代码助手解决方案。我们展示了如何在不微调大型模型的情况下，通过微调小型本地模型用于边缘设备，实现高质量的代码生成。我们的工具允许多个AI模型相互竞争，使用推理自动纠正错误，并通过在聊天界面中直接编译代码来验证代码的有效性。我们通过详尽的评估支持我们的方法，包括代码编译统计数据和用户评分。我们发现，通过广泛的提示工程和定向检索，基于检索增强生成（RAG）的代码助手可以在低数据域中有效运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of training coding assistants for industrial applications where Programmable Logic Controllers (PLCs) use proprietary code dialects. The authors propose a vendor-aware coding assistant that leverages Retrieval-Augmented Generation (RAG) to enable secure, on-premise code generation without requiring large-scale fine-tuning. Their approach involves fine-tuning small local models for edge device use and integrating multiple AI models to enhance reasoning, bug correction, and code validation through direct compilation in the chat interface. The extensive evaluation demonstrates that their RAG-enhanced assistant can produce high-quality IEC 61131-3 compatible code with strong user feedback and compilation success rates, even in low-data scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决工业可编程逻辑控制器（PLC）使用专有代码方言所带来的编码助手训练难题。作者提出了一种基于检索增强生成（RAG）的供应商感知编码助手，能够在不依赖大规模微调的情况下实现安全的本地代码生成。该方法采用小型本地模型进行边缘设备部署，并通过多个AI模型协作完成推理、自动纠错和代码编译验证。评估结果显示，通过广泛的提示工程和定向检索，RAG增强的助手能够在低数据场景下生成高质量的IEC 61131-3兼容代码。</div>
</details>
</div>
<div class="card">
<div class="title">OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding</div>
<div class="meta-line">Authors: Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui</div>
<div class="meta-line">First: 2026-01-15T12:36:08+00:00 · Latest: 2026-01-16T02:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10343v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OctoBench：基于仓库的智能编码中对支架感知指令遵循的基准测试</div>
<div class="mono" style="margin-top:8px">现代编码支架使LLMs成为有能力的软件代理，但它们遵循支架指定指令的能力仍被低估，尤其是在约束条件异构且持续跨交互的情况下。为填补这一空白，我们引入了OctoBench，用于评估基于仓库的智能编码中对支架感知指令遵循的能力。OctoBench包含34个环境和217个任务实例，涵盖三种支架类型，并配有7,098个目标检查清单项。为了将任务解决与规则遵循分离，我们提供了一个自动化的观察与评分工具包，可捕捉完整交互轨迹并进行细粒度检查。在八个代表性模型上的实验揭示了任务解决与支架感知合规之间存在系统性差距，强调了需要专门针对异构指令遵循进行训练和评估的重要性。我们发布该基准测试以支持可重复的基准测试，并加速开发更具备支架感知能力的编码代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the ability of large language models (LLMs) to follow scaffold-specified instructions in repository-grounded agentic coding, particularly under heterogeneous constraints. The authors introduce OctoBench, a benchmark that includes 34 environments and 217 tasks across three scaffold types, along with 7,098 objective checklist items. They also provide an automated observation-and-scoring toolkit to capture full interaction trajectories and perform fine-grained compliance checks. Experimental results on eight representative models show a systematic discrepancy between task-solving performance and scaffold-aware instruction following, highlighting the need for more targeted training and evaluation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型（LLMs）在基于仓库的代理编码中遵循支架指定指令的能力，尤其是在处理异构约束条件时的表现。作者提出了OctoBench基准，包含34个环境和217个任务，涵盖三种支架类型，并配有7098个目标检查清单项。他们还提供了一个自动化观察与评分工具包，用于追踪完整的交互轨迹并进行细粒度的合规性检查。在八个代表性模型上的实验结果显示，任务解决能力和支架意识指令遵循之间存在系统性差距，强调了需要更针对性的训练和评估方法来提升这些能力。</div>
</details>
</div>
<div class="card">
<div class="title">MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models</div>
<div class="meta-line">Authors: Muhammad Imran, Chi Lee, Yugyung Lee</div>
<div class="meta-line">First: 2026-01-16T01:18:02+00:00 · Latest: 2026-01-16T01:18:02+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11666v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX&#x27;s potential to enhance trust and transparency in radiological AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MATEX：面向医学视觉-语言模型的多尺度注意力与文本引导可解释性</div>
<div class="mono" style="margin-top:8px">我们引入了MATEX（多尺度注意力与文本引导可解释性），这是一种新颖的框架，通过结合解剖学引导的空间推理来提升医学视觉-语言模型的可解释性。MATEX通过多层注意力展开、文本引导的空间先验以及层间一致性分析，生成精确、稳定且具有临床意义的梯度归因图。通过解决现有方法的关键局限性，如空间不精确、缺乏解剖学基础以及注意力粒度有限，MATEX能够提供更忠实且可解释的模型解释。在MS-CXR数据集上的评估表明，MATEX在空间精度和与专家标注结果的一致性方面均优于最先进的M2IB方法。这些结果突显了MATEX在增强放射学AI应用中的信任和透明度方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of MATEX is to improve the interpretability of medical vision-language models by addressing their limitations in spatial precision, anatomical grounding, and attention granularity. The framework integrates multi-scale attention mechanisms, text-guided spatial priors, and layer consistency analysis to generate accurate and clinically relevant gradient attribution maps. Experimental results on the MS-CXR dataset demonstrate that MATEX achieves superior performance compared to the state-of-the-art M2IB approach, showing enhanced spatial precision and alignment with expert annotations.</div>
<div class="mono" style="margin-top:8px">MATEX的提出旨在提升医学视觉语言模型的可解释性，解决其在空间精度、解剖学基础和注意力粒度方面的不足。该框架结合多层注意力展开、文本引导的空间先验和层一致性分析，生成精确且具有临床意义的梯度归因图。在MS-CXR数据集上的实验结果表明，MATEX在空间精度和与专家标注的一致性方面均优于当前最先进的M2IB方法，从而增强了放射学AI应用的可信度和透明度。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents</div>
<div class="meta-line">Authors: Himanshu Thakur, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri</div>
<div class="meta-line">First: 2026-01-15T19:33:42+00:00 · Latest: 2026-01-15T19:33:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10820v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team&#x27;s unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team&#x27;s environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过LLM代理受限拓扑结构中的规划实现可靠的机器学习特征工程</div>
<div class="mono" style="margin-top:8px">最近代码生成模型的进展为自动化特征工程带来了前所未有的机会，但其在现实世界机器学习团队中的采用仍受到关键挑战的限制：(i) 缺乏能够捕捉生产级特征工程迭代和复杂编码过程的数据集；(ii) 常用编码代理（如CoPilot和Devin）与团队独特工具、代码库、工作流程和实践的集成和个性化有限；(iii) 由于反馈时机不当或不足，导致人机协作效果不佳。我们提出了一种由规划器引导的受限拓扑多代理框架，以分步骤方式为仓库生成代码。该规划器由大型语言模型驱动，利用团队环境（以图表示）来协调可用代理的调用，生成上下文感知的提示，并利用下游失败来回溯修正上游产物。它可以在关键步骤请求人工干预，从而确保生成的代码可靠、可维护，并符合团队期望。在我们新构建的内部数据集上，我们的方法在评估指标上分别比手动构建和未规划的流程提升了38%和150%。在实践中，当构建服务于超过1.2亿用户的推荐模型的特征时，我们的方法通过将特征工程周期从三周缩短至一天，实现了实际影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in automating feature engineering for machine learning by proposing a planner-guided, constrained-topology multi-agent framework that integrates LLM-powered planning with existing coding agents. The framework uses a team&#x27;s environment as a graph to coordinate agent interactions, generate context-aware prompts, and correct earlier steps based on downstream failures. It also allows for human intervention at critical points to ensure reliability and alignment with team practices. Experimental results on an in-house dataset show significant improvements in evaluation metrics, with 38% better performance than manual methods and 150% improvement over unplanned workflows. In practice, the approach has reduced feature engineering cycles from three weeks to one day for recommendation models serving over 120 million users.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将代码生成模型集成到实际机器学习团队中进行可靠特征工程所面临的挑战。提出的方法是一种由规划器引导、具有受限拓扑结构的多智能体框架，通过多步骤流程生成代码，利用团队环境的图表示来创建上下文感知的提示，并基于下游失败情况反向修正上游产物。该框架在关键步骤允许人类干预，以确保生成的代码可靠且符合团队期望。在一项新型内部数据集上的实验表明，该方法在评估指标上比手动构建的流程提升了38%，比未规划的流程提升了150%。在实际应用中，该方法已将服务于1.2亿用户的推荐模型的特征工程周期从三周缩短至一天。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</div>
<div class="meta-line">Authors: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-10T12:54:04+00:00 · Latest: 2026-01-15T13:22:05+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, accepted to AAAI 2026. Project page:https://github.com/CASIA-IVA-Lab/UrbanNav</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09607v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09607v2">PDF</a> · <a href="https://github.com/CASIA-IVA-Lab/UrbanNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanNav: 从大规模人类轨迹中学习语言引导的城市导航</div>
<div class="mono" style="margin-top:8px">使用自然语言指令在复杂的城市环境中导航对具身智能体提出了重大挑战，包括嘈杂的语言指令、模糊的空间参照、多样化的地标以及动态的街道场景。当前的视觉导航方法通常局限于模拟或非街道环境，并且往往依赖于精确的目标格式，如特定坐标或图像。这限制了它们在自主智能体（如最后一公里配送机器人）在陌生城市中导航时的有效性。为了解决这些限制，我们引入了UrbanNav，一个可扩展的框架，用于训练具身智能体在多样化城市环境中遵循自由形式的语言指令。通过利用大规模的城市步行视频，我们开发了一个可扩展的标注流程，将人类导航轨迹与基于真实地标的语言指令对齐。UrbanNav包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，涵盖了广泛的城市场景。我们的模型学习了鲁棒的导航策略以应对复杂的城市场景，展示了优越的空间推理能力、对嘈杂指令的鲁棒性以及对未见过的城市环境的泛化能力。实验结果表明，UrbanNav显著优于现有方法，突显了大规模网络视频数据在实现具身智能体语言引导的真实城市导航方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UrbanNav addresses the challenges of navigating complex urban environments using natural language instructions by introducing a scalable framework that trains embodied agents to follow free-form language guidance. The method leverages web-scale city walking videos and develops an annotation pipeline to align human trajectories with real-world landmarks and language instructions. The framework is trained on over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, enabling robust spatial reasoning and generalization. Experimental results demonstrate that UrbanNav significantly outperforms existing methods in handling noisy instructions and navigating unfamiliar urban settings.</div>
<div class="mono" style="margin-top:8px">UrbanNav旨在解决基于自然语言指令在复杂城市环境中导航的挑战，为具身智能体提供更有效的解决方案。该框架通过利用大规模网络城市步行视频，开发了一种可扩展的标注流程，将人类导航轨迹与基于真实地标的语言指令对齐。其数据集包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，覆盖了广泛的城市场景。实验结果表明，UrbanNav在空间推理、对噪声指令的鲁棒性以及对未见过的城市环境的泛化能力方面显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants</div>
<div class="meta-line">Authors: Tsvi Cherny-Shahar, Amiram Yehudai</div>
<div class="meta-line">First: 2026-01-15T06:42:45+00:00 · Latest: 2026-01-15T06:42:45+00:00</div>
<div class="meta-line">Comments: 35 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10112v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.
  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\% and reduces completion time by 53.9\%, yielding a mean 57.8\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\% in accuracy and 69.5\% in efficiency on average, compared to 6.6\% and 46.1\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge faced by repository-aware coding agents in understanding build and test structures, particularly in multilingual projects with complex and heterogeneous build systems. The authors propose the Repository Intelligence Graph (RIG), a deterministic and evidence-based architectural representation that captures components, aggregators, runners, tests, and package dependencies. They also introduce SPADE, an extractor that builds RIG from build and test artifacts, such as CMake and CTest data, and provides it in a JSON format suitable for large language models. Evaluation on eight repositories with three commercial agents shows that using RIG improves mean accuracy by 12.2% and reduces completion time by 53.9%, with more significant gains in multilingual projects.</div>
<div class="mono" style="margin-top:8px">本文针对仓库感知编码代理在理解和处理复杂构建与测试结构时面临的挑战，特别是在多语言项目中由于异构构建系统导致的跨语言依赖问题。作者提出了Repository Intelligence Graph（RIG），一种基于证据的确定性架构图，用于表示可构建组件、聚合器、运行器、测试、外部包及其管理工具之间的依赖和覆盖关系。他们还开发了SPADE工具，能够从构建和测试工件（如CMake和CTest数据）中自动构建RIG，并以适合大型语言模型的JSON格式提供。在八个仓库上对三个商业代理进行评估显示，使用RIG可将平均准确率提高12.2%，完成时间减少53.9%，在多语言仓库中提升更为显著，平均准确率提高17.7%，效率提升69.5%。</div>
</details>
</div>
<div class="card">
<div class="title">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</div>
<div class="meta-line">Authors: Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</div>
<div class="meta-line">First: 2026-01-12T16:26:42+00:00 · Latest: 2026-01-15T03:58:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07695v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes &quot;near-miss&quot; samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑操作符：平滑可验证奖励激活视觉-语言模型的空间推理能力</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在实现精确数值预测以理解3D场景时面临关键瓶颈。传统的强化学习（RL）方法主要基于相对排名，常受严重奖励稀疏性和梯度不稳定性困扰，无法有效利用3D物理约束提供的可验证信号。值得注意的是，在标准GRPO框架中，相对归一化会导致&quot;近失&quot;样本（具有小但非零误差的样本）出现优势坍塌。这导致了严重的数据利用瓶颈，即在优化过程中有价值的边界样本被丢弃。为了解决这一问题，我们引入了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA采用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励空间。同时，AP-GRPO整合了绝对标量梯度以缓解传统相对排名机制中固有的数值信息损失。通过这种方法，我们构建了Numerical3D-50k数据集，包含50,000个可验证的3D子任务。实证结果表明，AP-GRPO在保持更高数据效率的同时，实现了与大规模监督方法相当的性能，有效激活了VLMs中的潜在3D推理能力，而无需对模型架构进行修改。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge that Vision-Language Models (VLMs) face in achieving accurate numerical predictions for 3D scene understanding. Traditional reinforcement learning methods suffer from reward sparsity and gradient instability, which hinder the effective use of verifiable signals from 3D physical constraints. To overcome these issues, the authors propose the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA uses a dynamically parameterized Sigmoid function to generate dense, continuous rewards, while AP-GRPO incorporates absolute scalar gradients to preserve numerical information. The proposed approach is validated using Numerical3D-50k, a dataset of 50,000 verifiable 3D subtasks, and the results show that AP-GRPO matches the performance of large-scale supervised methods with better data efficiency, effectively enhancing the latent spatial reasoning capabilities of VLMs without altering their architecture.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉-语言模型（VLMs）在3D场景理解中进行精确数值预测所面临的挑战。传统强化学习方法由于依赖相对排名，导致奖励稀疏性和梯度不稳定性，进而引发优势坍塌并丢弃有价值的数据样本。为此，作者提出了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA利用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励，而AP-GRPO则通过引入绝对标量梯度来保留数值信息。实验在包含50,000个可验证3D子任务的Numerical3D-50k数据集上进行，结果表明AP-GRPO在数据效率上优于传统方法，且性能与大规模监督方法相当，有效激活了VLMs中潜在的3D推理能力，而无需修改模型结构。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲区</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲区。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，其训练方法往往将图像扁平化为1D的图像块序列，丢弃了空间推理所需的2D结构。我们认为这种缺乏空间感知是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the spatial reasoning limitations of vision-language models (VLMs), which are typically trained using CLIP-style image encoders that flatten images into 1D sequences, neglecting their 2D structure. The authors propose two approaches to enhance spatial awareness: training image encoders with alternative objectives and incorporating 2D positional encodings. Experimental results demonstrate that these modifications improve spatial reasoning performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在空间推理方面的不足，指出当前模型通常采用对比语言-图像预训练（CLIP）风格的图像编码器，将图像扁平化为1D序列，忽略了其2D结构。作者提出了两种改进方法：使用不同训练目标的图像编码器以及引入2D位置编码。实验结果表明，这些架构改进在多个基准测试中提升了空间推理能力，为机器人和具身AI等应用提供了新的方向。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
