<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-11 04:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260211_0417</div>
    <div class="row"><div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-09T15:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v2">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在单个问题的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念性任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ProjDevBench is to address the gap in evaluating coding agents for end-to-end project development, as current benchmarks focus on individual bug fixes rather than comprehensive system building. The benchmark combines Online Judge testing with LLM-assisted code review to assess agents on system architecture design, functional correctness, and iterative solution refinement. It includes 20 programming problems across 8 categories, evaluating six coding agents based on different LLM backends. The main experimental results show an overall acceptance rate of 27.38%, indicating that while agents can handle basic functionality and data structures, they face significant challenges in complex system design, time complexity optimization, and resource management.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，现有评估主要集中在单个问题的修复上，而忽略了完整的端到端项目开发。ProjDevBench提出一个全新的基准，为编码代理提供完整的项目需求，并对其生成的代码库进行系统架构设计、功能正确性以及迭代解决方案优化的评估。该基准涵盖8个类别中的20个编程问题，包括概念性任务和现实应用场景，并对基于不同LLM后端构建的六个编码代理进行了测试。实验结果显示，尽管代理在基础功能和数据结构方面表现良好，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难，整体接受率为27.38%。</div>
</details>
</div>
<div class="card">
<div class="title">From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki</div>
<div class="meta-line">First: 2026-02-09T14:39:43+00:00 · Latest: 2026-02-09T14:39:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08735v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08735v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从对应关系到行动：多模态大语言模型中类人多图像空间推理</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在单图像空间推理方面取得了显著进展，但需要整合多个视角信息的多图像空间推理仍然具有挑战性。认知研究指出，人类通过两种机制解决此类任务：跨视角对应关系，用于识别不同视角中对应同一物理位置的区域；以及逐步视角转换，通过依次组合相对视角变化来完成推理。然而，现有研究仅部分且通常隐式地引入了这些机制，缺乏对两者进行显式监督。我们提出了一种名为HATCH（跨视角对应关系和视角变化的人类感知训练）的训练框架，包含两个互补的目标：（1）块级空间对齐，鼓励块表示在空间对应区域中对齐；（2）行动后回答推理，要求模型在预测最终答案前生成显式的视角转换动作。在三个基准测试上的实验表明，HATCH在保持单图像推理能力的同时，显著优于同等规模的基线模型，并且在性能上与更大规模的模型相竞争。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-image spatial reasoning in multi-modal large language models (MLLMs), which remains underdeveloped despite progress in single-image tasks. The authors propose HATCH, a training framework that explicitly incorporates two human-like mechanisms: cross-view correspondence and stepwise viewpoint transformation. By introducing Patch-Level Spatial Alignment and Action-then-Answer Reasoning as complementary objectives, HATCH enables the model to align spatial information across views and generate sequential viewpoint transitions. Experimental results on three benchmarks show that HATCH significantly outperforms existing baselines of similar size and achieves performance comparable to much larger models, while maintaining effective single-image reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对多图像空间推理在多模态大语言模型中的挑战，提出了一种名为HATCH的训练框架，以显式地模拟人类的两种推理机制：跨视图对应和逐步视角变换。HATCH包含两个互补的目标：图像块级空间对齐和动作-答案推理，前者促使模型在不同视角间对齐对应区域，后者要求模型在预测最终答案前生成明确的视角转换动作。实验结果表明，HATCH在三个基准测试中显著优于同类规模的基线模型，并在性能上接近更大规模的模型，同时保持了单图像推理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-09T14:24:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展边际效益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，通过提升多样性而非原始数据量来扩展代码代理的性能。在固定训练预算下，增加轨迹多样性所带来的性能提升比增加轨迹数量更大，从而优化代理训练的性能-成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，捕捉真实服务中的逻辑依赖关系；（2）蓝图驱动的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙盒环境中的代码工具，缓解内在编码能力的灾难性遗忘。我们在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力上均取得显著提升。我们计划在论文发表后公开完整的代码库和合成数据集（包含30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of code large language models (LLMs) as they evolve into tool-interactive agents, particularly the issues caused by low-quality synthetic data and the diminishing returns of quantity-based scaling. The proposed method, TDScaling, introduces a trajectory diversity scaling framework that enhances agent performance by increasing the diversity of training trajectories rather than their quantity. Experimental results on various benchmarks and code agent tasks show that TDScaling significantly improves both tool-use generalization and coding proficiency under a fixed training budget.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）在转变为工具交互代理过程中遇到的性能瓶颈，指出低质量合成数据和数据量增长带来的边际效益递减是主要限制因素。为此，提出了TDScaling框架，通过提升轨迹多样性而非单纯增加数据量来优化性能与成本的平衡。该框架包含四项创新：业务聚类机制用于捕捉实际服务中的逻辑依赖关系，蓝图驱动的多代理范式确保轨迹的一致性，基于领域熵、推理模式熵和累积动作复杂度的自适应演化机制防止模式坍缩，以及一个沙盒环境的代码工具以保持内在编码能力。实验结果表明，TDScaling在多个基准测试中有效提升了工具使用泛化能力和编码能力。</div>
</details>
</div>
<div class="card">
<div class="title">Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models</div>
<div class="meta-line">Authors: Yueyan Li, Chenggong Zhao, Zeyuan Zang, Caixia Yuan, Xiaojie Wang</div>
<div class="meta-line">First: 2025-09-23T16:07:18+00:00 · Latest: 2026-02-09T10:18:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19191v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19191v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the &quot;what&quot; and &quot;where&quot; pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model&#x27;s perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像阅读文本一样阅读图像：视觉-语言模型中的序列化图像理解</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多种现实任务中表现出色。然而，现有VLMs通常通过序列化图像处理视觉信息，这种方法与人类视觉的并行特性存在显著差异。此外，其内部机制不透明，阻碍了更深入的理解和架构创新。受人类视觉双流假说的启发，我们将VLMs中的视觉处理分解为对象识别和空间感知，分别进行研究。对于对象识别，我们将图像转换为文本标记图，并发现模型对图像内容的感知是一个从浅层到深层的两阶段过程，从属性识别开始，最终实现语义消歧。对于空间感知，我们从理论上推导并实证验证了VLMs中位置表示的几何结构。基于这些发现，我们引入了一种基于即插即用视觉解码器的指令无关标记压缩算法以提高解码效率，并提出了一种RoPE缩放技术以增强空间推理能力。通过严谨的实验，我们的工作验证了这些分析，提供了对VLM内部机制更深入的理解，并为设计更强大的未来架构提供了清晰的指导原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Vision-Language Models (VLMs) by analyzing their visual processing mechanisms, which differ from the parallel nature of human vision. Inspired by the dual-stream hypothesis, the authors decompose visual understanding into object recognition and spatial perception, revealing a two-stage content perception process in object recognition and a geometric structure in spatial representation. They propose an instruction-agnostic token compression algorithm and a RoPE scaling technique to improve decoding efficiency and spatial reasoning, respectively. Experimental results confirm these insights, enhancing the transparency and design principles of VLMs for future improvements.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言模型（VLMs）在处理图像时依赖序列化方法的局限性，指出其与人类并行视觉处理方式存在显著差异。受双流假说启发，作者将视觉处理分解为物体识别和空间感知两个部分，揭示了VLMs通过浅层到深层的两阶段过程理解图像内容的机制。他们进一步理论推导并实验证实了位置表示中的几何结构，并提出两种技术：基于可插拔视觉解码器的指令无关标记压缩算法和RoPE缩放技术以提升空间推理能力。实验结果验证了这些分析，为未来模型架构的设计提供了更深入的理解和明确的指导原则。</div>
</details>
</div>
<div class="card">
<div class="title">BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models</div>
<div class="meta-line">Authors: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li</div>
<div class="meta-line">First: 2026-02-09T08:47:14+00:00 · Latest: 2026-02-09T08:47:14+00:00</div>
<div class="meta-line">Comments: 38 pages, 9 figures. Project page:https://bimanibench.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08392v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bimanibench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiManiBench：一种用于评估多模态大语言模型双臂协调的分层基准</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）显著推动了具身人工智能的发展，使用它们来评估机器人智能已成为关键趋势。然而，现有框架主要局限于单臂操作，无法捕捉如提起重锅等双臂任务所需的时空协调。为此，我们引入BiManiBench，这是一个分层基准，从三个层面评估MLLMs：基础空间推理、高级动作规划和低级末端执行器控制。我们的框架隔离了双臂任务中的独特挑战，如手臂可达性与运动学约束，从而区分感知幻觉与规划失败。对超过30个最先进的模型的分析表明，尽管在高级推理方面表现优异，MLLMs在双臂空间定位和控制方面仍存在困难，经常导致相互干扰和顺序错误。这些发现表明当前范式缺乏对相互运动学约束的深入理解，强调未来研究应关注双臂间的碰撞避免和精细时间序列控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the bimanual coordination capabilities of Multimodal Large Language Models (MLLMs) in the context of embodied AI and robotic intelligence. To achieve this, the authors propose BiManiBench, a hierarchical benchmark that assesses MLLMs across three levels: spatial reasoning, action planning, and end-effector control. The main experimental results show that while MLLMs perform well in high-level reasoning, they face significant challenges in dual-arm spatial grounding and control, often leading to mutual interference and sequencing errors. This highlights the need for improved understanding and modeling of inter-arm kinematic constraints in future research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估多模态大语言模型（MLLMs）在具身AI中的双臂协调能力，因为现有基准主要局限于单臂操作。为此，提出了BiManiBench这一分层基准，用于评估MLLMs在空间推理、动作规划和末端执行器控制三个层面的表现。实验结果表明，尽管MLLMs在高层推理方面表现良好，但在双臂空间定位和控制上存在困难，常导致相互干扰和顺序错误，反映出当前范式对双臂运动学约束理解不足。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science</div>
<div class="meta-line">Authors: Jie Zhang, Xingtong Yu, Yuan Fang, Rudi Stouffs, Zdravko Trivic</div>
<div class="meta-line">First: 2026-02-09T07:28:49+00:00 · Latest: 2026-02-09T07:28:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08342v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanGraphEmbeddings: 学习和评估具有空间基础的多模态嵌入以用于城市科学</div>
<div class="mono" style="margin-top:8px">在城市环境中学习可迁移的多模态嵌入具有挑战性，因为城市理解本质上是空间的，但现有的数据集和基准缺乏街道视图图像与城市结构之间的显式对齐。我们引入了UGData，这是一个具有空间基础的数据集，将街道视图图像锚定到结构化的空间图上，并通过空间推理路径和空间上下文描述提供图对齐的监督，揭示超出图像内容的距离、方向性、连通性和邻里上下文。基于UGData，我们提出了UGE，这是一种两阶段的训练策略，通过结合指令引导的对比学习和基于图的空间编码，逐步且稳定地对齐图像、文本和空间结构。最后，我们引入了UGBenchmark，这是一个全面的基准，用于评估具有空间基础的嵌入如何支持多样化的城市理解任务，包括地理定位排序、图像检索、城市感知和空间锚定。我们在多个最先进的视觉语言模型（VLM）主干网络上开发了UGE，包括Qwen2-VL、Qwen2.5-VL、Phi-3-Vision和LLaVA1.6-Mistral，并使用LoRA微调训练固定维度的空间嵌入。基于Qwen2.5-VL-7B主干网络的UGE在训练城市中实现了图像检索44%的提升和地理定位排序30%的提升，在未见过的城市中分别实现了超过30%和22%的提升，证明了显式空间锚定在空间密集型城市任务中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of learning transferable multimodal embeddings for urban environments, where understanding is inherently spatial but current datasets lack explicit alignment between street-view images and urban structure. To tackle this, the authors introduce UGData, a dataset that links street-view images to spatial graphs and provides graph-aligned supervision through spatial reasoning paths and context captions. They propose UGE, a two-stage training strategy that combines instruction-guided contrastive learning with graph-based spatial encoding to progressively align images, text, and spatial structures. Experimental results show that UGE achieves significant improvements in image retrieval and geolocation ranking, with up to 44% and 30% gains on training cities, and over 30% and 22% on held-out cities, highlighting the effectiveness of spatially grounded embeddings for urban understanding tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决城市环境中学习可迁移的多模态嵌入的挑战，因为城市理解本质上具有空间属性，但现有数据集缺乏街道视图图像与空间结构之间的显式对齐。作者提出了UGData数据集，将街道视图图像锚定到结构化的空间图上，并通过空间推理路径和上下文描述提供图对齐的监督。在此基础上，他们提出了UGE，一种两阶段训练策略，结合了指令引导的对比学习和基于图的空间编码，以逐步且稳定地对齐图像、文本和空间结构。实验结果表明，基于Qwen2.5-VL-7B骨干的UGE在训练城市中分别实现了图像检索和地理定位排名44%和30%的提升，在未见过的城市中分别达到30%和22%的提升，证明了显式空间对齐在空间密集型城市任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</div>
<div class="meta-line">Authors: Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal</div>
<div class="meta-line">First: 2026-02-09T03:21:48+00:00 · Latest: 2026-02-09T03:21:48+00:00</div>
<div class="meta-line">Comments: the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adaptive-visual-tts.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时以及想象多少：使用世界模型进行视觉空间推理的自适应测试时缩放</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）取得了快速进展，但当正确答案依赖于未见过或替代视角下的场景外观时，视觉空间推理仍然不可靠。最近的研究通过引入世界模型来增强推理过程中的视觉想象能力，但关于何时需要想象、想象多少是有益的，以及何时会带来负面影响等问题仍缺乏深入理解。在实践中，无差别地使用想象可能会增加计算量，甚至通过引入误导性证据而降低性能。在本文中，我们深入分析了测试时的视觉想象作为一种可控资源在空间推理中的作用。我们研究了静态视觉证据何时足够、何时能提升推理能力，以及过度或不必要的想象如何影响准确性和效率。为支持这一分析，我们提出了AVIC，一个基于世界模型的自适应测试时框架，它在选择性调用和缩放视觉想象之前，会显式地推理当前视觉证据是否足够。在空间推理基准（SAT、MMSI）和一个具身导航基准（R2R）上的实验结果表明，存在明确的场景，其中想象是关键、边缘或有害的，且选择性控制策略在大幅减少世界模型调用和语言标记数量的情况下，可以达到或超越固定想象策略的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of visual spatial reasoning in Multimodal Large Language Models (MLLMs), where performance is unreliable when answers depend on unseen viewpoints. The authors propose AVIC, an adaptive framework that uses world models to selectively control the use of visual imagination during inference, determining when static visual evidence is sufficient or when imagination is needed. Their experiments on spatial reasoning benchmarks and an embodied navigation task show that controlled use of imagination can improve accuracy and efficiency, outperforming fixed strategies by reducing unnecessary world-model calls and language tokens.</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在视觉空间推理方面取得了快速进展，但当正确答案依赖于未见过或替代视角的场景时，其可靠性仍存在问题。本文提出AVIC框架，通过世界模型在测试时选择性地控制视觉想象的使用，以判断当前视觉证据是否足够。实验结果表明，在不同的任务中，视觉想象可能具有关键作用、边际效益或有害影响，而AVIC在减少世界模型调用和语言标记数量的同时，实现了与固定想象策略相当甚至更优的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ViT-5: Vision Transformers for The Mid-2020s</div>
<div class="meta-line">Authors: Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad, Cihang Xie, Alan Yuille</div>
<div class="meta-line">First: 2026-02-08T18:03:44+00:00 · Latest: 2026-02-08T18:03:44+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/wangf3014/ViT-5</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08071v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08071v1">PDF</a> · <a href="https://github.com/wangf3014/ViT-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViT-5：面向2020年代中期的视觉Transformer</div>
<div class="mono" style="margin-top:8px">本工作系统地探讨了通过利用过去五年架构上的进步来现代化视觉Transformer主干的方法。在保留经典注意力-前馈网络结构的前提下，我们对各个组件进行了精细化改进，包括归一化、激活函数、位置编码、门控机制以及可学习标记。这些改进构成了新一代的视觉Transformer，我们称之为ViT-5。大量实验表明，ViT-5在理解和生成基准测试中均优于当前最先进的普通视觉Transformer。在ImageNet-1k分类任务中，ViT-5-Base在计算资源相当的情况下达到84.2%的top-1准确率，超过了DeiT-III-Base的83.8%。此外，ViT-5也作为生成建模的更强主干：当将其嵌入到SiT扩散框架中时，其FID为1.84，而使用普通ViT主干则为2.06。除了主要指标外，ViT-5在表示学习和空间推理方面也表现出改进，并且在任务间具有良好的迁移能力。其设计符合当前基础模型的实践，为2020年代中期的视觉主干提供了一个简单的替换升级方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to modernize Vision Transformer (ViT) architectures by incorporating recent advancements in transformer design. The authors refine key components of the ViT, including normalization, activation functions, positional encoding, and gating mechanisms, while retaining the canonical Attention-FFN structure. The resulting model, ViT-5, demonstrates superior performance compared to previous versions on both classification and generative tasks. On ImageNet-1k, ViT-5-Base achieves 84.2% top-1 accuracy, surpassing DeiT-III-Base. In generative modeling, ViT-5 improves the FID score to 1.84 when used with an SiT diffusion framework, compared to 2.06 with a standard ViT backbone.</div>
<div class="mono" style="margin-top:8px">本文旨在通过引入过去五年中变压器架构的最新进展，对视觉变换器（ViT）进行现代化改进。ViT-5在保留经典Attention-FFN结构的基础上，对归一化、激活函数、位置编码和门控机制等关键组件进行了优化。实验结果表明，ViT-5在图像分类和生成任务中均优于现有ViT变体，其在ImageNet-1k上的top-1准确率达到84.2%，超越了DeiT-III-Base的83.8%。在扩散模型中，ViT-5的FID得分为1.84，优于传统ViT的2.06，显示出更优的图像生成质量。该模型在表示学习和空间推理方面表现更佳，并具备良好的任务迁移能力，适用于2020年代中期的视觉任务。</div>
</details>
</div>
<div class="card">
<div class="title">View-Centric Multi-Object Tracking with Homographic Matching in Moving UAV</div>
<div class="meta-line">Authors: Deyi Ji, Lanyun Zhu, Siqi Gao, Qi Zhu, Yiru Zhao, Peng Xu, Yue Ding, Hongtao Lu, Jieping Ye, Feng Wu, Feng Zhao</div>
<div class="meta-line">First: 2024-03-16T06:48:33+00:00 · Latest: 2026-02-08T10:40:20+00:00</div>
<div class="meta-line">Comments: TGRS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.10830v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.10830v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we address the challenge of Multi-Object Tracking (MOT) in moving Unmanned Aerial Vehicle (UAV) scenarios, where irregular flight trajectories, such as hovering, turning left/right, and moving up/down, lead to significantly greater complexity compared to fixed-camera MOT. Specifically, changes in the scene background not only render traditional frame-to-frame object IoU association methods ineffective but also introduce significant view shifts in the objects, which complicates tracking. To overcome these issues, we propose a novel HomView-MOT framework, which for the first time, harnesses the view homography inherent in changing scenes to solve MOT challenges in moving environments, incorporating homographic matching and view-centric concepts. We introduce a Fast Homography Estimation (FHE) algorithm for rapid computation of homography matrices between video frames, enabling object View-Centric ID Learning (VCIL) and leveraging multi-view homography to learn cross-view ID features. Concurrently, our Homographic Matching Filter (HMF) maps object bounding boxes from different frames onto a common view plane for a more realistic physical IoU association. Extensive experiments have proven that these innovations allow HomView-MOT to achieve state-of-the-art performance on prominent UAV MOT datasets VisDrone and UAVDT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于同构匹配的移动无人机多目标跟踪</div>
<div class="mono" style="margin-top:8px">本文针对移动无人机场景下的多目标跟踪（MOT）挑战，提出了一种新颖的HomView-MOT框架。该框架首次利用变化场景中固有的视图同构性，解决移动环境下的MOT问题，结合了同构匹配和以视图为中心的概念。我们引入了快速同构估计（FHE）算法，用于快速计算视频帧之间的同构矩阵，从而实现目标视图中心ID学习（VCIL）并利用多视图同构学习跨视图ID特征。同时，我们的同构匹配滤波器（HMF）将不同帧中的目标边界框映射到同一视图平面上，以实现更符合物理现实的帧间IoU关联。大量实验表明，这些创新使HomView-MOT在VisDrone和UAVDT等主流无人机MOT数据集上达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-object tracking (MOT) in moving UAV scenarios, where irregular flight paths and changing backgrounds complicate traditional frame-to-frame association methods. The proposed HomView-MOT framework introduces homographic matching and view-centric concepts to handle view shifts by leveraging the homography inherent in dynamic scenes. A Fast Homography Estimation (FHE) algorithm enables efficient computation of homography matrices, supporting View-Centric ID Learning (VCIL) and cross-view ID feature learning. The Homographic Matching Filter (HMF) further improves tracking by mapping bounding boxes to a common view plane for more accurate IoU association. Experimental results on VisDrone and UAVDT datasets demonstrate that HomView-MOT achieves state-of-the-art performance in moving UAV MOT tasks.</div>
<div class="mono" style="margin-top:8px">本文针对移动无人机（UAV）场景下的多目标跟踪（MOT）挑战，提出了一种新的HomView-MOT框架。由于无人机的不规则飞行轨迹和场景背景的变化，传统基于帧间IoU关联的方法效果不佳，而目标视角变化进一步增加了跟踪难度。该框架首次利用场景中的视图同构性，结合快速同构估计（FHE）算法和视图中心概念，实现跨视图ID特征学习和目标视图中心ID学习。Homographic Matching Filter（HMF）通过将不同帧的目标边界框映射到统一视图平面，提升了物理IoU关联的准确性。在VisDrone和UAVDT等主流无人机MOT数据集上的实验表明，HomView-MOT在移动无人机场景中达到了最先进的跟踪性能。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents</div>
<div class="meta-line">Authors: Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, Lingxiao Jiang</div>
<div class="meta-line">First: 2026-02-08T10:26:31+00:00 · Latest: 2026-02-08T10:26:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07900v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考基于LLM的软件工程代理中代理生成测试的价值</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）代码代理越来越多地通过迭代编辑代码、调用工具和验证候选补丁来解决仓库级别的问题。在这些工作流中，代理经常即兴编写测试，这一范式被许多SWE-bench排行榜上的高排名代理所采用。然而，我们观察到，GPT-5.2几乎不编写新测试，却仍能实现与顶级代理相当的性能。这引发了关键问题：这些测试是否真正有助于问题解决，还是仅仅模仿了人类的测试实践，同时消耗了大量交互预算？
为了揭示代理编写测试的影响，我们进行了一项实证研究，分析了在SWE-bench Verified上六种最先进的LLM代理的轨迹。我们的结果显示，尽管测试编写被广泛采用，但同一模型中已解决和未解决的任务在测试编写频率上表现出相似性。此外，这些测试通常作为观察反馈渠道，代理更倾向于使用揭示值的打印语句，而不是正式的断言检查。基于这些见解，我们通过修改四个代理的提示来增加或减少测试编写量，并进行了一项受控实验。结果表明，代理编写测试的数量变化不会显著影响最终结果。综上所述，我们的研究揭示了当前测试编写实践在自主软件工程任务中可能仅提供有限的效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the value of agent-generated tests in LLM-based software engineering agents, motivated by the observation that high-performing agents on the SWE-bench leaderboard often write tests on the fly, yet models like GPT-5.2, which generate minimal tests, can still achieve comparable performance. The authors conducted an empirical analysis of agent trajectories across six advanced LLMs on SWE-bench Verified, finding that test writing is common but not strongly correlated with task resolution. Their controlled experiments showed that altering the volume of test writing in four agents did not significantly affect final outcomes, suggesting that current test-writing practices may offer limited utility in autonomous software engineering tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于大语言模型（LLM）的软件工程代理中生成测试的价值，动机源于观察到几乎没有生成新测试的GPT-5.2在SWE-bench排行榜上表现与顶尖代理相当。通过分析六种先进LLM在SWE-bench Verified上的代理轨迹，研究发现测试生成虽普遍，但与任务解决与否无明显关联。结果表明，测试通常作为观察反馈机制，代理更倾向于使用输出值的打印语句而非正式断言。一项控制实验通过修改四个代理的提示以增加或减少测试生成，结果显示测试生成量的改变对最终结果影响不大，表明当前测试生成实践在自主软件工程任务中可能提供有限的效用。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds</div>
<div class="meta-line">Authors: Chen Yang, Guanxin Lin, Youquan He, Peiyao Chen, Guanghe Liu, Yufan Mo, Zhouyuan Xu, Linhao Wang, Guohui Zhang, Zihang Zhang, Shenxiang Zeng, Chen Wang, Jiansheng Fan</div>
<div class="meta-line">First: 2026-02-08T08:29:38+00:00 · Latest: 2026-02-08T08:29:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ssi-bench.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构思维：通过受限流形上的推理评估空间智能</div>
<div class="mono" style="margin-top:8px">空间智能对于视觉-语言模型（VLMs）在物理世界中的表现至关重要，但许多基准测试主要评估不受限的场景，使得模型可以利用2D捷径。我们引入了SSI-Bench，这是一个针对受限流形上空间推理的VQA基准测试，基于复杂的真实世界3D结构，其可行配置严格受几何、拓扑和物理约束的控制。SSI-Bench包含1,000个排序问题，涵盖几何和拓扑推理，并需要多样化的组合空间操作，如心理旋转、截面推理、遮挡推理和力-路径推理。该基准测试通过完全以人类为中心的流程创建：十位研究人员花费超过400小时筛选图像、标注结构组件并设计问题，以最小化像素级线索。评估31个广泛使用的VLMs显示，与人类相比存在显著差距：最佳开源模型准确率为22.2%，最强闭源模型准确率为33.6%，而人类准确率为91.6%。鼓励模型进行思考仅带来边际提升，错误分析表明模型在结构锚定和约束一致的3D推理方面存在缺陷。项目页面：https://ssi-bench.github.io.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SSI-Bench, a new VQA benchmark designed to evaluate spatial intelligence in vision--language models (VLMs) by focusing on constrained manifolds derived from complex real-world 3D structures. The motivation stems from the observation that existing benchmarks often assess models on unconstrained scenes, allowing them to rely on 2D shortcuts rather than true 3D understanding. SSI-Bench includes 1,000 ranking questions that require geometric and topological reasoning, as well as compositional spatial operations like mental rotation and force-path reasoning. The benchmark was created through a human-centered process involving extensive curation and annotation. Experimental results show that even the best VLMs significantly underperform compared to humans, with open-source models achieving 22.2% accuracy and closed-source models reaching 33.6%, while humans score 91.6%. The analysis highlights challenges in structural grounding and constraint-consistent 3D reasoning.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的视觉问答基准SSI-Bench，旨在通过评估受限流形上的空间推理能力来检验视觉-语言模型（VLMs）的空间智能。研究动机源于现有基准主要评估无约束场景，使模型可以依赖2D捷径而非真正的3D推理。SSI-Bench包含1000个排序问题，涵盖几何和拓扑推理，以及如心理旋转、截面推断、遮挡推理和力-路径推理等组合空间操作。该基准通过完全以人类为中心的流程创建，涉及大量图像整理、结构组件标注和问题设计，以减少像素级线索的影响。对31个广泛使用的VLMs进行评估发现，其表现与人类存在显著差距，最佳开源模型准确率为22.2%，最强闭源模型为33.6%，而人类准确率高达91.6%。错误分析表明，模型在结构定位和符合约束的3D推理方面存在明显不足。</div>
</details>
</div>
<div class="card">
<div class="title">rePIRL: Learn PRM with Inverse RL for LLM Reasoning</div>
<div class="meta-line">Authors: Xian Wu, Kaijie Zhu, Ying Zhang, Lun Wang, Wenbo Guo</div>
<div class="meta-line">First: 2026-02-08T05:47:27+00:00 · Latest: 2026-02-08T05:47:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07832v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>rePIRL: 通过逆强化学习学习LLM推理的进程奖励模型</div>
<div class="mono" style="margin-top:8px">进程奖励在深度强化学习中被广泛用于提高训练效率、减少方差并防止奖励黑客行为。在LLM推理中，现有工作也探索了各种学习有效进程奖励模型（PRM）的解决方案，无论是否借助专家策略。然而，现有方法要么依赖于对专家策略的强假设（例如需要其奖励函数），要么存在内在限制（例如熵崩溃），导致PRM效果较弱或泛化能力有限。本文提出rePIRL，这是一种受逆强化学习启发的框架，能够在对专家策略的假设极小的情况下学习有效的PRM。具体而言，我们设计了一个双学习过程，交替更新策略和PRM。我们的学习算法包含定制技术，以应对将传统逆强化学习扩展到LLM的挑战。我们从理论上证明，所提出的框架能够统一在线和离线PRM学习方法，从而证明rePIRL可以在极小假设下学习PRM。我们在标准化的数学和编程推理数据集上的实证评估展示了rePIRL相较于现有方法的有效性。我们进一步展示了所训练的PRM在测试时训练、测试时扩展以及为训练困难问题提供早期信号方面的应用。最后，我们通过详细消融实验验证了我们的训练方案和关键设计选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of learning effective process reward models (PRMs) for large language model (LLM) reasoning, where existing methods either require strong assumptions about expert policies or face limitations like entropy collapse. The proposed rePIRL framework is inspired by inverse reinforcement learning and employs a dual learning process that iteratively updates the policy and the PRM. Theoretical analysis shows that rePIRL unifies online and offline PRM learning, while empirical results on math and coding reasoning tasks demonstrate its superior performance compared to existing approaches. Additionally, the study highlights the utility of the trained PRM in test-time training, scaling, and identifying challenging problems early in the training process.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）推理中学习有效过程奖励模型（PRM）的挑战，指出现有方法要么依赖专家奖励函数，要么面临如熵坍塌等问题。提出rePIRL框架，基于逆强化学习，采用交替更新策略同步优化策略和PRM。理论分析表明，该框架能够统一在线与离线PRM学习方法，实验结果在数学和编程推理数据集上验证了其优于现有方法的有效性。此外，研究还探讨了PRM在测试时训练、扩展以及识别训练难题中的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Towards an Understanding of Context Utilization in Code Intelligence</div>
<div class="meta-line">Authors: Yanlin Wang, Kefeng Duan, Dewu Zheng, Ensheng Shi, Fengji Zhang, Yanli Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Hongyu Zhang, Qianxiang Wang, Zibin Zheng</div>
<div class="meta-line">First: 2025-04-11T17:59:53+00:00 · Latest: 2026-02-07T19:37:03+00:00</div>
<div class="meta-line">Comments: Accepted by ACM Computing Surveys</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.08734v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.08734v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Code intelligence is an emerging domain in software engineering, aiming to improve the effectiveness and efficiency of various code-related tasks. Recent research suggests that incorporating contextual information beyond the basic original task inputs (i.e., source code) can substantially enhance model performance. Such contextual signals may be obtained directly or indirectly from sources such as API documentation or intermediate representations like abstract syntax trees can significantly improve the effectiveness of code intelligence. Despite growing academic interest, there is a lack of systematic analysis of context in code intelligence. To address this gap, we conduct an extensive literature review of 146 relevant studies published between September 2007 and August 2024. Our investigation yields four main contributions. (1) A quantitative analysis of the research landscape, including publication trends, venues, and the explored domains; (2) A novel taxonomy of context types used in code intelligence; (3) A task-oriented analysis investigating context integration strategies across diverse code intelligence tasks; (4) A critical evaluation of evaluation methodologies for context-aware methods. Based on these findings, we identify fundamental challenges in context utilization in current code intelligence systems and propose a research roadmap that outlines key opportunities for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代码智能中上下文利用的理解</div>
<div class="mono" style="margin-top:8px">代码智能是软件工程中的一个新兴领域，旨在提高各种代码相关任务的效果和效率。近期研究表明，结合超出基础任务输入（即源代码）的上下文信息可以显著提升模型性能。这些上下文信号可能直接或间接地从API文档或抽象语法树等中间表示中获取，从而显著提升代码智能的效果。尽管学术界对此兴趣日益增长，但对代码智能中上下文的系统性分析仍显不足。为解决这一问题，我们对2007年9月至2024年8月间发表的146项相关研究进行了广泛的文献综述。我们的调查得出四个主要贡献：(1) 对研究领域进行定量分析，包括出版趋势、发表平台和研究领域；(2) 提出一种用于代码智能的新型上下文类型分类法；(3) 以任务为导向的分析，探讨不同代码智能任务中的上下文整合策略；(4) 对上下文感知方法的评估方法进行批判性评估。基于这些发现，我们识别了当前代码智能系统中上下文利用的基本挑战，并提出了一个研究路线图，概述了未来研究的关键机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of context in code intelligence, a field focused on enhancing code-related tasks through advanced techniques. Motivated by the potential of contextual information beyond source code to improve model performance, the study reviews 146 papers from 2007 to 2024. It presents four key contributions: a quantitative analysis of the research landscape, a new taxonomy of context types, a task-oriented analysis of context integration strategies, and a critical evaluation of current evaluation methods. The findings highlight challenges in context utilization and suggest a roadmap for future research.</div>
<div class="mono" style="margin-top:8px">本文探讨了上下文在代码智能中的作用，该领域旨在通过提升效果和效率来改进与代码相关的任务。研究指出，需要系统理解上下文（如API文档或抽象语法树）如何影响模型性能。通过对2007年至2024年间发表的146篇相关文献进行全面综述，作者提供了研究趋势的定量分析，提出了新的上下文类型分类体系，分析了不同代码智能任务中的上下文整合策略，并评估了当前上下文感知方法的评估方法。研究结果揭示了当前代码智能系统在上下文利用方面的主要挑战，并提出了未来研究的关键方向。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning</div>
<div class="meta-line">Authors: Yancheng Long, Yankai Yang, Hongyang Wei, Wei Chen, Tianke Zhang, Haonan fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</div>
<div class="meta-line">First: 2026-02-07T09:23:34+00:00 · Latest: 2026-02-07T09:23:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term &quot;Attention Collapse,&quot; where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialReward：通过显式空间推理弥合图像编辑在线强化学习中的感知鸿沟</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）为复杂的图像编辑提供了有前景的途径，但目前受到可靠且细粒度奖励信号稀缺的限制。现有评估器经常面临我们称之为『注意力崩溃』的关键感知鸿沟问题，即模型忽略了跨图像比较，无法捕捉细粒度细节，导致感知不准确和评分失调。为了解决这些限制，我们提出了SpatialReward，这是一种通过显式空间推理强制进行精确验证的奖励模型。通过将推理锚定在预测的编辑区域，SpatialReward在像素级证据基础上进行语义判断，显著提升了评估的准确性。我们的模型在精心挑选的26万条空间感知数据集上进行训练，在MMRB2和EditReward-Bench上取得了最先进的性能，并在我们提出的MultiEditReward-Bench上优于专有评估器。此外，SpatialReward在在线RL中作为强大的信号，使OmniGen2在GEdit-Bench上提升+0.90，超越了领先的判别模型，并且GPT-4.1的提升幅度翻倍（+0.45）。这些结果表明，空间推理对于解锁图像编辑中的有效对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of online reinforcement learning (RL) in image editing, particularly the issue of unreliable and coarse reward signals. The proposed method, SpatialReward, introduces explicit spatial reasoning to bridge the perception gap by focusing on predicted edit regions and grounding semantic judgments in pixel-level evidence. Experimental results show that SpatialReward achieves state-of-the-art performance on MMRB2 and EditReward-Bench, outperforms proprietary evaluators on MultiEditReward-Bench, and significantly improves the performance of OmniGen2 on GEdit-Bench, surpassing the leading discriminative model and doubling the gain of GPT-4.1.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在线强化学习（RL）在图像编辑中的局限性，尤其是由于感知差距导致的奖励信号不可靠问题，这种差距被称为&#x27;注意力坍塌&#x27;。所提出的方法SpatialReward通过引入显式的空间推理，将语义评估锚定在像素级证据上，从而提升奖励信号的准确性。实验结果表明，SpatialReward在MMRB2和EditReward-Bench上达到最先进的性能，在MultiEditReward-Bench上优于专有评估器，并在GEdit-Bench上使OmniGen2提升+0.90，超越了领先的判别模型，且GPT-4.1的提升幅度翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-07T01:15:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使大型语言模型在推荐系统中首次具备扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这可能是由于先前持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性所致。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，表明其在学习可泛化的用户偏好模式方面具有优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具备稳健的幂律扩展特性。我们的实验表明，在多种合成数据模态下，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of developing Large Language Models (LLMs) for recommender systems, where the lack of predictable scaling laws has hindered progress. The authors propose a principled synthetic data generation framework that creates a curated, pedagogical curriculum to overcome the noise, bias, and incompleteness of real user interaction data. Experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, achieving a $+130\%$ improvement in recall@100 for SasRec. Furthermore, the study demonstrates robust power-law scaling for LLMs pre-trained on recommendation-specific synthetic data, with consistent perplexity reduction across different data modalities, establishing a reliable foundation for scaling LLM capabilities in the recommendation domain.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在推荐系统中的发展难题，提出了一种基于原则的合成数据生成框架。研究动机源于真实用户交互数据中存在的噪声、偏差和不完整性，这些因素阻碍了可预测的扩展定律的建立。方法采用分层方式生成高质量、结构化的合成数据，作为LLM训练的精心设计课程。实验结果表明，使用该合成数据训练的模型在排序任务中显著优于基于真实数据训练的模型，其中SasRec在recall@100指标上提升了130%。此外，研究首次实证了LLMs在基于高质量推荐数据的持续预训练中表现出稳健的幂律扩展特性，显示出跨不同数据模态的一致性和可预测的困惑度下降，为推荐领域中LLM的可扩展应用奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</div>
<div class="meta-line">Authors: Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</div>
<div class="meta-line">First: 2026-02-06T18:59:59+00:00 · Latest: 2026-02-06T18:59:59+00:00</div>
<div class="meta-line">Comments: 21 pages, 6 figures and 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://genmilab.github.io/MedMO-Page">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO&#x27;s broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedMO：面向医学图像的多模态大语言模型的定位与理解</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）迅速发展，但其在医学领域的应用仍受限于领域覆盖不足、模态对齐和基于上下文的推理能力的差距。本文提出MedMO，这是一个基于通用MLLM架构并仅在大规模、领域特定数据上训练的医学基础模型。MedMO采用多阶段训练方案：(i) 跨模态预训练，将异构视觉编码器与医学语言主干对齐；(ii) 在涵盖图像描述、视觉问答（VQA）、报告生成、检索和基于边界框的疾病定位等多任务监督下的指令调优；(iii) 采用可验证奖励的强化学习，结合事实性检查和基于边界框的GIoU奖励，以增强复杂临床场景中的空间定位和逐步推理能力。MedMO在多个模态和任务上均优于强大的开源医学MLLMs。在VQA基准测试中，MedMO在基线基础上平均准确率提升了13.7%，且性能接近SOTA模型Fleming-VL（仅差1.9%）。在基于文本的问答任务中，其准确率分别比基线提升6.9%和比Fleming-VL提升14.5%。在医学报告生成任务中，MedMO在语义和临床准确性方面均取得显著提升。此外，它展现出强大的定位能力，IoU指标比基线提升40.4%，比Fleming-VL提升37.0%，突显了其在空间推理和定位方面的稳健性能。在放射学、眼科和病理学显微镜领域的评估验证了MedMO在跨模态任务中的广泛泛化能力。我们发布了两个版本的MedMO：4B和8B。项目地址为https://genmilab.github.io/MedMO-Page</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing multimodal large language models (MLLMs) in the medical domain, particularly in domain coverage, modality alignment, and grounded reasoning. MedMO is a medical foundation model developed using a generalized MLLM architecture and trained on large-scale, domain-specific data. It employs a multi-stage training approach, including cross-modal pretraining, instruction tuning on multiple medical tasks, and reinforcement learning with a combination of factuality checks and box-level GIoU rewards. The model demonstrates significant performance improvements across various tasks, such as VQA, text-based QA, and medical report generation, achieving higher accuracy and better spatial grounding compared to existing models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有多模态大语言模型（MLLMs）在医学领域中的不足，特别是在领域覆盖、模态对齐和基于上下文的推理方面。MedMO 是一个基于通用 MLLM 架构的医学基础模型，专门使用大规模、领域相关的数据进行训练。该模型采用多阶段训练方法，包括跨模态预训练、多任务指令调优以及结合事实性检查和框级 GIoU 奖励的强化学习。在多个任务中，如视觉问答（VQA）、基于文本的问答和医学报告生成，MedMO 表现出显著的性能提升，其准确率和空间定位能力均优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Beyond Redundancy: Task Complexity&#x27;s Role in Vision Token Specialization in VLLMs</div>
<div class="meta-line">Authors: Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</div>
<div class="meta-line">First: 2026-02-06T18:13:01+00:00 · Latest: 2026-02-06T18:13:01+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06914v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越冗余：VLLMs中视觉标记专业化与任务复杂度的关系</div>
<div class="mono" style="margin-top:8px">视觉大语言模型（VLLMs）的视觉能力一直落后于其语言能力。特别是，许多基准研究已经表明，当需要细粒度视觉信息或空间推理时，VLLMs表现不佳。然而，我们尚未完全理解为何VLLMs在这些任务上相较于其他模型表现如此困难。一些研究将视觉冗余作为解释，认为高层视觉信息均匀分布在多个标记中，而具体的细粒度视觉信息被舍弃。在本研究中，我们更详细地探讨这一前提，旨在更好地理解模型如何处理不同类型视觉信息以及哪些信息被舍弃。为此，我们引入了一个简单的人工合成基准数据集，专门用于探测各种视觉特征，并设计了一组衡量视觉冗余的指标，从而更深入地理解它们之间的关系。随后，我们探索了在多个复杂视觉任务上微调VLLMs，以更好地理解冗余和压缩如何随着训练数据复杂度的变化而变化。我们发现任务复杂度与视觉压缩之间存在联系，这意味着拥有足够比例的高复杂度视觉数据对于改变VLLMs的视觉表示分布方式至关重要，从而提高其在复杂视觉任务上的表现。我们希望这项研究能为下一代VLLMs的训练提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why vision large language models (VLLMs) perform poorly on complex visual tasks compared to linguistic ones. It challenges the common assumption that visual redundancy is the main cause by introducing a synthetic benchmark dataset and visual redundancy metrics to analyze how different types of visual information are processed. The findings reveal a link between task complexity and visual compression, suggesting that incorporating high-complexity visual data during training can reshape the model&#x27;s visual representation distribution and enhance its performance on challenging visual tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了为何视觉大语言模型（VLLMs）在复杂视觉任务上的表现不如语言任务。它质疑了视觉冗余是主要原因的常见假设，通过引入一个合成基准数据集和测量视觉冗余的指标进行分析。研究结果表明，任务复杂度影响视觉压缩，模型在训练中包含足够比例的高复杂度视觉数据有助于改变其视觉表示的分布方式，从而提升在复杂视觉任务上的表现。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCode: A Benchmark for Evaluating Software Engineering Agents</div>
<div class="meta-line">Authors: Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Simon Alford, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta</div>
<div class="meta-line">First: 2026-02-02T16:04:10+00:00 · Latest: 2026-02-06T15:49:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02262v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02262v2">PDF</a> · <a href="https://github.com/seal-research/OmniCode">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCode：评估软件工程代理的基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型的编码代理正在重新定义现实世界软件的开发方式。为了推动对更优编码代理的研究，我们需要能够严格评估此类代理执行各种软件工程任务能力的挑战性基准。然而，像HumanEval和SWE-Bench这样的流行编码基准主要关注狭窄范围的任务，如编程竞赛和补丁生成。实际上，软件工程师在现实世界软件开发中需要处理更广泛的任务。为了解决这一问题，我们提出了OmniCode，一个包含超越代码或补丁生成的更广泛和多样化任务类别的新型软件工程基准。总体而言，OmniCode包含1794个任务，涵盖三种编程语言（Python、Java和C++）以及四个关键类别：错误修复、测试生成、代码审查修复和风格修复。与之前的软件工程基准相比，OmniCode的任务（1）经过人工验证以消除定义不清的问题，（2）合成构建或最近整理以避免数据泄露问题，提供了一种从有限现实数据中合成生成多样化软件任务的新框架。我们使用流行的代理框架如SWE-Agent对OmniCode进行了评估，结果显示虽然它们在Python错误修复方面表现良好，但在测试生成等任务以及C++和Java等语言中表现不足。例如，在Java测试生成任务中，SWE-Agent使用DeepSeek-V3.1最多仅达到20.9%。OmniCode旨在成为一种稳健的基准，推动开发能够在软件开发不同方面表现良好的代理。代码和数据可在https://github.com/seal-research/OmniCode获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind OmniCode is to provide a comprehensive benchmark for evaluating the capabilities of LLM-powered coding agents in real-world software engineering tasks. The proposed benchmark includes 1794 tasks across Python, Java, and C++, covering four categories: bug fixing, test generation, code review fixing, and style fixing. Unlike existing benchmarks that focus on narrow tasks, OmniCode features manually validated and synthetically generated tasks to avoid data leakage and ensure diversity. Evaluation with popular agent frameworks like SWE-Agent shows that while they perform reasonably on Python bug fixing, they struggle with tasks like test generation in Java and C++. For example, SWE-Agent achieves only 20.9% accuracy on Java test generation using DeepSeek-V3.1.</div>
<div class="mono" style="margin-top:8px">LLM驱动的编码代理正在改变现实世界的软件开发方式，但现有的基准测试如HumanEval和SWE-Bench在任务范围上较为狭窄，主要集中在编程竞赛和补丁生成。为弥补这一不足，OmniCode被提出作为一个更全面的基准测试，涵盖四个关键类别：错误修复、测试生成、代码审查修复和风格调整，共包含1794个任务，覆盖Python、Java和C++三种编程语言。这些任务经过人工验证并合成生成，以避免数据泄露问题，确保对代理能力的严格评估。使用SWE-Agent等框架进行评估表明，尽管它们在Python错误修复方面表现良好，但在Java和C++的测试生成等任务上仍存在明显不足。</div>
</details>
</div>
<div class="card">
<div class="title">TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation</div>
<div class="meta-line">Authors: Biao Xiong, Zhen Peng, Ping Wang, Qiegen Liu, Xian Zhong</div>
<div class="meta-line">First: 2026-02-06T15:36:50+00:00 · Latest: 2026-02-06T15:36:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07100v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07100v1">PDF</a> · <a href="https://github.com/rosolose/TLC-PLAN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TLC-Plan：一种基于两级码本的网络用于端到端向量平面图生成</div>
<div class="mono" style="margin-top:8px">自动化平面图生成旨在通过联合建模全局空间组织和精确几何细节来提高设计质量、建筑效率和可持续性。然而，现有方法在光栅空间中操作，并依赖后处理向量化，这会引入结构不一致并阻碍端到端学习。受组合空间推理的启发，我们提出了TLC-Plan，这是一种分层生成模型，可直接从输入边界合成向量平面图，基于模块化和可重用的模式与人类建筑工作流程对齐。TLC-Plan采用两级VQ-VAE，将全局布局编码为语义标记的房间边界框，并利用多边形级码细化局部几何。这种层次结构通过CodeTree表示统一，同时，一个自回归Transformer根据边界采样码，生成多样且拓扑有效的设计，无需显式的房间拓扑或尺寸先验。大量实验表明，该方法在RPLAN数据集上达到最先进的性能（FID = 1.84，MSE = 2.06），并在LIFULL数据集上取得领先结果。所提出的框架推进了面向现实建筑应用的约束感知和可扩展的向量平面图生成。源代码和训练模型已发布在https://github.com/rosolose/TLC-PLAN。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automated floorplan generation seeks to enhance design quality and efficiency by integrating global spatial organization with precise geometric details. Current methods, however, operate in raster space and depend on post hoc vectorization, leading to structural inconsistencies and limiting end-to-end learning. To address these issues, TLC-Plan introduces a hierarchical generative model based on a two-level VQ-VAE, which encodes global layouts as semantically labeled room bounding boxes and refines local geometries using polygon-level codes. The model uses a CodeTree representation to unify these levels and an autoregressive transformer to sample codes conditioned on input boundaries, enabling the creation of diverse and topologically valid designs. Experimental results on the RPLAN and LIFULL datasets demonstrate state-of-the-art performance, with FID scores of 1.84 and MSE of 2.06, highlighting its effectiveness in constraint-aware and scalable vector floorplan generation.</div>
<div class="mono" style="margin-top:8px">自动平面图生成旨在通过联合建模全局空间组织和精确几何细节来提升设计质量、建筑效率和可持续性。传统方法在光栅空间中操作，并依赖后处理向量化，导致结构不一致并限制端到端学习。为了解决这些问题，TLC-Plan 提出了一种基于两级 VQ-VAE 的分层生成模型，将全局布局编码为语义标记的房间边界框，并利用多边形级代码细化局部几何。该模型通过 CodeTree 表示统一了层次结构，并采用自回归 Transformer 根据边界条件采样代码，从而生成多样且拓扑有效的设计。在 RPLAN 和 LIFULL 数据集上的实验结果表明，其 FID 为 1.84，MSE 为 2.06，分别达到最先进的水平，展示了其在约束感知和可扩展的向量平面图生成中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks</div>
<div class="meta-line">Authors: Junxian Li, Kai Liu, Leyang Chen, Weida Wang, Zhixin Wang, Jiaqi Xu, Fan Li, Renjing Pei, Linghe Kong, Yulun Zhang</div>
<div class="meta-line">First: 2026-02-06T12:47:16+00:00 · Latest: 2026-02-06T12:47:16+00:00</div>
<div class="meta-line">Comments: The main part of our paper: PlanViz Code is at: https://github.com/lijunxian111/PlanViz Supplementary material is at: https://github.com/lijunxian111/PlanViz/releases/tag/v1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06663v1">PDF</a> · <a href="https://github.com/lijunxian111/PlanViz">Code1</a> · <a href="https://github.com/lijunxian111/PlanViz/releases/tag/v1">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&amp;UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PlanViz：评估面向计算机使用任务的规划导向图像生成与编辑</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在生成自然图像和多模态推理方面展现出令人印象深刻的能力。然而，它们在支持与我们的生活密切相关的计算机使用规划任务方面的潜力仍待探索。在计算机使用任务中，图像生成和编辑需要空间推理和程序理解等能力，目前尚不清楚UMMs是否具备完成这些任务的能力。因此，我们提出了PlanViz，一个专门用于评估计算机使用任务中图像生成与编辑的新基准。为了实现我们的评估目标，我们关注那些频繁出现在日常生活中并需要规划步骤的子任务。具体来说，设计了三个新的子任务：路线规划、工作流程图示和网页与用户界面展示。我们通过整理人工标注的问题和参考图像，并实施质量控制流程，来应对数据质量方面的挑战。针对全面且精确的评估问题，我们提出了一个任务自适应评分系统PlanScore，该评分系统有助于理解生成图像的正确性、视觉质量和效率。通过实验，我们突出了该领域研究的关键局限性与未来研究机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to explore the potential of unified multimodal models (UMMs) in supporting computer-use planning tasks, which are essential for daily life. To evaluate this, the authors introduce PlanViz, a benchmark that includes three sub-tasks: route planning, work diagramming, and web&amp;UI displaying, which require spatial reasoning and procedural understanding. They curate high-quality human-annotated data and propose a task-adaptive evaluation metric called PlanScore, which assesses the correctness, visual quality, and efficiency of generated images. Experimental results reveal the current limitations of UMMs in these tasks and identify opportunities for future research.</div>
<div class="mono" style="margin-top:8px">本文提出了PlanViz，这是一个用于评估统一多模态模型（UMMs）在计算机使用任务中进行规划导向图像生成和编辑能力的基准测试。研究动机源于UMMs在涉及空间推理和程序理解的日常任务中潜力尚未被充分探索，例如路线规划、工作流程图绘制和网页与用户界面展示。作者通过整理人工标注的问题和参考图像，并结合质量控制流程，确保数据质量。同时，提出了一种任务自适应的评估指标PlanScore，用于衡量生成图像的正确性、视觉质量和效率。实验结果揭示了当前UMMs在这些规划任务中的主要局限，并指出了未来研究的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</div>
<div class="meta-line">Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</div>
<div class="meta-line">First: 2026-02-05T18:59:55+00:00 · Latest: 2026-02-06T06:50:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06041v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06041v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视角描述预测相机姿态以实现空间推理</div>
<div class="mono" style="margin-top:8px">多图像空间推理对于当前的多模态大语言模型（MLLMs）仍然是一个挑战。虽然单视角感知本质上是2D的，但跨视角推理需要在不同视角之间建立一致的场景理解。我们特别研究了视角转换（perspective taking），其中模型必须从多视角观察中构建一致的3D理解，并据此在新的语言指定视角下进行推理。我们提出了CAMCUE，这是一个具有姿态感知的多图像框架，它将相机姿态作为跨视角融合和新视角推理的显式几何锚点。CAMCUE将每个视角的姿态注入视觉标记中，将自然语言视角描述锚定到目标相机姿态，并合成基于姿态的想象目标视角以支持回答。为支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例将多视角图像与姿态配对，并包含多样化的目标视角描述和视角转换问题。我们还在测试集中加入了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确率提升了9.06%，并且能够从自然语言视角描述中预测目标姿态，其旋转准确率超过90%（在20°以内），平移准确率在0.5误差阈值内。这种直接的锚定避免了昂贵的测试时搜索和匹配过程，将推理时间从每个实例256.6秒减少到1.45秒，从而实现了快速、交互式的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance spatial reasoning capabilities in multi-image settings for multimodal large language models (MLLMs), which struggle with coherent scene understanding across different viewpoints. The proposed method, CAMCUE, introduces a pose-aware framework that explicitly uses camera poses as geometric anchors for cross-view fusion and novel-view reasoning. It integrates per-view camera poses into visual tokens, maps natural-language viewpoint descriptions to target poses, and synthesizes imagined views to support reasoning. The main experimental results show that CAMCUE improves overall accuracy by 9.06% and achieves over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold when predicting target poses from language descriptions. This approach significantly reduces inference time from 256.6s to 1.45s per example, enabling efficient and interactive use in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升多图像环境下多模态大语言模型（MLLMs）的空间推理能力，因为当前模型在不同视角间建立连贯场景理解方面仍存在挑战。提出的方法CAMCUE是一个基于相机姿态的框架，通过将相机姿态作为显式的几何锚点，实现跨视角融合和新视角推理。该方法将自然语言视角描述映射到目标相机姿态，并合成基于姿态的想象目标视角以支持回答。实验结果显示，CAMCUE整体准确率提升了9.06%，在旋转误差小于20°、平移误差小于0.5的条件下，旋转准确率超过90%。该方法避免了耗时的测试时搜索匹配过程，将推理时间从每例256.6秒降至1.45秒，从而实现了高效的实时应用。</div>
</details>
</div>
<div class="card">
<div class="title">MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation</div>
<div class="meta-line">Authors: Haoming Wang, Qiyao Xue, Weichen Liu, Wei Gao</div>
<div class="meta-line">First: 2026-02-06T06:17:29+00:00 · Latest: 2026-02-06T06:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07082v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07082v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM&#x27;s spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM&#x27;s spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MosaicThinker：通过多帧空间表示的迭代构建实现具身AI的本地化视觉空间推理</div>
<div class="mono" style="margin-top:8px">当具身AI从传统的物体检测和识别扩展到更高级的任务，如机器人操作和执行规划时，需要从视频输入中进行视觉空间推理以感知物体的空间关系并指导设备动作。然而，现有的视觉语言模型（VLMs）在空间推理方面能力较弱，主要是由于缺乏对3D空间信息的知识，尤其是在涉及多个视频帧的复杂空间关系推理任务中。本文提出了一种新的推理时计算技术，用于本地化具身AI，即\emph{MosaicThinker}，该技术增强了本地化小型VLM在困难的跨帧推理任务中的空间推理能力。我们的基本思路是将多帧中的碎片化空间信息整合到全局语义地图的统一空间表示中，并通过视觉提示进一步引导VLM在语义地图上进行空间推理。实验结果表明，我们的技术可以显著提升资源受限的具身AI设备在跨帧空间推理任务中的准确性，适用于多种类型和复杂度的推理任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this paper is to improve spatial reasoning capabilities in on-device embodied AI systems, particularly for complex cross-frame tasks that existing visual language models (VLMs) struggle with due to their limited understanding of 3D spatial information. The proposed method, MosaicThinker, constructs a global semantic map by integrating spatial information from multiple video frames and uses a visual prompt to guide the VLM&#x27;s reasoning over this map. Experimental results demonstrate that this approach significantly enhances the accuracy of cross-frame spatial reasoning on resource-constrained devices.</div>
<div class="mono" style="margin-top:8px">随着具身AI从目标检测扩展到机器人操作和运动规划，跨多个视频帧进行视觉空间推理的能力变得至关重要。然而，现有的视觉语言模型在处理复杂的三维空间关系方面存在不足。为此，本文提出了一种名为MosaicThinker的设备端推理时间技术，通过将多帧中的碎片化空间信息整合到全局语义地图中，并利用视觉提示引导模型进行空间推理，从而增强小型VLM的空间推理能力。实验结果表明，该方法在资源受限的设备上显著提升了跨帧空间推理任务的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics</div>
<div class="meta-line">Authors: Zhuofan Shi, Hubao A, Yufei Shao, Dongliang Huang, Hongxu An, Chunxiao Xin, Haiyang Shen, Zhenyu Wang, Yunshan Na, Gang Huang, Xiang Jing</div>
<div class="meta-line">First: 2026-01-05T12:56:51+00:00 · Latest: 2026-02-06T05:47:09+00:00</div>
<div class="meta-line">Comments: 24 pages,4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02075v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.02075v4">PDF</a> · <a href="https://github.com/FredericVAN/PKU_MDAgent2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&amp;A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MDAgent2：分子动力学领域的大语言模型用于代码生成和知识问答</div>
<div class="mono" style="margin-top:8px">分子动力学（MD）模拟对于理解材料科学中的原子尺度行为至关重要，但编写LAMMPS脚本仍然是高度专业化且耗时的任务。尽管大语言模型（LLMs）在代码生成和领域特定问答方面展现出潜力，但其在MD场景中的表现受到领域数据稀缺、先进LLMs的高部署成本以及生成代码可执行性低的限制。基于我们之前提出的MDAgent，我们提出了MDAgent2，这是首个能够在MD领域内同时进行知识问答和代码生成的端到端框架。我们构建了一个领域特定的数据构建流水线，生成了三个高质量的数据集，涵盖MD知识、问答和代码生成。基于这些数据集，我们采用三阶段后训练策略——持续预训练（CPT）、监督微调（SFT）和强化学习（RL）——来训练两个领域适应模型：MD-Instruct和MD-Code。此外，我们引入了MD-GRPO，一种闭环强化学习方法，利用模拟结果作为奖励信号，并重用低奖励轨迹以实现持续优化。我们还构建了MDAgent2-RUNTIME，一个可部署的多智能体系统，集成了代码生成、执行、评估和自我修正功能。结合本文提出的MD-EvalBench，这是首个针对LAMMPS代码生成和问答的基准测试，我们的模型和系统在性能上超越了多个强大的基线模型。本工作系统地展示了大语言模型在工业模拟任务中的适应性和泛化能力，为科学计算和大规模工业模拟中的自动代码生成奠定了方法论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to address the challenges of code generation and knowledge question answering in molecular dynamics (MD) simulations, where existing large language models (LLMs) face limitations due to scarce domain data, high deployment costs, and low code executability. MDAgent2 introduces a domain-specific data-construction pipeline and a three-stage post-training strategy—continued pre-training, supervised fine-tuning, and reinforcement learning—to train two models, MD-Instruct and MD-Code. Additionally, it proposes MD-GRPO, a closed-loop reinforcement learning method that uses simulation outcomes as reward signals and refines low-reward trajectories. The system also includes MDAgent2-RUNTIME, a deployable multi-agent framework that integrates code generation, execution, evaluation, and self-correction. Experimental results show that the proposed models and system outperform several strong baselines on MD-EvalBench, a newly introduced benchmark for LAMMPS code generation and question answering.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决分子动力学（MD）模拟中代码生成和知识问答的挑战，现有大语言模型（LLMs）在该领域面临数据稀缺、部署成本高和代码可执行性低等问题。MDAgent2引入了一个领域专用的数据构建管道，并采用三阶段后训练策略，包括持续预训练、监督微调和强化学习，训练出两个模型：MD-Instruct用于问答，MD-Code用于代码生成。此外，提出了MD-GRPO，一种利用模拟结果作为奖励信号的闭环强化学习方法，以及MDAgent2-RUNTIME，一个可部署的多智能体系统，支持代码生成、执行、评估和自我修正。实验结果表明，所提出的模型和系统在MD-EvalBench基准测试中优于多个强基线模型，展示了LLMs在工业模拟任务中的适应性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the effects of data contamination in Code Intelligence</div>
<div class="meta-line">Authors: Zhen Yang, Hongyi Lin, Yifan He, Junqi Wang, Zeyu Sun, Shuo Liu, Jie Xu, Pengpeng Wang, Zhongxing Yu, Qingyuan Liang</div>
<div class="meta-line">First: 2025-06-03T12:15:44+00:00 · Latest: 2026-02-06T05:44:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.02791v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.02791v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation. Previous studies mainly focused on sample-level contamination, ignoring partial contamination scenarios that are pervasive in code intelligence. This paper fills this gap and presents a systematic empirical study to investigate the fine-grained data contamination on mainstream code tasks. Our study involves diverse representative PLMs: RoBERTa and GPT-2, and LLMs: LLaMA and StarCoder, covering three major tasks: code translation, code generation, and code summarization, across two Programming Languages (PLs): Java and Python. We categorize contamination scenarios into four types according to the code intelligence practice, namely input-only, output-only, unpaired, and paired contamination settings, and construct corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination. Apart from the above, other contamination scenarios have no impact on both PLMs and LLMs. Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考数据污染对代码智能的影响</div>
<div class="mono" style="margin-top:8px">近年来，代码智能在自动化软件工程领域的重要性日益增加。同时，预训练语言模型（PLMs）和大语言模型（LLMs）的广泛应用引发了对数据污染及其对模型性能评估潜在影响的关注。以往的研究主要关注样本级别的污染，忽略了在代码智能中普遍存在的部分污染场景。本文填补了这一空白，提出一项系统性的实证研究，探讨主流代码任务中细粒度的数据污染。我们的研究涵盖了多种具有代表性的PLMs：RoBERTa和GPT-2，以及LLMs：LLaMA和StarCoder，涉及三个主要任务：代码翻译、代码生成和代码摘要，覆盖两种编程语言（Java和Python）。我们根据代码智能实践将污染场景分为四类：仅输入污染、仅输出污染、非配对污染和配对污染，并构建了相应的实验组和对照组进行探索。实验结果表明，在PLMs采用的预训练、微调和推理范式下，即使故意注入配对污染，也不会导致显著的性能高估。但直接推理或小规模微调则会揭示污染的影响。相比之下，采用预训练和推理范式的LLMs则显著受到配对污染的影响。除此之外，其他污染场景对PLMs和LLMs均无影响。我们的研究结果挑战了传统观点，即污染必然导致性能高估，为代码智能模型的评估和部署提供了新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the effects of data contamination in code intelligence, addressing the gap in previous studies that mainly focused on sample-level contamination while neglecting partial contamination scenarios. The authors conduct a systematic empirical study on mainstream code tasks—code translation, code generation, and code summarization—using PLMs like RoBERTa and GPT-2, and LLMs like LLaMA and StarCoder, across Java and Python. They categorize contamination into four types and design corresponding experiments. Results indicate that under the pre-training, fine-tuning, and inference paradigm, paired contamination does not significantly overestimate performance, but direct inference or small-scale fine-tuning reveals its impact. LLMs are more vulnerable to paired contamination than PLMs, challenging the assumption that contamination always leads to performance overestimation.</div>
<div class="mono" style="margin-top:8px">本文探讨了数据污染对代码智能模型的影响，挑战了数据污染必然导致性能高估的传统观点。研究系统分析了四种数据污染场景——输入仅污染、输出仅污染、未配对污染和配对污染，涵盖代码翻译、代码生成和代码摘要三项任务，使用了RoBERTa、GPT-2等PLM模型以及LLaMA、StarCoder等LLM模型。实验结果表明，在标准的预训练、微调和推理范式下，数据污染对模型性能影响不大，但直接推理或小规模微调则能揭示其影响。值得注意的是，LLM模型相较于PLM模型更容易受到配对污染的影响，为代码智能模型的评估与部署提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning</div>
<div class="meta-line">Authors: Bangji Yang, Ruihan Guo, Jiajun Fan, Chaoran Cheng, Ge Liu</div>
<div class="meta-line">First: 2026-02-05T20:10:27+00:00 · Latest: 2026-02-05T20:10:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06166v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>M3：通过多模态、多智能体和多轮次视觉推理实现高保真文本到图像生成</div>
<div class="mono" style="margin-top:8px">生成模型在文本到图像合成中已实现了令人印象深刻的保真度，但在涉及多个约束的复杂组合提示中仍存在困难。我们引入了\textbf{M3（多模态、多智能体、多轮次）}，这是一种无需训练的框架，通过迭代推理时的细化系统性地解决这些失败。M3在一个稳健的多智能体循环中协调现成的基础模型：一个规划器将提示分解为可验证的清单，而专门的检查器、细化器和编辑器智能体则依次精确修正约束，验证器确保持续改进。在开源模型上应用M3，在具有挑战性的OneIG-EN基准测试中取得了显著成果，我们的Qwen-Image+M3超越了包括Imagen4（0.515）和Seedream 3.0（0.530）在内的商业旗舰系统，达到最先进的性能（总体0.532）。这表明智能多智能体推理可以将开源模型提升至超越专有模型的水平。此外，M3还显著提升了GenEval组合指标，在强化测试集上有效将空间推理性能翻倍。作为一个兼容任何预训练文本到图像模型的即插即用模块，M3为组合生成建立了一种新的范式，无需昂贵的重新训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of current text-to-image generative models in handling complex compositional prompts with multiple constraints. The proposed M3 framework employs a training-free approach that utilizes multi-modal, multi-agent, and multi-round visual reasoning to iteratively refine image generation at inference time. It integrates a Planner, Checker, Refiner, Editor, and Verifier to systematically decompose and correct constraints, leading to improved image quality. Experimental results show that M3 significantly enhances performance on the OneIG-EN benchmark, with Qwen-Image+M3 achieving a score of 0.532, surpassing commercial systems like Imagen4 and Seedream 3.0. Additionally, M3 improves compositional metrics in GenEval, doubling spatial reasoning performance on hardened test sets.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前文本到图像生成模型在处理包含多个约束条件的复杂组合提示时的不足。提出的M3框架采用多模态、多智能体和多轮次的方法，在推理过程中迭代优化图像生成，无需重新训练。它通过Planner将提示分解为可验证的清单，并利用专门的Checker、Refiner和Editor智能体依次修正约束，Verifier确保生成过程的持续改进。实验结果表明，M3在OneIG-EN基准测试中显著提升了性能，Qwen-Image+M3达到最先进的结果，超越了包括Imagen4和Seedream 3.0在内的商业旗舰模型。此外，M3在GenEval组合指标上也表现出显著提升，使空间推理性能在强化测试集上翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</div>
<div class="meta-line">Authors: Shravan Chaudhari, Rahul Thomas Jacob, Mononito Goswami, Jiajun Cao, Shihab Rashid, Christian Bock</div>
<div class="meta-line">First: 2025-12-18T01:32:25+00:00 · Latest: 2026-02-05T19:34:42+00:00</div>
<div class="meta-line">Comments: Initial preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16956v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving code functions, classes or files that are relevant in order to solve a given user query, bug report or feature request from large codebases is a fundamental challenge for Large Language Model (LLM)-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify semantically relevant units. While embedding-based approaches can outperform BM25 by large margins, they often don&#x27;t take into consideration the underlying graph-structured characteristics of the codebase. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that integrates LLM-based reasoning along with auxiliary information obtained from graph-based exploration of the codebase. We further introduce SpIDER-Bench, a graph-structured evaluation benchmark curated from SWE-PolyBench, SWEBench-Verified and Multi-SWEBench, spanning codebases from Python, Java, JavaScript and TypeScript programming languages. Empirical results show that SpIDER consistently improves dense retrieval performance by at least 13% across programming languages and benchmarks in SpIDER-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpIDER：基于空间信息的密集嵌入检索用于软件问题定位</div>
<div class="mono" style="margin-top:8px">从大型代码库中检索与给定用户查询、错误报告或功能请求相关的代码函数、类或文件，是基于大型语言模型（LLM）的编码代理的基本挑战。代理方法通常采用BM25等稀疏检索方法或密集嵌入策略来识别语义相关的单元。虽然基于嵌入的方法在性能上通常远超BM25，但它们往往忽略了代码库的底层图结构特性。为了解决这一问题，我们提出了SpIDER（空间信息密集嵌入检索），这是一种增强的密集检索方法，结合了基于LLM的推理以及从代码库图结构探索中获得的辅助信息。我们进一步引入了SpIDER-Bench，这是一个从SWE-PolyBench、SWEBench-Verified和Multi-SWEBench中整理的图结构评估基准，涵盖Python、Java、JavaScript和TypeScript等编程语言的代码库。实证结果表明，SpIDER在SpIDER-Bench中的各种编程语言和基准测试中，密集检索性能至少提升了13%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The challenge of retrieving relevant code units from large codebases for software issue localization is addressed by SpIDER, which enhances dense retrieval by incorporating spatial information from the codebase&#x27;s graph structure. SpIDER integrates LLM-based reasoning with graph-derived auxiliary data to better capture semantic and structural relationships. Experimental results on SpIDER-Bench, a benchmark combining multiple codebase datasets, demonstrate that SpIDER improves retrieval performance by at least 13% across different programming languages and benchmarks.</div>
<div class="mono" style="margin-top:8px">在大型代码库中检索与用户查询、错误报告或功能请求相关的代码单元是软件问题定位的核心挑战。为了解决这一问题，SpIDER提出了一种结合LLM推理和代码库图结构信息的密集检索方法。在SpIDER-Bench上进行的实验表明，该方法在多种编程语言和基准测试中将检索性能提升了至少13%。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-05T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v1">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近期在多模态大语言模型（MLLMs）中进行空间推理的研究越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这通常会导致语义与几何信息的错位以及冗余信号。我们提出了GeoThinker框架，将整合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力机制选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控机制进行校准，该机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面取得了新的最先进水平，在VSI-Bench上达到了72.6的峰值得分。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力和显著提升的空间感知能力，包括具身指称和自动驾驶。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), where geometric information is often misaligned with semantic content and leads to redundancy. The proposed GeoThinker framework introduces an active perception approach by enabling the model to selectively retrieve geometric evidence based on its internal reasoning needs. It achieves this through Spatial-Grounded Fusion at specific VLM layers, using frame-strict cross-attention and Importance Gating to focus on task-relevant geometric structures. Experimental results on VSI-Bench show that GeoThinker achieves a peak score of 72.6, outperforming existing methods and demonstrating strong generalization in complex scenarios such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）在空间推理任务中被动几何融合的局限性，指出其常导致语义与几何信息错位及冗余信号的问题。提出GeoThinker框架，采用主动感知方法，使模型能够根据内部推理需求选择性地检索几何证据。该方法通过在特定VLM层应用空间基础融合，结合帧严格交叉注意力和重要性门控机制，聚焦于任务相关的几何结构。在VSI-Bench上的实验结果表明，GeoThinker取得72.6的峰值得分，优于现有方法，并在复杂场景如具身指称和自动驾驶中展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-05T17:10:26+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cioutn.github.io/context-bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：面向编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估框架。ContextBench包含来自8种编程语言、66个仓库的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四个前沿的LLM和五个编码代理。我们的结果表明，复杂的代理结构在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有端到端基准，从而揭示了问题解决过程。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。数据和代码可在：https://cioutn.github.io/context-bench/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate how coding agents retrieve and utilize code context during issue resolution, rather than just focusing on final task success. ContextBench introduces a process-oriented benchmark with 1,136 tasks from 66 repositories across eight programming languages, each annotated with gold contexts. The authors developed an automated evaluation framework to track agent behavior and measure context recall, precision, and efficiency. Experimental results reveal that advanced agent scaffolding provides only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between explored and used context. These findings highlight the importance of intermediate context metrics in understanding and improving coding agent performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，即主要关注最终任务成功而忽视上下文检索过程。ContextBench引入了一个面向过程的基准测试，包含来自66个仓库、涵盖八种编程语言的1,136个问题解决任务，每个任务都附有人工标注的黄金上下文。作者开发了一个自动化评估框架，用于追踪代理的行为并评估问题解决过程中上下文的召回率、精确率和效率。实验结果表明，先进的代理结构在上下文检索方面仅带来小幅提升，LLMs倾向于优先召回而非精确，且存在探索与实际使用上下文之间的显著差距。这些发现强调了在软件任务中引入中间上下文指标的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution</div>
<div class="meta-line">Authors: Philipp D. Siedler</div>
<div class="meta-line">First: 2025-05-21T22:00:20+00:00 · Latest: 2026-02-05T16:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16048v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.16048v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPhyR：基于拓扑优化的材料分布空间-物理推理基准数据集</div>
<div class="mono" style="margin-top:8px">我们引入了一个新颖的数据集，用于基于拓扑优化方法评估大型语言模型（LLM）的物理和空间推理能力。该方法用于在给定载荷和支撑条件下计算设计空间内的最优材料分布。在该数据集中，LLM会接收到诸如2D边界、施加的力和支撑等条件，并需推理出相应的最优材料分布。该数据集包含多种任务，从填充部分结构中的掩码区域到预测完整的材料分布。解决这些任务需要理解力的流动以及在给定约束下的所需材料分布，而无需访问仿真工具或显式的物理模型，从而挑战模型对结构稳定性和空间组织的推理能力。我们的数据集旨在评估二维环境下的空间和物理推理能力，为传统的语言和逻辑基准提供补充视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SPhyR, a novel dataset aimed at evaluating the spatial and physical reasoning capabilities of Large Language Models (LLMs) in material distribution tasks. The dataset is based on topology optimization, where LLMs are given 2D boundary conditions, applied forces, and supports, and must infer the optimal material layout without access to simulation tools or explicit physical models. Experimental results show that models are able to predict material distributions and fill in masked regions in partial structures, demonstrating their ability to reason about force flow and structural stability in a 2D context.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为SPhyR的新数据集，旨在评估大型语言模型（LLM）在材料分布任务中的空间和物理推理能力。该数据集基于拓扑优化方法，为LLMs提供二维边界条件、施加的力和支撑条件，要求其在没有仿真工具或显式物理模型的情况下推断出最优材料分布。任务包括预测完整的材料分布和填充部分结构的掩码区域，需要模型理解力的流动和结构稳定性。实验结果表明，当前的LLMs在这些任务上表现不佳，凸显了在物理和空间推理领域提升模型能力的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非自我中心感知者：通过帧实例化将非自我中心推理与自我中心视觉先验分离</div>
<div class="mono" style="margin-top:8px">随着对空间基础任务（如视觉-语言导航/动作）的需求增加，视觉-语言模型（VLMs）中的非自我中心感知能力正受到越来越多的关注。然而，VLMs在需要显式视角转换的非自我中心空间查询上仍表现脆弱，答案依赖于目标中心的推理框架，而非观察到的相机视角。因此，我们引入了Allocentric Perceiver，这是一种无需训练的策略，通过使用现成的几何专家从一张或多张图像中恢复度量3D状态，然后实例化一个与指令语义意图对齐的查询条件非自我中心参考框架。通过确定性地将重建的几何结构转换到目标框架，并使用结构化的、基于几何的表示提示主干VLM，Allocentric Perceiver将心理旋转从隐式推理转移到显式计算。我们在多个主干家族上评估Allocentric Perceiver在空间推理基准测试中的表现，观察到在非自我中心任务中取得了稳定且显著的提升（约10%），同时保持了强大的自我中心性能，并超越了空间感知微调模型以及最先进的开源和专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing demand for spatially grounded tasks such as Vision-Language Navigation and Action has highlighted the limitations of Vision-Language Models (VLMs) in handling allocentric spatial queries that require explicit perspective shifts. To address this, the Allocentric Perceiver is introduced as a training-free method that reconstructs metric 3D states from images using off-the-shelf geometric experts and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. This approach transforms reconstructed geometry into the target frame deterministically and prompts the VLM with structured, geometry-grounded representations, shifting mental rotation from implicit reasoning to explicit computation. Experimental results show consistent and significant improvements ($\sim$10%) on allocentric tasks across multiple backbone families, while maintaining strong performance on egocentric tasks, and outperforming both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div>
<div class="mono" style="margin-top:8px">随着对空间感知任务如视觉-语言导航和行动的需求增加，视觉-语言模型（VLMs）在处理需要视角转换的分配中心空间查询时表现出局限性。为此，本文提出了Allocentric Perceiver，一种无需训练的方法，通过几何专家从图像中恢复度量3D状态，并构建与指令语义意图一致的查询条件分配中心参考系。该方法通过确定性地将重建的几何信息转换为目标视角，并使用结构化的几何基础表示来提示主干VLM，将心理旋转从隐式推理转移到显式计算。实验结果表明，在分配中心任务上取得了显著提升（约10%），同时保持了在分配中心任务上的强性能，优于空间感知微调模型和当前最先进的开源及专有模型。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
