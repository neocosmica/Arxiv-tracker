<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-22 03:45</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260222_0345</div>
    <div class="row"><div class="card">
<div class="title">HiMAP: History-aware Map-occupancy Prediction with Fallback</div>
<div class="meta-line">Authors: Yiming Xu, Yi Yang, Hao Cheng, Monika Sester</div>
<div class="meta-line">First: 2026-02-19T10:24:02+00:00 · Latest: 2026-02-19T10:24:02+00:00</div>
<div class="meta-line">Comments: Accepted in 2026 IEEE International Conference on Robotics and Automation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17231v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17231v1">PDF</a> · <a href="https://github.com/XuYiMing83/HiMAP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiMAP：具有回退机制的历史感知地图占用预测</div>
<div class="mono" style="margin-top:8px">准确的运动预测对于自动驾驶至关重要，但大多数预测方法依赖于基于身份关联的多目标跟踪（MOT），假设目标能够被正确且连续地跟踪。当由于遮挡、身份切换或检测丢失等原因导致跟踪失败时，预测质量会下降，安全风险增加。我们提出了\textbf{HiMAP}，一种无需跟踪的轨迹预测框架，在MOT失败时仍能保持可靠性。HiMAP将过去的检测结果转换为时空不变的历史占用地图，并引入一个历史查询模块，根据当前智能体状态从无标签的占用表示中迭代检索特定智能体的历史信息。检索到的历史信息通过时间映射嵌入进行总结，并与最终查询和地图上下文结合，驱动类似DETR的解码器生成多模态的未来轨迹。这种设计消除了对身份的依赖，支持通过可重用编码进行流式推理，并在跟踪不可用时作为稳健的回退方案。在Argoverse~2数据集上，HiMAP在不使用身份信息的情况下实现了与基于跟踪方法相当的性能，并在无跟踪设置下显著优于强大的基线模型，相较于微调后的QCNet，在FDE上相对提升11\%，在ADE上相对提升12\%，MR降低4\%。除了整体指标外，HiMAP能够在不等待跟踪恢复的情况下同时为所有智能体提供稳定的预测，突显了其在安全关键型自动驾驶中的实际价值。代码可在以下链接获取：https://github.com/XuYiMing83/HiMAP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate motion forecasting is essential for autonomous driving, but traditional methods depend on multi-object tracking (MOT) with identity association, which can fail under occlusion or missed detections. HiMAP is a tracking-free framework that generates reliable predictions even when MOT fails. It transforms past detections into spatiotemporally invariant occupancy maps and uses a historical query module to retrieve agent-specific history from unlabeled data. The retrieved history, combined with current state and map context, is used to generate multi-modal future trajectories through a DETR-style decoder. HiMAP performs comparably to tracking-based methods on Argoverse~2 without using object IDs and significantly outperforms baselines in the no-tracking setting, achieving 11% and 12% relative improvements in FDE and ADE, respectively, and a 4% reduction in MR compared to QCNet.</div>
<div class="mono" style="margin-top:8px">HiMAP 是为了解决自动驾驶系统中运动预测在多目标跟踪（MOT）失败时的局限性而提出的，特别是在遮挡、身份切换或检测遗漏等情况下。与依赖目标身份的传统方法不同，HiMAP 采用无跟踪方法，通过将过去的检测转换为时空不变的历史占用地图来实现。它引入了一个历史查询模块，从无标签的占用表示中检索特定代理的历史信息，并通过时间映射嵌入进行总结。结合当前代理状态和地图上下文，HiMAP 使用 DETR 风格的解码器生成多模态的未来轨迹。在 Argoverse~2 数据集上，HiMAP 在不使用目标 ID 的情况下实现了与基于跟踪方法相当的性能，并在无跟踪设置下显著优于其他基线模型，FDE、ADE 分别提升了 11% 和 12%，MR 减少了 4%。</div>
</details>
</div>
<div class="card">
<div class="title">Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering</div>
<div class="meta-line">Authors: Kishan Maharaj, Nandakishore Menon, Ashita Saxena, Srikanth Tamilselvam</div>
<div class="meta-line">First: 2026-02-19T09:05:03+00:00 · Latest: 2026-02-19T09:05:03+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 Figures, 5 Tables, Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17183v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17183v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型在长上下文代码问答中的鲁棒性与推理保真度</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）越来越多地用于需要对长代码上下文进行推理的软件工程任务，但其在不同输入条件下的鲁棒性仍不明确。我们通过受控的消融实验系统地研究了长上下文代码问答的敏感性，包括答案格式、干扰项和上下文规模。我们扩展了LongCodeBench Python数据集，新增了COBOL和Java的问答集，并在三种设置下评估了最先进的模型：(i) 混淆的多项选择题，(ii) 开放式问题，以及(iii) 包含相关和对抗性无关信息的“大海捞针”式上下文。结果表明，在混淆的多项选择题和开放式问题中，模型性能显著下降，并且在存在无关提示的情况下表现出脆弱的行为。我们的研究突显了当前长上下文评估的局限性，并为评估传统和现代系统中的代码推理提供了更广泛的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the robustness and reasoning fidelity of large language models (LLMs) in long-context code question answering, as LLMs are increasingly used in software engineering tasks that require understanding extensive codebases. The researchers extend the LongCodeBench dataset with COBOL and Java question-answer pairs and evaluate state-of-the-art models under three scenarios: shuffled multiple-choice options, open-ended questions, and contexts with irrelevant information. The results indicate significant performance degradation in shuffled options and open-ended settings, as well as fragile responses when irrelevant cues are present, highlighting the limitations of current long-context evaluation methods and the need for more comprehensive benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在长上下文代码问答任务中的鲁棒性和推理保真度，动机源于LLMs在软件工程任务中的广泛应用。研究人员扩展了LongCodeBench数据集，增加了COBOL和Java的问答对，并在三种场景下评估了最先进的模型：打乱的多项选择项、开放式问题以及包含相关和对抗性无关信息的上下文。实验结果表明，在打乱选项和开放式问题设置下，模型性能显著下降，且在存在无关提示时表现出脆弱性，揭示了当前长上下文评估方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses</div>
<div class="meta-line">Authors: Kan Watanabe, Rikuto Tsuchida, Takahiro Monno, Bin Huang, Kazuma Yamasaki, Youmei Fan, Kazumasa Shimari, Kenichi Matsumoto</div>
<div class="meta-line">First: 2026-02-19T05:06:31+00:00 · Latest: 2026-02-19T05:06:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17084v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17084v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何沟通：对拉取请求描述特征和人类审查响应的研究</div>
<div class="mono" style="margin-top:8px">大型语言模型的快速采用催生了能够在GitHub上自主创建拉取请求的AI编码代理。然而，这些代理在拉取请求描述特征上的差异，以及人类审查者如何响应它们，仍缺乏深入研究。在本研究中，我们使用AIDev数据集对五个AI编码代理创建的拉取请求进行了实证分析。我们分析了代理在拉取请求描述特征上的差异，包括结构特征，并从审查活动、响应时间、情感和合并结果等方面考察了人类审查者的响应。我们发现，AI编码代理表现出不同的拉取请求描述风格，这些风格与审查者的参与度、响应时间和合并结果存在关联。我们观察到，代理在审查者互动指标和合并率方面存在显著差异。这些发现突显了拉取请求呈现方式和审查者互动动态在人机协作软件开发中的作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents generate pull request descriptions and how human reviewers respond to them. With the increasing use of large language models in software development, understanding the characteristics of AI-generated pull requests and their impact on human review processes is crucial. By analyzing pull requests from five AI coding agents using the AIDev dataset, the research identifies differences in description styles and their correlations with reviewer engagement, response timing, and merge outcomes. The results show that variations in PR descriptions significantly influence human reviewer behavior and the likelihood of successful merges.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理生成拉取请求描述的方式以及人类审阅者对此的反应。通过分析AIDev数据集中五个AI编码代理生成的拉取请求，研究识别了它们在描述结构上的差异，并考察了审阅者的互动行为，包括活跃度、响应时间、情感倾向和合并结果。研究结果表明，AI代理具有不同的拉取请求描述风格，这影响了审阅者的参与度、响应速度以及合并的可能性，突显了有效沟通在人机协作软件开发中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Wink: Recovering from Misbehaviors in Coding Agents</div>
<div class="meta-line">Authors: Rahul Nanda, Chandra Maddila, Smriti Jha, Euna Mehnaz Khan, Matteo Paltenghi, Satish Chandra</div>
<div class="meta-line">First: 2026-02-19T03:15:00+00:00 · Latest: 2026-02-19T03:15:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user&#x27;s instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wink：在编码代理中恢复误行为</div>
<div class="mono" style="margin-top:8px">基于大型语言模型（LLMs）的自主编码代理正越来越多地被软件行业采用，以自动化复杂的工程任务。然而，这些代理容易出现各种误行为，例如偏离用户指令、陷入重复循环，或未能正确使用工具。这些失败会干扰开发流程，通常需要耗费大量资源的人工干预。在本文中，我们提出了一种大规模自动恢复代理误行为的系统。我们首先基于生产流量分析，引入了一种误行为分类体系，识别出三种主要类别：规范漂移、推理问题和工具调用失败，我们发现这三种误行为约占所有代理轨迹的30%。
  为了解决这些问题，我们开发了一种轻量级、异步的自我干预系统，名为Wink。Wink观察代理轨迹，并提供有针对性的纠偏指导，以引导代理回到高效的工作路径。我们在超过10,000条真实代理轨迹上评估了该系统，发现它成功解决了约90%需要单次干预的误行为。此外，在我们的生产环境中进行的实时A/B测试表明，该系统在工具调用失败、每会话令牌数和每会话工程师干预次数方面实现了统计学意义上的显著降低。我们分享了设计和部署该系统的经验，提供了在大规模构建弹性代理系统时面临的挑战的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing adoption of autonomous coding agents in the software industry has highlighted the need for robust recovery mechanisms due to frequent misbehaviors such as specification drift, reasoning problems, and tool call failures, which occur in about 30% of agent trajectories. To address these issues, the authors developed Wink, a lightweight, asynchronous self-intervention system that observes agent behavior and provides targeted guidance to correct deviations. Evaluation on over 10,000 real-world trajectories showed that Wink successfully resolves 90% of single-intervention misbehaviors, and a live A/B test demonstrated significant reductions in tool call failures, tokens per session, and engineer interventions per session.</div>
<div class="mono" style="margin-top:8px">随着自主编码代理在软件行业的广泛应用，处理其行为偏差成为保障开发流程顺畅的重要课题。这些偏差包括规范漂移、推理问题和工具调用失败，约占所有代理轨迹的30%。为解决这些问题，作者开发了轻量级的异步自我干预系统Wink，该系统通过观察代理行为并提供针对性指导，引导其回到有效的工作路径。在超过10,000条真实轨迹上的评估显示，Wink能够成功解决90%需要单次干预的行为问题，且在生产环境中的A/B测试表明，其显著降低了工具调用失败、每会话的标记数和工程师干预次数。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Multiagent Learning Algorithms with Large Language Models</div>
<div class="meta-line">Authors: Zun Li, John Schultz, Daniel Hennes, Marc Lanctot</div>
<div class="meta-line">First: 2026-02-18T22:41:00+00:00 · Latest: 2026-02-18T22:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16928v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型发现多智能体学习算法</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）在不完美信息博弈中的进展历史上很大程度上依赖于人工迭代优化基线。尽管基础方法家族如反事实遗憾最小化（CFR）和策略空间响应预言机（PSRO）建立在坚实的理论基础上，但其最有效的变体设计往往依赖于人类直觉来探索庞大的算法设计空间。在本工作中，我们提出使用AlphaEvolve，一种由大语言模型驱动的进化编码智能体，以自动发现新的多智能体学习算法。我们通过在两种不同的博弈论学习范式中演化出新颖的变体，展示了该框架的通用性。首先，在迭代遗憾最小化领域，我们演化出控制遗憾累积和策略推导的逻辑，发现了一种新的算法：波动自适应折扣（VAD-CFR）。VAD-CFR采用新颖且非直观的机制，包括波动敏感的折扣、一致性强化的乐观策略以及硬启动策略累积时间表，从而超越了诸如折扣预测CFR+等最先进的基线。其次，在基于种群的训练算法领域，我们演化出PSRO的训练时和评估时元策略求解器，发现了一种新的变体：平滑混合乐观遗憾（SHOR-PSRO）。SHOR-PSRO引入了一种混合元策略求解器，通过线性融合乐观遗憾匹配与平滑、温度控制的最佳纯策略分布。通过在训练过程中动态调整该融合因子和平滑多样性奖励，该算法实现了从种群多样性到严格均衡寻找的自动化过渡，相较于标准静态元策略求解器表现出更优的实证收敛性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of manually designing effective multiagent learning algorithms in imperfect-information games by proposing the use of AlphaEvolve, an evolutionary coding agent based on large language models. The framework is applied to two paradigms of game-theoretic learning: iterative regret minimization and population-based training. In the first, it discovers VAD-CFR, an algorithm that outperforms existing baselines through volatility-sensitive discounting and consistency-enforced optimism. In the second, it evolves SHOR-PSRO, a hybrid meta-solver that dynamically adjusts its strategy during training, leading to better convergence and equilibrium finding than traditional static approaches.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在不完全信息博弈中手动优化多智能体学习算法的挑战，传统方法如CFR和PSRO往往依赖人类直觉。作者提出使用基于大语言模型的进化编码代理AlphaEvolve，以自动发现新的算法。通过演化，他们提出了两个新变体：用于迭代遗憾最小化的VAD-CFR，该算法通过波动敏感的折扣机制和一致性强化的乐观策略优于现有方法；以及用于基于种群的训练的SHOR-PSRO，该算法结合了乐观遗憾匹配与平滑的策略分布，实现了优于标准静态元求解器的收敛性能。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid-Gym: Training Coding Agents to Generalize Across Tasks</div>
<div class="meta-line">Authors: Yiqing Xie, Emmy Liu, Gaokai Zhang, Nachiket Kotalwar, Shubham Gandhi, Sathwik Acharya, Xingyao Wang, Carolyn Rose, Graham Neubig, Daniel Fried</div>
<div class="meta-line">First: 2026-02-18T19:30:55+00:00 · Latest: 2026-02-18T19:30:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16819v1">PDF</a> · <a href="https://github.com/yiqingxyq/Hybrid-Gym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Hybrid-Gym: 训练编码代理跨任务泛化</div>
<div class="mono" style="margin-top:8px">在评估编码代理的质量时，主流基准主要关注在GitHub上解决单一问题，例如SWE-Bench。相反，在实际使用中，这些代理需要解决更多样化和复杂的任务，这些任务涉及探索代码库、测试软件和设计架构等其他技能。在本文中，我们首先通过将轨迹分解为细粒度组件，来表征一些跨任务的可迁移技能，并推导出一套设计辅助训练任务的原则，以教授语言模型这些技能。基于这些原则，我们提出了一种训练环境Hybrid-Gym，包含一系列可扩展的合成任务，如函数定位和依赖搜索。实验表明，使用我们合成任务训练的代理能够有效泛化到训练中未出现的多样化现实任务，在SWE-Bench Verified上提升25.4%，在SWT-Bench Verified上提升7.9%，在Commit-0 Lite上提升5.1%。Hybrid-Gym还能够补充下游任务构建的数据集（例如，在SWT-Bench Verified上提升SWE-Play 4.9%）。代码可在：https://github.com/yiqingxyq/Hybrid-Gym 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing coding agent benchmarks, which primarily evaluate models on solving single, specific coding tasks. To better reflect real-world scenarios where agents must handle diverse and complex tasks, the authors identify transferable skills through trajectory decomposition and propose a set of principles for designing auxiliary training tasks. Based on these, they introduce Hybrid-Gym, a scalable synthetic training environment that includes tasks like function localization and dependency search. Experimental results demonstrate that agents trained on Hybrid-Gym show significant improvements in generalization across real-world tasks, achieving absolute gains of 25.4% on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Additionally, Hybrid-Gym enhances the performance of existing datasets for downstream tasks.</div>
<div class="mono" style="margin-top:8px">本文针对现有编码代理基准测试的局限性，指出这些测试主要评估模型在GitHub上解决单一问题的能力，如SWE-Bench。作者通过分解任务轨迹，识别出跨任务的可迁移技能，并提出一套设计辅助训练任务的原则。基于这些原则，他们构建了Hybrid-Gym这一可扩展的合成训练环境，包含函数定位和依赖搜索等任务。实验结果表明，使用Hybrid-Gym训练的代理在泛化到未在训练中出现的真实任务上表现优异，分别在SWE-Bench Verified、SWT-Bench Verified和Commit-0 Lite上取得25.4%、7.9%和5.1%的绝对提升。此外，Hybrid-Gym还提升了现有下游任务数据集（如SWE-Play）的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Situated Awareness in the Real World</div>
<div class="meta-line">Authors: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan, Jiajun Wu, Ming-Hsuan Yang, Xin Eric Wang</div>
<div class="meta-line">First: 2026-02-18T18:22:52+00:00 · Latest: 2026-02-18T18:22:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16682v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent&#x27;s viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model&#x27;s observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在现实世界中学习情境意识</div>
<div class="mono" style="margin-top:8px">人类感知的一个核心方面是情境意识，即能够将自己与周围的物理环境建立联系，并在特定情境中推理可能的行动。然而，大多数现有的多模态基础模型（MFMs）基准测试强调以环境为中心的空间关系（场景中物体之间的关系），而忽视了以观察者为中心的关系，这种关系需要基于代理的视角、姿态和运动进行推理。为弥合这一差距，我们引入了SAW-Bench（现实世界中的情境意识），这是一个用于评估以自我为中心的情境意识的新基准测试，基于真实世界视频。SAW-Bench包含786段使用Ray-Ban Meta（Gen 2）智能眼镜自拍录制的视频，涵盖多样化的室内外环境，以及超过2,071对人工标注的问题-答案对。它通过六个不同的意识任务来探测模型对以观察者为中心的理解能力。我们的全面评估显示，即使使用表现最好的多模态基础模型Gemini 3 Flash，人类与模型在性能上仍存在37.66%的差距。此外，我们的深入分析揭示了几个显著发现；例如，虽然模型可以利用以自我为中心视频中的部分几何线索，但它们常常无法推断出连贯的摄像机几何结构，导致系统性的空间推理错误。我们将SAW-Bench定位为情境空间智能的基准测试，超越被动观察，转向对物理基础、以观察者为中心动态的理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the limitation of existing multimodal foundation models (MFMs) in capturing observer-centric spatial relationships, which are essential for situated awareness. SAW-Bench, a new benchmark using real-world videos from Ray-Ban Meta smart glasses, evaluates models&#x27; ability to understand and reason about the environment from an agent&#x27;s perspective. The benchmark includes 786 videos and 2,071 annotated question-answer pairs, testing six awareness tasks. Evaluation shows a significant performance gap between humans and models, with the best MFM, Gemini 3 Flash, achieving only 37.66% accuracy. The analysis highlights that models struggle with coherent spatial reasoning, often failing to infer correct camera geometry from egocentric viewpoints.</div>
<div class="mono" style="margin-top:8px">该研究针对现有多模态基础模型（MFMs）在捕捉以观察者为中心的空间关系方面的不足，这些关系对情境感知至关重要。SAW-Bench 是一个使用真实世界视频的新基准，通过六种情境感知任务评估模型从智能体视角进行推理的能力。该基准包含786段视频和2071对人工标注的问题-答案对，评估结果显示，即使是最优的MFM模型Gemini 3 Flash，其准确率也仅为62.34%，与人类表现存在37.66%的差距。</div>
</details>
</div>
<div class="card">
<div class="title">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</div>
<div class="meta-line">Authors: Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield</div>
<div class="meta-line">Venue: CVPR 2025 Oral</div>
<div class="meta-line">First: 2024-11-25T16:21:34+00:00 · Latest: 2026-02-18T04:26:35+00:00</div>
<div class="meta-line">Comments: CVPR 2025 (Oral); Project Website: https://chanh.ee/RoboSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.16537v5">Abs</a> · <a href="https://arxiv.org/pdf/2411.16537v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image datasets that often lack sophisticated spatial understanding. For example, datasets frequently do not capture reference frame comprehension, yet effective spatial reasoning requires understanding whether to reason from ego-, world-, or object-centric perspectives. To address this issue, we introduce RoboSpatial, a large-scale dataset for spatial understanding in robotics. It consists of real indoor and tabletop scenes, captured as 3D scans and egocentric images, and annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial relationships, and the pairing of 2D egocentric images with 3D scans makes it both 2D- and 3D- ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robot manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSpatial：为机器人赋予空间理解能力的2D和3D视觉-语言模型</div>
<div class="mono" style="margin-top:8px">空间理解是使机器人能够感知环境、推理环境并有意义地与环境互动的关键能力。在现代机器人技术中，这些能力越来越多地由视觉-语言模型提供。然而，这些模型在空间推理任务中面临重大挑战，因为它们的训练数据基于通用图像数据集，通常缺乏复杂的空间理解。例如，数据集经常无法捕捉参考系的理解，而有效的空间推理需要明确是从自体中心、世界中心还是物体中心视角进行推理。为了解决这一问题，我们引入了RoboSpatial，这是一个用于机器人空间理解的大规模数据集。它包含真实室内外和桌面场景，以3D扫描和第一视角图像形式采集，并标注了与机器人相关的丰富空间信息。该数据集包含100万张图像、5000个3D扫描和300万个标注的空间关系，且2D第一视角图像与3D扫描的配对使其同时适用于2D和3D任务。我们的实验表明，使用RoboSpatial训练的模型在空间能力预测、空间关系预测和机器人操作等下游任务中均优于基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboSpatial addresses the challenge of spatial understanding in vision-language models used for robotics, where existing datasets lack the necessary spatial reasoning information. The dataset includes real-world indoor and tabletop scenes, annotated with rich spatial relationships, and combines 3D scans with 2D egocentric images to support both modalities. Experimental results demonstrate that models trained on RoboSpatial achieve superior performance in tasks like spatial affordance prediction, spatial relationship prediction, and robot manipulation compared to baseline models.</div>
<div class="mono" style="margin-top:8px">RoboSpatial旨在解决现有视觉-语言模型在空间推理任务中的不足，通过提供一个专门针对机器人应用的大规模标注数据集来实现这一目标。该数据集包含通过3D扫描和第一视角图像捕捉的真实室内和桌面场景，并标注了与机器人相关的丰富空间信息。它支持2D和3D模型，包含100万张图像、5000个3D扫描和300万个空间标注。实验结果表明，使用RoboSpatial训练的模型在空间能力预测、空间关系预测和机器人操作等下游任务中表现优于现有基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis</div>
<div class="meta-line">Authors: Tianwei Lin, Zhongwei Qiu, Wenqiao Zhang, Jiang Liu, Yihan Xie, Mingjian Gao, Zhenxuan Fan, Zhaocheng Li, Sijing Li, Zhongle Xie, Peng LU, Yueting Zhuang, Yingda Xia, Ling Zhang, Beng Chin Ooi</div>
<div class="meta-line">First: 2026-02-18T00:42:41+00:00 · Latest: 2026-02-18T00:42:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16110v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16110v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCT：面向全面CT分析的统一切片-体积大视觉-语言模型</div>
<div class="mono" style="margin-top:8px">计算机断层扫描（CT）是最广泛使用且诊断信息密集的成像方式之一，涵盖心脏、肺、肝脏和结肠等关键器官。临床解释依赖于切片驱动的局部特征（如亚厘米结节、病灶边界）和体积驱动的空间表示（如肿瘤浸润、器官间解剖关系）。然而，现有的大视觉-语言模型（LVLMs）在CT切片与体积理解方面仍存在碎片化问题：切片驱动的LVLMs表现出强大的泛化能力，但缺乏跨切片的空间一致性；而体积驱动的LVLMs明确捕捉体积语义，但存在粒度粗糙且与切片输入兼容性差的问题。缺乏统一的建模范式是医学LVLMs临床转化的主要瓶颈。我们提出了OmniCT，一种强大的统一切片-体积LVLM，用于CT场景，其贡献包括：(i) 空间一致性增强（SCE）：结合体积切片组合与三轴位置嵌入，引入体积一致性，并通过MoE混合投影实现高效的切片-体积适配；(ii) 器官级语义增强（OSE）：分割和感兴趣区域（ROI）定位明确对齐解剖区域，强调病灶和器官级语义；(iii) MedEval-CT：最大的切片-体积CT数据集和混合基准，整合全面的评估指标。OmniCT在多种临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理能力。更重要的是，它为跨模态医学影像理解建立了一个新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing Large Vision-Language Models (LVLMs) in handling both slice and volumetric data in CT imaging, which hinder their clinical application. OmniCT introduces a unified approach by integrating spatial consistency enhancement through volumetric slice composition and tri-axial positional embedding, along with an MoE hybrid projection for efficient adaptation between slice and volume modalities. It also enhances organ-level semantics by aligning anatomical regions with segmentation and ROI localization. The main experimental results show that OmniCT significantly outperforms previous methods across various clinical tasks, demonstrating strong performance in both micro-level detail detection and macro-level spatial reasoning.</div>
<div class="mono" style="margin-top:8px">OmniCT旨在解决现有大型视觉-语言模型（LVLMs）在处理CT切片和体积数据时的局限性。当前模型要么专注于切片级特征但缺乏空间一致性，要么捕捉体积语义但粒度较粗。为此，OmniCT提出了一种统一的切片-体积LVLM，包含三个主要贡献：空间一致性增强（SCE）以提升体积理解能力，器官级语义增强（OSE）以实现精确的解剖区域对齐，以及MedEval-CT，这是目前最大的切片-体积CT数据集和混合基准测试集。实验结果表明，OmniCT在多种临床任务中显著优于现有方法，展现出对切片细节和体积整体结构的高敏感性与理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-17T22:24:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到政策：为内容感知布局设计增强语言模型的空间推理</div>
<div class="mono" style="margin-top:8px">我们引入了LaySPA，这是一个强化学习框架，为大型语言模型（LLMs）赋予明确且可解释的空间推理能力，以实现内容感知的图形布局设计。LaySPA解决了两个关键挑战：LLMs在空间推理方面的局限性以及设计决策过程中缺乏透明度。我们不以像素级操作为基础，而是将布局设计重新表述为一个在结构化文本空间环境中的策略学习问题，该环境明确编码了画布几何、元素属性以及元素间的关系。LaySPA生成包含可解释推理轨迹和结构化布局规范的双层级输出，从而实现透明且可控的设计决策。布局设计策略通过多目标空间批评进行优化，将布局质量分解为几何有效性、关系一致性以及审美一致性，并采用相对群体优化进行训练，以稳定开放设计空间中的学习过程。实验表明，LaySPA在结构有效性与视觉质量方面均有提升，优于更大的专有LLMs，并在性能上与专门的SOTA布局生成器相当，同时需要更少的标注样本和更低的延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces LaySPA, a reinforcement learning framework designed to enhance the spatial reasoning capabilities of large language models (LLMs) for content-aware graphic layout design. The motivation stems from the limitations of LLMs in understanding spatial relationships and the opacity of design decision-making processes. LaySPA reformulates layout design as a policy learning task in a structured textual spatial environment, encoding canvas geometry, element attributes, and inter-element relationships. The framework generates interpretable reasoning traces and structured layout specifications, allowing for transparent and controllable design. It optimizes layout policies using a multi-objective spatial critique that evaluates geometric validity, relational coherence, and aesthetic consistency, and employs relative group optimization to stabilize training. Experimental results show that LaySPA improves structural validity and visual quality, surpassing larger proprietary LLMs and matching the performance of specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">本文提出LaySPA，一种强化学习框架，旨在提升大型语言模型（LLMs）在内容感知图形布局设计中的空间推理能力。研究动机源于当前LLMs在空间理解方面的不足以及设计决策过程的不透明性。LaySPA将布局设计问题转化为在结构化文本空间环境中的策略学习任务，该环境显式编码画布几何、元素属性及元素间关系。框架生成可解释的推理轨迹和结构化的布局规范，从而实现透明可控的设计决策。通过多目标空间批评机制，将布局质量分解为几何有效性、关系一致性与审美一致性，并采用相对群体优化方法稳定学习过程。实验结果表明，LaySPA在结构有效性和视觉质量方面表现优异，超越了更大规模的专有LLMs，并在较少标注样本和更低延迟的情况下达到与专用顶级布局生成器相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families</div>
<div class="meta-line">Authors: Yuval Levental</div>
<div class="meta-line">First: 2026-02-17T19:06:19+00:00 · Latest: 2026-02-17T19:06:19+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, 2 tables. Workshop-length paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15950v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15950v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型能识别正方形吗？文本识别在三种模型家族中中介了空间推理</div>
<div class="mono" style="margin-top:8px">我们提出一个简单的实验，揭示了视觉-语言模型（VLMs）的一个基本限制：当单元格没有文本标识时，无法准确定位二进制网格中的填充单元格。我们生成了十五个15x15的网格，填充单元格密度从10.7%到41.8%不等，并将每个网格渲染为两种图像类型——文本符号（. 和 #）和无网格线的填充正方形。然后我们让三种前沿的VLMs（Claude Opus、ChatGPT 5.2 和 Gemini 3 Thinking）进行转录。在文本符号条件下，Claude和ChatGPT分别达到约91%的单元格准确率和84%的F1分数，而Gemini达到84%准确率和63% F1分数。在填充正方形条件下，所有三个模型的准确率都降至60-73%，F1分数降至29-39%。关键的是，所有条件都通过相同的视觉编码器——文本符号是图像，而非标记化文本。文本与正方形之间的F1分数差距在模型间从34到54分不等，这表明VLMs似乎具备一种高保真度的文本识别路径，用于空间推理，其表现远超其原生的视觉路径。每个模型在正方形条件下都表现出不同的失败模式——系统性低估（Claude）、大量高估（ChatGPT）以及模板幻觉（Gemini）——但它们都共享相同的根本缺陷：对非文本视觉元素的空间定位严重退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates a fundamental limitation in vision-language models (VLMs) by testing their ability to transcribe filled cells in binary grids. The researchers created 15 15x15 grids with varying densities and rendered them using two image types: text symbols (e.g., . and #) and filled squares without gridlines. Three state-of-the-art VLMs were evaluated, showing significantly higher performance in the text-symbol condition compared to the filled-squares condition. The results reveal that VLMs rely heavily on text-recognition for spatial reasoning, with their native visual processing being less accurate, especially for non-textual elements. The performance gap highlights a critical deficit in spatial localization for visual elements that lack textual identity.</div>
<div class="mono" style="margin-top:8px">本研究通过测试视觉语言模型（VLMs）在二进制网格中转录填充单元格的能力，揭示了其在视觉推理中的一个根本性限制。研究人员创建了15个15x15的网格，填充密度从10.7%到41.8%不等，并以文本符号（如.和#）和无网格线的填充方块两种形式呈现。三个前沿的VLMs在文本符号条件下表现良好，准确率约为91%，F1分数为84%；而在填充方块条件下，所有模型的准确率和F1分数均显著下降，仅为60-73%和29-39%。这表明VLMs在处理非文本视觉元素时存在严重的空间定位缺陷，其表现似乎依赖于高保真的文本识别路径而非其原生的视觉处理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Gradients for Cumulative Prospect Theory in Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Lepel, Anas Barakat</div>
<div class="meta-line">First: 2024-10-03T15:45:39+00:00 · Latest: 2026-02-17T17:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.02605v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.02605v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive a policy gradient theorem for Cumulative Prospect Theory (CPT) objectives in finite-horizon Reinforcement Learning (RL), generalizing the standard policy gradient theorem and encompassing distortion-based risk objectives as special cases. Motivated by behavioral economics, CPT combines an asymmetric utility transformation around a reference point with probability distortion. Building on our theorem, we design a first-order policy gradient algorithm for CPT-RL using a Monte Carlo gradient estimator based on order statistics. We establish statistical guarantees for the estimator and prove asymptotic convergence of the resulting algorithm to first-order stationary points of the (generally non-convex) CPT objective. Simulations illustrate qualitative behaviors induced by CPT and compare our first-order approach to existing zeroth-order methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中基于累积前景理论的策略梯度</div>
<div class="mono" style="margin-top:8px">我们在有限时间范围的强化学习（RL）中推导出一个针对累积前景理论（CPT）目标的策略梯度定理，该定理推广了标准的策略梯度定理，并涵盖了基于概率扭曲的风险目标作为特例。受行为经济学启发，CPT结合了围绕参考点的非对称效用变换和概率扭曲。基于我们的定理，我们设计了一个基于顺序统计量的蒙特卡洛梯度估计器的一阶策略梯度算法用于CPT-RL。我们建立了该估计器的统计保证，并证明了所得到的算法在（通常非凸的）CPT目标上渐近收敛于一阶平稳点。仿真展示了CPT引发的定性行为，并将我们的第一阶方法与现有的零阶方法进行了比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of incorporating Cumulative Prospect Theory (CPT) into reinforcement learning by deriving a policy gradient theorem for finite-horizon CPT objectives. The method extends standard policy gradients by integrating an asymmetric utility function and probability distortion, which are key components of CPT. The authors propose a first-order policy gradient algorithm using Monte Carlo estimators based on order statistics and provide theoretical guarantees for its statistical properties and asymptotic convergence. Experimental results demonstrate the qualitative behavior of CPT in RL settings and highlight the advantages of the first-order approach over existing zeroth-order methods.</div>
<div class="mono" style="margin-top:8px">本文旨在将累积前景理论（CPT）应用于强化学习，通过推导有限时间范围内CPT目标的策略梯度定理来解决这一问题。研究动机来源于行为经济学，指出决策者通常表现出非线性的风险偏好和不对称的效用函数。作者提出了一种基于蒙特卡洛估计和顺序统计量的首阶策略梯度算法，并提供了统计保证，证明了该算法在CPT目标（通常非凸）上能够渐近收敛到首阶平稳点。实验模拟展示了CPT在强化学习中的定性行为，并对比了首阶方法与现有零阶方法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery</div>
<div class="meta-line">Authors: Zelin Xu, Yupu Zhang, Saugat Adhikari, Saiful Islam, Tingsong Xiao, Zibo Liu, Shigang Chen, Da Yan, Zhe Jiang</div>
<div class="meta-line">First: 2026-02-17T06:08:43+00:00 · Latest: 2026-02-17T06:08:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15918v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EarthSpatialBench：评估多模态大语言模型在地球影像中的空间推理能力</div>
<div class="mono" style="margin-top:8px">评估多模态大语言模型（MLLMs）中的空间推理能力在计算机视觉领域引起了越来越多的关注，因其对具身AI及其他需要与物理世界进行精确交互的代理系统至关重要。然而，地球影像中的空间推理仍相对滞后，因为其独特之处在于需要将对象定位在地理参考图像中，并利用视觉线索和向量几何坐标（如2D边界框、折线和多边形）进行距离、方向和拓扑关系的定量推理。现有的地球影像基准测试主要关注二维空间定位、图像描述和粗略空间关系（如简单的方向或邻近线索），缺乏对定量方向和距离推理、系统化拓扑关系以及超出边界框的复杂对象几何的支持。为填补这一空白，我们提出了\textbf{EarthSpatialBench}，这是一个全面的基准测试，用于评估MLLMs在地球影像中的空间推理能力。该基准包含超过325,000个问答对，涵盖以下方面：(1) 空间距离和方向的定性和定量推理；(2) 系统化的拓扑关系；(3) 单对象查询、对象对查询和组合聚合组查询；(4) 通过文本描述、视觉叠加和显式几何坐标（包括2D边界框、折线和多边形）表达的对象引用。我们在开源和专有模型上进行了广泛的实验，以识别MLLMs在空间推理方面的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to address the lack of comprehensive benchmarks for evaluating spatial reasoning capabilities of multimodal large language models (MLLMs) on Earth imagery, which is critical for embodied AI and agentic systems. The proposed EarthSpatialBench introduces a benchmark that covers qualitative and quantitative spatial reasoning, systematic topological relations, and complex object geometries beyond simple bounding boxes. It includes over 325K question-answer pairs with diverse query types and object references. Experimental results on both open-source and proprietary models reveal significant limitations in their ability to perform precise quantitative spatial reasoning and handle complex geometric relationships.</div>
<div class="mono" style="margin-top:8px">提出EarthSpatialBench的动机是填补对地球影像中多模态大语言模型（MLLMs）空间推理能力评估的空白，这对具身AI和需要与物理世界精确交互的代理系统至关重要。该基准包含超过325,000个问答对，涵盖空间距离和方向的定性与定量推理、系统性的拓扑关系以及复杂对象几何形状。实验结果揭示了现有模型在利用视觉和几何信息进行距离、方向和拓扑关系推理方面存在明显局限。</div>
</details>
</div>
<div class="card">
<div class="title">Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery</div>
<div class="meta-line">Authors: Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Abhay Yadav, Ananth Vishnu Bhaskar, Hardik Prajapati, Cheng Peng, Rama Chellappa, Shivkumar Chandrasekaran, B. S. Manjunath</div>
<div class="meta-line">First: 2026-02-16T17:06:54+00:00 · Latest: 2026-02-16T17:06:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14929v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14929v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wrivinder：面向地面图像在卫星影像上地理定位的空间智能方法</div>
<div class="mono" style="margin-top:8px">将地面图像与地理注册的卫星地图对齐对于地图绘制、导航和态势感知至关重要，但在视角差异较大或GPS不可靠的情况下仍具挑战性。我们提出了Wrivinder，这是一个零样本、基于几何的框架，通过聚合多张地面照片来重建一致的3D场景，并将其与上方的卫星影像对齐。Wrivinder结合了SfM重建、3D高斯点绘制、语义定位以及基于单目深度的度量线索，生成一个稳定的顶视渲染，可以直接与卫星上下文匹配，实现度量准确的相机地理定位。为了支持该任务的系统性评估（该任务缺乏合适的基准数据集），我们还发布了MC-Sat，这是一个经过整理的数据集，连接了多视角地面图像与地理注册的卫星瓦片，涵盖多种户外环境。Wrivinder和MC-Sat共同提供了首个全面的基线和测试平台，用于研究无需成对监督的几何中心跨视角对齐。在零样本实验中，Wrivinder在密集和大范围场景中均实现了低于30米的地理定位精度，突显了基于几何的聚合方法在稳健地面到卫星定位方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of aligning ground-level images with satellite maps in scenarios with large viewpoint differences or unreliable GPS. Wrivinder, a zero-shot, geometry-driven framework, is introduced to reconstruct a consistent 3D scene from multiple ground photographs and align it with overhead satellite imagery. It integrates SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth-based metric cues to generate a stable zenith-view rendering for accurate camera geo-localization. The main experimental results show that Wrivinder achieves sub-30-meter geolocation accuracy across various scenes, demonstrating the effectiveness of geometry-based aggregation for robust ground-to-satellite alignment.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在大视角差异或GPS不可靠的情况下，将地面图像与卫星地图对齐的挑战。提出了一种零样本、基于几何的框架Wrivinder，通过多张地面照片重建一致的3D场景并将其与俯视卫星图像对齐。该框架结合了结构从运动（SfM）、3D高斯点云、语义定位和单目深度估计，生成稳定的顶视渲染以实现精确的相机地理定位。在新发布的MC-Sat数据集上的实验结果表明，Wrivinder在密集和大范围场景中均实现了低于30米的地理定位精度，展示了基于几何的方法在无配对监督下的跨视角对齐的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Haichao Zhu, Zhaorui Yang, Qian Zhang</div>
<div class="meta-line">First: 2026-02-16T02:26:59+00:00 · Latest: 2026-02-16T02:26:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14409v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14409v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习提出，几何处理：一种用于高效空间推理的模块化框架</div>
<div class="mono" style="margin-top:8px">空间感知旨在从视觉观测中估计相机运动和场景结构，这一问题传统上通过几何建模和物理一致性约束来解决。近年来，基于学习的方法在几何感知中展现出强大的表示能力，并在实践中越来越多地用于增强以几何为中心的经典系统。然而，学习组件是否应直接替代几何估计，还是应作为此类流程中的中间模块，仍是一个开放性问题。
  在这项工作中，我们解决了这一问题，并研究了一种端到端的模块化框架，以实现有效的空间推理，其中学习用于提出几何假设，而几何算法用于处理估计决策。特别地，我们在RGB-D序列的相对相机姿态估计背景下探讨了这一原则。使用VGGT作为代表性学习模型，我们在不同运动幅度和场景动态下评估了基于学习的姿态和深度提案，并随后采用经典点对平面RGB-D ICP作为几何后端。我们在TUM RGB-D基准上的实验揭示了三个一致的发现：(1) 仅依靠基于学习的姿态提案是不可靠的；(2) 当学习提出的几何未正确对齐相机内参时，性能会下降；(3) 当学习提出的深度在几何上对齐并随后经过几何处理阶段时，在中等挑战性的刚性场景中会出现一致的性能提升。
  这些结果表明，几何不仅仅是优化组件，而是必不可少的仲裁者，用于验证和吸收基于学习的几何观测。我们的研究强调了模块化、几何感知系统设计在实现稳健空间感知中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of learning-based components in spatial perception systems, specifically focusing on relative camera pose estimation using RGB-D sequences. The study proposes a modular framework where neural networks generate geometric hypotheses, and classical geometric algorithms are used to refine and validate these proposals. Experimental results on the TUM RGB-D benchmark show that learning-based pose proposals alone are unreliable, improper alignment of learned geometry with camera intrinsics can degrade performance, but when properly aligned and followed by a geometric disposal stage, consistent improvements are achieved in rigid environments.</div>
<div class="mono" style="margin-top:8px">本文探讨了学习组件在空间感知系统中的作用，特别是在使用RGB-D序列进行相对相机姿态估计的场景中。研究提出了一种模块化框架，其中学习模型生成几何假设，而经典几何算法负责细化和验证这些假设。作者采用VGGT作为学习模型，评估其在不同运动和场景条件下的姿态和深度估计性能，并通过点对平面ICP算法进行几何优化。在TUM RGB-D基准测试中，实验结果表明仅依赖学习生成的姿态估计不可靠，若学习得到的几何信息未正确对齐相机内参，可能降低性能，但在对齐后并结合几何处理阶段，可在中等挑战性的刚性环境中实现一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines</div>
<div class="meta-line">Authors: Yifan Wu, Yiran Peng, Yiyu Chen, Jianhao Ruan, Zijie Zhuang, Cheng Yang, Jiayi Zhang, Man Chen, Yenchi Tseng, Zhaoyang Yu, Liang Chen, Yuyao Zhai, Bang Liu, Chenglin Wu, Yuyu Luo</div>
<div class="meta-line">First: 2026-02-15T20:03:19+00:00 · Latest: 2026-02-15T20:03:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14296v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoWebWorld: 通过有限状态机合成无限可验证的网络环境</div>
<div class="mono" style="margin-top:8px">自主Web GUI代理的性能严重依赖于其训练数据的质量和数量。然而，一个根本性的瓶颈仍然存在：从真实网站中收集交互轨迹既昂贵又难以验证。底层状态转换是隐藏的，导致需要依赖不一致且成本高昂的外部验证器来评估每一步的正确性。为了解决这一问题，我们提出了AutoWebWorld，这是一个新颖的框架，通过将网络环境建模为有限状态机（FSMs）来合成可控且可验证的网络环境，并利用编码代理将FSMs转换为可交互的网站。与真实网站中隐式的状态转换不同，AutoWebWorld明确定义了所有状态、动作和转换规则。这使得可以进行程序化验证：动作的正确性通过预定义的规则进行检查，任务成功则通过在FSM图中达到目标状态来确认。AutoWebWorld实现了完全自动化的搜索与验证流程，仅以每轨迹0.04美元的成本，从29个多样化的网络环境中生成超过11,663条验证轨迹。在这些合成数据上进行训练显著提升了实际应用中的性能。我们的7B参数Web GUI代理在WebVoyager上15步内优于所有基线。此外，我们观察到一个清晰的扩展定律：随着合成数据量的增加，AutoWebWorld在WebVoyager和Online-Mind2Web上的性能持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of AutoWebWorld is to overcome the challenges of collecting and verifying interaction trajectories for training autonomous Web GUI agents, which is both costly and error-prone in real-world environments. The framework models web environments as Finite State Machines (FSMs), explicitly defining states, actions, and transition rules, allowing for programmatic verification of agent behavior. By using coding agents to translate FSMs into interactive websites, AutoWebWorld generates a large number of verified trajectories at a low cost, achieving over 11,663 verified trajectories from 29 diverse environments for $0.04 per trajectory. Training on this synthetic data significantly improves the performance of real-world agents, with the 7B Web GUI agent outperforming all baselines within 15 steps on WebVoyager and showing consistent performance improvements as synthetic data volume increases.</div>
<div class="mono" style="margin-top:8px">AutoWebWorld的动机是解决自主Web GUI代理训练中收集和验证大规模交互轨迹的困难。该框架通过将Web环境建模为有限状态机（FSMs），显式定义状态、动作和转移规则，从而实现程序化验证。利用编码代理将FSMs转换为可交互的网站，AutoWebWorld构建了一个全自动的搜索与验证流程，生成超过11,663条验证轨迹，成本仅为每条0.04美元。在这些合成数据上训练的代理在真实环境中表现显著提升，其中7B参数的Web GUI代理在WebVoyager上15步内超越所有基线，并且随着合成数据量的增加，其在WebVoyager和Online-Mind2Web上的性能持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-15T19:48:43+00:00 · Latest: 2026-02-15T19:48:43+00:00</div>
<div class="meta-line">Comments: 15 pages, 33 pages with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KernelBlaster: 通过记忆增强的上下文强化学习实现持续跨任务CUDA优化</div>
<div class="mono" style="margin-top:8px">在多代GPU架构上优化CUDA代码具有挑战性，因为实现最佳性能需要在日益复杂且特定于硬件的优化空间中进行广泛探索。传统编译器受限于固定的启发式方法，而微调大型语言模型（LLMs）则成本较高。然而，CUDA代码优化的代理工作流在整合先前探索的知识方面能力有限，导致采样偏差和次优解。我们提出KernelBlaster，这是一个基于记忆增强的上下文强化学习（MAIC-RL）框架，旨在提升基于LLM的GPU编码代理的CUDA优化搜索能力。KernelBlaster通过将知识积累到可检索的持久CUDA知识库中，使代理能够从经验中学习，并在未来的任务中做出系统性的决策。我们提出了一种新颖的基于配置文件引导和文本梯度的代理流程，用于CUDA生成和优化，以在多代GPU架构上实现高性能。KernelBlaster引导LLM代理系统地探索超越简单重写的高潜力优化策略。与PyTorch基线相比，我们的方法在KernelBench Level 1、2、3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。我们发布了KernelBlaster作为一个开源代理框架，附带测试框架、验证组件和可复现的评估流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of KernelBlaster is to address the challenges of optimizing CUDA code across different GPU architectures, where traditional methods and fixed heuristics fall short. The framework introduces a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) approach that allows LLM-based agents to accumulate and retrieve optimization knowledge from prior tasks, improving their decision-making for future ones. Experimental results show that KernelBlaster achieves significant speedups, with geometric mean improvements of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively, compared to the PyTorch baseline.</div>
<div class="mono" style="margin-top:8px">KernelBlaster的研究动机是解决在不同GPU架构上优化CUDA代码的挑战，克服传统编译器和LLM微调方法的局限性。该方法采用Memory-Augmented In-context Reinforcement Learning（MAIC-RL）框架，使基于LLM的GPU编码代理能够通过Persistent CUDA Knowledge Base积累和检索优化知识。提出了一种基于配置文件和文本梯度的代理流程，用于CUDA生成和优化，使代理能够基于经验做出系统性决策。实验结果表明，与PyTorch基线相比，KernelBlaster在KernelBench Levels 1、2和3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。</div>
</details>
</div>
<div class="card">
<div class="title">SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement</div>
<div class="meta-line">Authors: Xiaojun Jia, Jie Liao, Simeng Qin, Jindong Gu, Wenqi Ren, Xiaochun Cao, Yang Liu, Philip Torr</div>
<div class="meta-line">First: 2026-02-15T16:09:48+00:00 · Latest: 2026-02-15T16:09:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14211v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkillJect：面向编码代理的基于技能的隐蔽提示注入自动化框架</div>
<div class="mono" style="margin-top:8px">代理技能正逐渐成为编码代理的核心抽象，通过封装长文本指令和辅助脚本扩展工具增强行为。这种抽象引入了一个未被充分衡量的攻击面：基于技能的提示注入，其中被污染的技能可能引导代理偏离用户意图和安全策略。在实践中，简单的注入往往失败，因为恶意意图过于明显或偏离原始技能，导致代理忽略或拒绝这些注入；现有的攻击方法也大多是手工构建的。我们提出了首个针对代理技能的隐蔽提示注入自动化框架。该框架与三个代理形成闭环：一个攻击代理在显式隐蔽约束下合成注入技能，一个代码代理在一个现实的工具环境中使用注入技能执行任务，一个评估代理记录操作轨迹（如工具调用和文件操作）并验证是否发生了目标恶意行为。我们还提出了一种恶意负载隐藏策略，通过在辅助脚本中隐藏对抗性操作，同时注入优化的诱导提示以触发工具执行。在多种编码代理设置和现实软件工程任务上的广泛实验表明，我们的方法在现实环境中能够持续实现高攻击成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of skill-based abstractions in coding agents creates a new vulnerability, as these skills can be manipulated through prompt injection to deviate from user intent and safety policies. To address this, the authors introduce SkillJect, an automated framework for stealthy skill-based prompt injection. The framework employs a closed-loop system involving three agents: an Attack Agent that generates injection skills under stealth constraints, a Code Agent that executes tasks in a realistic environment, and an Evaluate Agent that monitors action traces and confirms the occurrence of malicious behaviors. The method also includes a payload hiding strategy that embeds adversarial operations within auxiliary scripts, making the attacks more effective and harder to detect. Experimental results across various coding-agent scenarios and real-world software engineering tasks demonstrate that SkillJect achieves high attack success rates under realistic conditions.</div>
<div class="mono" style="margin-top:8px">该研究关注编码代理中基于技能的提示注入带来的安全风险，可能导致偏离用户意图和安全策略。提出SkillJect框架，这是一个自动化生成隐蔽注入技能的系统，包含三个代理：攻击代理负责在隐秘约束下合成注入技能，代码代理在真实环境中执行任务，评估代理则记录操作轨迹并验证是否发生目标恶意行为。框架还引入了恶意负载隐藏策略，将对抗性操作隐藏在辅助脚本中。在多种编码代理场景和现实软件工程任务中的实验表明，SkillJect在保持隐秘性的同时实现了高攻击成功率。</div>
</details>
</div>
<div class="card">
<div class="title">EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming</div>
<div class="meta-line">Authors: Sen Fang, Weiyuan Ding, Mengshi Zhang, Zihao Chen, Bowen Xu</div>
<div class="meta-line">First: 2025-05-18T01:02:33+00:00 · Latest: 2026-02-15T05:28:25+00:00</div>
<div class="meta-line">Comments: 27 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12185v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.12185v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI&#x27;s o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVALOOOP：一种以自一致性为中心的框架，用于评估大型语言模型在编程中的鲁棒性</div>
<div class="mono" style="margin-top:8px">评估大型语言模型（LLMs）在编程中的鲁棒性对于确保其在基于人工智能的软件开发中的可靠性至关重要。然而，对抗性攻击存在根本性局限，影响了公平的鲁棒性评估：它们在不同攻击策略下会产生矛盾的评估结果，并且更关键的是，它们仅通过外部扰动进行操作，无法捕捉到自主编码代理所需的内在稳定性，因为后续输入是由模型自身生成的。我们引入了EVALOOOP，这是一种新颖的评估框架，从自一致性角度评估鲁棒性，利用软件工程任务中固有的自然二元性（例如代码生成和代码摘要）。EVALOOOP建立了一个自包含的反馈循环，其中LLM在代码和自然语言之间反复转换，直到功能失效。鲁棒性通过一种新的平均可持续循环（ASL）指标进行量化——即在基准任务中保持功能正确性的迭代次数的平均值。这种循环策略无需依赖外部攻击配置，内在地评估鲁棒性，提供了一个统一的指标，揭示了LLMs在持续自指转换中如何有效保持语义完整性。我们在EVALOOOP上评估了96个流行的LLMs，参数范围从0.5B到685B，并发现EVALOOOP通常在十次循环内导致pass@1准确率下降2.65%-47.62%。有趣的是，鲁棒性并不总是与初始性能（即单次查询）一致；例如，Qwen3-235B-A22B-Instruct-2507虽然在初始代码生成方面不如OpenAI的o系列模型和DeepSeek-V3，但其鲁棒性（ASL得分）却更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of adversarial attacks in assessing the robustness of large language models (LLMs) for programming tasks, as these attacks often produce inconsistent results and fail to capture intrinsic model stability. EVALOOOP is introduced as a self-consistency-centered framework that evaluates LLMs by establishing a feedback loop where the model iteratively transforms between code and natural language until functional failure occurs. The framework uses a novel metric, Average Sustainable Loops (ASL), to quantify robustness by measuring the mean number of iterations maintaining correctness. Experimental results on 96 LLMs with the MBPP Plus benchmark show that EVALOOOP typically leads to a 2.65%-47.62% drop in pass@1 accuracy within ten loops, highlighting that robustness does not always correlate with initial performance, as demonstrated by Qwen3-235B-A22B-Instruct-2507 achieving superior ASL scores despite lower initial code generation quality.</div>
<div class="mono" style="margin-top:8px">本研究提出EVALOOOP框架，旨在解决现有对抗攻击在评估大语言模型（LLMs）编程鲁棒性时存在的局限性，如结果不一致和无法反映模型内在稳定性。EVALOOOP基于自一致性视角，通过构建一个闭环反馈机制，让LLM在代码与自然语言之间反复转换直至功能失效，从而评估其鲁棒性。该框架引入了新的平均可持续循环（ASL）指标，用于衡量模型在基准任务中保持功能正确的迭代次数。在MBPP Plus基准上对96个LLM进行评估发现，EVALOOOP通常导致pass@1准确率下降2.65%-47.62%，表明鲁棒性并不总是与初始性能相关，例如Qwen3-235B-A22B-Instruct-2507虽然初始代码生成能力不如OpenAI的o系列模型和DeepSeek-V3，但其鲁棒性表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Xiaoyu Li, Yitao Wu, Xian Wu, Haolin Zhuo, Lijun Zhao, Lining Sun</div>
<div class="meta-line">First: 2026-02-14T13:34:21+00:00 · Latest: 2026-02-14T13:34:21+00:00</div>
<div class="meta-line">Comments: Based on this work, we achieved 1st place on the KITTI tracking leaderboard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13772v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13772v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Offline-Poly：面向离线三维多目标跟踪的多面体框架</div>
<div class="mono" style="margin-top:8px">离线三维多目标跟踪（MOT）是4D自动标注（4DAL）流程中的关键组成部分。它通过引入时间上下文来增强高性能检测器生成的伪标签。然而，现有的离线三维MOT方法通常是在线框架的直接扩展，未能充分利用离线设置的优势。此外，这些方法往往依赖固定的上游模块和定制化架构，限制了其适应性。为了解决这些问题，我们提出了Offline-Poly，一种基于以跟踪为中心设计的通用离线三维MOT方法。我们引入了一种标准化范式，称为Tracking-by-Tracking（TBT），该范式仅依赖于任意现成的跟踪输出，并生成离线优化的轨迹片段。这种设计将离线跟踪器与特定的上游检测器或跟踪器解耦。在TBT范式下，Offline-Poly接受一个或多个粗略的跟踪结果，并通过包含预处理、分层匹配与融合以及轨迹片段优化的结构化流程进行处理。每个模块都旨在利用离线跟踪的两个基本特性：资源不受限性，允许在实时限制之外进行全局优化；以及未来可观测性，使得可以在完整的时间范围内进行轨迹推理。Offline-Poly首先利用全局场景上下文消除短期鬼轨迹片段，并通过重新识别碎片化片段进行轨迹恢复。然后，它构建场景级相似性以关联多个输入源的轨迹片段。最后，Offline-Poly通过联合利用局部和全局运动模式对轨迹片段进行优化。在nuScenes数据集上，我们实现了77.6%的AMOTA最优性能；在KITTI数据集上，实现了83.00%的HOTA领先结果。全面的实验进一步验证了Offline-Poly的灵活性、通用性和模块有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Offline-Poly is proposed to address the limitations of existing offline 3D multi-object tracking methods, which are often direct extensions of online frameworks and lack adaptability. The method introduces a tracking-centric design based on a standardized paradigm called Tracking-by-Tracking (TBT), which operates on arbitrary off-the-shelf tracking outputs to generate refined tracklets. Offline-Poly employs a structured pipeline that includes pre-processing, hierarchical matching and fusion, and tracklet refinement, leveraging the global optimization and full temporal observability inherent in offline settings. It achieves state-of-the-art performance on nuScenes with 77.6% AMOTA and leading results on KITTI with 83.00% HOTA, demonstrating its flexibility and effectiveness.</div>
<div class="mono" style="margin-top:8px">Offline-Poly 是为了解决现有离线 3D 多目标跟踪方法的局限性而提出的，这些方法通常是对在线框架的直接扩展，缺乏适应性。该方法引入了一种以跟踪为中心的设计，称为 Tracking-by-Tracking (TBT)，它基于任意现成的跟踪输出生成优化后的轨迹片段，而不依赖特定的检测器或跟踪器。Offline-Poly 利用了离线设置的优势，如资源不受限和未来可观测性，通过包含预处理、分层匹配与融合以及轨迹片段优化的结构化流程实现。在 nuScenes 数据集上，它取得了 77.6% AMOTA 的最先进性能，在 KITTI 数据集上达到了 83.00% HOTA 的领先结果，验证了其灵活性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird&#x27;s-Eye View with Deformable Attention and Sparse Goal Proposals</div>
<div class="meta-line">Authors: Minsang Kong, Myeongjun Kim, Sang Gu Kang, Hejiu Lu, Yupeng Zhong, Sang Hun Lee</div>
<div class="meta-line">First: 2025-09-12T09:17:54+00:00 · Latest: 2026-02-14T08:37:57+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Intelligent Transportation Systems (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10080v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10080v2">PDF</a> · <a href="https://github.com/Kongminsang/bevtraj">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In autonomous driving, trajectory prediction is essential for safe and efficient navigation. While recent methods often rely on high-definition (HD) maps to provide structured environmental priors, such maps are costly to maintain, geographically limited, and unreliable in dynamic or unmapped scenarios. Directly leveraging raw sensor data in Bird&#x27;s-Eye View (BEV) space offers greater flexibility, but BEV features are dense and unstructured, making agent-centric spatial reasoning challenging and computationally inefficient. To address this, we propose Bird&#x27;s-Eye View Trajectory Prediction (BEVTraj), a map-free framework that employs deformable attention to adaptively aggregate task-relevant context from sparse locations in dense BEV features. We further introduce a Sparse Goal Candidate Proposal (SGCP) module that predicts a small set of realistic goals, enabling fully end-to-end multimodal forecasting without heuristic post-processing. Extensive experiments show that BEVTraj achieves performance comparable to state-of-the-art HD map-based methods while providing greater robustness and flexibility without relying on pre-built maps. The source code is available at https://github.com/Kongminsang/bevtraj.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BEVTraj：基于鸟瞰图、可变形注意力与稀疏目标提案的无地图端到端轨迹预测</div>
<div class="mono" style="margin-top:8px">在自动驾驶中，轨迹预测对于安全和高效导航至关重要。尽管近期方法通常依赖高精度（HD）地图来提供结构化的环境先验，但此类地图维护成本高、地理范围有限，并且在动态或未映射场景中不可靠。直接利用鸟瞰图（BEV）空间中的原始传感器数据提供了更大的灵活性，但BEV特征密集且无结构，使得以代理为中心的空间推理具有挑战性且计算效率低下。为了解决这一问题，我们提出了鸟瞰图轨迹预测（BEVTraj），这是一个无需地图的框架，通过可变形注意力机制从密集的BEV特征中自适应地聚合任务相关的上下文信息。我们进一步引入了稀疏目标候选提案（SGCP）模块，用于预测一组现实的目标，从而实现完全端到端的多模态预测，无需启发式后处理。大量实验表明，BEVTraj在性能上与基于HD地图的最先进方法相当，同时在不依赖预构建地图的情况下提供了更高的鲁棒性和灵活性。源代码可在 https://github.com/Kongminsang/bevtraj 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trajectory prediction is crucial for autonomous driving systems to navigate safely and efficiently. Traditional approaches often depend on high-definition maps, which are expensive, limited in scope, and unreliable in dynamic or unmapped environments. BEVTraj introduces a map-free framework that uses deformable attention to selectively gather context from dense Bird&#x27;s-Eye View features, enabling more flexible and efficient spatial reasoning. Additionally, it incorporates a Sparse Goal Candidate Proposal module to generate a small set of realistic goals, supporting end-to-end multimodal forecasting. Experimental results demonstrate that BEVTraj matches the performance of HD map-based methods while being more robust and adaptable in scenarios without pre-built maps.</div>
<div class="mono" style="margin-top:8px">轨迹预测对于自动驾驶系统的安全高效导航至关重要。传统方法通常依赖高精地图，但这类地图维护成本高、地理范围有限且在动态或未映射环境中不可靠。BEVTraj 提出了一种无需地图的框架，利用可变形注意力机制从密集的鸟瞰图特征中选择性提取任务相关上下文，从而实现更灵活高效的空间推理。同时，该框架引入了稀疏目标候选提案模块，生成少量现实目标，支持端到端的多模态预测。实验结果表明，BEVTraj 在无需地图的情况下，其性能与基于高精地图的最先进方法相当，且更具鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories</div>
<div class="meta-line">Authors: Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen</div>
<div class="meta-line">First: 2025-04-29T22:22:44+00:00 · Latest: 2026-02-14T01:32:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21205v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.21205v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 29 standalone LLMs and 15 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SecRepoBench：在现实仓库中评估代码代理的安全代码补全基准</div>
<div class="mono" style="margin-top:8px">本文介绍了SecRepoBench，这是一个用于评估代码代理在现实仓库中进行安全代码补全的基准。SecRepoBench包含27个C/C++仓库中的318个代码补全任务，涵盖15种CWE漏洞。我们使用该基准评估了29个独立的LLMs和15个代码代理。我们发现，最先进的LLMs在生成正确且安全的代码补全方面存在困难。然而，代码代理显著优于独立LLMs。我们展示了SecRepoBench比之前的先进基准更具挑战性。最后，我们的全面分析为提升代码代理在现实仓库中编写正确且安全代码的能力提供了潜在方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents SecRepoBench, a benchmark designed to assess code agents&#x27; performance in secure code completion within real-world repositories. The benchmark includes 318 tasks across 27 C/C++ repositories, addressing 15 common software vulnerabilities. Evaluating 29 standalone large language models (LLMs) and 15 code agents using three advanced agent frameworks, the study reveals that current LLMs struggle with generating both correct and secure code. In contrast, code agents demonstrate significantly better performance, highlighting their potential in improving code safety. The benchmark is shown to be more challenging than previous state-of-the-art benchmarks, offering valuable insights for future research on secure code generation.</div>
<div class="mono" style="margin-top:8px">本文提出了SecRepoBench，这是一个用于评估代码代理在真实仓库中安全代码补全能力的基准测试。该基准包含27个C/C++仓库中的318个代码补全任务，涵盖15种常见的软件漏洞。通过评估29个独立的大型语言模型（LLMs）和15个代码代理，使用三种最先进的代理框架，研究发现LLMs在生成正确且安全的代码方面存在困难。相比之下，代码代理表现出显著更好的性能，突显了其在提升代码安全性方面的潜力。该基准被证明比之前的先进基准更具挑战性，为未来改进安全代码生成提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Spear-Phishing Website Generation in Large Language Model Coding Agents</div>
<div class="meta-line">Authors: Tailia Malloy, Tegawende F. Bissyande</div>
<div class="meta-line">First: 2026-02-13T12:12:53+00:00 · Latest: 2026-02-13T12:12:53+00:00</div>
<div class="meta-line">Comments: 18 Pages, 7 Figures, 1 Table. Accepted to the conference Human Computer Interaction International</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are expanding beyond being a tool humans use and into independent agents that can observe an environment, reason about solutions to problems, make changes that impact those environments, and understand how their actions impacted their environment. One of the most common applications of these LLM Agents is in computer programming, where agents can successfully work alongside humans to generate code while controlling programming environments or networking systems. However, with the increasing ability and complexity of these agents comes dangers about the potential for their misuse. A concerning application of LLM agents is in the domain cybersecurity, where they have the potential to greatly expand the threat imposed by attacks such as social engineering. This is due to the fact that LLM Agents can work autonomously and perform many tasks that would normally require time and effort from skilled human programmers. While this threat is concerning, little attention has been given to assessments of the capabilities of LLM coding agents in generating code for social engineering attacks. In this work we compare different LLMs in their ability and willingness to produce potentially dangerous code bases that could be misused by cyberattackers. The result is a dataset of 200 website code bases and logs from 40 different LLM coding agents. Analysis of models shows which metrics of LLMs are more and less correlated with performance in generating spear-phishing sites. Our analysis and the dataset we present will be of interest to researchers and practitioners concerned in defending against the potential misuse of LLMs in spear-phishing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型编码代理在钓鱼网站生成中的能力</div>
<div class="mono" style="margin-top:8px">大型语言模型正在从人类使用的工具扩展为能够观察环境、推理解决问题、对环境产生影响并理解其行为后果的独立代理。这些LLM代理最常见的应用之一是计算机编程，它们可以与人类协作生成代码，同时控制编程环境或网络系统。然而，随着这些代理的能力和复杂性不断提升，其被滥用的潜在风险也随之增加。在网络安全领域，LLM代理的一个令人担忧的应用是它们可能大大扩展社会工程攻击的威胁。这是因为LLM代理可以自主工作，并执行通常需要熟练程序员投入大量时间和精力的任务。尽管这一威胁令人担忧，但对LLM编码代理在生成社会工程攻击代码方面的能力评估却很少受到关注。在本研究中，我们比较了不同LLM在生成可能被网络攻击者滥用的危险代码库方面的能力和意愿。结果是一个包含200个网站代码库和40个不同LLM编码代理日志的数据集。模型分析展示了哪些LLM指标与生成精准钓鱼网站的性能更为或更不相关。我们的分析和所呈现的数据集将对关注如何防范LLM在钓鱼攻击中被滥用的研究人员和实践者具有参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential misuse of large language model (LLM) coding agents in generating spear-phishing websites, a critical concern in cybersecurity. The research compares different LLMs based on their ability and willingness to produce malicious code bases that could be exploited by attackers. Through an extensive analysis, the study identifies which LLM metrics are more or less correlated with the effectiveness of generating such websites, resulting in a dataset of 200 code bases and logs from 40 agents. The findings provide valuable insights for researchers and practitioners aiming to mitigate risks associated with LLMs in social engineering attacks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在生成钓鱼网站方面的潜在滥用问题，这是网络安全领域的重要关注点。研究比较了不同LLMs在生成可用于社会工程攻击的代码库方面的能力和意愿，重点关注其在生成此类内容上的有效性。作者构建了一个包含200个网站代码库和40个LLM编码代理日志的数据集，揭示了某些LLM指标与生成钓鱼网站表现之间的相关性。研究结果为关注LLMs在钓鱼攻击中可能被滥用的学者和从业者提供了有价值的参考和资源。</div>
</details>
</div>
<div class="card">
<div class="title">3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset</div>
<div class="meta-line">Authors: Mehran Advand, Zahra Dehghanian, Navid Faraji, Reza Barati, Seyed Amir Ahmad Safavi-Naini, Hamid R. Rabiee</div>
<div class="meta-line">First: 2026-02-13T11:08:15+00:00 · Latest: 2026-02-13T11:08:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mehrn79.github.io/3DLAND">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DLAND：腹部异常病变三维定位数据集</div>
<div class="mono" style="margin-top:8px">现有的腹部CT医学影像数据集通常缺乏三维标注、多器官覆盖或精确的病灶-器官关联，阻碍了稳健的表示学习和临床应用。为解决这一问题，我们引入了3DLAND，这是一个大规模基准数据集，包含超过6,000个增强CT体数据，具有超过20,000个高保真度的三维病灶标注，涵盖七个腹部器官：肝脏、肾脏、胰腺、脾脏、胃和胆囊。我们的三阶段流水线整合了自动化空间推理、优化提示的2D分割以及记忆引导的3D传播，经专家放射科医生验证，表面Dice分数超过0.75。通过提供多样化的病灶类型和患者人口统计信息，3DLAND支持对医学AI异常检测、定位和跨器官迁移学习的可扩展评估。我们的数据集为评估器官感知的三维分割模型设立了新的基准，为面向医疗的AI发展铺平了道路。为促进可重复性和进一步研究，3DLAND数据集及实现代码已公开在https://mehrn79.github.io/3DLAND。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the 3DLAND dataset is to address the limitations of existing abdominal CT datasets, which often lack comprehensive 3D annotations, multi-organ coverage, and precise lesion-to-organ associations. The dataset introduces a large-scale benchmark with over 6,000 contrast-enhanced CT volumes and more than 20,000 high-fidelity 3D lesion annotations across seven abdominal organs. The proposed three-phase pipeline combines automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores above 0.75. This enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning in medical AI.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有腹部CT数据集在三维标注、多器官覆盖和病变与器官关联方面的不足。作者提出了3DLAND数据集，包含超过6000例增强CT影像，覆盖肝脏、肾脏、胰腺、脾脏、胃和胆囊等七个腹部器官，标注了超过20000个高保真度的三维病变。该数据集通过一个三阶段流程构建，结合了自动空间推理、提示优化的2D分割以及记忆引导的3D传播，经放射科专家验证，表面Dice分数超过0.75。3DLAND为评估医学AI中的异常检测、定位及跨器官迁移学习提供了重要的基准资源。</div>
</details>
</div>
<div class="card">
<div class="title">Easy-Poly: An Easy Polyhedral Framework For 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Peng Zhang, Xin Li, Xin Lin, Liang He</div>
<div class="meta-line">First: 2025-02-25T04:01:25+00:00 · Latest: 2026-02-13T08:05:15+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17822v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.17822v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent 3D multi-object tracking (3D MOT) methods mainly follow tracking-by-detection pipelines, but often suffer from high false positives, missed detections, and identity switches, especially in crowded and small-object scenarios. To address these challenges, we propose Easy-Poly, a filter-based 3D MOT framework with four key innovations: (1) CNMSMM, a novel Camera-LiDAR fusion detection method combining multi-modal augmentation and an efficient NMS with a new loss function to improve small target detection; (2) Dynamic Track-Oriented (DTO) data association that robustly handles uncertainties and occlusions via class-aware optimal assignment and parallel processing strategies; (3) Dynamic Motion Modeling (DMM) using a confidence-weighted Kalman filter with adaptive noise covariance to enhance tracking accuracy; and (4) an extended life-cycle management system reducing identity switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 65.65% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 75.6%), while also running in real-time. Our framework advances robustness and adaptability in complex driving environments, paving the way for safer autonomous driving perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Easy-Poly：一种用于3D多目标跟踪的简单多面体框架</div>
<div class="mono" style="margin-top:8px">最近的3D多目标跟踪(3D MOT)方法主要采用基于检测的跟踪流程，但在密集和小目标场景中常面临高误检率、漏检和身份切换的问题。为了解决这些挑战，我们提出了Easy-Poly，一个基于滤波的3D MOT框架，包含四项关键创新：(1) CNMSMM，一种结合多模态增强和高效NMS的新损失函数的相机-激光雷达融合检测方法，以提升小目标检测性能；(2) 动态目标导向(DTO)数据关联，通过类感知最优分配和平行处理策略，鲁棒地处理不确定性与遮挡；(3) 动态运动建模(DMM)，使用具有自适应噪声协方差的置信度加权卡尔曼滤波以提高跟踪精度；(4) 扩展生命周期管理系统，减少身份切换和误终止。实验结果表明，Easy-Poly在mAP（如LargeKernel3D中从63.30%提升至65.65%）和AMOTA（如从73.1%提升至75.6%）方面优于Poly-MOT和Fast-Poly等先进方法，同时支持实时运行。我们的框架提升了复杂驾驶环境下的鲁棒性和适应性，为更安全的自动驾驶感知铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the performance of 3D multi-object tracking in challenging scenarios such as crowded environments and small-object detection. Easy-Poly introduces a filter-based framework with four key innovations: CNMSMM for enhanced multi-modal detection, DTO data association for robust object matching, DMM for adaptive motion modeling, and an extended life-cycle management system for better identity consistency. Experimental results demonstrate that Easy-Poly achieves significant improvements in mAP and AMOTA metrics compared to existing methods like Poly-MOT and Fast-Poly, while maintaining real-time performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D多目标跟踪（3D MOT）在拥挤场景和小目标检测等挑战性环境中的性能。提出的Easy-Poly框架包含四项创新：一种结合多模态增强和高效NMS的新型相机-激光雷达融合检测方法（CNMSMM），一种通过类感知分配和并行处理策略处理不确定性和遮挡的动态轨迹导向（DTO）数据关联方法，一种使用置信度加权卡尔曼滤波器和自适应噪声协方差的动态运动建模（DMM）技术，以及一种扩展生命周期管理系统以减少身份切换和错误终止。实验结果表明，Easy-Poly在mAP和AMOTA指标上优于现有方法如Poly-MOT和Fast-Poly，同时保持实时运行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models</div>
<div class="meta-line">Authors: Joshua Ong Jun Leang, Yu Zhao, Mihaela Cătălina Stoian, Wenda Li, Shay B. Cohen, Eleonora Giunchiglia</div>
<div class="meta-line">First: 2026-02-13T03:56:22+00:00 · Latest: 2026-02-13T03:56:22+00:00</div>
<div class="meta-line">Comments: 8 pages, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12586v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我可以点单吗？扩散语言模型中用于槽填充顺序的蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">尽管在掩码扩散模型（MDMs）中，计划-填充解码在数学和代码推理方面展现出潜力，但其性能仍高度依赖于槽填充顺序，常常导致输出方差显著。我们引入了McDiffuSE框架，将槽选择建模为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。McDiffuSE利用前瞻模拟，在承诺之前评估部分完成情况，系统地探索生成顺序的组合空间。实验表明，与自回归基线相比平均提升3.2%，与基线计划-填充方法相比提升8.0%，在MBPP和MATH500数据集上分别获得19.5%和4.9%的显著提升。我们的分析表明，尽管McDiffuSE主要遵循顺序生成，但引入非顺序生成对于最大化性能是必要的。我们观察到，为了克服模型置信度偏差并发现有效的顺序，需要更大的探索常数，而不是增加模拟次数。这些发现确立了基于MCTS的规划作为提升MDMs生成质量的有效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of slot filling order sensitivity in Masked Diffusion Models (MDMs), which affects the consistency and quality of generated outputs, especially in mathematical and code reasoning tasks. The authors propose McDiffuSE, a framework that treats slot selection as a decision-making process and uses Monte Carlo Tree Search (MCTS) to optimize the infilling order. By employing look-ahead simulations, McDiffuSE evaluates partial completions before finalizing them, leading to more systematic exploration of generation orders. Experimental results demonstrate that McDiffuSE improves performance by 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill methods, with significant gains of 19.5% on MBPP and 4.9% on MATH500. The analysis indicates that while sequential ordering is common, non-sequential generation is crucial for optimal performance, and larger exploration constants are more effective than increasing the number of simulations in overcoming model confidence biases.</div>
<div class="mono" style="margin-top:8px">该研究针对Masked Diffusion Models（MDMs）在解码过程中因槽填充顺序敏感而导致的输出方差问题。提出McDiffuSE框架，将槽选择视为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。该方法利用前瞻模拟，在确定生成顺序前评估部分填充结果，从而系统地探索生成顺序的组合空间。实验结果显示，McDiffuSE在自回归基线方法上平均提升3.2%，在基线计划-填充方法上提升8.0%，在MBPP和MATH500数据集上分别取得19.5%和4.9%的显著提升。分析表明，尽管顺序填充较为常见，但非顺序生成对性能最大化至关重要，且更大的探索常数比增加模拟次数更有效，有助于克服模型置信度偏差并发现有效的填充顺序。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-12T21:47:09+00:00</div>
<div class="meta-line">Comments: added more results on scaling law analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使LLM在推荐系统中首次实现扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这可能是由于先前持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性所致。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，表明其在学习可泛化的用户偏好模式方面具有优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具有稳健的幂律扩展特性。我们的实验表明，在多种合成数据模式下，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of Large Language Models (LLMs) in recommender systems has been hindered by the lack of predictable scaling laws, which are essential for research guidance and resource optimization. This paper proposes a principled synthetic data generation framework that addresses data noise, bias, and incompleteness by creating a curated curriculum for LLM training. Experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, with a $+130\%$ improvement in recall@100 for SasRec. Furthermore, the study demonstrates robust power-law scaling for LLMs pre-trained on recommendation-specific synthetic data, revealing consistent perplexity reduction across different modalities, thus establishing a reliable foundation for scaling LLM capabilities in the recommendation domain.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在推荐系统中的发展难题，提出了一种基于原则的合成数据生成框架。研究动机源于真实用户交互数据中存在的噪声、偏差和不完整性，这些因素阻碍了可预测的扩展定律的建立。方法上，引入了一种分层课程设计，以生成高质量的合成数据，从而支持有效的持续预训练。实验结果显示，使用该合成数据训练的模型在排序任务中显著优于基于真实数据训练的模型，尤其在SasRec的recall@100指标上提升了130%。此外，研究首次实验证明了在推荐特定合成数据上持续预训练的LLM具有稳健的幂律扩展特性，表现出在不同数据模态下一致且可预测的困惑度下降，表明其在可扩展用户偏好学习方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study</div>
<div class="meta-line">Authors: Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim</div>
<div class="meta-line">First: 2025-10-26T16:49:06+00:00 · Latest: 2026-02-12T21:11:14+00:00</div>
<div class="meta-line">Comments: Accepted at LREC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22747v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Québec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Québec French LLMs on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型的低资源方言适应：一项法语方言案例研究</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）已被广泛采用，但其最强的能力仍主要局限于少数高资源语言，这些语言拥有大量训练数据。最近，持续预训练（CPT）作为一种方法被提出，用于微调这些模型以适应低资源的区域方言。本文研究了在数据和计算资源受限的情况下使用CPT进行方言学习的效果。我们利用低秩适应（LoRA）和计算高效的持续预训练技术，使用一个非常小的数据集将三个LLMs适配为魁北克法语方言，并在COLE数据集上进行基准测试。实验结果表明，在不到1%的模型参数更新的情况下，对少数方言基准的提升显著，而对标准语言基准的退化则非常有限。结果分析表明，提升效果高度依赖于语料库的构成。这些发现表明，结合参数高效微调（PEFT）的持续预训练可以有效缩小方言差距，通过提供成本效益高且可持续的语言资源创建方式，使高质量LLM能够扩展至少数语言群体。我们发布了首个魁北克法语LLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting large language models to low-resource dialects, focusing on Québec French. The motivation stems from the fact that LLMs are predominantly trained on high-resource languages, limiting their utility for minority dialects. The authors employ continual pre-training (CPT) combined with low-rank adaptation (LoRA) to fine-tune three LLMs using a minimal dataset. Their experiments show that the adapted models achieve performance improvements on Québec French benchmarks while maintaining strong results on the prestige language benchmarks, with less than 1% of model parameters updated. The results highlight the importance of corpus composition in achieving effective dialect adaptation through parameter-efficient methods.</div>
<div class="mono" style="margin-top:8px">本文探讨了在有限数据和计算资源下，通过持续预训练（CPT）方法对低资源方言进行大语言模型（LLM）适配的挑战，以魁北克法语为例。研究采用低秩适应（LoRA）和计算高效的CPT技术，仅更新不到1%的模型参数，对三个LLM进行适配，并在COLE数据集上进行基准测试。实验结果显示，模型在魁北克法语基准上有所提升，同时在标准法语（主流语言）上的表现未显著下降，表明CPT的效果高度依赖于语料库的构成。这些发现表明，使用参数高效微调（PEFT）方法的CPT能够以低成本和可持续的方式缩小方言差距，从而为少数语言群体提供更高质量的LLM访问。</div>
</details>
</div>
<div class="card">
<div class="title">LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens</div>
<div class="meta-line">Authors: Zekun Li, Sizhe An, Chengcheng Tang, Chuan Guo, Ivan Shugurov, Linguang Zhang, Amy Zhao, Srinath Sridhar, Lingling Tao, Abhay Mittal</div>
<div class="meta-line">First: 2026-02-12T20:02:21+00:00 · Latest: 2026-02-12T20:02:21+00:00</div>
<div class="meta-line">Comments: Project page: https://kunkun0w0.github.io/project/LLaMo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kunkun0w0.github.io/project/LLaMo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (&gt;30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaMo：通过连续自回归标记实现统一运动理解和生成的预训练语言模型扩展</div>
<div class="mono" style="margin-top:8px">近年来，大模型在统一多模态生成和理解方面取得了显著进展。然而，将运动与语言生成和理解统一的模型开发仍处于初步探索阶段。现有方法通常在配对的运动-文本数据上微调大型语言模型（LLMs），由于可用的文本-运动对规模有限，这可能导致语言能力的灾难性遗忘。此外，先前的方法通常通过量化将运动转换为离散表示，以与语言模型集成，从而引入了显著的抖动伪影。为了解决这些挑战，我们提出了LLaMo，一个统一框架，通过一种特定模态的混合变换器（MoT）架构扩展预训练LLMs。该设计在保留基础模型语言理解能力的同时，实现了可扩展的多模态适应。我们将人类运动编码为因果连续的潜在空间，并通过一个轻量级的流匹配头在解码器主干中保持下一个标记预测范式，从而支持实时流式运动生成（&gt;30 FPS）。借助预训练LLMs的全面语言理解和大规模运动-文本预训练，我们的实验表明，LLaMo在一般场景下实现了高保真度的文本到运动生成和运动到文本描述，尤其是在零样本运动生成方面，标志着向通用统一运动-语言大模型迈出的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop a unified model that can both understand and generate motion, addressing the limitations of existing methods that rely on discrete tokenization and suffer from catastrophic forgetting. LLaMo introduces a modality-specific Mixture-of-Transformers (MoT) architecture to extend pretrained language models, preserving their linguistic capabilities while enabling scalable multimodal adaptation. The model encodes human motion into a continuous latent space and uses a lightweight flow-matching head to maintain the next-token prediction paradigm, supporting real-time motion generation. Experimental results show that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, particularly in zero-shot scenarios, demonstrating its effectiveness in unifying motion and language understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的模型，用于运动理解和生成，以解决现有方法在灾难性遗忘和抖动伪影方面的不足。LLaMo提出了一种模态特定的Mixture-of-Transformers（MoT）架构，扩展了预训练语言模型，同时保留其语言理解能力，并实现可扩展的多模态适应。通过将人体运动编码为连续的潜在空间，并在解码器骨架中使用轻量级的流匹配头，LLaMo支持实时运动生成，速度超过30 FPS。实验结果表明，LLaMo在零样本运动生成等一般场景中实现了高质量的文本到运动生成和运动到文本描述，展示了其在统一运动语言理解方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态通常未能充分与语言语义对齐，影响了跨模态对齐的效果，尤其是在需要跨远距离区域或多个图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，一种将视觉操作重新定义为语言引导的特征调制的新框架。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重新编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们在ViLaVT中实例化了这一范式，ViLaVT是一个新型的LVLM，配备了专门用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the performance of large vision-language models (LVLMs) by addressing the limitations of single-pass visual encoding that result in the loss of fine-grained visual information. The proposed method, &#x27;&#x27;chatting with images&#x27;&#x27;, introduces a framework where visual manipulation is guided by language prompts, enabling dynamic joint re-encoding across multiple image regions. This approach enhances the alignment between linguistic and visual modalities, particularly in tasks requiring reasoning over spatial relationships or multiple images. Experimental results across eight benchmarks show that the model, ViLaVT, achieves significant improvements, especially in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型视觉-语言模型（LVLMs）在单次视觉编码过程中丢失细粒度视觉信息的问题，提出了&#x27;chatting with images&#x27;框架，将视觉操作重新定义为语言引导的特征调制。该方法通过表达性语言提示动态地对多个图像区域进行联合重编码，从而加强语言推理与视觉状态更新之间的耦合。该框架在ViLaVT中实现，ViLaVT是一个配备动态视觉编码器的新型LVLM，采用监督微调与强化学习相结合的双阶段课程进行训练。在八个基准测试中，实验结果表明ViLaVT在复杂多图像和基于视频的空间推理任务中表现出显著且一致的性能提升。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260221_0401.html">20260221_0401</a>
<a href="archive/20260220_0359.html">20260220_0359</a>
<a href="archive/20260219_0408.html">20260219_0408</a>
<a href="archive/20260218_0406.html">20260218_0406</a>
<a href="archive/20260217_0354.html">20260217_0354</a>
<a href="archive/20260216_0344.html">20260216_0344</a>
<a href="archive/20260215_0344.html">20260215_0344</a>
<a href="archive/20260213_0409.html">20260213_0409</a>
<a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
