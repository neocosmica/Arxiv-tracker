<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-17 03:54</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260217_0354</div>
    <div class="row"><div class="card">
<div class="title">3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset</div>
<div class="meta-line">Authors: Mehran Advand, Zahra Dehghanian, Navid Faraji, Reza Barati, Seyed Amir Ahmad Safavi-Naini, Hamid R. Rabiee</div>
<div class="meta-line">First: 2026-02-13T11:08:15+00:00 · Latest: 2026-02-13T11:08:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mehrn79.github.io/3DLAND">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DLAND：腹部异常病变三维定位数据集</div>
<div class="mono" style="margin-top:8px">现有的腹部CT医学影像数据集通常缺乏三维标注、多器官覆盖或精确的病灶-器官关联，阻碍了稳健的表示学习和临床应用。为解决这一问题，我们引入了3DLAND，这是一个大规模基准数据集，包含超过6,000个增强CT体数据，与七个腹部器官（肝脏、肾脏、胰腺、脾脏、胃和胆囊）相关联的超过20,000个高保真度三维病灶标注。我们的三阶段简化流程集成了自动化空间推理、优化提示的2D分割以及记忆引导的3D传播，经专家放射科医生验证，表面Dice评分超过0.75。通过提供多样化的病灶类型和患者人口统计信息，3DLAND支持对医学AI异常检测、定位和跨器官迁移学习的可扩展评估。我们的数据集为评估器官感知的三维分割模型设立了新的基准，为面向医疗的AI发展铺平了道路。为促进可重复性和进一步研究，3DLAND数据集及实现代码已公开在https://mehrn79.github.io/3DLAND。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind 3DLAND is to address the limitations of existing abdominal CT datasets, which often lack comprehensive 3D annotations, multi-organ coverage, and accurate lesion-to-organ associations. The dataset introduces a large-scale benchmark with over 6,000 contrast-enhanced CT volumes and more than 20,000 high-fidelity 3D lesion annotations across seven abdominal organs. The core method involves a three-phase pipeline that combines automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores above 0.75. The main experimental results demonstrate that 3DLAND supports scalable evaluation of anomaly detection, localization, and cross-organ transfer learning, offering a valuable resource for improving organ-aware 3D segmentation models in medical AI.</div>
<div class="mono" style="margin-top:8px">本研究提出3DLAND数据集，旨在解决现有腹部CT数据集中缺乏三维标注、多器官覆盖以及精确病灶-器官关联的问题。该数据集包含超过6000个增强CT体积，覆盖肝脏、肾脏、胰腺、脾脏、胃和胆囊等七个腹部器官，拥有超过20000个高保真三维病灶标注。通过一个三阶段的流程，结合自动化空间推理、提示优化的2D分割和记忆引导的3D传播，经放射科专家验证，表面Dice分数超过0.75。3DLAND数据集为医学人工智能中的异常检测、定位和跨器官迁移学习提供了可扩展的评估平台。</div>
</details>
</div>
<div class="card">
<div class="title">Easy-Poly: An Easy Polyhedral Framework For 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Peng Zhang, Xin Li, Xin Lin, Liang He</div>
<div class="meta-line">First: 2025-02-25T04:01:25+00:00 · Latest: 2026-02-13T08:05:15+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17822v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.17822v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent 3D multi-object tracking (3D MOT) methods mainly follow tracking-by-detection pipelines, but often suffer from high false positives, missed detections, and identity switches, especially in crowded and small-object scenarios. To address these challenges, we propose Easy-Poly, a filter-based 3D MOT framework with four key innovations: (1) CNMSMM, a novel Camera-LiDAR fusion detection method combining multi-modal augmentation and an efficient NMS with a new loss function to improve small target detection; (2) Dynamic Track-Oriented (DTO) data association that robustly handles uncertainties and occlusions via class-aware optimal assignment and parallel processing strategies; (3) Dynamic Motion Modeling (DMM) using a confidence-weighted Kalman filter with adaptive noise covariance to enhance tracking accuracy; and (4) an extended life-cycle management system reducing identity switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 65.65% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 75.6%), while also running in real-time. Our framework advances robustness and adaptability in complex driving environments, paving the way for safer autonomous driving perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Easy-Poly：一种用于3D多目标跟踪的简单多面体框架</div>
<div class="mono" style="margin-top:8px">最近的3D多目标跟踪(3D MOT)方法主要采用基于检测的跟踪流程，但在拥挤和小目标场景中，常常面临高误检率、漏检和身份切换的问题。为了解决这些挑战，我们提出了Easy-Poly，一个基于滤波的3D MOT框架，包含四项关键创新：(1) CNMSMM，一种结合多模态增强和高效NMS的新损失函数的相机-激光雷达融合检测方法，以提升小目标检测性能；(2) 动态目标导向(DTO)数据关联，通过类感知最优分配和并行处理策略，鲁棒地处理不确定性与遮挡；(3) 动态运动建模(DMM)，使用带有自适应噪声协方差的置信度加权卡尔曼滤波器以提高跟踪精度；(4) 扩展生命周期管理系统，减少身份切换和误终止。实验结果表明，Easy-Poly在mAP（如LargeKernel3D中从63.30%提升至65.65%）和AMOTA（如从73.1%提升至75.6%）等指标上优于Poly-MOT和Fast-Poly等最先进的方法，同时还能实现实时运行。我们的框架提升了复杂驾驶环境下的鲁棒性和适应性，为更安全的自动驾驶感知铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of Easy-Poly is to improve the performance of 3D multi-object tracking in challenging scenarios such as crowded environments and small-object detection. The framework introduces four key innovations: a novel Camera-LiDAR fusion detection method with multi-modal augmentation and an efficient NMS, a class-aware data association strategy for handling occlusions and uncertainties, a confidence-weighted Kalman filter for dynamic motion modeling, and an extended life-cycle management system to reduce identity switches. Experimental results demonstrate that Easy-Poly achieves significant improvements in mAP and AMOTA compared to existing methods like Poly-MOT and Fast-Poly, while maintaining real-time performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D多目标跟踪在拥挤场景和小目标检测中的性能。Easy-Poly提出了一种基于滤波器的框架，包含四项关键创新：一种结合多模态增强和高效NMS的新Camera-LiDAR融合检测方法（CNMSMM），用于提升小目标检测效果；一种动态目标导向（DTO）的数据关联方法，通过类感知最优分配和并行处理策略有效应对不确定性与遮挡问题；一种使用置信度加权卡尔曼滤波器和自适应噪声协方差的动态运动建模（DMM）方法，以提高跟踪精度；以及一种扩展生命周期管理系统，减少身份切换和错误终止。实验结果表明，Easy-Poly在mAP和AMOTA指标上优于现有方法如Poly-MOT和Fast-Poly，同时保持实时运行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models</div>
<div class="meta-line">Authors: Joshua Ong Jun Leang, Yu Zhao, Mihaela Cătălina Stoian, Wenda Li, Shay B. Cohen, Eleonora Giunchiglia</div>
<div class="meta-line">First: 2026-02-13T03:56:22+00:00 · Latest: 2026-02-13T03:56:22+00:00</div>
<div class="meta-line">Comments: 8 pages, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12586v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我可以点单吗？扩散语言模型中用于槽填充顺序的蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">尽管在掩码扩散模型（MDMs）中，计划-填充解码方法在数学和代码推理方面展现出潜力，但其性能仍高度依赖于槽填充顺序，常常导致输出方差显著。我们引入了McDiffuSE框架，将槽选择建模为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。McDiffuSE利用前瞻模拟在承诺之前评估部分完成情况，系统地探索生成顺序的组合空间。实验表明，与自回归基线相比，平均提升3.2%，与基线计划-填充方法相比提升8.0%，在MBPP和MATH500数据集上分别获得19.5%和4.9%的显著提升。我们的分析表明，虽然McDiffuSE主要遵循顺序生成，但引入非顺序生成对于最大化性能是必要的。我们观察到，为了克服模型置信度偏差并发现有效的顺序，需要更大的探索常数，而不是增加模拟次数。这些发现确立了基于MCTS的规划作为提升MDMs生成质量的有效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of slot infilling order sensitivity in Masked Diffusion Models (MDMs), which can lead to significant output variance in tasks like mathematical and code reasoning. The authors propose McDiffuSE, a framework that treats slot selection as a decision-making process and optimizes the infilling order using Monte Carlo Tree Search (MCTS). By employing look-ahead simulations, McDiffuSE evaluates partial completions before finalizing them, enabling systematic exploration of generation orders. Experimental results demonstrate that McDiffuSE achieves an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill methods, with particularly strong performance on MBPP (19.5%) and MATH500 (4.9%). The analysis shows that while sequential ordering is common, non-sequential generation is crucial for optimal performance, and larger exploration constants are more effective than increased simulation counts in mitigating model confidence biases.</div>
<div class="mono" style="margin-top:8px">本文针对Masked Diffusion Models（MDMs）在数学和代码推理任务中因槽填充顺序敏感而导致的输出方差问题。提出McDiffuSE框架，将槽选择视为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序，利用前瞻模拟评估部分填充结果。实验表明，McDiffuSE在自回归基线方法上平均提升3.2%，在基线计划填充方法上提升8.0%，在MBPP和MATH500任务上分别达到19.5%和4.9%的显著提升。分析显示，尽管多数情况下采用顺序填充，但引入非顺序生成对性能提升至关重要，且增大探索常数比增加模拟次数更有效克服模型置信偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-12T21:47:09+00:00</div>
<div class="meta-line">Comments: added more results on scaling law analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使LLM在推荐系统中首次实现扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这可能是由于先前持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性所致。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，表明其在学习可泛化的用户偏好模式方面的优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具有稳健的幂律扩展特性。我们的实验表明，在多种合成数据模式下，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of large language models (LLMs) in recommender systems has been hindered by the lack of predictable scaling laws, which are essential for guiding research and resource optimization. This paper proposes a principled synthetic data generation framework that addresses the noise, bias, and incompleteness in real user interaction data by creating a structured, pedagogical curriculum for LLM training. Experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, with a $+130\%$ improvement in recall@100 for SasRec. Furthermore, the study demonstrates robust power-law scaling for LLMs pre-trained on this recommendation-specific data, revealing consistent perplexity reduction across different synthetic data modalities.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在推荐系统中的应用受到缺乏可预测的扩展定律的阻碍，这限制了研究方向和资源分配。本文提出了一种基于原则的合成数据生成框架，通过构建一个精心设计的教育性课程来解决真实用户交互数据中的噪声、偏差和不完整性问题。实验结果表明，使用该合成数据训练的模型在排序任务中显著优于基于真实数据训练的模型，其中SasRec在recall@100指标上提升了130%。此外，研究首次展示了在推荐特定合成数据上持续预训练的LLM具有稳健的幂律扩展特性，揭示了在多种合成数据模式下一致的困惑度下降，为推荐领域中LLM的可靠扩展提供了基础方法。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study</div>
<div class="meta-line">Authors: Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim</div>
<div class="meta-line">First: 2025-10-26T16:49:06+00:00 · Latest: 2026-02-12T21:11:14+00:00</div>
<div class="meta-line">Comments: Accepted at LREC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22747v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Québec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Québec French LLMs on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型的低资源方言适配：一项法语方言案例研究</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）已被广泛采用，但其最强的能力仍主要局限于少数高资源语言，这些语言拥有大量训练数据。最近，持续预训练（CPT）作为一种方法，被用于微调这些模型以适应低资源的区域方言。在本文中，我们研究了在数据和计算资源受限的情况下使用CPT进行方言学习的效果。通过低秩适配（LoRA）和计算高效的持续预训练，我们使用一个非常小的数据集将三个LLMs适配为魁北克法语方言，并在COLE数据集上进行基准测试。实验结果表明，在不到1%的模型参数更新的情况下，对少数方言基准的提升显著，而对标准语言基准的退化则非常有限。结果分析表明，提升效果高度依赖于语料库的构成。这些发现表明，结合参数高效微调（PEFT）的持续预训练可以有效缩小方言差距，通过提供成本效益高且可持续的语言资源创建，使高质量LLM能够惠及少数语言群体。我们发布了首个魁北克法语LLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting large language models (LLMs) to low-resource dialects, focusing on Québec French. The motivation stems from the fact that LLMs&#x27; strongest performance is typically limited to high-resource languages with extensive training data. The authors employ continual pre-training (CPT) combined with low-rank adaptation (LoRA) and compute-efficient techniques to fine-tune three LLMs using a minimal dataset. Experimental results on the COLE suite show significant improvements in performance for the Québec French dialect while maintaining strong performance on the prestige language benchmarks, with less than 1% of model parameters updated. The analysis highlights that the effectiveness of the adaptation is highly dependent on the composition of the training corpus.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型在低资源方言上的适应问题，以魁北克法语为例。动机源于当前大型语言模型主要依赖高资源语言训练，限制了其在少数方言中的应用。作者采用持续预训练（CPT）结合低秩适应（LoRA）技术，使用极小的数据集对三个大型语言模型进行微调。实验表明，适应后的模型在魁北克法语基准测试中表现提升，同时在标准法语基准测试中几乎没有性能下降，仅更新了不到1%的模型参数。结果分析表明，语料库的构成对方言适应效果有显著影响，表明通过参数高效微调方法可以有效缩小方言差距。</div>
</details>
</div>
<div class="card">
<div class="title">LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens</div>
<div class="meta-line">Authors: Zekun Li, Sizhe An, Chengcheng Tang, Chuan Guo, Ivan Shugurov, Linguang Zhang, Amy Zhao, Srinath Sridhar, Lingling Tao, Abhay Mittal</div>
<div class="meta-line">First: 2026-02-12T20:02:21+00:00 · Latest: 2026-02-12T20:02:21+00:00</div>
<div class="meta-line">Comments: Project page: https://kunkun0w0.github.io/project/LLaMo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kunkun0w0.github.io/project/LLaMo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (&gt;30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaMo：通过连续自回归标记实现统一运动理解和生成的预训练语言模型扩展</div>
<div class="mono" style="margin-top:8px">近年来，大模型在统一多模态生成和理解方面取得了显著进展。然而，将运动与语言生成和理解统一的模型开发仍处于初步探索阶段。现有方法通常在配对的运动-文本数据上微调大型语言模型（LLMs），由于可用的文本-运动对规模有限，这可能导致语言能力的灾难性遗忘。此外，先前的方法通常通过量化将运动转换为离散表示，以与语言模型集成，从而引入了显著的抖动伪影。为了解决这些挑战，我们提出了LLaMo，一个统一框架，通过一种特定模态的混合变换器（MoT）架构扩展预训练的LLMs。该设计在保持基础模型语言理解能力的同时，实现了可扩展的多模态适应。我们将人体运动编码为因果连续的潜在空间，并通过一个轻量级的流匹配头在解码器主干中保持下一个标记预测范式，从而支持实时流式运动生成（&gt;30 FPS）。通过利用预训练LLMs的全面语言理解和大规模运动-文本预训练，我们的实验表明，LLaMo在一般场景下实现了高保真度的文本到运动生成和运动到文本描述，尤其是在零样本运动生成方面，标志着向通用统一运动-语言大模型迈出的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop a unified model that can both understand and generate motion, addressing the limitations of existing methods that suffer from catastrophic forgetting and jitter artifacts. LLaMo introduces a modality-specific Mixture-of-Transformers (MoT) architecture to extend pretrained language models, preserving their linguistic capabilities while enabling scalable multimodal adaptation. The model encodes human motion into a continuous latent space and uses a lightweight flow-matching head to maintain the next-token prediction paradigm, supporting real-time motion generation at over 30 FPS. Experimental results show that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, particularly in zero-shot scenarios, demonstrating its effectiveness in unifying motion and language understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的模型，用于运动理解和生成，以解决现有方法因离散标记化导致的抖动伪影和灾难性遗忘问题。LLaMo提出了一种模态特定的Mixture-of-Transformers（MoT）架构，扩展了预训练语言模型，同时保留其语言理解能力，并实现可扩展的多模态适应。该模型将人体运动编码为连续的潜在空间，并通过轻量级的流匹配头保持下一个标记预测范式，支持实时运动生成（&gt;30 FPS）。实验结果表明，LLaMo在一般场景下实现了高质量的文本到运动生成和运动到文本描述，尤其在零样本运动生成方面表现突出，标志着向通用运动-语言大模型迈出重要一步。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态通常未能充分与语言语义对齐，影响了跨模态对齐的效果，尤其是在需要跨远距离区域或多张图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，一种将视觉操作重新定义为语言引导的特征调制的新框架。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们在ViLaVT中实例化了这一范式，ViLaVT是一个新型的LVLM，配备了专门用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the performance of large vision-language models (LVLMs) by addressing the limitations of single-pass visual encoding that result in the loss of fine-grained visual information. The proposed method, &#x27;chatting with images,&#x27; introduces a framework where visual manipulation is guided by language prompts, enabling dynamic joint re-encoding across multiple image regions. This approach enhances the alignment between linguistic and visual modalities. Experimental results across eight benchmarks show that the novel model, ViLaVT, achieves significant improvements, especially in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型视觉-语言模型（LVLMs）依赖单次视觉编码导致细粒度视觉信息丢失的问题，提出了一种新的框架&#x27;chatting with images&#x27;，将视觉操作重新定义为语言引导的特征调制，实现对多个图像区域的动态联合重编码。该方法通过紧密耦合语言推理与视觉状态更新，提升跨模态对齐效果。ViLaVT作为该框架的实现，是一款配备动态视觉编码器的新型LVLM，采用监督微调与强化学习相结合的双阶段课程进行训练。在八个基准测试中，ViLaVT表现出显著的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中效果更为突出。</div>
</details>
</div>
<div class="card">
<div class="title">3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu</div>
<div class="meta-line">First: 2026-02-12T16:41:26+00:00 · Latest: 2026-02-12T16:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aczheng-cai.github.io/3dgsnav.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DGSNav：通过主动3D高斯点云增强视觉-语言模型的物体导航推理</div>
<div class="mono" style="margin-top:8px">物体导航是具身智能的核心能力，使代理能够在未知环境中定位目标物体。近年来，视觉-语言模型（VLMs）的进步推动了零样本物体导航（ZSON）的发展。然而，现有方法通常依赖于场景抽象，将环境转换为语义地图或文本表示，导致高层决策受限于低层感知的准确性。在本工作中，我们提出了3DGSNav，一个新颖的ZSON框架，通过将3D高斯点云（3DGS）作为持久记忆嵌入VLMs中，以增强空间推理能力。通过主动感知，3DGSNav逐步构建环境的3DGS表示，从而实现前沿感知的第一视角视图的轨迹引导自由视角渲染。此外，我们设计了结构化的视觉提示，并将其与思维链（Chain-of-Thought, CoT）提示相结合，进一步提升VLM推理能力。在导航过程中，实时物体检测器过滤潜在目标，而由VLM驱动的主动视角切换则执行目标重新验证，确保高效且可靠的识别。在多个基准测试和四足机器人上的实验证明，我们的方法在鲁棒性和竞争力方面均优于最先进的方法。项目页面：https://aczheng-cai.github.io/3dgsnav.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to improve the spatial reasoning capabilities of vision-language models (VLMs) for zero-shot object navigation (ZSON) by addressing the limitations of scene abstraction methods. 3DGSNav introduces a novel framework that integrates 3D Gaussian Splatting (3DGS) as a persistent memory mechanism, allowing VLMs to maintain a detailed 3D representation of the environment through active perception. This enables the agent to generate trajectory-guided free-viewpoint renderings and use structured visual prompts combined with Chain-of-Thought (CoT) prompting to enhance reasoning. The method employs a real-time object detector and VLM-driven active viewpoint switching to ensure accurate target re-verification, leading to robust and competitive performance across multiple benchmarks and real-world experiments on a quadruped robot.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉语言模型（VLMs）在未知环境中进行零样本物体导航（ZSON）的空间推理能力。3DGSNav提出了一种新颖框架，将3D高斯点云作为持久记忆机制嵌入其中，以增强VLMs对复杂场景的理解与导航能力。通过主动感知，该方法逐步构建环境的3DGS表示，并支持轨迹引导的自由视角渲染。同时，引入结构化的视觉提示与链式思维提示以提升推理效果。在导航过程中，实时物体检测器用于筛选潜在目标，而由VLM驱动的主动视角切换则用于目标重新验证，从而实现高效可靠的导航。在多个基准测试和四足机器人的真实世界实验中，结果表明该方法具有鲁棒性和竞争力，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Adoption of AI Coding Agents in Open-source Android and iOS Development</div>
<div class="meta-line">Authors: Muhammad Ahmad Khan, Hasnain Ali, Muneeb Rana, Muhammad Saqib Ilyas, Abdul Ali Bangash</div>
<div class="meta-line">First: 2026-02-12T16:30:29+00:00 · Latest: 2026-02-12T16:30:29+00:00</div>
<div class="meta-line">Comments: Accepted at MSR 2026 Mining Challenge track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于在开源Android和iOS开发中采用AI编码代理的影响</div>
<div class="mono" style="margin-top:8px">AI编码代理在软件开发中的作用日益增加，但其对移动开发的影响却鲜有实证研究。本文提出了首个针对开源移动应用项目中代理生成代码的类别级实证研究。我们利用AIDev数据集中的193个已验证的Android和iOS开源GitHub仓库中的2,901个AI撰写的拉取请求（PRs），分析了跨移动平台、代理和任务类别的PR接受行为。我们发现，Android项目接收的AI撰写的PR数量是iOS的两倍，并且Android的PR接受率（71%）高于iOS（63%），且在Android上存在显著的代理间差异。在任务类别方面，常规任务（功能、修复和UI）的PR接受率最高，而重构和构建等结构性变更的PR成功率较低且解决时间较长。此外，我们的演化分析表明，Android的PR解决时间在2025年中期有所改善，之后又开始下降。我们的研究结果为AI代理对开源移动项目的影响提供了首个基于证据的描述，并为评估代理生成贡献对设计平台感知代理系统的影响建立了实证基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the adoption of AI coding agents in open-source Android and iOS development, aiming to understand their impact on code contributions. By analyzing 2,901 AI-authored pull requests across 193 verified repositories, the research identifies differences in PR acceptance rates between platforms, with Android showing higher acceptance (71%) compared to iOS (63%). It also reveals that routine tasks such as feature additions and bug fixes are more likely to be accepted, whereas structural changes like refactoring and build updates face lower success rates and longer resolution times. The study further observes a trend of declining PR resolution times on Android after mid-2025, suggesting evolving dynamics in the integration of AI agents into mobile development workflows.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在开源移动开发中的采用及其影响，重点关注Android和iOS项目。通过对193个仓库中2,901个AI生成的拉取请求（PR）进行分析，研究发现Android项目的PR接受率（71%）高于iOS（63%），且在Android平台上存在显著的代理间差异。日常任务如功能添加和修复的PR接受率最高，而重构和构建等结构性任务则接受率较低且处理时间较长。此外，分析还显示Android项目的PR处理时间在2025年中期有所改善，但之后又出现下降趋势，表明AI生成代码在开源移动生态中的整合正在经历变化。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?</div>
<div class="meta-line">Authors: Thibaud Gloaguen, Niels Mündler, Mark Müller, Veselin Raychev, Martin Vechev</div>
<div class="meta-line">First: 2026-02-12T14:15:22+00:00 · Latest: 2026-02-12T14:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11988v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents&#x27; task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估AGENTS.md：仓库级上下文文件对编码代理是否有帮助？</div>
<div class="mono" style="margin-top:8px">在软件开发中，一种普遍的做法是通过手动或自动生成上下文文件（如AGENTS.md）来针对特定仓库定制编码代理。尽管这种做法被代理开发者强烈推荐，但目前尚无严谨的研究探讨此类上下文文件是否对实际任务真正有效。在本研究中，我们探讨了这一问题，并在两个互补的设置中评估了编码代理完成任务的性能：一是使用LLM生成的上下文文件，按照代理开发者建议，针对流行仓库中的SWE-bench任务；二是从包含开发者提交的上下文文件的仓库中收集的新问题集。我们发现，在多个编码代理和LLM中，上下文文件往往会降低任务的成功率，同时增加推理成本超过20%。行为上，无论是LLM生成的还是开发者提供的上下文文件，都会鼓励更广泛的探索（例如更彻底的测试和文件遍历），而编码代理往往遵循这些指令。最终，我们得出结论：上下文文件中不必要的要求会使任务更加困难，而人工撰写的上下文文件应仅描述最低限度的要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the effectiveness of repository-level context files, such as AGENTS.md, in improving the performance of coding agents. The research is motivated by the common practice of using these files to tailor agents to specific repositories, despite a lack of empirical evidence on their impact. The authors evaluate coding agents on two settings: one using LLM-generated context files based on developer recommendations, and the other using real issues from repositories with existing context files. Their findings show that context files generally lower task success rates and increase inference costs by more than 20%, suggesting that they may introduce unnecessary complexity. The results indicate that coding agents tend to follow context file instructions, but excessive or irrelevant requirements can hinder task completion.</div>
<div class="mono" style="margin-top:8px">本研究探讨了诸如AGENTS.md等仓库级上下文文件对编码代理性能的影响。研究动机源于编码代理常用这些文件来适配特定仓库的实践，但缺乏实证验证。作者在两个互补的设置中评估了编码代理的表现：一个使用基于开发者建议的LLM生成的上下文文件，另一个使用实际开发者提交的上下文文件。实验结果显示，上下文文件通常会降低任务成功率并增加推理成本，表明它们可能引入不必要的复杂性。研究还指出，编码代理倾向于遵循上下文文件的指令，但过多或不相关的条件会阻碍任务完成。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</div>
<div class="meta-line">Authors: Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang, Hongyang Wei, Yi-Fan Zhang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Long Chen</div>
<div class="meta-line">First: 2026-02-12T14:12:14+00:00 · Latest: 2026-02-12T14:12:14+00:00</div>
<div class="meta-line">Comments: 19 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11980v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11980v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model&#x27;s layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间链式思维：连接理解与生成模型以实现空间推理生成</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在美学图像合成方面表现出色，但它们在复杂空间理解和推理方面常常面临挑战。现有方法通常借助多模态大语言模型（MLLMs）来增强这一能力。然而，这些方法要么通过联合训练带来高昂的计算成本，要么仅依赖文本提示导致空间信息丢失。为了解决这些问题，我们提出了一种空间链式思维（SCoT）框架，这是一种即插即用的方法，能够有效连接MLLMs的推理能力与扩散模型的生成能力。具体而言，我们首先通过交错的文本-坐标指令格式训练扩散模型，以增强其布局感知能力。随后，我们利用最先进的MLLMs作为规划器，生成全面的布局计划，并将它们的空间规划能力直接转移到生成过程中。大量实验表明，我们的方法在图像生成基准测试中达到了最先进的性能，并在复杂推理任务中显著优于基线方法，同时在图像编辑场景中也表现出良好的效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of diffusion models in spatial understanding and reasoning by proposing the Spatial Chain-of-Thought (SCoT) framework. The motivation stems from the fact that while diffusion models excel in image generation, they lack the ability to reason about spatial layouts effectively. The method combines the reasoning power of Multimodal Large Language Models (MLLMs) with the generative strength of diffusion models through a plug-and-play approach. By training diffusion models on interleaved text-coordinate instructions and using MLLMs to generate detailed layout plans, the framework enhances spatial reasoning. Experimental results show that SCoT achieves state-of-the-art performance on image generation benchmarks and outperforms existing methods in complex reasoning tasks, as well as demonstrating effectiveness in image editing.</div>
<div class="mono" style="margin-top:8px">本文旨在解决扩散模型在空间理解和推理方面的不足，提出了空间链式思维（SCoT）框架。动机源于扩散模型虽然在图像生成方面表现优异，但缺乏对空间结构的推理能力，通常依赖于多模态大语言模型（MLLMs），这要么需要高昂的计算成本，要么在仅使用文本提示时会丢失空间信息。方法通过将MLLMs作为规划器生成详细的布局计划，并利用交错的文本-坐标指令格式引导扩散模型的生成过程。实验结果表明，SCoT在图像生成基准测试中达到最先进水平，在复杂空间推理任务中显著优于基线方法，同时在图像编辑场景中也表现出良好的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Suraj Ranganath, Anish Patnaik, Vaishak Menon</div>
<div class="meta-line">First: 2026-02-12T12:32:51+00:00 · Latest: 2026-02-12T12:32:51+00:00</div>
<div class="meta-line">Comments: Workshop submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11882v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11882v1">PDF</a> · <a href="https://github.com/suraj-ranganath/DINO-MBQuant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在世界模型规划中比特的重要性：一种用于高效空间推理的配对混合比特研究</div>
<div class="mono" style="margin-top:8px">高效的空问推理需要在有限精度预算下保持可靠的世界模型。我们研究低比特规划行为是否主要由总比特宽度决定，还是由各模块中比特的分配位置决定。在Wall规划任务中，我们使用DINO-WM对均匀、混合、非对称和逐层变体进行了配对目标混合比特评估，并在两种规划预算下观察到一致的三种状态模式：8比特和6比特设置接近FP16，3比特设置崩溃，而4比特设置则对分配敏感。在这一过渡区域，保持编码器精度的规划效果优于均匀量化，且接近大小的非对称变体也表现出相同的编码器侧方向。在后续的严格22单元复制实验中，使用更小的每单元回合数，混合与均匀INT4的符号变得依赖预算，这进一步突显了该过渡区域的敏感性。这些发现为高效空间推理的模块感知和预算感知量化策略提供了研究方向。代码和运行产物可在https://github.com/suraj-ranganath/DINO-MBQuant获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of bit allocation on the reliability of world models in spatial reasoning tasks under limited precision budgets. The research focuses on determining whether low-bit planning performance is primarily influenced by total bitwidth or the distribution of bits across different modules. Using DINO-WM on the Wall planning task, the authors evaluate various quantization strategies, including uniform, mixed, asymmetric, and layerwise, under two planner budgets. Their results reveal a three-regime pattern, where 8-bit and 6-bit settings maintain performance close to FP16, 3-bit settings experience significant degradation, and 4-bit settings show sensitivity to bit allocation. Preserving encoder precision in mixed-bit configurations improves planning performance compared to uniform quantization, and asymmetric variants demonstrate similar trends. These findings suggest the need for more nuanced, module- and budget-aware quantization approaches in efficient spatial reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在有限精度预算下，位宽分配对世界模型空间推理可靠性的影响。通过在Wall规划任务上评估DINO-WM的不同量化策略，包括均匀、混合、非对称和分层方式，研究发现性能呈现出三种状态：8位和6位设置保持接近FP16的性能，3位设置出现显著退化，而4位设置则对位分配敏感。结果表明，保持编码器的精度可以提升规划性能，而非对称变体也表现出类似优势。这些发现提示了在高效空间推理中需要更细致的、模块感知和预算感知的量化策略。</div>
</details>
</div>
<div class="card">
<div class="title">Code2Worlds: Empowering Coding LLMs for 4D World Generation</div>
<div class="meta-line">Authors: Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang</div>
<div class="meta-line">First: 2026-02-12T09:34:28+00:00 · Latest: 2026-02-12T09:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11757v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11757v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/Code2Worlds">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://aigeeksgroup.github.io/Code2Worlds">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Code2Worlds: 为4D世界生成赋能的编码大语言模型</div>
<div class="mono" style="margin-top:8px">实现空间智能需要超越视觉合理性，构建基于物理定律的世界模拟器。尽管编码大语言模型在静态3D场景生成方面取得了进展，但将其范式扩展到4D动态仍然是一个关键的前沿领域。该任务面临两个根本性挑战：多尺度上下文纠缠，其中整体生成无法在局部物体结构与全局环境布局之间取得平衡；以及语义-物理执行鸿沟，其中开环代码生成会导致缺乏动态保真的物理幻觉。我们提出了Code2Worlds框架，将4D生成建模为语言到模拟代码生成。首先，我们提出了一种双流架构，将检索增强的物体生成与分层环境编排解耦。其次，为确保动态保真度，我们建立了一种物理感知的闭环机制，其中PostProcess Agent编写动态，同时结合VLM-Motion Critic进行自我反思，以迭代优化模拟代码。在Code4D基准上的评估表明，Code2Worlds在SGS指标上比基线提升41%，在丰富度上高出49%，并且能够生成此前静态方法中缺失的物理感知动态。代码：https://github.com/AIGeeksGroup/Code2Worlds。网站：https://aigeeksgroup.github.io/Code2Worlds。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance coding LLMs&#x27; ability to generate 4D dynamic worlds by addressing the limitations of static 3D scene generation. The proposed method, Code2Worlds, introduces a dual-stream architecture that separates object generation from environmental orchestration and employs a physics-aware closed-loop mechanism involving a PostProcess Agent and a VLM-Motion Critic for iterative refinement. Experimental results on the Code4D benchmark demonstrate significant improvements, with a 41% increase in SGS and 49% higher Richness compared to existing baselines, showcasing the framework&#x27;s effectiveness in generating physically accurate dynamics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升编码大语言模型在生成4D动态世界方面的能力，克服静态3D场景生成的局限性。Code2Worlds方法引入了双流架构，将物体生成与环境编排分离，并采用了一种物理感知的闭环机制，包括PostProcess Agent和VLM-Motion Critic，用于动态代码的迭代优化。在Code4D基准测试中，实验结果表明该框架在空间接地得分（SGS）上提升了41%，在丰富度上提高了49%，显示出其在生成物理准确动态世界方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation</div>
<div class="meta-line">Authors: Shuo Lu, Jianjie Cheng, Yinuo Xu, Yongcan Yu, Lijun Sheng, Peijie Wang, Siru Jiang, Yongguan Hu, Run Ling, Yihua Shao, Ao Ma, Wei Feng, Lingxiao He, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang</div>
<div class="meta-line">First: 2026-02-12T06:37:55+00:00 · Latest: 2026-02-12T06:37:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态大语言模型真的理解空间吗？一项数学推理评估</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在面向感知的任务中表现出色，但其进行数学空间推理的能力，即解析和操作二维和三维关系的能力，仍不清楚。人类在解决教科书风格的空间推理问题时准确率超过95\%，但我们发现大多数领先的MLLMs在相同任务上的准确率甚至无法达到60\%。这种显著的差距突显了空间推理是当前模型的基本弱点。为研究这一差距，我们提出了MathSpatial，一个用于评估和提升MLLMs空间推理能力的统一框架。MathSpatial包含三个互补的组成部分：(i) MathSpatial-Bench，一个涵盖三个类别和十一个子类型的2000个问题的基准测试集，旨在将推理难度与感知噪声分离；(ii) MathSpatial-Corpus，一个包含8000个已验证解法的额外训练数据集；以及(iii) MathSpatial-SRT，它将推理建模为由三个原子操作——关联、约束和推断——组成的结构化轨迹。实验表明，使用MathSpatial对Qwen2.5-VL-7B进行微调，可以在减少25\% token数量的同时实现具有竞争力的准确率。MathSpatial提供了首个将感知与推理分离的大规模资源，使我们能够精确测量和全面理解MLLMs中的数学空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the spatial reasoning capabilities of multimodal large language models (MLLMs), revealing a significant gap between human performance and model accuracy on mathematical spatial reasoning tasks. The researchers introduce MathSpatial, a framework that includes a benchmark dataset, a training corpus, and a structured reasoning trace model, to evaluate and enhance spatial reasoning in MLLMs. Experimental results show that fine-tuning Qwen2.5-VL-7B on MathSpatial improves accuracy while reducing token usage by 25%, indicating the framework&#x27;s effectiveness in isolating reasoning challenges from perceptual noise.</div>
<div class="mono" style="margin-top:8px">本研究探讨了多模态大语言模型（MLLMs）在数学空间推理方面的局限性，这是一种关键但尚未充分研究的能力。研究人员提出了MathSpatial框架，包括基准测试集、训练语料库和结构化推理轨迹模型，用于评估和提升空间推理性能。实验结果显示，对Qwen2.5-VL-7B进行MathSpatial微调可提高准确率，同时减少25%的token使用量，表明该框架在分离推理挑战与感知噪声方面具有有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</div>
<div class="meta-line">Authors: Dewi S. W. Gould, Bruno Mlodozeniec, Samuel F. Brown</div>
<div class="meta-line">First: 2025-08-08T08:16:40+00:00 · Latest: 2026-02-12T04:11:52+00:00</div>
<div class="meta-line">Comments: 7 pages and appendices</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06111v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06111v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the capabilities and risks of foundation models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others&#x27; weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that: (1) weaker models can reliably differentiate and score stronger ones, (2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and (3) SKATE automatically surfaces fine-grained capability differences between models. Our findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SKATE：一种可扩展的竞赛评估框架：较弱的LLM通过可验证挑战区分更强的LLM</div>
<div class="mono" style="margin-top:8px">评估基础模型的能力和风险至关重要，但当前方法需要大量领域专业知识，阻碍了其可扩展性，因为这些模型迅速发展。我们引入SKATE：一种新颖的评估框架，其中大语言模型（LLMs）通过为彼此生成和解决可验证任务进行竞争。我们的核心观点是将评估视为一种游戏：模型同时扮演任务设置者和解决者，被激励创建能够突出自身优势并暴露他人弱点的问题。SKATE提供了几个关键优势，平衡了可扩展性、开放性和客观性。它完全自动化，无需数据和人工干预，且具有可扩展性。通过使用可验证任务而非LLM裁判，评分具有客观性。与领域受限的程序生成基准（如下棋或空间推理）不同，让LLM创造性地提出挑战使评估更加开放和可扩展。作为概念验证，我们引入了由LLM设定的代码输出预测（COP）挑战，作为测试我们方法的可验证且可扩展的框架。使用基于TrueSkill的排名系统，我们评估了六个前沿LLM，并发现：(1) 较弱的模型可以可靠地区分和评分更强的模型；(2) 基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题；(3) SKATE能够自动揭示模型之间的细粒度能力差异。我们的发现是迈向通用、可扩展评估框架的重要一步，这些框架能够跟上LLM的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SKATE is to develop a scalable and objective evaluation framework for large language models (LLMs) that does not rely on domain expertise or human input. The core method involves treating evaluation as a competitive game where LLMs generate and solve verifiable tasks for each other, allowing them to highlight their strengths and expose others&#x27; weaknesses. The main experimental results show that weaker models can reliably differentiate and score stronger ones, LLMs exhibit self-preferencing behavior by generating challenges aligned with their own capabilities, and SKATE effectively uncovers fine-grained capability differences between models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种无需领域专业知识或人工干预的可扩展且客观的大型语言模型（LLM）评估方法。提出的框架SKATE将评估视为一种竞争游戏，其中LLM既作为任务生成者又作为解题者，从而突出自身优势并揭示他人的不足。主要实验结果表明，较弱的模型能够可靠地区分并评分更强的模型，LLM表现出自我偏好行为，生成与其自身能力相符的挑战，且SKATE能够自动揭示模型间的细粒度能力差异。</div>
</details>
</div>
<div class="card">
<div class="title">GameDevBench: Evaluating Agentic Capabilities Through Game Development</div>
<div class="meta-line">Authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten, Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar, Chris Donahue</div>
<div class="meta-line">First: 2026-02-11T18:15:11+00:00 · Latest: 2026-02-11T18:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5&#x27;s performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GameDevBench：通过游戏开发评估代理能力</div>
<div class="mono" style="margin-top:8px">尽管在代码代理方面取得了快速进展，但其多模态对应物的进展却相对滞后。一个关键挑战是缺乏结合软件开发复杂性与深度多模态理解需求的评估测试平台。游戏开发提供了这样的测试平台，因为代理必须在视觉游戏场景中处理诸如着色器、精灵和动画等本质上多模态的资产，同时应对庞大的代码库。我们提出了GameDevBench，这是首个用于评估代理在游戏开发任务上的基准。GameDevBench包含132个任务，来源于网络和视频教程。这些任务需要显著的多模态理解能力，并且复杂度较高——平均解决方案所需的代码行数和文件更改量是之前软件开发基准的三倍以上。目前代理在游戏开发任务上仍存在困难，表现最好的代理仅能完成54.5%的任务。我们发现任务难度与多模态复杂度之间存在强相关性，成功率从以游戏玩法为导向的任务的46.9%下降到2D图形任务的31.6%。为了提升多模态能力，我们引入了两种基于图像和视频的简单反馈机制。尽管这些方法简单，但它们持续提升了代理的性能，其中最大的提升体现在Claude Sonnet 4.5的性能从33.3%提升到47.7%。我们公开发布GameDevBench，以支持进一步的代理游戏开发研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to address the lack of comprehensive evaluation frameworks for multimodal coding agents, which are essential for complex tasks requiring both code and visual understanding. The authors introduce GameDevBench, a benchmark composed of 132 tasks drawn from web and video tutorials, designed to assess agents&#x27; ability to handle game development, which involves intricate multimodal elements such as shaders, sprites, and animations. Experimental results show that even the best agent only solves 54.5% of the tasks, with success rates significantly lower for tasks involving 2D graphics compared to gameplay-oriented ones. The study also proposes two simple feedback mechanisms based on image and video input, which improve agent performance, particularly for Claude Sonnet 4.5, increasing its success rate from 33.3% to 47.7%.</div>
<div class="mono" style="margin-top:8px">GameDevBench的提出旨在解决多模态编码代理缺乏全面评估框架的问题，因为这些代理的发展仍落后于单模态代理。该基准通过游戏开发任务来评估代理的复杂软件开发能力，这些任务需要对视觉和文本元素有深入理解。它包含132个任务，来源于网络和视频教程，要求代理具备显著的多模态推理能力。实验结果显示，即使是最优代理也只能完成54.5%的任务，其中面向游戏玩法的任务成功率为46.9%，而2D图形任务则降至31.6%。研究引入了两种基于图像和视频的简单反馈机制，显著提升了代理的表现，尤其是Claude Sonnet 4.5的成功率从33.3%提升至47.7%。</div>
</details>
</div>
<div class="card">
<div class="title">CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning</div>
<div class="meta-line">Authors: Hang Wu, Yujun Cai, Zehao Li, Haonan Ge, Bowen Sun, Junsong Yuan, Yiwei Wang</div>
<div class="meta-line">First: 2026-01-30T04:45:43+00:00 · Latest: 2026-02-11T17:26:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00181v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00181v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamReasoner：通过结构化空间推理强化摄像机运动理解</div>
<div class="mono" style="margin-top:8px">理解摄像机动态是视频空间智能的核心支柱。然而，现有的多模态模型大多将此任务视为黑箱分类，往往依赖于表面的视觉模式而非几何线索，从而混淆了物理上不同的运动。我们提出了CamReasoner框架，将摄像机运动理解重新表述为一个结构化的推理过程，以弥合感知与电影逻辑之间的鸿沟。我们的方法以观察-思考-回答（O-T-A）范式为核心，迫使模型在一个显式的推理模块中解码时空线索，如轨迹和视锥。为了培养这一能力，我们构建了一个大规模的推理轨迹套件，包含18,000个SFT推理链和38,000个RL反馈样本。值得注意的是，我们是首个在该领域使用强化学习进行逻辑对齐的团队，确保运动推理基于物理几何而非情境猜测。通过将强化学习应用于观察-思考-回答（O-T-A）推理范式，CamReasoner有效抑制了幻觉现象，并在多个基准测试中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the understanding of camera movements in videos by moving beyond superficial visual patterns and incorporating geometric reasoning. CamReasoner introduces an Observation-Thinking-Answer (O-T-A) framework that forces the model to perform structured spatial inference, decoding spatio-temporal cues like trajectories and view frustums. The framework is trained using a Large-scale Inference Trajectory Suite with 18k SFT reasoning chains and 38k RL feedback samples, and it is the first to apply reinforcement learning for logical alignment in this domain. Experimental results show that CamReasoner effectively reduces hallucinations and achieves state-of-the-art performance on multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过引入几何推理提升视频中相机运动的理解，突破现有模型依赖表面视觉模式的局限。CamReasoner提出了一种观察-思考-回答（O-T-A）框架，强制模型进行结构化的空间推理，解析轨迹和视锥等时空线索。该框架基于包含18000个SFT推理链和38000个RL反馈样本的大规模推理轨迹集进行训练，并首次在该领域应用强化学习实现逻辑对齐。实验结果表明，CamReasoner有效抑制了幻觉现象，在多个基准测试中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting</div>
<div class="meta-line">Authors: Rishikesh Bhyri, Brian R Quaranto, Philip J Seger, Kaity Tung, Brendan Fox, Gene Yang, Steven D. Schwaitzberg, Junsong Yuan, Nan Xi, Peter C W Kim</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-11T16:49:37+00:00 · Latest: 2026-02-11T16:49:37+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026. This version includes additional authors who contributed during the rebuttal phase</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11024v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于密集手术器械计数的链式观察空间推理</div>
<div class="mono" style="margin-top:8px">准确计数手术室（OR）中的手术器械是确保手术期间患者安全的关键前提。尽管大型视觉-语言模型和代理AI取得了近期进展，但准确计数这些器械仍然极具挑战性，尤其是在器械密集排列的场景中。为了解决这一问题，我们引入了Chain-of-Look，这是一种新颖的视觉推理框架，通过强制执行结构化的视觉链来模拟人类的顺序计数过程，而不是依赖传统的无序目标检测方法。这种视觉链引导模型沿着连贯的空间轨迹进行计数，从而在复杂场景中提高计数准确性。为了进一步强化视觉链的物理合理性，我们引入了邻近损失函数，该函数显式建模了密集排列手术器械所固有的空间约束。我们还提出了SurgCount-HD，一个包含1,464张高密度手术器械图像的新数据集。大量实验表明，我们的方法在密集手术器械计数这一具有挑战性的任务中，优于现有的计数方法（如CountGD、REC）以及多模态大语言模型（如Qwen、ChatGPT）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate counting of surgical instruments in operating rooms is essential for patient safety, yet remains challenging in dense scenarios due to the tight clustering of objects. The Chain-of-Look framework addresses this by simulating the sequential human counting process through a structured visual chain, which enables the model to count along a coherent spatial path. This approach is enhanced with a neighboring loss function that enforces spatial constraints, improving the physical plausibility of the counting process. Evaluations on the newly introduced SurgCount-HD dataset show that Chain-of-Look outperforms existing methods including CountGD, REC, and large language models like Qwen and ChatGPT in dense surgical instrument counting tasks.</div>
<div class="mono" style="margin-top:8px">手术器械在手术室中的准确计数对于保障患者安全至关重要，但在密集场景下由于器械紧密排列，这一任务仍面临挑战。为解决这一问题，本文提出Chain-of-Look框架，通过模拟人类按顺序计数的过程，采用结构化的视觉链来引导模型进行空间轨迹上的计数，从而提升复杂场景下的准确性。为了增强视觉链的物理合理性，引入了邻近损失函数以显式建模密集排列器械的空间约束。在新构建的高密度手术器械数据集SurgCount-HD上进行的实验表明，该方法在密集手术器械计数任务中优于现有方法如CountGD、REC以及大型语言模型如Qwen和ChatGPT。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning GPT-5 for GPU Kernel Generation</div>
<div class="meta-line">Authors: Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah</div>
<div class="meta-line">First: 2026-02-11T16:22:54+00:00 · Latest: 2026-02-11T16:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora&#x27;s environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在GPU内核生成中微调GPT-5</div>
<div class="mono" style="margin-top:8px">开发高效的GPU内核对于扩展现代AI系统至关重要，但由于复杂的硬件架构和对专门优化知识的需求，这一任务仍然具有挑战性。尽管大型语言模型（LLMs）在通用顺序代码生成方面表现出强大的能力，但在GPU代码生成方面却面临显著挑战，原因包括高质量标记训练数据的稀缺、合成解决方案生成时的编译器偏见，以及在不同硬件世代间的泛化能力有限。这使得监督微调（SFT）难以作为可扩展的方法来改进当前的LLMs。相比之下，强化学习（RL）提供了一种数据高效且自适应的替代方案，但需要访问相关工具、仔细选择训练问题，并具备强大的评估环境。我们介绍了Makora的环境和工具，用于前沿模型的强化学习微调，并报告了我们对GPT-5进行微调以生成Triton代码的结果。在单次尝试设置下，我们的微调模型将内核正确性从43.7%提升至77.0%（+33.3个百分点），并使在KernelBench基准测试中优于TorchInductor的问题比例从14.8%提升至21.8%（+7个百分点）。与基线GPT-5相比，我们的模型在KernelBench上超过了先前的最先进模型。当集成到完整的编码代理中时，它能够在扩展的KernelBench套件中解决高达97.4%的问题，并在72.9%的问题上优于PyTorch的TorchInductor编译器，几何平均速度提升2.12倍。我们的工作表明，针对特定任务的强化学习后训练可以释放LLMs在高度专业化的技术领域中的能力，这些领域传统监督学习受限于数据可用性，从而为AI辅助的加速器编程开辟了新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenges in generating efficient GPU kernels using Large Language Models (LLMs), as supervised fine-tuning is not scalable due to limited high-quality training data and hardware-specific complexities. The authors propose a reinforcement learning (RL) approach, utilizing Makora&#x27;s environment and tools, to fine-tune GPT-5 for Triton code generation. Their experiments show that the fine-tuned model significantly improves kernel correctness, achieving 77.0% compared to 43.7% in the baseline, and outperforms TorchInductor on 72.9% of problems when integrated into a coding agent, with a geometric mean speedup of 2.12x.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决使用大语言模型（LLMs）生成高效GPU内核的挑战，特别是由于高质量训练数据不足和监督微调的局限性。作者提出了一种基于强化学习（RL）的方法，利用Makora环境和工具对GPT-5进行微调，以优化Triton代码生成。实验结果显示，微调后的模型在内核正确性方面显著提升，达到77.0%，相比基线模型提高了33.3个百分点；在21.8%的问题上超越了TorchInductor，优于先前的最先进模型。当集成到编码代理中时，该模型能够在扩展版的KernelBench测试套件中解决97.4%的问题，相比PyTorch编译器在72.9%的问题上表现更优，几何平均加速比为2.12倍。</div>
</details>
</div>
<div class="card">
<div class="title">City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</div>
<div class="meta-line">Authors: Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</div>
<div class="meta-line">First: 2025-12-17T19:59:31+00:00 · Latest: 2026-02-11T06:31:53+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026 (ORAL)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15933v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.15933v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dwipddalal.github.io/AgentNav/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environment. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs, reasoning techniques (e.g., GEPA, chain-of-thought, reflection) and competitive baseline PReP significantly underperform in this challenging setting. To address this, we propose Verbalization of Path(VoP), which explicitly grounds the agent&#x27;s internal reasoning by probing city-scale cognitive maps (key landmarks and directions toward the destination) from the MLLM, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外城市导航：从大规模知识中探索MLLMs的涌现导航</div>
<div class="mono" style="margin-top:8px">利用多模态大语言模型（MLLMs）开发具身智能体为解决复杂的现实世界任务提供了巨大潜力。然而，当前的评估基准主要以语言为中心或严重依赖模拟环境，很少涉及实际现实场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们引入了稀疏锚定视觉导航任务，专门设计用于评估MLLMs在具有挑战性的、知识密集型的现实环境中的顺序决策能力。我们通过CityNav这一涵盖四个不同全球城市的综合性基准来实现该任务，专门用于评估原始的MLLM驱动智能体在城市导航中的表现。智能体仅需依赖视觉输入和内部多模态推理，无需额外的环境标注或专门的架构修改，即可在50多个决策点中进行顺序导航。关键的是，智能体必须通过解读城市特定线索和识别地标自主实现定位，进行空间推理，并战略性地规划和执行前往目的地的路线。通过广泛的评估，我们表明当前最先进的MLLMs、推理技术（如GEPA、思维链、反思）以及具有竞争力的基线PReP在这一具有挑战性的场景中表现显著不足。为了解决这一问题，我们提出了路径语言化（Verbalization of Path, VoP），通过从MLLM中探查城市级认知地图（关键地标和朝向目的地的方向）来显式锚定智能体的内部推理，从而显著提升导航成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of evaluating multimodal large language models (MLLMs) in real-world navigation tasks, which are often overlooked in current benchmarks. The authors propose the Sparsely Grounded Visual Navigation task, implemented through the CityNav benchmark, to assess MLLMs&#x27; ability to navigate complex urban environments using only visual inputs and internal reasoning. Their experiments show that existing methods, including state-of-the-art MLLMs and techniques like GEPA, chain-of-thought, and reflection, perform poorly in this setting. To improve navigation performance, they introduce VoP, a method that explicitly verbalizes the agent&#x27;s internal reasoning by extracting city-scale cognitive maps, leading to significant improvements in success rates.</div>
<div class="mono" style="margin-top:8px">本文旨在解决当前评估多模态大语言模型（MLLMs）在现实世界导航任务中的不足，特别是现有基准主要依赖语言或模拟环境。作者提出了稀疏视觉导航任务，并通过CityNav基准进行实现，该基准测试了代理在复杂城市环境中仅依靠视觉输入和内部推理进行导航的能力。实验表明，现有的MLLMs及推理技术如GEPA、思维链和反思在该任务中表现不佳，凸显了更优推理机制的必要性。为此，他们提出了路径语言化（VoP）方法，通过提取城市级认知地图来显式地锚定代理的推理过程，从而显著提升了导航成功率。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Federica Sarro, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-11T04:58:49+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：一种针对编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估框架。ContextBench包含来自8种编程语言、66个仓库的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四种前沿LLM和五个编码代理。我们的结果表明，复杂的代理结构在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有端到端基准，揭示了问题解决过程中的关键环节。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate how coding agents retrieve and use code context during issue resolution, rather than just focusing on final task success. ContextBench is introduced as a process-oriented benchmark that includes 1,136 issue-resolution tasks across eight programming languages, each with human-annotated gold contexts. The authors implemented an automated evaluation framework to track agent behavior and measure context recall, precision, and efficiency. Experimental results reveal that advanced agent scaffolding provides only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between the context explored and the context actually used.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估编码代理在问题解决过程中如何检索和利用代码上下文，而不仅仅关注最终任务的成功率。ContextBench 提供了一个面向过程的基准测试，包含来自66个仓库、涵盖八种编程语言的1136个问题解决任务，每个任务都附有人工标注的黄金上下文。作者开发了一个自动化评估框架，用于追踪代理的行为并测量上下文的召回率、精确率和效率。实验结果表明，先进的代理结构在上下文检索方面仅带来小幅提升，LLMs倾向于优先召回而非精确，且存在显著的探索与利用上下文之间的差距。这些发现强调了在评估编码代理时引入更详细的中间过程指标的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps</div>
<div class="meta-line">Authors: Sharat Bhat, Harshita Khandelwal, Tushar Kataria, Vivek Gupta</div>
<div class="meta-line">First: 2026-02-11T04:36:14+00:00 · Latest: 2026-02-11T04:36:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10518v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10518v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapVerse：一个用于多样化真实地图的地理问答基准</div>
<div class="mono" style="margin-top:8px">地图是结构化和上下文知识的强大载体，涵盖地理、人口、基础设施和环境模式。对这类知识进行推理需要模型能够整合空间关系、视觉线索、现实世界背景和领域专业知识——这些能力当前的大语言模型（LLMs）和视觉-语言模型（VLMs）仍难以一致展现。然而，目前用于评估VLMs地图推理能力的数据集范围狭窄，局限于特定领域，并且严重依赖人工生成内容（如LLMs或基于流水线的方法的输出），难以深入评估真实的地理推理能力。为了解决这一问题，我们提出了MapVerse，一个基于真实地图的大规模基准。该基准包含1,025张地图上的11,837对人工撰写的问答对，涵盖十种多样的地图类别以及每种地图下的多种问题类别。该数据集为评估地图阅读、解释和多模态推理提供了丰富的环境。我们对十种最先进的模型进行了评估，以建立基准并量化推理能力的差距。除了整体表现外，我们还进行了细粒度的类别分析，以评估模型推理在多个维度上的表现，并探讨影响推理结果的视觉因素。我们的研究发现，尽管当前的VLMs在分类式任务上表现具有竞争力，但在需要复杂空间推理的高级任务上，无论是开源还是闭源模型都存在不足。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MapVerse is to address the limitations of existing datasets for evaluating geospatial reasoning in vision-language models (VLMs), which are often narrow in scope and rely on artificial content. MapVerse is a large-scale benchmark built on real-world maps, containing 11,837 human-authored question-answer pairs across 1,025 maps, covering ten diverse map categories and multiple question types. The benchmark evaluates ten state-of-the-art models, revealing that while they perform well on classification tasks, they struggle with more complex spatial reasoning tasks, highlighting the need for improved capabilities in this domain.</div>
<div class="mono" style="margin-top:8px">该研究提出了MapVerse，一个用于评估多样化真实地图上的地理问答的大型基准数据集。由于现有数据集范围狭窄且依赖人工生成内容，研究者收集了1,025张地图上的11,837对人工撰写的问答对，涵盖十种地图类型和多种问题类别。该基准旨在测试模型对空间关系、视觉线索和现实世界背景的整合能力。对十种最先进的模型进行评估发现，尽管它们在分类任务上表现良好，但在需要复杂空间推理的高级任务上仍存在明显不足，揭示了当前视觉-语言模型能力的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</div>
<div class="meta-line">Authors: Tianjiao Yu, Xinzhuo Li, Yifan Shen, Yuanzhe Liu, Ismini Lourentzou</div>
<div class="meta-line">First: 2025-12-14T17:05:11+00:00 · Latest: 2026-02-10T22:27:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12768v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12768v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoRe3D：以协作推理为基础的三维智能</div>
<div class="mono" style="margin-top:8px">近期在大型多模态模型方面的进展表明，显式的推理机制在提高模型可靠性、可解释性和跨模态对齐方面起着关键作用。尽管这种以推理为中心的方法在语言和视觉任务中已被证明是有效的，但将其扩展到三维领域仍处于初步阶段。CoRe3D引入了一个统一的三维理解和生成推理框架，该框架在语义和空间抽象上联合运作，使从语言中推断出的高层意图能够直接指导低层三维内容的生成。该设计的核心是一个基于空间的推理表示，将三维潜在空间分解为局部区域，使模型能够以组合和程序化的方式对几何结构进行推理。通过紧密耦合语义链式推理与结构化空间推理，CoRe3D生成的三维输出表现出强局部一致性和与语言描述的高度对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of CoRe3D is to enhance the reliability, interpretability, and cross-modal alignment of 3D intelligence systems by integrating explicit reasoning mechanisms. The method proposes a unified framework for 3D understanding and generation that operates over both semantic and spatial abstractions, using a spatially grounded reasoning representation to decompose the 3D latent space into localized regions. This allows the model to generate 3D content in a compositional and procedural way guided by high-level language intent. The main experimental results demonstrate that CoRe3D produces 3D outputs with strong local consistency and faithful alignment with linguistic descriptions, outperforming existing methods in these aspects.</div>
<div class="mono" style="margin-top:8px">CoRe3D的研究动机是通过引入显式的推理机制来提升3D智能系统的可靠性、可解释性和跨模态对齐能力。该方法提出了一种统一的推理框架，同时处理语义和空间抽象，使语言中的高层意图能够直接指导3D内容的生成。关键实验结果表明，CoRe3D生成的3D输出具有良好的局部一致性和与语言描述的忠实对齐，验证了其在连接语言与3D理解方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning</div>
<div class="meta-line">Authors: Yancheng Long, Yankai Yang, Hongyang Wei, Wei Chen, Tianke Zhang, Haonan fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</div>
<div class="meta-line">First: 2026-02-07T09:23:34+00:00 · Latest: 2026-02-10T19:38:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07458v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term &quot;Attention Collapse,&quot; where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialReward：通过显式空间推理弥合在线强化学习在图像编辑中的感知鸿沟</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）为复杂的图像编辑提供了有前景的途径，但目前受到可靠且细粒度奖励信号稀缺的限制。现有评估器经常面临我们称之为『注意力崩溃』的关键感知鸿沟，其中模型忽略了跨图像比较，无法捕捉细粒度细节，导致感知不准确和评分失调。为了解决这些限制，我们提出了SpatialReward，这是一种通过显式空间推理强制进行精确验证的奖励模型。通过将推理锚定在预测的编辑区域，SpatialReward在像素级证据基础上进行语义判断，显著提高了评估的准确性。该模型在精心挑选的26万条空间感知数据集上进行训练，在MMRB2和EditReward-Bench上取得了最先进的性能，并在我们提出的MultiEditReward-Bench上优于专有评估器。此外，SpatialReward在在线RL中作为强大的信号，使OmniGen2在GEdit-Bench上提升+0.90，超越了领先的判别模型，并且GPT-4.1的提升幅度翻倍（+0.45）。这些结果表明，空间推理对于在图像编辑中实现有效的对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of reliable reward signals in online reinforcement learning for image editing, where existing evaluators suffer from a critical perception gap known as &quot;Attention Collapse.&quot; This phenomenon leads to models ignoring cross-image comparisons and missing fine-grained details, resulting in inaccurate evaluations. To resolve this, the authors introduce SpatialReward, a reward model that integrates explicit spatial reasoning to ground semantic judgments in pixel-level evidence. The model is trained on a large-scale spatial-aware dataset and demonstrates superior performance on multiple benchmarks, including MMRB2, EditReward-Bench, and MultiEditReward-Bench, while significantly improving the performance of OmniGen2 on GEdit-Bench compared to other models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在线强化学习在图像编辑中的局限性，尤其是奖励信号的不可靠和粗糙问题。提出的方法SpatialReward通过引入显式的空间推理来弥合感知差距，将语义评估建立在预测编辑区域的像素级证据之上。实验结果表明，SpatialReward在MMRB2和EditReward-Bench上达到最先进的性能，并在MultiEditReward-Bench上超越现有评估器，显著提升了OmniGen2在GEdit-Bench上的表现，提升值为+0.90，超越了领先的判别模型，并将GPT-4.1的提升值翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到动作：在空间基础先验中构建视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-动作（VLA）模型在三维真实世界中进行操作，但通常基于二维编码器，这导致了空间推理的缺失，限制了其泛化能力和适应性。近期的VLA三维集成技术要么需要专用传感器且跨模态迁移效果差，要么注入的线索较弱，缺乏几何信息并损害了视觉-语言对齐。在本工作中，我们引入FALCON（从空间到动作），这是一种新颖范式，将丰富的三维空间标记注入到动作头中。FALCON利用空间基础模型，仅通过RGB图像即可提供强大的几何先验，并包含一个具身空间模型，可选择性地融合深度或姿态信息以提高保真度，而无需重新训练或架构修改。为了保持语言推理能力，空间标记被输入到空间增强的动作头中，而不是简单地连接到视觉-语言主干网络中。这些设计使FALCON能够解决空间表示、模态迁移性和对齐性方面的局限。在三个模拟基准和十一项真实世界任务的全面评估中，我们提出的FALCON取得了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化下保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing vision-language-action (VLA) models, which operate in 3D environments but rely on 2D encoders, leading to a spatial reasoning gap. The proposed FALCON model introduces a novel approach by incorporating rich 3D spatial tokens into the action head, utilizing spatial foundation models to extract geometric priors from RGB data. It also features an optional Embodied Spatial Model that can integrate depth or pose information for enhanced accuracy. The Spatial-Enhanced Action Head is designed to maintain language reasoning while processing spatial tokens separately from the vision-language backbone. Experimental results across three simulation benchmarks and eleven real-world tasks show that FALCON achieves state-of-the-art performance, outperforms baselines, and demonstrates robustness in challenging conditions such as clutter and varying object scales.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言-动作（VLA）模型在3D环境中依赖2D编码器导致的空间推理不足问题，提出了一种新的方法FALCON。该模型通过在动作头中注入丰富的3D空间标记，利用空间基础模型从RGB数据中提取强几何先验信息，并可选地融合深度或姿态信息以提高精度。空间增强的动作头独立处理这些空间标记，而非将其与视觉-语言主干网络拼接，从而保持语言推理能力。在三个模拟基准和十一项真实世界任务的综合评估中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化等挑战性条件下表现出鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Mindset: Reasoning with Adaptive Cognitive Modes</div>
<div class="meta-line">Authors: Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen</div>
<div class="meta-line">First: 2026-02-10T18:31:47+00:00 · Latest: 2026-02-10T18:31:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10063v1">PDF</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset">Code1</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维链：基于自适应认知模式的推理</div>
<div class="mono" style="margin-top:8px">人类解决问题的过程从不是单一思维模式的重复，这里的思维模式指的是不同的认知处理方式。在解决特定任务时，我们并不依赖单一思维模式，而是将多种思维模式整合到同一个解决方案中。然而，现有的LLM推理方法陷入了一个普遍的误区：它们在所有步骤中都使用相同的固定思维模式，忽视了解决同一问题的不同阶段需要根本不同的思维模式。这种单一假设阻碍了模型向更高层次智能的发展。为了解决这一局限性，我们提出了思维链（Chain of Mindset, CoM），这是一种无需训练的代理框架，能够实现步骤级的自适应思维模式协调。CoM将推理分解为四种功能上异质的思维模式：空间思维、收敛思维、发散思维和算法思维。一个元代理（Meta-Agent）会根据推理状态的演变动态选择最优的思维模式，而双向上下文门（Context Gate）则过滤跨模块的信息流动，以保持推理的有效性和效率。我们在涵盖数学、代码生成、科学问答和空间推理的六个具有挑战性的基准上进行了实验，结果表明CoM在整体准确率上分别比最强基线模型高出4.96\%和4.72\%，同时保持推理效率。我们的代码可在\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing large language models (LLMs) in reasoning tasks, where they often apply a fixed mindset across all steps, failing to adapt to the different cognitive requirements of problem-solving stages. The authors propose Chain of Mindset (CoM), a training-free agentic framework that dynamically orchestrates four distinct mindsets—Spatial, Convergent, Divergent, and Algorithmic—based on the evolving reasoning state. A Meta-Agent selects the optimal mindset, and a bidirectional Context Gate manages information flow between modules. Experiments on six benchmarks show that CoM achieves state-of-the-art performance, improving overall accuracy by 4.96\% and 4.72\% on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash respectively, while maintaining reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过模拟人类在解决问题过程中切换不同认知模式的能力，提升大语言模型（LLM）的推理能力。提出的Chain of Mindset（CoM）框架引入了四种功能各异的认知模式——空间型、收敛型、发散型和算法型，以实现每一步推理的自适应。一个Meta-Agent根据当前推理状态动态选择最合适的认知模式，而双向上下文门控机制则管理模块间的信息流动。在六个涵盖数学、代码生成、科学问答和空间推理的基准测试中，实验结果表明CoM达到了最先进的性能，分别在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上将整体准确率提升了4.96\%和4.72\%，同时保持推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge</div>
<div class="meta-line">Authors: Yijie Lin, Guofeng Ding, Haochen Zhou, Haobin Li, Mouxing Yang, Xi Peng</div>
<div class="meta-line">First: 2026-02-10T14:45:02+00:00 · Latest: 2026-02-10T14:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARK：一个结合推理与知识的双轴多模态检索基准</div>
<div class="mono" style="margin-top:8px">现有的多模态检索基准主要强调日常图像的语义匹配，并对专业领域知识和复杂推理的诊断能力有限。为了解决这一问题，我们引入了ARK，一个从两个互补视角分析多模态检索的基准：(i) 知识领域（五个领域，十七个子类），这些领域描述了检索所依赖的内容和专业知识；(ii) 推理能力（六个类别），这些类别描述了在识别正确候选时所需进行的多模态证据推理类型。具体而言，ARK评估了单模态和多模态查询与候选的检索，涵盖十六种异构视觉数据类型。为了避免评估中的捷径匹配，大多数查询都与需要多步推理的目标困难负样本配对。我们在ARK上评估了23种代表性的基于文本和多模态的检索器，并观察到知识密集型与推理密集型检索之间存在显著差距，其中细粒度视觉和空间推理成为持续存在的瓶颈。我们进一步表明，简单的增强方法如重排序和重写能够带来一致的改进，但仍存在大量提升空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the ARK benchmark is to address the limitations of existing multimodal retrieval systems in handling professional knowledge and complex reasoning tasks. ARK introduces a dual-axis evaluation framework, focusing on knowledge domains and reasoning skills, with 17 subtypes across five domains and six reasoning categories respectively. The benchmark includes 16 heterogeneous visual data types and employs hard negative samples to prevent shortcut matching. Experimental results show a significant performance gap between knowledge-intensive and reasoning-intensive retrieval, highlighting fine-grained visual and spatial reasoning as persistent challenges. Simple techniques like re-ranking and rewriting provide consistent improvements, but there is still substantial room for advancement.</div>
<div class="mono" style="margin-top:8px">本研究旨在弥补现有多模态检索基准在专业领域知识和复杂推理能力上的不足，这些基准主要关注日常图像的语义匹配。提出的ARK基准从两个互补角度评估多模态检索：知识领域（五个领域，17个子类）和推理能力（六个类别）。它涵盖了16种异构视觉数据类型，并采用针对性的困难负样本以避免捷径匹配。实验结果显示，知识密集型与推理密集型检索之间存在显著性能差距，其中细粒度视觉和空间推理是主要瓶颈。简单的改进方法如重排序和重写能带来一致的提升，但仍存在较大的优化空间。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-10T14:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v2">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型（MLLMs）在空间推理方面取得了显著进展，越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式融合，这常常导致语义与几何的不匹配以及冗余信号。我们提出GeoThinker框架，将整合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控进行校准，以偏重任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面设立了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力，并显著提升了空间感知能力，包括具身指称和自动驾驶等任务。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), where geometric information is often misaligned with semantic content and leads to redundant signals. The proposed GeoThinker framework introduces an active perception approach by selectively retrieving geometric evidence based on the model&#x27;s internal reasoning needs. It employs Spatial-Grounded Fusion at specific Vision-Language Model (VLM) layers, allowing semantic visual priors to query and integrate relevant geometry through frame-strict cross-attention, further refined by Importance Gating. Experimental results on the VSI-Bench show that GeoThinker achieves a state-of-the-art score of 72.6, demonstrating improved spatial perception and robust generalization in complex scenarios such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对当前多模态大语言模型（MLLMs）在空间推理中存在的问题，如语义与几何信息错位和冗余信号，提出了一种主动感知的GeoThinker框架。该框架通过在特定视觉-语言模型（VLM）层应用空间基础融合，并结合帧严格交叉注意力与重要性门控机制，使模型能够根据内部推理需求选择性地检索几何证据。实验结果表明，GeoThinker在VSI-Bench上取得了72.6的峰值得分，优于现有方法，并在复杂任务如具身指称和自动驾驶中展现出强大的泛化能力与空间感知提升。</div>
</details>
</div>
<div class="card">
<div class="title">EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems</div>
<div class="meta-line">Authors: Wentao Zhang, Jianfeng Wang, Liheng Liang, Yilei Zhao, HaiBin Wen, Zhe Zhao</div>
<div class="meta-line">First: 2026-02-10T14:04:22+00:00 · Latest: 2026-02-10T14:04:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10171v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvoCodeBench：一种用于自进化的LLM驱动编码系统的类人表现基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在编程任务中的持续进步，LLM驱动的编码系统已从单次代码生成演进为能够在推理过程中进行迭代改进的复杂系统。然而，现有的代码基准主要强调静态正确性，并隐式地假设推理过程中模型能力是固定的。因此，它们无法捕捉推理时的自我进化特性，例如代理在迭代优化解决方案时准确性与效率是否提升。此外，这些基准对资源成本的考量有限，并且很少将模型性能与人类程序员进行对比校准。而且，许多基准主要针对高资源语言，忽视了跨语言鲁棒性和长尾语言的稳定性。因此，我们提出了EvoCodeBench，这是一个用于评估自进化LLM驱动编码系统在多种编程语言中表现的基准，并与人类表现进行直接对比。EvoCodeBench追踪性能动态，测量解决方案的正确性以及解决时间、内存消耗等效率指标，并在重复问题解决尝试中评估算法设计的改进。为了将评估建立在以人类为中心的参考框架上，我们直接在相同任务上将模型性能与人类程序员进行比较，从而在人类能力分布中实现相对性能评估。此外，EvoCodeBench支持多种编程语言，能够在统一协议下进行系统性的跨语言和长尾语言稳定性分析。我们的结果表明，自进化系统在时间推移中表现出可衡量的效率提升，而与人类相对的多语言分析提供了仅凭准确性无法获得的见解。EvoCodeBench为评估不断进化的LLM驱动编码智能奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind EvoCodeBench is to address the limitations of existing code benchmarks that focus on static correctness and ignore the dynamic self-evolution of LLM-driven coding systems during inference. The benchmark evaluates the performance of self-evolving systems across multiple programming languages by tracking correctness, solving time, memory consumption, and algorithmic improvements over repeated attempts. Experimental results show that these systems achieve measurable gains in efficiency over time and that comparing their performance to human programmers provides valuable insights into their capabilities beyond mere accuracy.</div>
<div class="mono" style="margin-top:8px">EvoCodeBench的提出旨在弥补现有代码基准在评估LLM驱动编码系统自进化能力方面的不足，这些基准主要关注静态正确性，而忽略了推理过程中的动态改进。该基准通过跟踪重复问题解决过程中的效率和正确性变化，评估系统性能，包括解决时间、内存消耗和算法优化等指标。实验结果表明，自进化系统在时间推移中表现出可衡量的效率提升，与人类程序员的对比分析也揭示了仅凭准确率无法获得的洞察力，同时支持多语言评估，为研究编码智能的演进提供了基础。</div>
</details>
</div>
<div class="card">
<div class="title">From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki</div>
<div class="meta-line">First: 2026-02-09T14:39:43+00:00 · Latest: 2026-02-10T08:48:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08735v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08735v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从对应关系到行动：多模态大语言模型中类人多图像空间推理</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在单图像空间推理方面取得了显著进展，但需要整合多个视角信息的多图像空间推理仍然具有挑战性。认知研究指出，人类通过两种机制解决此类任务：跨视角对应关系，用于识别不同视角中对应同一物理位置的区域；以及逐步视角转换，通过依次组合相对视角变化来推理。然而，现有研究仅部分且通常隐式地引入了这些机制，缺乏对两者显式的监督。我们提出了一种名为HATCH（用于跨视角对应关系和视角变化的人类感知训练）的训练框架，包含两个互补的目标：（1）块级空间对齐，鼓励块表示在空间对应区域中对齐；（2）行动后回答推理，要求模型在预测最终答案前生成显式的视角转换动作。在三个基准测试上的实验表明，HATCH在保持单图像推理能力的同时，显著优于同等规模的基线模型，并且在性能上与更大规模的模型竞争。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-image spatial reasoning in multimodal large language models (MLLMs), which remains underdeveloped despite progress in single-image tasks. The authors propose HATCH, a training framework that explicitly incorporates two human-like mechanisms: cross-view correspondence and stepwise viewpoint transformation. By introducing Patch-Level Spatial Alignment and Action-then-Answer Reasoning as complementary objectives, HATCH enables the model to align spatial information across views and generate sequential viewpoint transitions. Experimental results on three benchmarks show that HATCH significantly outperforms existing baselines of similar size and achieves performance comparable to much larger models, while maintaining effective single-image reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对多图像空间推理在多模态大语言模型（MLLMs）中的挑战，指出其相较于单图像推理发展不足。作者提出HATCH框架，显式地融合了两种人类推理机制：跨视图对应和逐步视角变换。通过引入图像块级空间对齐和动作-答案推理两个互补目标，HATCH使模型能够对齐不同视角下的空间信息并生成连续的视角转换动作。在三个基准测试中，实验结果表明HATCH在性能上显著优于同类规模的基线模型，并且在与更大模型的对比中表现竞争力，同时保持了良好的单图像推理能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260216_0344.html">20260216_0344</a>
<a href="archive/20260215_0344.html">20260215_0344</a>
<a href="archive/20260213_0409.html">20260213_0409</a>
<a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
