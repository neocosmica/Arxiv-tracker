<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 03:52</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0352</div>
    <div class="row"><div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-01-28T17:27:08+00:00</div>
<div class="meta-line">Comments: 21 main pages, 7 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该比闭源系统具有根本性的优势：它们可以专门针对私有代码库，直接在权重中编码仓库特定的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，使得能够快速且低成本地创建专门针对私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法、代码）模型中实现了最先进的结果，同时与前沿的开放权重模型（如Devstral-Small-2）表现相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到同等性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数千条轨迹。结合成本效益，这使得专门针对私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过200,000条合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和混淆因素进行了详细分析。总体而言，我们认为我们的工作将大大加速开放编码代理的研究，并展示能够专门针对私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SERA is to enable open-weight coding agents to effectively specialize to private codebases, leveraging their ability to encode repository-specific information directly into their weights. The proposed method, Soft Verified Generation (SVG), allows for the efficient creation of such agents using only supervised fine-tuning, without the need for reinforcement learning or expensive synthetic data generation. SERA achieves state-of-the-art performance among fully open-source models and matches the capabilities of leading open-weight models like Devstral-Small-2, while being 26x cheaper than reinforcement learning and 57x cheaper than prior synthetic data methods. Additionally, SVG is applied to a larger corpus of codebases, generating over 200,000 synthetic trajectories for further analysis of training dynamics and scaling laws.</div>
<div class="mono" style="margin-top:8px">本研究的动机是证明开放权重的编码代理可以实际应用于私有代码库，通过在模型权重中直接编码特定仓库的信息来实现专业化。作者提出了SERA方法，利用仅监督微调（SFT）训练编码代理，实现了开放源模型的最先进性能，并与前沿的开放权重模型如Devstral-Small-2表现相当。实验结果表明，SERA在达到相同性能时，比强化学习便宜26倍，比之前合成数据方法便宜57倍。此外，Soft Verified Generation（SVG）技术可以从单个代码库生成数千条合成轨迹，支持对编码代理训练中扩展定律、消融实验和干扰因素的深入分析。</div>
</details>
</div>
<div class="card">
<div class="title">On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents</div>
<div class="meta-line">Authors: Jai Lal Lulla, Seyedmoein Mohsenimofidi, Matthias Galster, Jie M. Zhang, Sebastian Baltes, Christoph Treude</div>
<div class="meta-line">First: 2026-01-28T09:09:30+00:00 · Latest: 2026-01-28T09:09:30+00:00</div>
<div class="meta-line">Comments: 5 pages, 1 figure, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20404v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于AGENTS.md文件对AI编码代理效率的影响</div>
<div class="mono" style="margin-top:8px">诸如Codex和Claude Code等AI编码代理正越来越多地用于自主贡献软件仓库。然而，关于仓库级配置工件如何影响代理操作效率的了解仍然有限。本文研究了AGENTS.md文件对在GitHub拉取请求上运行的AI编码代理的运行时间和令牌消耗的影响。我们分析了10个仓库和124个拉取请求，在有和无AGENTS.md文件的两种条件下执行代理。我们测量了代理执行期间的实时时钟执行时间和令牌使用情况。我们的结果表明，存在AGENTS.md文件与较低的中位数运行时间（Δ28.64%）和减少的输出令牌消耗（Δ16.58%）相关，同时保持了相当的任务完成行为。基于这些结果，我们讨论了在实践中对AI编码代理配置和部署的直接影响，并概述了关于仓库级指令在塑造AI编码代理行为、效率和软件开发流程中整合作用的更广泛研究议程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how AGENTS.md files influence the efficiency of AI coding agents when interacting with GitHub pull requests. As AI coding agents like Codex and Claude Code become more prevalent in software development, understanding the effect of repository-level configuration artifacts is crucial. The study analyzes 10 repositories and 124 pull requests, comparing agent performance with and without AGENTS.md files. Results indicate that the presence of AGENTS.md reduces median runtime by 28.64% and lowers output token consumption by 16.58%, without compromising task completion. These findings highlight the importance of proper configuration in optimizing the performance of AI coding agents.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AGENTS.md文件对GitHub拉取请求中AI编码代理效率的影响。随着Codex和Claude Code等AI编码代理在软件开发中的广泛应用，了解其与仓库级配置工件的交互变得尤为重要。研究分析了10个仓库和124个拉取请求，对比了代理在有无AGENTS.md文件情况下的运行表现。结果表明，AGENTS.md的存在使中位运行时间减少了28.64%，输出令牌消耗降低了16.58%，同时保持了任务完成率的相当水平。这些发现提示仓库级指令在提升AI编码代理运行效率方面具有重要作用。</div>
</details>
</div>
<div class="card">
<div class="title">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</div>
<div class="meta-line">Authors: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T08:15:00+00:00 · Latest: 2026-01-28T08:15:00+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20354v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20354v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>各就各位：文本到图像模型空间智能的基准测试</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型在生成高保真图像方面取得了显著成功，但在处理复杂的空间关系（如空间感知、推理或交互）时往往表现不佳。这些关键方面由于当前基准测试的提示设计较短或信息稀疏，而被严重忽视。在本文中，我们引入了SpatialGenEval，这是一个旨在系统评估T2I模型空间智能的新基准，涵盖两个关键方面：(1) SpatialGenEval包含25个现实场景中的1,230个长且信息密集的提示，每个提示整合了10个空间子领域和相应的10个多选问答对，涵盖从物体位置和布局到遮挡和因果关系等多个方面。我们对21个最先进的模型进行了广泛评估，发现高阶空间推理仍然是主要瓶颈。(2) 为了证明我们信息密集设计的实用性不仅限于简单的评估，我们还构建了SpatialT2I数据集。该数据集包含15,400个文本-图像对，通过重写提示确保图像一致性，同时保持信息密度。在当前基础模型（如Stable Diffusion-XL、Uniworld-V1、OmniGen2）上进行微调后，结果显示出一致的性能提升（+4.2%、+5.7%、+4.4%），并且在空间关系上更加真实，突显了一种以数据为中心的范式，以实现T2I模型的空间智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current text-to-image models in handling complex spatial relationships, which are often neglected in existing benchmarks due to their sparse prompts. The authors propose SpatialGenEval, a comprehensive benchmark with 1,230 long, information-dense prompts across 25 real-world scenes, covering 10 spatial sub-domains and 10 multi-choice questions per prompt. Evaluation of 21 state-of-the-art models shows that higher-order spatial reasoning is a major challenge. Additionally, the paper introduces the SpatialT2I dataset, which contains 15,400 text-image pairs with rewritten prompts to maintain information density while ensuring image consistency. Fine-tuning on this dataset leads to performance improvements of up to +5.7% and more realistic spatial relations in generated images.</div>
<div class="mono" style="margin-top:8px">本文针对当前文本到图像（T2I）模型在处理复杂空间关系方面的不足，指出现有基准测试由于提示语简短或信息稀疏而忽视了这些关键能力。作者提出了SpatialGenEval，这是一个包含1,230个信息密集型提示语的全面基准测试，覆盖25个现实场景，每个提示语涉及10个空间子领域和10组多选问答对。对21个最先进的模型进行评估发现，高阶空间推理仍是主要瓶颈。此外，本文还构建了SpatialT2I数据集，包含15,400个文本-图像对，通过重写提示语确保图像一致性的同时保持信息密度。在该数据集上微调当前基础模型（如Stable Diffusion-XL、Uniworld-V1、OmniGen2）后，模型在空间关系任务上表现出一致的性能提升（+4.2%、+5.7%、+4.4%），凸显了以数据为中心的方法在提升T2I模型空间智能方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding</div>
<div class="meta-line">Authors: Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao</div>
<div class="meta-line">First: 2024-02-24T08:57:12+00:00 · Latest: 2026-01-28T08:05:32+00:00</div>
<div class="meta-line">Comments: Accepted by Empirical Software Engineering (EMSE) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.15769v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.15769v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenCode：一种通用的数据增强框架，用于提升基于深度学习的代码理解</div>
<div class="mono" style="margin-top:8px">预训练代码模型引领了代码智能的时代，多个模型表现出令人印象深刻的表现。然而，一个重要的问题——用于自动帮助开发者准备训练数据的代码数据增强方法——在该领域尚未得到充分研究。本文介绍了一种通用的数据增强框架GenCode，以增强代码理解模型的训练。简而言之，GenCode遵循生成与选择的范式来准备有用的训练代码数据。具体来说，它首先使用代码增强技术生成新的代码候选，然后通过影响力评分识别重要的代码作为训练数据。为了评估GenCode的有效性，我们在四个代码理解任务（如代码克隆检测）和三个预训练代码模型（如CodeT5）以及两个最近发布的代码专用大语言模型（LLMs）（如Qwen2.5-Coder）上进行了实验。与最先进的代码增强方法MixCode相比，GenCode在预训练代码模型上平均提高了2.92%的准确率和4.90%的对抗鲁棒性。对于代码专用的LLMs，GenCode在准确率和自然鲁棒性上平均分别提高了0.93%和0.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the lack of effective data augmentation methods for code data in the context of deep learning-based code understanding. GenCode is introduced as a generic framework that follows a generation-and-selection approach, first generating new code candidates through augmentation techniques and then selecting the most informative ones using influence scores. Experimental results on four code understanding tasks and two types of pre-trained models show that GenCode improves accuracy by 2.92% and adversarial robustness by 4.90% compared to the state-of-the-art method MixCode, with additional benefits for code-specific LLMs.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决代码数据增强方法不足的问题，这对训练基于深度学习的代码理解模型至关重要。GenCode提出了一种通用框架，采用生成与选择的范式，首先利用代码增强技术生成新的代码候选，然后通过影响力评分选择最有信息量的代码作为训练数据。在四个代码理解任务和三个预训练模型（包括两个最近发布的代码专用大语言模型）上的实验表明，与最先进的代码增强方法MixCode相比，GenCode在准确率上提高了2.92%，在对抗鲁棒性上提高了4.90%。</div>
</details>
</div>
<div class="card">
<div class="title">Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction</div>
<div class="meta-line">Authors: Genyuan Zhang, Zihao Wang, Zhifan Gao, Lei Xu, Zhen Zhou, Haijun Yu, Jianjia Zhang, Xiujian Liu, Weiwei Zhang, Shaoyu Wang, Huazhu Fu, Fenglin Liu, Weiwen Wu</div>
<div class="meta-line">First: 2026-01-28T06:54:06+00:00 · Latest: 2026-01-28T06:54:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构约束的语言引导扩散模型用于非配对低剂量计算机断层扫描血管造影重建</div>
<div class="mono" style="margin-top:8px">碘对比剂（ICM）的应用提高了计算机断层扫描（CT）在多种临床指征中的敏感性和特异性。然而，过量使用ICM可能导致肾损伤和危及生命的过敏反应。深度学习方法可以从低剂量ICM生成正常剂量ICM的CT图像，从而在保持诊断能力的同时减少所需剂量。然而，现有方法在使用不完全配对图像进行准确增强时存在困难，主要原因是模型对特定结构的识别能力有限。为克服这一限制，我们提出了一种结构约束的语言引导扩散模型（SLDM），这是一种集成了结构协同和空间智能的统一医学生成模型。首先，有效提取图像的结构先验信息以约束模型推理过程，从而确保增强过程中的结构一致性。随后，引入具有空间智能的语义监督策略，该策略集成了视觉感知和空间推理的功能，从而促使模型实现准确的增强。最后，应用减影血管造影增强模块，其作用是提高ICM剂区域的对比度，使其处于适合观察的区间。视觉比较的定性分析和多个指标的定量结果表明，我们的方法在低剂量对比剂CT血管造影的血管重建中是有效的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of accurately reconstructing high-quality CT angiography images from low-dose iodinated contrast media (ICM) scans, which is critical for reducing patient exposure while maintaining diagnostic accuracy. The proposed Structure-constrained Language-informed Diffusion Model (SLDM) integrates structural prior information and semantic supervision with spatial intelligence to enhance image quality. The model constrains inference using structural features, incorporates semantic guidance for accurate enhancement, and applies a subtraction angiography module to improve contrast. Experimental results show that the method outperforms existing approaches in both qualitative visual analysis and quantitative metrics, demonstrating its effectiveness in low-dose ICM CT angiography reconstruction.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决从低剂量碘对比剂CT血管造影扫描中准确重建高质量CT图像的挑战，以减少患者暴露同时保持诊断准确性。提出的结构约束语言引导扩散模型（SLDM）结合了结构先验信息和语义监督策略，以提升图像重建效果。该模型通过结构特征约束推理过程，利用语义理解实现精确增强，并应用减影血管造影模块提升对比剂区域的对比度。实验结果表明，SLDM在定性视觉分析和定量指标上均优于现有方法，验证了其在低剂量对比剂CT血管造影重建中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</div>
<div class="meta-line">Authors: Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long</div>
<div class="meta-line">First: 2026-01-27T17:40:07+00:00 · Latest: 2026-01-27T17:40:07+00:00</div>
<div class="meta-line">Comments: Project page: https://thuml.github.io/Reasoning-Visual-World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://thuml.github.io/Reasoning-Visual-World">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉生成通过多模态世界模型实现类人推理</div>
<div class="mono" style="margin-top:8px">人类通过构建内部世界模型并操作其中的概念来进行推理。最近的AI进展，特别是链式思维（CoT）推理，已经近似模拟了这种人类认知能力，其中世界模型被认为嵌入在大型语言模型中。当前系统主要依赖语言推理，在数学和编程等正式和抽象领域已达到专家级表现。然而，在需要更丰富表示和先验知识的物理和空间智能领域，它们仍远不如人类。因此，统一多模态模型（UMMs）的出现，引发了对基于互补多模态路径的更类人推理方式的兴趣，尽管其优势尚不明确。从世界模型的角度来看，本文首次系统地研究了视觉生成何时以及如何促进推理。我们的核心观点是视觉优势假说：对于某些任务，尤其是与物理世界相关的任务，视觉生成更自然地作为世界模型，而纯语言世界模型则因表示限制或先验知识不足而遇到瓶颈。理论上，我们将内部世界建模形式化为CoT推理的核心组成部分，并分析不同形式世界模型之间的区别。实证上，我们识别出需要交替使用视觉和语言CoT推理的任务，构建了一个新的评估套件VisWorld-Eval。在最先进的UMM上进行的受控实验表明，在有利于视觉世界建模的任务中，交替CoT显著优于纯语言CoT，但在其他任务中则没有明显优势。综上，本工作阐明了多模态世界建模在构建更强大、更类人多模态AI方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores how visual generation can enhance human-like reasoning in AI systems by leveraging multimodal world models. It argues that visual generation provides a more natural and effective way to construct world models for tasks involving physical and spatial understanding, which are challenging for purely verbal models. The study introduces the visual superiority hypothesis and develops a new evaluation framework, VisWorld-Eval, to assess tasks requiring interleaved visual-verbal reasoning. Experimental results on a state-of-the-art unified multimodal model show that interleaved chain-of-thought reasoning significantly improves performance on tasks that benefit from visual world modeling, but does not offer clear advantages in other domains.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉生成如何通过多模态世界模型提升人工智能系统的人类类比推理能力。论文提出了视觉优越性假说，认为在涉及物理和空间理解的任务中，视觉生成比纯语言推理更有效，因为语言模型在表示和先验知识方面存在局限。通过在先进统一多模态模型上的受控实验，研究发现交错的视觉与语言推理在需要视觉世界建模的任务中显著优于纯语言推理，而在纯语言任务中则没有明显优势。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-27T12:16:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的代理原生中间训练</div>
<div class="mono" style="margin-top:8px">最近，大型语言模型（LLM）的能力前沿已从单轮代码生成转向代理软件工程——一种模型能够自主导航、编辑和测试复杂仓库的范式。尽管后训练方法已成为代码代理的主流方法，但**代理中间训练**（agentic mid-training）——即在大规模数据上进行中间训练，这些数据模拟真实的代理工作流——由于资源需求巨大，仍被严重忽视。然而，与仅依赖昂贵的强化学习相比，它提供了一条更具可扩展性的路径，以培养基础的代理行为。实现有效的代理中间训练的核心挑战在于静态训练数据与真实开发中动态且反馈丰富的环境之间的分布不匹配。为了解决这一问题，我们系统地研究了代理中间训练，建立了适用于大规模代理开发的有效数据合成原则和训练方法。我们的方法核心在于**代理原生数据**——包含两种互补类型的轨迹：**上下文原生轨迹**，保留代理经历的完整信息流，提供广泛覆盖和多样性；以及**环境原生轨迹**，从可执行仓库中收集，其观察来源于实际工具调用和测试执行，提供深度和交互的真实性。我们在 `SWE-Bench Verified` 上验证了模型的代理能力。在使用对齐基础模型和代理框架的两种后训练设置下，我们的方法在使用不到一半中间训练令牌（73.1B）的情况下，优于之前的开源软件工程中间训练方案 `Kimi-Dev`。除了相对优势外，我们的表现最佳的32B和72B模型分别实现了**56.1%**和**58.5%**的解决率，分别...</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to explore agentic mid-training as a more scalable and cost-effective alternative to expensive reinforcement learning for developing code agents. The authors introduce agent-native data, which includes contextually-native and environmentally-native trajectories to better align training data with real development workflows. Their experiments on SWE-Bench Verified show that their approach outperforms Kimi-Dev with fewer training tokens, achieving resolution rates of 56.1% and 58.5% for their 32B and 72B models respectively.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在代理软件工程中的训练挑战，提出了一种模拟真实开发流程的中期训练方法。研究引入了代理原生数据，包括上下文原生轨迹和环境原生轨迹，以弥合静态训练数据与动态开发环境之间的差距。实验表明，该方法在与Kimi-Dev等现有方法的对比中表现更优，使用更少的训练标记实现了更高的解决率。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</div>
<div class="meta-line">Authors: Daniel Ogenrwot, John Businge</div>
<div class="meta-line">First: 2026-01-24T20:27:04+00:00 · Latest: 2026-01-27T06:14:36+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17581v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff&#x27;s $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何修改代码：GitHub拉取请求的大型研究</div>
<div class="mono" style="margin-top:8px">AI编码代理正越来越多地作为自主贡献者，通过生成和提交拉取请求（PRs）参与开发。然而，我们缺乏关于这些由代理生成的PR与人类贡献之间差异的实证证据，尤其是在代码修改方式和PR描述方面。理解这些差异对于评估其可靠性和对开发流程的影响至关重要。我们使用MSR 2026 Mining Challenge版本的AIDev数据集，分析了24,014个已合并的Agentic PR（440,295次提交）和5,081个已合并的人类PR（23,242次提交）。我们考察了新增内容、删除内容、提交记录和涉及的文件，并通过词法和语义相似性评估PR描述与代码差异的一致性。在提交次数方面，Agentic PR与Human PR存在显著差异（Cliff&#x27;s δ=0.5429），在涉及的文件和删除行方面表现出中等差异。它们在所有衡量标准下也显示出略微更高的描述与差异相似性。这些发现提供了关于AI编码代理如何参与开源开发的大型实证特征描述。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents contribute to open source development by analyzing a large-scale dataset of GitHub pull requests. The motivation is to understand the differences between AI-generated and human pull requests, particularly in terms of code modification patterns and description accuracy. Using the AIDev dataset, the researchers compare 24,014 merged Agentic PRs with 5,081 merged Human PRs, examining commit counts, files touched, and the semantic consistency between PR descriptions and code changes. The results show that Agentic PRs have significantly higher commit counts and moderate differences in files touched and deleted lines, while also demonstrating slightly better alignment between descriptions and diffs across various metrics.</div>
<div class="mono" style="margin-top:8px">本研究通过分析AI编码代理生成的拉取请求（PR）与人类贡献的PR，探讨AI在开源开发中的贡献方式。研究基于MSR 2026 Mining Challenge版本的AIDev数据集，考察了24,014个合并的Agentic PR和5,081个合并的人类PR，重点关注代码修改、提交模式以及PR描述与代码变更的一致性。结果表明，Agentic PR在提交数量上与Human PR存在显著差异，文件修改和删除行数有中等程度的不同，且在描述与代码变更的一致性方面略高。</div>
</details>
</div>
<div class="card">
<div class="title">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</div>
<div class="meta-line">Authors: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou</div>
<div class="meta-line">First: 2025-10-13T05:51:22+00:00 · Latest: 2026-01-27T05:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vlaser：具有协同具身推理能力的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管已有大量研究致力于利用视觉-语言模型（VLMs）开发具身推理能力，或将其集成到视觉-语言-动作（VLA）模型中以实现端到端的机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。在本工作中，我们通过引入Vlaser——一个具有协同具身推理能力的视觉-语言-动作模型，迈出初步一步，弥合具身推理与VLA策略学习之间的鸿沟。Vlaser是一个基础的视觉-语言模型，旨在将高层次推理与低层次控制相结合，以支持具身智能体。基于高质量的Vlaser-6M数据集，Vlaser在一系列具身推理基准测试中取得了最先进的性能，包括空间推理、具身锚定、具身问答和任务规划。此外，我们系统地研究了不同VLM初始化方式对监督VLA微调的影响，提供了关于缓解互联网规模预训练数据与具身特定策略学习数据之间领域偏移的新见解。基于这些见解，我们的方法在WidowX基准测试中取得了最先进的结果，并在Google Robot基准测试中表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of integrating high-level vision-language reasoning with low-level action control in embodied agents, aiming to bridge the gap between upstream vision-language models and downstream vision-language-action policy learning. The proposed Vlaser model introduces synergistic embodied reasoning, combining a foundational vision-language model with task-specific control mechanisms. Evaluated on multiple benchmarks, Vlaser demonstrates state-of-the-art performance in spatial reasoning, embodied grounding, embodied QA, and task planning, and provides insights into improving supervised fine-tuning by examining the impact of different VLM initializations on domain adaptation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将高阶视觉语言推理与低阶动作控制相结合的问题，以弥合上游视觉语言模型与下游视觉语言动作策略学习之间的鸿沟。提出的Vlaser模型引入了协同具身推理机制，将基础的视觉语言模型与任务特定的控制模块相结合。在多个基准测试中，Vlaser在空间推理、具身定位、具身问答和任务规划方面均表现出最先进的性能，并通过系统分析不同VLM初始化对监督具身动作学习的影响，提供了关于领域迁移的新见解。</div>
</details>
</div>
<div class="card">
<div class="title">m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</div>
<div class="meta-line">Authors: Yosub Shin, Michael Buriek, Igor Molybog</div>
<div class="meta-line">First: 2026-01-27T02:01:56+00:00 · Latest: 2026-01-27T02:01:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>m2sv：一种用于地图到街景空间推理的可扩展基准</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在许多多模态基准上表现出色，但在需要将抽象的俯视图表示与以自我为中心视角对齐的空间推理任务中仍显得脆弱。我们引入了m2sv，这是一个用于地图到街景空间推理的可扩展基准，要求模型通过将北向俯视地图与同一现实交叉路口拍摄的街景图像对齐，推断相机的观看方向。我们发布了m2sv-20k，这是一个地理多样性且具有可控模糊性的基准，以及m2sv-sft-11k，一个用于监督微调的精心整理的结构化推理轨迹集合。
尽管在现有多模态基准上表现强劲，但评估最好的VLM在m2sv上的准确率仅为65.2%，远低于人类基线的95%。虽然监督微调和强化学习带来了持续的性能提升，但跨基准评估显示其迁移能力有限。除了整体准确率外，我们还通过结构信号和人类努力系统地分析了地图到街景推理的难度，并对适应的开源模型进行了广泛的失败分析。我们的研究结果突显了几何对齐、证据聚合和推理一致性方面的持续差距，为未来多视角的基于场景的空间推理研究提供了动机。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces m2sv, a scalable benchmark designed to evaluate spatial reasoning capabilities of vision--language models (VLMs) in aligning abstract overhead map representations with egocentric street-view images. The benchmark includes m2sv-20k, a geographically diverse dataset with controlled ambiguity, and m2sv-sft-11k, a set of structured reasoning traces for supervised fine-tuning. Despite strong performance on other multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, significantly lower than the human baseline of 95%. The analysis reveals challenges in geometric alignment, evidence aggregation, and reasoning consistency, suggesting the need for further research into grounded spatial reasoning across different viewpoints.</div>
<div class="mono" style="margin-top:8px">本文提出m2sv，这是一个用于评估视觉语言模型在地图到街景空间推理能力的可扩展基准。该基准要求模型通过将北向俯视地图与同一现实交叉口拍摄的街景图像对齐，推断相机的视角方向。研究包含m2sv-20k，一个地理多样性且具有可控模糊性的数据集，以及m2sv-sft-11k，用于监督微调的结构化推理轨迹集。尽管在其他多模态基准上表现优异，但最佳评估的视觉语言模型在m2sv上的准确率仅为65.2%，远低于人类基线的95%。研究显示，监督微调和强化学习都能提升性能，但跨基准迁移效果有限。作者还通过结构信号和人工分析系统地探讨了地图到街景推理的难度，并对调整后的开放模型进行了广泛失败分析，揭示了几何对齐、证据聚合和推理一致性方面的持续差距。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward</div>
<div class="meta-line">Authors: Dipendra Misra, Aldo Pacchiano, Ta-Chung Chi, Ge Gao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-27T00:31:04+00:00 · Latest: 2026-01-27T00:31:04+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19055v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent&#x27;s response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains adapted from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于用户编辑的LLM原理化微调：偏好、监督与奖励的融合</div>
<div class="mono" style="margin-top:8px">我们研究如何利用由用户编辑生成的部署数据来微调LLM，该数据集包含上下文、代理的响应以及用户的编辑。这种部署数据自然地出现在基于LLM的写作助手和编码代理等应用中。由于用户编辑的自然来源，它成为适应和个性化LLM的理想数据源。在此设置下，各种通常在文献中独立研究的反馈类型（偏好、监督标签和成本）得以统一。本文我们启动了从用户编辑中学习的理论研究。我们首先推导出从这些反馈类型中学习的算法的学习界。我们证明这些算法根据用户、数据分布和模型类别具有不同的权衡。随后，我们提出了一种简单的集成方法，以联合学习这些反馈类型。在两个从Gao等人（2024）改编的领域上，我们展示了我们的集成方法优于仅从单一反馈类型学习的方法。此外，我们还证明了所提出的程序在测试时能够稳健地适应不同的用户编辑分布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the fine-tuning of large language models (LLMs) using user-edit data, which consists of context, agent responses, and user modifications. The motivation stems from the natural occurrence of such edits in real-world applications like writing assistants and coding agents, making them a valuable resource for personalization. The authors unify preference, supervision, and reward signals into a single learning framework and derive theoretical bounds for each feedback type. They propose an ensembling method that combines these signals, demonstrating improved performance over individual feedback-based approaches on two domains adapted from Gao et al. 2024. The results show that their method can robustly adapt to varying user-edit distributions during testing.</div>
<div class="mono" style="margin-top:8px">本文研究了如何利用用户编辑数据对大语言模型（LLMs）进行微调，该数据包含上下文、代理响应和用户修改。研究动机源于这类编辑在实际应用如基于LLM的写作助手和编码代理中自然产生，使其成为个性化和适应的重要数据来源。作者将偏好、监督标签和奖励信号统一到一个学习框架中，并为每种反馈类型推导了学习算法的理论界限。他们提出了一种简单的集成方法，联合利用这些信号，在两个改编自Gao等人2024年的领域上展示了其优于单独使用每种反馈的方法。实验结果表明，该方法在测试时能够稳健地适应不同的用户编辑分布。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Foundation Models for Radar Scene Understanding</div>
<div class="meta-line">Authors: Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia</div>
<div class="meta-line">First: 2025-11-26T06:41:00+00:00 · Latest: 2026-01-26T20:56:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21105v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21105v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>雷达场景理解的基础模型扩展</div>
<div class="mono" style="margin-top:8px">雷达传感器在恶劣天气、光照和远距离条件下提供了可靠的感知能力。近年来，基础模型在视觉和语言理解方面取得了重大进展，但其与雷达感知的结合仍处于初步探索阶段。现有的雷达方法往往是碎片化的且任务特定的；每个下游任务都采用不同的架构和训练目标，阻碍了任务间的迁移能力。在本文中，我们提出了RadarFM：一种通过结构化空间语言监督学习统一场景表示的雷达基础模型。我们做出两项关键贡献：(1) 一种结构化描述框架，能够以原生雷达坐标编码车辆分布；(2) 一种哈希感知的对比学习目标，通过量化连续场景相似性而非二元匹配，实现细粒度的空间推理。我们利用CARLA模拟器，在多样化的驾驶场景中生成大规模且标注良好的雷达数据集。此外，我们还提出了定位感知的评估指标，以超越传统检测度量的方式评估空间准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating foundation models into radar scene understanding, where existing methods are task-specific and lack transferability. The authors propose RadarFM, a foundation model that learns unified scene-level representations using structured spatial language supervision. Two key contributions include a structured caption framework for encoding vehicle distributions in radar coordinates and a hash-aware contrastive learning objective for continuous scene similarity. Experimental results on large-scale radar datasets generated via CARLA demonstrate improved spatial reasoning and accuracy compared to traditional detection-based metrics.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决雷达场景理解中基础模型整合的挑战，现有方法多为任务特定且缺乏迁移能力。提出的RadarFM模型引入了结构化描述框架，用于在原生雷达坐标中编码车辆分布，并采用基于哈希的对比学习目标，通过量化连续场景相似性实现更精细的空间推理。在使用CARLA模拟器生成的大型雷达数据集上进行的实验表明，该模型在空间理解方面优于传统的基于检测的评估方法，所提出的定位感知度量进一步提升了空间准确性的评估效果。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests</div>
<div class="meta-line">Authors: Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang</div>
<div class="meta-line">First: 2026-01-08T12:31:02+00:00 · Latest: 2026-01-26T17:05:34+00:00</div>
<div class="meta-line">Comments: Accepted by MSR&#x27;26 Mining Challenge Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04886v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04886v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that &quot;descriptions claim unimplemented changes&quot; was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5 times longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分析AI编码代理撰写的拉取请求中的消息-代码不一致问题</div>
<div class="mono" style="margin-top:8px">由AI编码代理生成的拉取请求（PR）描述是向人类审阅者传达代码更改的主要渠道。然而，这些消息与实际更改之间的对齐情况尚未被探索，引发了对AI代理可信度的担忧。为填补这一空白，我们使用PR消息-代码不一致（PR-MCI）分析了五个代理的23,247个代理PR。我们贡献了974个手动标注的PR，发现其中406个（1.7%）表现出高PR-MCI，并识别出八种PR-MCI类型，其中最常见的问题是“描述声称未实现的更改”（占45.4%）。统计检验表明，高MCI的PR接受率低51.7%（28.3% vs. 80.0%），且合并所需时间是普通PR的3.5倍（55.8小时 vs. 16.0小时）。我们的研究结果表明，不可靠的PR描述削弱了对AI代理的信任，强调了需要PR-MCI验证机制和改进PR生成以实现可信的人机协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the issue of message-code inconsistency in pull requests authored by AI coding agents, as PR descriptions are critical for human reviewers to understand code changes. By analyzing 23,247 PRs across five agents, the researchers identified 406 PRs with high inconsistency, contributing 974 manually annotated examples. They categorized the inconsistencies into eight types, with &#x27;descriptions claim unimplemented changes&#x27; being the most prevalent. The results show that high-MCI PRs have significantly lower acceptance rates and longer merge times, indicating that unreliable descriptions hinder trust in AI agents and emphasize the importance of verification mechanisms and improved PR generation for effective human-AI collaboration.</div>
<div class="mono" style="margin-top:8px">本研究探讨了由AI编码代理撰写的拉取请求（PR）中消息与代码不一致的问题，因为PR描述是人类审阅者理解代码变更的关键渠道。通过对五个代理生成的23,247个PR进行分析，研究人员发现了406个存在高不一致性的PR，并贡献了974个手动标注的案例。他们将不一致类型分为八类，其中&#x27;描述声称未实现的变更&#x27;最为常见。实验结果显示，高不一致PR的接受率显著降低，合并时间也更长，表明不可靠的描述会损害对AI代理的信任，凸显了改进PR生成和验证机制的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过预测编码实现共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是一个关键挑战，部分可观测性和有限带宽常常导致协调中的灾难性失败。我们提出了一种多智能体预测编码框架，将协调建模为智能体之间相互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习应向谁、传达什么信息，还学习何时进行通信。该框架的基础是一种类似网格细胞的度量，作为内部空间编码用于自我定位，这种编码自发地从自监督运动预测中产生。在此基础上，智能体逐步发展出一种带宽高效的通信机制，并形成专门的神经群体以编码伙伴的位置——这是人工模拟的海马体社会位置细胞（SPCs）。这些社会表征进一步被分层强化学习策略所利用，该策略主动探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法表现出对带宽限制的卓越鲁棒性：当带宽从128位/步减少到4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则从67.6%骤降至28.6%。我们的研究结果为复杂社会表征如何从统一的预测驱动中涌现提供了理论上有依据且符合生物机制的基础，从而实现集体智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of maintaining a consistent shared spatial memory in multi-agent systems, particularly under conditions of partial observability and limited communication bandwidth. The proposed method introduces a predictive coding framework that minimizes mutual uncertainty among agents by learning who, what, and when to communicate. This framework uses a grid-cell-like metric for self-localization, which arises from self-supervised motion prediction. The main experimental results show that the approach maintains high performance under bandwidth constraints, achieving 64.4% success when bandwidth is reduced to 4 bits/step, compared to a sharp drop to 28.6% in a full-broadcast baseline on the Memory-Maze benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多智能体系统中保持一致共享空间记忆的挑战，特别是在部分可观测性和通信带宽受限的情况下。提出的方法是一种基于预测编码的框架，通过最小化智能体间的相互不确定性来实现协调，学习谁、什么以及何时进行通信。该框架利用类似网格细胞的度量作为内部空间编码，用于自定位，并从自监督运动预测中自发产生。智能体随后发展出高效的通信机制和专门的神经群体，用于编码同伴的位置。在Memory-Maze基准测试中，该方法在带宽受限的情况下仍表现出良好的鲁棒性，当带宽从128位/步降至4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则急剧降至28.6%。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-26T10:49:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, SWE-EVO comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on SWE-EVO, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长周期软件演进场景中对编码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI编码代理基准主要关注孤立的、单一问题的任务，如修复错误或实现小功能。然而，现实中的软件工程本质上是一项长周期的工作：开发者需要解读高层需求，协调多个文件的更改，并在多次迭代中演进代码库，同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长周期软件演进挑战上的基准。SWE-EVO基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而相比之下在单一问题的SWE-Bench Verified上解决率为65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长周期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing benchmarks for AI coding agents, which focus on isolated, single-issue tasks rather than the complex, long-term nature of real-world software engineering. SWE-EVO is introduced as a new benchmark that evaluates agents on multi-step, multi-file software evolution tasks derived from seven mature open-source Python projects. The benchmark includes 48 tasks requiring modifications across an average of 21 files and is validated against extensive test suites. Experimental results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on SWE-Bench Verified, highlighting the difficulty agents face in sustained, multi-file reasoning. The paper also proposes Fix Rate, a new metric to assess partial progress in solving these complex tasks.</div>
<div class="mono" style="margin-top:8px">该研究提出了SWE-EVO基准，用于评估AI编码代理在长周期软件演进任务中的表现，这类任务涉及跨多个文件的多步骤修改并保持原有功能。与现有专注于独立任务（如修复错误或实现小功能）的基准不同，SWE-EVO基于七个成熟Python项目的发布说明和版本历史构建，包含48个任务，每个任务平均涉及21个文件，每个实例平均有874个测试用例。实验结果显示，即使像GPT-5结合OpenHands这样的先进模型，在SWE-EVO上的解决率也只有21%，远低于单任务SWE-Bench Verified的65%，表明当前代理在持续的多文件推理方面存在显著不足。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Conditioned Reasoning in Long-Egocentric Videos</div>
<div class="meta-line">Authors: James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi</div>
<div class="meta-line">First: 2026-01-26T03:21:35+00:00 · Latest: 2026-01-26T03:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时程第一视角视频中的空间条件推理</div>
<div class="mono" style="margin-top:8px">由于视角漂移和缺乏持久的几何上下文，长时程第一视角视频对视觉导航提出了重大挑战。尽管近期的视觉-语言模型在图像和短视频推理上表现良好，但它们在长第一视角序列中的空间推理能力仍有限。在本工作中，我们研究了显式空间信号如何影响基于VLM的视频理解，而无需修改模型架构或推理过程。我们引入了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在面向导航的空间查询上对多个VLM进行了基准测试。为了考察输入级别的归纳偏置，我们进一步将深度图与RGB帧融合，并评估其对空间推理的影响。我们的结果揭示了通用准确性和空间特异性之间的权衡，表明具有深度感知和空间基础的表示可以提高对行人和障碍物检测等安全关键任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenges of visual navigation in long-horizon egocentric videos, where viewpoint drift and lack of persistent geometric context hinder spatial understanding. The authors investigate how explicit spatial signals can enhance video understanding by VLMs without altering model structures or inference processes. They propose Sanpo-D, a refined annotation of the Google Sanpo dataset, and evaluate various VLMs on spatial navigation tasks. By integrating depth maps with RGB frames, they demonstrate that depth-aware representations improve performance in critical tasks like pedestrian and obstruction detection, albeit at the cost of reduced general-purpose accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对长时地平线视角视频中因视角漂移和缺乏持久几何上下文而带来的视觉导航挑战。作者探讨了在不修改模型结构或推理流程的前提下，显式空间信号如何提升基于视觉语言模型的视频理解能力。他们提出了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在导航相关的空间查询上评估了多种VLM的表现。通过将深度图与RGB帧融合，他们发现深度感知的表示方法在行人和障碍物检测等关键任务中提升了性能，但以牺牲通用准确性为代价。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-26T00:36:33+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com First two authors contribute equally. The 3th - 6th authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能来寻找共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力，才能有效地作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的超过600个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为在人类团队中，增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道充斥着模糊、时机不当和不准确的信息；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CooperBench, a benchmark designed to evaluate the coordination abilities of AI coding agents in collaborative tasks. Motivated by the need for AI agents to function effectively as teammates, the researchers created over 600 tasks across 12 libraries and four programming languages, where two agents are assigned conflicting features that require coordination to resolve. Experimental results show that current coding agents perform significantly worse when working together, achieving only 30% success rate on average compared to individual performance. This highlights the &#x27;curse of coordination&#x27; in AI agents, as they struggle with communication, commitment adherence, and accurate expectation management. The study also identifies some emergent coordination behaviors, such as role division and negotiation, suggesting potential pathways for future improvements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨当前编码代理在协作环境中的局限性，特别是它们在协调方面的能力不足。作者提出了CooperBench，这是一个包含超过600个协作编码任务的基准测试平台，涵盖四种编程语言和十二个库，旨在评估代理的协调能力。每个任务涉及两个代理，分别实现独立但可能冲突的特性，并基于真实的开源仓库和专家编写的测试用例。主要实验结果显示，当前最先进的编码代理在协作时平均成功率比单独完成任务低30%，突显了AI代理中的&#x27;协调诅咒&#x27;，与人类团队协作通常提升生产力形成鲜明对比。分析指出三个关键问题：沟通不畅、承诺偏离以及对他人计划的错误预期。通过大规模模拟，研究还观察到了一些罕见但有趣的协调行为，如角色分工、资源共享和协商。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</div>
<div class="meta-line">First: 2025-09-21T03:02:59+00:00 · Latest: 2026-01-25T23:11:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16891v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.16891v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM作为布局设计师：增强的空间推理能力用于内容感知布局生成</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）在文本领域展示了出色的推理和规划能力，并能有效遵循复杂任务的指令，但它们对空间关系的理解和操控能力仍有限。这种能力对于内容感知的图形布局设计至关重要，其目标是将异构元素排列在画布上，以确保最终设计在视觉上平衡且结构上可行。该问题需要在受限的视觉空间中精确协调多个元素的放置、对齐和结构组织。为了解决这一局限，我们引入了LaySPA，一个基于强化学习的框架，通过为基于LLM的智能体添加显式空间推理能力来增强布局设计。LaySPA采用混合奖励信号，共同捕捉几何约束、结构保真度和视觉质量，使智能体能够在画布上导航，建模元素间的关系，并优化空间布局。通过群体相对策略优化，智能体生成的内容感知布局能够反映显著区域，尊重空间约束，并产生可解释的推理轨迹以解释放置决策和结构化布局规范。实验结果表明，LaySPA显著提升了生成结构有效且视觉吸引人的布局的能力，优于更大的通用LLM，并且其性能与最先进的专用布局模型相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of Large Language Models (LLMs) in understanding and manipulating spatial relationships, which is essential for content-aware graphic layout design. The authors propose LaySPA, a reinforcement learning framework that enhances LLM-based agents with explicit spatial reasoning capabilities. LaySPA uses hybrid reward signals to incorporate geometric constraints, structural fidelity, and visual quality, allowing agents to generate layouts that are both structurally valid and visually appealing. Experimental results demonstrate that LaySPA outperforms larger general-purpose LLMs and achieves performance comparable to specialized layout models.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在理解与操控空间关系方面的不足，这对于内容感知的图形布局设计至关重要。作者提出了LaySPA，一个基于强化学习的框架，通过混合奖励信号增强LLM代理的空间推理能力，涵盖几何约束、结构保真度和视觉质量。实验结果表明，LaySPA在生成结构合理且视觉吸引人的布局方面表现优异，超越了更大的通用LLM，并与最先进的专用布局模型性能相当。</div>
</details>
</div>
<div class="card">
<div class="title">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</div>
<div class="meta-line">Authors: Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao</div>
<div class="meta-line">First: 2026-01-25T15:29:32+00:00 · Latest: 2026-01-25T15:29:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17885v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://peafowlvla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PEAfowl：增强感知的多视角视觉-语言-动作策略用于双臂操作</div>
<div class="mono" style="margin-top:8px">在杂乱场景中进行双臂操作需要在遮挡、视角和场景变化下保持稳定的策略。现有视觉-语言-动作模型往往难以泛化，原因包括：(i) 多视角特征通过无视角感知的token拼接进行融合，导致弱的3D一致空间理解；(ii) 语言作为全局条件注入，导致粗略的指令定位。本文提出PEAfowl，一种增强感知的多视角VLA策略用于双臂操作。在空间推理方面，PEAfowl预测每个token的深度分布，执行可微分的3D提升，并聚合局部跨视角邻居以形成几何基础且跨视角一致的表示。在指令定位方面，我们提出用Perceiver风格的文本感知读出机制替代全局条件注入，利用冻结的CLIP视觉特征实现迭代证据积累。为克服无噪声和不完整的商品深度数据且不增加推理开销，我们应用仅训练的深度蒸馏，从预训练的深度教师模型中监督深度分布头，为感知前端提供几何感知先验。在领域随机化的RoboTwin 2.0数据集上，PEAfowl将最强基线的成功率提升了23.0个百分点，真实机器人实验进一步验证了其可靠的模拟到现实迁移能力以及深度蒸馏带来的持续改进。项目网站：https://peafowlvla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Bimanual manipulation in cluttered environments demands robust policies that maintain stability despite occlusions and viewpoint changes. PEAfowl addresses these challenges by enhancing spatial reasoning through per-token depth prediction, differentiable 3D lifting, and cross-view neighbor aggregation, creating geometrically consistent representations. For instruction grounding, it uses a Perceiver-style readout over frozen CLIP features to iteratively accumulate evidence, improving alignment with language. The model also employs depth distillation during training to leverage a pretrained depth teacher, offering geometry-aware priors without increasing inference cost. Experimental results on RoboTwin 2.0 show a 23.0 percentage point improvement in success rate over the strongest baseline, with real-robot tests confirming reliable sim-to-real transfer and consistent performance gains.</div>
<div class="mono" style="margin-top:8px">在杂乱场景中进行双臂操作需要能够应对遮挡和视角变化的稳定策略。PEAfowl通过预测每个token的深度分布、进行可微分的3D提升以及聚合跨视角的局部邻居信息，增强了空间推理能力，从而形成几何一致的跨视角表示。为了解决指令对齐问题，它采用基于冻结CLIP特征的Perceiver风格文本感知读出机制，实现证据的迭代积累。此外，通过训练阶段的深度蒸馏，利用预训练的深度教师提供几何感知先验，无需增加推理开销。在RoboTwin 2.0数据集上的实验表明，PEAfowl比最强基线提升了23.0个百分点的成功率，真实机器人实验进一步验证了其可靠的仿真到现实迁移能力和深度蒸馏带来的持续性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</div>
<div class="meta-line">Authors: Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</div>
<div class="meta-line">First: 2025-08-19T15:58:25+00:00 · Latest: 2026-01-25T02:33:47+00:00</div>
<div class="meta-line">Comments: EACL 2026 Camera-Ready. Code and data: https://github.com/tianyiniu/RotBench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13968v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13968v3">PDF</a> · <a href="https://github.com/tianyiniu/RotBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench, a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270° rotated images. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models&#x27; ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs&#x27; spatial reasoning capabilities and human perception in identifying rotation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RotBench：评估多模态大语言模型在识别图像旋转方向上的能力</div>
<div class="mono" style="margin-top:8px">我们研究多模态大语言模型（MLLMs）在识别输入图像旋转方向（0°, 90°, 180°, 270°）方面的准确性。该任务需要强大的视觉推理能力，以检测旋转线索并理解图像中的空间关系，无论其方向如何。为评估MLLMs在这些能力上的表现，我们引入了RotBench，这是一个包含生活方式、人像和风景图像的350张手动筛选的基准数据集。尽管该任务相对简单，但我们发现，包括GPT-5、o3和Gemini-2.5-Pro在内的多个最先进的开源和专有MLLMs都无法可靠地识别输入图像的旋转方向。即使为模型提供辅助信息（如标题、深度图等）或使用链式推理提示，也只能带来微小且不一致的提升。我们的结果表明，大多数模型能够可靠地识别正向（0°）图像，而某些模型能够识别倒置（180°）图像。但没有任何模型能够可靠地区分90°和270°旋转的图像。同时展示不同方向旋转的图像，对推理模型的性能有适度提升，而使用投票机制的修改设置则能提升较弱模型的性能。此外，我们还发现，微调虽然显著提升了模型识别180°旋转图像的能力，但并未改善其区分90°和270°旋转图像的能力。这些结果揭示了MLLMs在空间推理能力与人类识别旋转方向感知之间存在显著差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to assess the ability of Multimodal Large Language Models (MLLMs) to identify image rotation angles of 0°, 90°, 180°, and 270°, highlighting the need for better spatial reasoning in these models. The authors introduce RotBench, a manually curated benchmark with 350 images across different categories, to evaluate this capability. Their experiments show that while most models can reliably detect 0° and 180° rotations, they struggle to distinguish between 90° and 270° rotations. Providing additional information or using reasoning prompts yields limited improvement, and fine-tuning does not enhance the ability to differentiate 90° and 270° rotations, despite helping with 180°.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估多模态大语言模型（MLLMs）识别图像旋转角度（0°, 90°, 180°, 270°）的能力，揭示了这些模型在空间推理方面的不足。作者提出了RotBench，一个包含350张人工筛选图像的基准数据集，用于测试该能力。实验结果表明，尽管大多数模型能可靠识别0°和180°旋转，但在区分90°和270°旋转方面表现较差，且微调对这一能力的提升有限。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing the Impact of Code Changes on the Fault Localizability of Large Language Models</div>
<div class="meta-line">Authors: Sabaat Haroon, Ahmad Faraz Khan, Ahmad Humayun, Waris Gill, Abdul Haddi Amjad, Ali R. Butt, Mohammad Taha Khan, Muhammad Ali Gulzar</div>
<div class="meta-line">First: 2025-04-06T05:59:29+00:00 · Latest: 2026-01-24T10:52:01+00:00</div>
<div class="meta-line">Comments: This paper is currently Under Review. It consists of 12 pages, 11 Figures, and 5 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.04372v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.04372v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative Large Language Models (LLMs) are increasingly used in non-generative software maintenance tasks, such as fault localization (FL). Success in FL depends on a models ability to reason about program semantics beyond surface-level syntactic and lexical features. However, widely used LLM benchmarks primarily evaluate code generation, which differs fundamentally from semantic program reasoning. Meanwhile, traditional FL benchmarks such as Defect4J and BugsInPy are either not scalable or obsolete, as their datasets have become part of LLM training data, leading to biased results. This paper presents the first large-scale empirical investigation into the robustness of LLMs fault localizability. Inspired by mutation testing, we develop an end-to-end evaluation framework that addresses key limitations in existing LLM evaluation, including data contamination, scalability, automation, and extensibility. Using real-world programs with specifications, we inject unseen faults and ask LLMs to localize them, filtering out underspecified programs where localization is ambiguous. For each successfully localized program, we apply semantic-preserving mutations (SPMs) and rerun localization to assess robustness and determine whether LLM reasoning relies on syntactic cues rather than semantics. We evaluate 10 state-of-the-art LLMs on 750,013 fault localization tasks from over 1,300 Java and Python programs. We find that SPMs cause LLMs to fail on previously localized faults in 78% of cases, and that reasoning is stronger when relevant code appears earlier in context. These results indicate that LLM code reasoning is often tied to features irrelevant to semantics. We also identify code patterns that are challenging for LLMs to reason about. Overall, our findings motivate fundamental advances in how LLMs represent, interpret, and prioritize code semantics to reason more deeply about program logic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估代码更改对大型语言模型故障定位能力的影响</div>
<div class="mono" style="margin-top:8px">生成式大型语言模型（LLMs）越来越多地用于非生成式软件维护任务，如故障定位（FL）。在FL中取得成功取决于模型能够超越表面语法和词汇特征，对程序语义进行推理。然而，广泛使用的LLM基准测试主要评估代码生成，这与语义程序推理有本质区别。同时，传统FL基准如Defect4J和BugsInPy要么不具备可扩展性，要么已经过时，因为它们的数据集已成为LLM训练数据的一部分，导致结果偏倚。本文提出了首个大规模实证研究，探讨LLMs在故障定位方面的鲁棒性。受变异测试启发，我们开发了一个端到端的评估框架，解决了现有LLM评估中的关键限制，包括数据污染、可扩展性、自动化和可扩展性。我们使用带有规范的真实世界程序，注入未见过的故障，并要求LLMs进行定位，过滤掉定位模糊的未规范程序。对于每个成功定位的程序，我们应用语义保持变异（SPMs），并重新运行定位以评估鲁棒性，并确定LLM推理是否依赖于语法线索而非语义。我们对1200多个Java和Python程序中的750,013个故障定位任务进行了评估，涉及10个最先进的LLMs。我们发现，在78%的情况下，SPMs会导致LLMs在之前定位的故障上失败，且当相关代码在上下文中出现得更早时，推理能力更强。这些结果表明，LLM的代码推理往往依赖于与语义无关的特征。我们还识别了LLMs难以推理的代码模式。总体而言，我们的发现推动了LLMs在表示、解释和优先处理代码语义方面进行根本性改进，以更深入地理解程序逻辑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how code changes affect the fault localizability of large language models, motivated by the need for more accurate and robust evaluation of LLMs in software maintenance tasks. The authors propose an end-to-end evaluation framework inspired by mutation testing, which injects unseen faults into real-world programs with specifications and assesses LLMs&#x27; ability to localize them. They find that semantic-preserving mutations significantly reduce the models&#x27; success rate in fault localization, with 78% of previously localized faults no longer being identified. Additionally, they observe that reasoning performance improves when relevant code appears earlier in the context, suggesting that LLMs may rely more on syntactic features than on semantic understanding. These findings highlight the limitations of current LLMs in semantic program reasoning and identify challenging code patterns.</div>
<div class="mono" style="margin-top:8px">本文探讨了代码更改对大型语言模型（LLMs）在软件维护任务中故障定位能力的影响。由于现有基准主要评估代码生成而非语义推理，研究提出了一种受突变测试启发的端到端评估框架。该框架通过向真实程序中注入未见过的故障，评估LLMs的定位能力，并过滤掉定位不明确的程序。实验结果显示，78%的情况下，语义保持的突变会导致LLMs无法定位之前成功识别的故障，表明其推理可能更多依赖于语法特征而非程序语义。此外，研究还识别出一些对LLMs语义理解构成挑战的代码模式，强调了改进代码语义表示的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</div>
<div class="meta-line">Authors: Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-10T20:04:08+00:00 · Latest: 2026-01-23T21:03:18+00:00</div>
<div class="meta-line">Comments: Conference: NeurIPS 2025 (main)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10046v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10046v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimWorld-Robotics：为多模态机器人导航与协作合成逼真且动态的城市环境</div>
<div class="mono" style="margin-top:8px">近年来，基础模型在开发能够根据多模态输入在开放场景中执行多样化任务的通用型机器人方面取得了令人鼓舞的成果。然而，当前的研究主要集中在室内和家庭场景。在本工作中，我们提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境的具身AI模拟平台。SWR基于Unreal Engine 5构建，能够程序化生成包含行人和交通系统等动态元素的无限逼真城市场景，其在真实感、复杂性和可扩展性方面超越了以往的城市模拟。它还支持多机器人控制与通信。借助这些关键特性，我们构建了两个具有挑战性的机器人基准测试：(1) 多模态指令跟随任务，其中机器人必须在行人和交通存在的情况下，根据视觉-语言导航指令到达目标；(2) 多智能体搜索任务，其中两个机器人必须通过通信协作定位并相遇。与现有基准不同，这两个新基准在现实场景中全面评估了多种关键机器人能力，包括(1) 多模态指令的语义对齐，(2) 大规模环境中的三维空间推理，(3) 在行人和交通中的安全长距离导航，(4) 多机器人协作，以及(5) 基于环境的通信。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SimWorld-Robotics (SWR), a simulation platform designed to support the development of generalist robotics in complex, large-scale urban environments. The motivation stems from the need to evaluate and improve robot capabilities in realistic, dynamic settings beyond traditional indoor scenarios. SWR, built on Unreal Engine 5, procedurally generates photorealistic urban scenes with dynamic elements such as pedestrians and traffic systems, enabling robust testing of multimodal navigation and multi-robot collaboration. The platform introduces two benchmarks: a vision-language navigation task and a multi-agent search task, which assess a range of critical robot abilities, including perception, spatial reasoning, and communication. Experimental results show that current state-of-the-art models, such as vision-language models, struggle with these tasks, highlighting the challenges in adapting them to urban environments.</div>
<div class="mono" style="margin-top:8px">本文提出了SimWorld-Robotics（SWR），一个用于构建大规模、逼真城市环境的模拟平台，旨在推动具身AI在复杂场景中的发展。研究动机源于对更真实动态训练环境的需求，SWR基于Unreal Engine 5生成包含行人和交通系统的无限逼真城市场景。平台支持多机器人控制与通信，并构建了两个具有挑战性的基准任务：多模态指令跟随任务和多智能体搜索任务。实验结果表明，当前最先进的模型，包括视觉-语言模型，在这些任务中表现不佳，凸显了在城市环境中所需感知、推理和规划能力的不足。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</div>
<div class="meta-line">Authors: Riyang Bao, Cheng Yang, Dazhou Yu, Zhexiang Tang, Gengchen Mai, Liang Zhao</div>
<div class="meta-line">First: 2026-01-23T18:33:45+00:00 · Latest: 2026-01-23T18:33:45+00:00</div>
<div class="meta-line">Comments: 15pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16965v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spatial-Agent: 基于科学核心概念的代理式地理空间推理</div>
<div class="mono" style="margin-top:8px">地理空间推理对于现实世界应用如城市分析、交通规划和灾害响应至关重要。然而，现有的基于大语言模型（LLM）的代理通常无法进行真正的地理空间计算，而是依赖网络搜索或模式匹配，并在空间关系上产生幻觉。我们提出了Spatial-Agent，这是一种基于地理空间信息科学基础理论的AI代理。我们的方法将地理分析问题回答形式化为概念转换问题，其中自然语言问题被解析为可执行的工作流，表示为GeoFlow图——一种有向无环图，节点对应空间概念，边表示转换。基于空间信息理论，Spatial-Agent通过基于模板的生成方式提取空间概念，分配功能角色并遵循原则性的顺序约束，进而组合转换序列。在MapEval-API和MapQA基准上的大量实验表明，Spatial-Agent显著优于现有基线方法，包括ReAct和Reflexion，同时生成可解释且可执行的地理空间工作流。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial-Agent was developed to address the limitations of current LLM-based agents in performing genuine geospatial computations. These agents often rely on web search or pattern matching, leading to hallucinations in spatial relationships. The approach formalizes geospatial reasoning as a concept transformation problem, translating natural language questions into executable GeoFlow Graphs, which are directed acyclic graphs representing spatial concepts and their transformations. By leveraging spatial information theory, the agent extracts and structures spatial concepts with principled ordering constraints, enabling the generation of interpretable and executable workflows. Experimental results on MapEval-API and MapQA benchmarks show that Spatial-Agent significantly outperforms existing methods like ReAct and Reflexion in geospatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">Spatial-Agent 是为了解决当前基于大语言模型的代理在进行准确地理空间推理时的局限性而开发的。现有代理往往依赖网络搜索或模式匹配，导致空间关系的错误假设。该方法融合了空间信息科学的基础理论，将自然语言问题转化为可执行的 GeoFlow 图，这是一种表示空间概念及其转换的有向无环图。在 MapEval-API 和 MapQA 基准上的实验结果表明，Spatial-Agent 显著优于 ReAct 和 Reflexion 等现有方法，能够生成可解释且可执行的地理空间工作流。</div>
</details>
</div>
<div class="card">
<div class="title">EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents</div>
<div class="meta-line">Authors: Xinze Li, Ziyue Zhu, Siyuan Liu, Yubo Ma, Yuhang Zang, Yixin Cao, Aixin Sun</div>
<div class="meta-line">First: 2026-01-23T12:09:59+00:00 · Latest: 2026-01-23T12:09:59+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent&#x27;s own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMemBench：面向VLM代理的事件记忆交互基准测试</div>
<div class="mono" style="margin-top:8px">我们引入了EMemBench，这是一个用于通过交互游戏评估代理长期记忆的程序化基准。与使用固定问题集不同，EMemBench从每个代理自身的轨迹中生成问题，涵盖文本和视觉游戏环境。每个模板通过底层游戏信号计算可验证的基准答案，具有可控的可回答性，并在记忆技能上保持平衡覆盖：单跳/多跳回忆、归纳、时间、空间、逻辑和对抗性。我们使用上下文提示作为基线，评估具有强大语言模型/VLM作为骨干的记忆代理。在15个文本游戏和多个视觉种子上，结果远未饱和：归纳和空间推理仍然是持续的瓶颈，尤其是在视觉环境中。持久记忆在文本游戏中为开放骨干模型带来了明显提升，但对VLM代理的改进则不够一致，这表明基于视觉的事件记忆仍然是一个开放性挑战。一项人类研究进一步验证了EMemBench的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EMemBench is introduced as a programmatic benchmark to evaluate the long-term memory capabilities of agents in interactive environments. Unlike traditional benchmarks that rely on fixed questions, EMemBench dynamically generates questions based on each agent&#x27;s trajectory, encompassing both text and visual game settings. The benchmark computes verifiable ground truth from game signals and covers various memory skills with controlled answerability. Evaluation shows that induction and spatial reasoning are significant challenges, particularly in visual settings, and while persistent memory benefits open backbones in text games, improvements for VLM agents are less consistent, indicating ongoing difficulties in visually grounded episodic memory.</div>
<div class="mono" style="margin-top:8px">EMemBench 是一种程序化基准，用于评估智能体在交互环境中的长期记忆能力。它根据每个智能体的轨迹生成问题，涵盖文本和视觉游戏环境，并通过底层游戏信号确保可验证的正确答案，控制答案的可回答性以覆盖多种记忆技能。该基准评估了基于强大语言模型和视觉语言模型的记忆智能体，采用上下文提示作为基线。实验结果表明，在文本游戏和多个视觉种子环境中，归纳推理和空间推理仍是主要瓶颈，特别是在视觉设置中，说明基于视觉的事件记忆仍是一个开放性问题。</div>
</details>
</div>
<div class="card">
<div class="title">TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning</div>
<div class="meta-line">Authors: Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin, Haoyu Cao, Xing Sun, Ying Shen, Hai-Tao Zheng, Liang Lin, Philip S. Yu</div>
<div class="meta-line">First: 2026-01-23T07:35:05+00:00 · Latest: 2026-01-23T07:35:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16520v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramPuzzle：通过组合空间推理评估多模态大语言模型</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在视觉识别和语义理解方面取得了显著进展。然而，它们在精确的组合空间推理方面的能力仍 largely 未被探索。现有基准测试通常涉及相对简单的任务，并依赖语义近似或粗略的相对位置，而其评估指标通常有限且缺乏严谨的数学定义。为弥合这一差距，我们引入 TangramPuzzle，这是一个基于几何的基准测试，旨在通过经典的 Tangram 游戏视角评估组合空间推理。我们提出了 Tangram Construction Expression（TCE），一种符号几何框架，通过精确、可由机器验证的坐标规范来定义 Tangram 组装，以减少视觉近似带来的歧义。我们设计了两个互补任务：轮廓预测，要求从局部组件推断整体形状；以及端到端代码生成，要求解决逆向几何组装问题。我们在多个先进的开源和专有模型上进行了广泛评估实验，发现了一个有趣的见解：MLLMs 倾向于优先匹配目标轮廓，而忽视几何约束，导致组件发生形变或扭曲。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of rigorous evaluation of compositional spatial reasoning in Multimodal Large Language Models (MLLMs). The authors introduce TangramPuzzle, a geometry-based benchmark inspired by the classic Tangram game, which assesses MLLMs&#x27; ability to reason about spatial compositions through precise coordinate-based tasks. Two tasks, Outline Prediction and End-to-End Code Generation, are designed to evaluate the models&#x27; performance in reconstructing shapes from components and solving inverse geometric problems. Experimental results show that MLLMs often focus on matching the target silhouette rather than adhering to geometric constraints, resulting in distorted or incorrect piece arrangements.</div>
<div class="mono" style="margin-top:8px">本研究旨在弥补多模态大语言模型（MLLMs）在组合空间推理能力评估方面的不足。作者提出了TangramPuzzle，这是一个基于几何的经典七巧板游戏启发的基准测试，用于通过精确的坐标规范评估MLLMs的空间推理能力。他们设计了Tangram Construction Expression（TCE）这一符号化框架，并构建了两个互补任务：轮廓预测和端到端代码生成。实验结果表明，MLLMs往往更关注匹配目标轮廓，而忽视几何约束，导致拼图组件出现变形或错误排列。</div>
</details>
</div>
<div class="card">
<div class="title">Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models</div>
<div class="meta-line">Authors: Bridget Leonard, Scott O. Murray</div>
<div class="meta-line">First: 2026-01-23T00:21:27+00:00 · Latest: 2026-01-23T00:21:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent&#x27;s visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于认知的标记克服多模态模型中的自我中心偏差</div>
<div class="mono" style="margin-top:8px">多模态语言模型（MLMs）在语义视觉-语言任务上表现良好，但在需要采用其他代理视觉视角的空间推理任务中表现不佳。这些错误反映了持续存在的自我中心偏差，并引发了关于当前模型是否支持 allocentric 推理的疑问。受人类空间认知启发，我们引入了视角标记，这些专门的嵌入通过以下两种方式编码方向：(1) 身体关键点提示或 (2) 支持心理旋转的抽象表示。将这些标记整合到 LLaVA-1.5-13B 中，可实现对二级视觉视角任务的性能提升。在合成和自然基准（Isle Bricks V2、COCO、3DSRBench）中，视角标记提高了准确性，基于旋转的标记可泛化到非人类参考代理。表示分析表明，微调增强了基础模型中已有的潜在方向敏感性，这表明 MLMs 包含 allocentric 推理的前身，但缺乏适当的内部结构。总体而言，将基于认知的空间结构直接嵌入到标记空间中，为视角采取和更类人空间推理提供了一种轻量且模型无关的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of multimodal language models (MLMs) in spatial reasoning tasks that require adopting another agent&#x27;s visual perspective, a challenge stemming from their egocentric bias. To overcome this, the authors propose perspective tokens, which are specialized embeddings that encode spatial orientation using either body-keypoint cues or abstract mental rotation representations. By integrating these tokens into LLaVA-1.5-13B, the model achieves improved performance on level-2 visual perspective-taking tasks. Experimental results across synthetic and naturalistic benchmarks show that perspective tokens significantly enhance accuracy, with rotation-based tokens demonstrating better generalization to non-human reference agents. The study suggests that while MLMs may already possess latent orientation sensitivity, they lack the internal structure necessary for allocentric reasoning, and directly embedding cognitively inspired spatial structures into the token space offers a lightweight and model-agnostic solution.</div>
<div class="mono" style="margin-top:8px">本文旨在解决多模态语言模型（MLMs）在空间推理中表现出的自我中心偏差问题，该偏差限制了模型从他人视角理解视觉信息的能力。作者提出了一种称为视角标记（perspective tokens）的特殊嵌入，通过身体关键点提示或抽象的旋转表示来编码空间方向。将这些标记整合到LLaVA-1.5-13B模型中，使其在二级视觉视角任务中表现更优。在合成和自然基准测试（如Isle Bricks V2、COCO和3DSRBench）中，视角标记显著提升了模型的准确性，其中基于旋转的标记在非人类参考代理上表现出更强的泛化能力。研究发现，微调增强了基础模型中已有的潜在方向敏感性，表明MLMs可能已经具备分配中心推理的雏形，但缺乏相应的内部结构。总体而言，直接在标记空间中嵌入基于认知的空间结构，为视角转换和更接近人类的空间推理提供了一种轻量且模型无关的机制。</div>
</details>
</div>
<div class="card">
<div class="title">VOCAL: Visual Odometry via ContrAstive Learning</div>
<div class="meta-line">Authors: Chi-Yao Huang, Zeel Bhatt, Yezhou Yang</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-06-30T20:26:13+00:00 · Latest: 2026-01-22T22:04:57+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.00243v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.00243v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Breakthroughs in visual odometry (VO) have fundamentally reshaped the landscape of robotics, enabling ultra-precise camera state estimation that is crucial for modern autonomous systems. Despite these advances, many learning-based VO techniques rely on rigid geometric assumptions, which often fall short in interpretability and lack a solid theoretical basis within fully data-driven frameworks. To overcome these limitations, we introduce VOCAL (Visual Odometry via ContrAstive Learning), a novel framework that reimagines VO as a label ranking challenge. By integrating Bayesian inference with a representation learning framework, VOCAL organizes visual features to mirror camera states. The ranking mechanism compels similar camera states to converge into consistent and spatially coherent representations within the latent space. This strategic alignment not only bolsters the interpretability of the learned features but also ensures compatibility with multimodal data sources. Extensive evaluations on the KITTI dataset highlight VOCAL&#x27;s enhanced interpretability and flexibility, pushing VO toward more general and explainable spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOCAL: 通过对比学习实现的视觉里程计</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）领域的突破彻底改变了机器人学的格局，使超高精度的相机状态估计成为现代自主系统的关键。尽管取得了这些进展，许多基于学习的VO技术仍依赖于刚性的几何假设，这些假设在可解释性和理论基础方面往往存在不足，特别是在完全数据驱动的框架中。为克服这些局限性，我们引入了VOCAL（通过对比学习实现的视觉里程计），一个全新的框架，将VO重新构想为一个标签排序问题。通过将贝叶斯推断与表示学习框架相结合，VOCAL将视觉特征组织成与相机状态相对应的形式。排序机制迫使相似的相机状态在潜在空间中收敛为一致且空间连贯的表示。这种策略性的对齐不仅增强了所学特征的可解释性，还确保了其与多模态数据源的兼容性。在KITTI数据集上的广泛评估突显了VOCAL在可解释性和灵活性方面的提升，推动了视觉里程计向更通用和可解释的空间智能发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual odometry (VO) is a critical component in autonomous systems, yet existing learning-based approaches often rely on rigid geometric assumptions that limit interpretability and theoretical grounding. VOCAL introduces a novel framework that redefines VO as a label ranking problem, leveraging Bayesian inference and representation learning to align visual features with camera states. By enforcing spatial coherence in the latent space through a ranking mechanism, VOCAL improves the interpretability and adaptability of the model. Evaluations on the KITTI dataset demonstrate its superior performance in terms of both interpretability and flexibility, advancing the field toward more general and explainable spatial intelligence solutions.</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）近年来取得了显著进展，但许多基于学习的方法受限于刚性的几何假设，影响了其可解释性和理论基础。VOCAL提出了一种新框架，将VO视为标签排序问题，结合贝叶斯推理与表征学习，使视觉特征与相机状态对齐。排序机制促使相似的相机状态在潜在空间中形成空间一致的表征，从而提升特征的可解释性与多模态数据的兼容性。在KITTI数据集上的广泛评估表明，VOCAL在可解释性和灵活性方面表现更优，推动了VO向更通用和可解释的空间智能发展。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-22T19:05:41+00:00</div>
<div class="meta-line">Comments: Work done as part of the EleutherAI SOAR Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，其训练方法往往将图像扁平化为1D的图像块序列，丢弃了空间推理所需的2D结构。我们认为，这种缺乏空间感知是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the spatial reasoning limitations of vision-language models (VLMs), which are typically trained using contrastive language-image pretraining (CLIP) style image encoders that flatten images into 1D sequences, thereby losing essential 2D spatial structure. The authors propose two approaches to enhance spatial awareness: training image encoders with alternative objectives and incorporating 2D positional encodings. Their experiments demonstrate that these modifications significantly improve spatial reasoning performance across multiple benchmarks, highlighting the importance of spatial structure in VLM design for applications like robotics and embodied AI.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）虽然发展迅速，但在捕捉空间关系方面仍存在盲点，这对机器人和具身AI等应用至关重要。本文探讨了两种改进空间感知的方法：使用不同训练目标的图像编码器和引入二维位置编码。实验结果表明，这些架构改进在多个基准测试中提升了空间推理能力，突显了空间结构在VLM设计中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知</div>
<div class="mono" style="margin-top:8px">大型音频语言模型（LALMs）在语音识别、音频描述和听觉问答任务中最近取得了显著进展。然而，这些模型是否能够感知空间动态，特别是声源的运动，仍不清楚。在本研究中，我们发现当前音频语言模型（ALLMs）存在系统性的运动感知缺陷。为探讨这一问题，我们引入了AudioMotionBench，这是首个专门设计用于评估听觉运动理解的基准测试。AudioMotionBench引入了一个受控的问答基准测试，用于评估音频语言模型（LALMs）能否从双耳音频中推断出移动声源的方向和轨迹。全面的定量和定性分析表明，当前模型在可靠识别运动线索或区分方向模式方面存在困难。平均准确率仍低于50\%，突显了听觉空间推理的基本局限性。我们的研究指出了人类与模型在听觉空间推理之间的根本差距，为未来增强音频语言模型的空间认知能力提供了诊断工具和新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of Large Audio-Language Models (LALMs) to perceive auditory motion, specifically the direction and trajectory of moving sound sources, from binaural audio. The motivation stems from the observation that while LALMs excel in speech recognition and audio captioning, their capacity to understand spatial dynamics is underexplored. To address this, the authors introduce AudioMotionBench, a novel benchmark designed to evaluate motion perception in audio models. Experimental results show that current models achieve average accuracy below 50\%, indicating a significant deficit in their ability to reliably interpret motion cues and distinguish directional patterns.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型音频语言模型（LALMs）在感知听觉运动方面的能力，特别是对移动声源方向和轨迹的识别。研究人员提出了AudioMotionBench，这是首个专门用于评估听觉运动理解的基准测试。实验结果表明，当前模型在识别运动线索和区分方向模式方面表现不佳，平均准确率低于50%，揭示了听觉空间推理能力的根本性缺陷。这突显了人类与机器在听觉空间理解上的差距，并为未来提升音频语言模型的空间认知能力提供了诊断工具和新思路。</div>
</details>
</div>
<div class="card">
<div class="title">Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</div>
<div class="meta-line">Authors: Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</div>
<div class="meta-line">First: 2025-05-19T16:56:31+00:00 · Latest: 2026-01-22T14:25:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13353v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.13353v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code&#x27;s operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs&#x27; accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval&#x27;s 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知与敏感性：探究语义回忆对长上下文代码推理的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用于理解大型代码库，但它们是通过理解长代码上下文的操作语义还是依赖模式匹配捷径仍不清楚。我们区分了词汇回忆（逐字检索代码）和语义回忆（理解操作语义）。通过评估10种最先进的LLMs，我们发现虽然前沿模型能够实现近乎完美的、位置无关的词汇回忆，但当代码位于长上下文的中心位置时，语义回忆会严重退化。我们引入了语义回忆敏感性这一指标，用于衡量任务是否需要理解代码的操作语义，还是允许使用模式匹配捷径。通过一种新颖的反事实测量方法，我们表明模型在解决现有代码理解基准时严重依赖模式匹配捷径。我们提出了一项新任务SemTrace，其通过不可预测的操作实现高语义回忆敏感性；LLMs的准确率表现出严重的位移效应，当相关代码片段接近输入代码上下文的中间位置时，中位准确率下降了92.73%，而CRUXEval则下降了53.36%。我们的研究结果表明，当前的评估方法在长上下文代码理解中严重低估了语义回忆失败的情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how large language models (LLMs) handle long code contexts, focusing on whether they rely on pattern matching or truly understand the operational semantics. The research differentiates between lexical recall, which involves retrieving code verbatim, and semantic recall, which requires comprehension of code behavior. By evaluating ten state-of-the-art LLMs, the authors found that while models perform well in lexical recall, their semantic recall significantly declines when code is located in the central part of long contexts. They introduce a metric called semantic recall sensitivity to assess the necessity of semantic understanding in tasks. Using a counterfactual measurement approach, they demonstrate that models heavily depend on pattern matching shortcuts for existing benchmarks. Their proposed task, SemTrace, highlights the severe positional impact on model accuracy, with a median drop of 92.73% compared to CRUXEval&#x27;s 53.36% as the relevant code moves toward the center of the input.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）在处理长代码上下文时是否依赖模式匹配还是真正理解代码的操作语义。作者区分了字面回溯（逐字检索代码）和语义回溯（理解代码含义）两种方式。通过对10个最先进的模型进行评估，发现虽然字面回溯接近完美，但语义回溯在代码位于长上下文中央时严重下降。他们引入了语义回溯敏感性这一指标，用于评估任务是否需要语义理解。通过一种新颖的反事实测量方法，研究显示现有代码理解基准测试主要依赖模式匹配。提出的SemTrace任务通过不可预测的操作实现了高语义回溯敏感性，结果显示当相关代码接近输入中间时，LLMs的准确率下降高达92.73%，表明当前评估方法在捕捉语义回溯失败方面存在严重低估。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
