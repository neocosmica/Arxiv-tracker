<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-12 04:16</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260212_0416</div>
    <div class="row"><div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到动作：在空间基础先验中构建视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-动作（VLA）模型在三维真实世界中进行操作，但通常基于二维编码器，这导致了空间推理的缺失，限制了其泛化能力和适应性。近期的VLA三维集成技术要么需要专用传感器且跨模态迁移效果差，要么注入的线索较弱，缺乏几何信息并损害了视觉-语言对齐。在本工作中，我们引入了FALCON（从空间到动作），这是一种新颖范式，将丰富的三维空间标记注入到动作头中。FALCON利用空间基础模型，仅通过RGB图像即可提供强大的几何先验，并包含一个可选的具身空间模型，当有深度或姿态信息时，可以融合这些信息以提高保真度，而无需重新训练或改变架构。为了保持语言推理能力，空间标记被输入到空间增强的动作头中，而不是简单地连接到视觉-语言主干网络中。这些设计使FALCON能够解决空间表示、模态迁移性和对齐方面的局限性。在三个模拟基准和十一项真实世界任务的全面评估中，我们提出的FALCON取得了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化的情况下保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing vision-language-action (VLA) models that operate in 3D environments but rely on 2D encoders, leading to a spatial reasoning gap. The proposed FALCON model introduces rich 3D spatial tokens into the action head, utilizing spatial foundation models to extract strong geometric priors from RGB data. It also incorporates an Embodied Spatial Model that can optionally integrate depth or pose information for enhanced accuracy. The Spatial-Enhanced Action Head is designed to process these spatial tokens without altering the vision-language backbone, preserving language reasoning. Experimental results across three simulation benchmarks and eleven real-world tasks show that FALCON achieves state-of-the-art performance, outperforming baselines and maintaining robustness in challenging conditions such as clutter and varying object scales.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言-动作（VLA）模型在3D环境中依赖2D编码器导致的空间推理不足问题，提出了一种新的解决方案。FALCON模型通过在动作头中注入丰富的3D空间标记，利用空间基础模型从RGB数据中提取强几何先验信息。它还包含一个可选的具身空间模型，能够融合深度或姿态信息以提高精度，而无需重新训练或改变模型结构。空间增强的动作头保留了语言推理能力，不修改视觉-语言主干。在三个模拟基准和十一项真实世界任务的综合评估中，FALCON表现出最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化等挑战性条件下保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Mindset: Reasoning with Adaptive Cognitive Modes</div>
<div class="meta-line">Authors: Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen</div>
<div class="meta-line">First: 2026-02-10T18:31:47+00:00 · Latest: 2026-02-10T18:31:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10063v1">PDF</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset">Code1</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维链：基于自适应认知模式的推理</div>
<div class="mono" style="margin-top:8px">人类解决问题的过程从不是单一思维模式的重复，这里的思维模式指的是不同的认知处理方式。在解决特定任务时，我们并非依赖单一思维模式，而是将多种思维模式整合到同一个解决方案中。然而，现有的LLM推理方法陷入了一个普遍的误区：它们在所有步骤中都使用相同的固定思维模式，忽视了解决同一问题的不同阶段需要根本不同的思维模式。这种单一假设阻碍了模型向更高层次智能的发展。为了解决这一局限性，我们提出了Chain of Mindset（CoM），一个无需训练的代理框架，能够实现步骤级的自适应思维模式协调。CoM将推理分解为四种功能上异质的思维模式：空间思维、收敛思维、发散思维和算法思维。一个元代理根据推理状态的演变动态选择最优的思维模式，同时双向上下文门控机制过滤跨模块的信息流动，以保持推理的有效性和效率。我们在涵盖数学、代码生成、科学问答和空间推理的六个具有挑战性的基准测试中进行了实验，结果表明CoM在整体准确率上分别比最强基线模型高出4.96\%和4.72\%，同时保持推理效率。我们的代码可在\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitation of existing large language models (LLMs) in reasoning tasks, where they often apply a fixed mindset across all steps, failing to adapt to the varying cognitive demands of different problem-solving stages. The proposed Chain of Mindset (CoM) framework introduces a training-free agentic approach that dynamically orchestrates four distinct cognitive mindsets—Spatial, Convergent, Divergent, and Algorithmic—at the step level. A Meta-Agent selects the optimal mindset based on the current reasoning state, and a bidirectional Context Gate manages information flow between modules. Experimental results across six benchmarks in mathematics, code generation, scientific QA, and spatial reasoning show that CoM achieves state-of-the-art performance, improving overall accuracy by 4.96\% and 4.72\% on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash respectively, while maintaining reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有大语言模型（LLMs）在推理任务中的局限性，即它们通常在所有步骤中使用固定的心智模式，未能适应不同问题解决阶段所需的多样化认知处理方式。提出的Chain of Mindset（CoM）框架采用无训练的代理方法，在步骤层面动态协调四种功能各异的心智模式：空间、收敛、发散和算法模式。一个元代理根据当前推理状态选择最优的心智模式，而双向上下文门控机制则管理模块间的跨信息流动。实验在数学、代码生成、科学问答和空间推理等六个基准测试中进行，结果显示CoM达到了最先进的性能，分别在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上将整体准确率提升了4.96\%和4.72\%，同时保持推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-10T16:46:20+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：面向编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估基准。ContextBench包含来自8种编程语言、66个仓库的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四个前沿的LLM和五个编码代理。我们的结果表明，复杂的代理框架在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有的端到端基准，揭示了问题解决过程中的关键环节。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of current evaluations of coding agents, which primarily focus on final task success without considering the context retrieval process. ContextBench introduces a process-oriented benchmark that includes 1,136 issue-resolution tasks across eight programming languages, each with human-annotated gold contexts. The authors developed an automated evaluation framework to track agent behavior and assess context recall, precision, and efficiency during problem-solving. Experimental results reveal that advanced agent scaffolding provides only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between the context explored and the context actually used. These findings highlight the importance of incorporating intermediate context metrics into benchmarking to better understand and improve the reasoning process of coding agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估编码代理在问题解决过程中如何检索和使用代码上下文，而不仅仅关注最终任务的成功率。ContextBench 提供了一个面向过程的基准测试，包含来自66个仓库、涵盖八种编程语言的1136个问题解决任务，每个任务都附有人工标注的上下文。作者开发了一个自动化框架，用于追踪代理的行为并评估上下文的召回率、精确率和效率。实验结果表明，先进的代理结构在上下文检索方面仅带来小幅提升，LLMs倾向于优先召回而非精确，且存在显著的探索上下文与实际使用上下文之间的差距。这些发现强调了对编码代理工作流程中间步骤进行更细致评估的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge</div>
<div class="meta-line">Authors: Yijie Lin, Guofeng Ding, Haochen Zhou, Haobin Li, Mouxing Yang, Xi Peng</div>
<div class="meta-line">First: 2026-02-10T14:45:02+00:00 · Latest: 2026-02-10T14:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARK：一个结合推理与知识的双轴多模态检索基准</div>
<div class="mono" style="margin-top:8px">现有的多模态检索基准主要强调日常图像的语义匹配，并对专业领域知识和复杂推理的诊断能力有限。为解决这一问题，我们引入了ARK，一个从两个互补视角分析多模态检索的基准：(i) 知识领域（五个领域，17个子类），这些领域描述了检索所依赖的内容和专业知识；(ii) 推理能力（六个类别），这些类别描述了在识别正确候选时所需进行的多模态证据推理类型。具体而言，ARK评估单模态和多模态查询与候选之间的检索，涵盖16种异构视觉数据类型。为了避免评估中的捷径匹配，大多数查询都与需要多步推理的针对性难负样本配对。我们在ARK上评估了23种代表性的基于文本和多模态的检索器，并观察到知识密集型与推理密集型检索之间存在显著差距，其中细粒度视觉和空间推理成为持续存在的瓶颈。我们进一步表明，简单的增强方法如重排序和重写能够带来一致的提升，但仍存在大量改进空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the ARK benchmark is to address the limitations of existing multimodal retrieval systems in handling professional knowledge and complex reasoning tasks. ARK introduces a dual-axis evaluation framework, focusing on knowledge domains and reasoning skills, with five knowledge domains and six reasoning categories. The benchmark includes 16 heterogeneous visual data types and employs hard negative examples to prevent shortcut matching. Experimental results show a significant performance gap between knowledge-intensive and reasoning-intensive retrieval, highlighting fine-grained visual and spatial reasoning as persistent challenges. Simple techniques like re-ranking and rewriting provide consistent improvements, but there is still substantial room for advancement.</div>
<div class="mono" style="margin-top:8px">本研究旨在弥补现有多模态检索基准在专业领域知识和复杂推理能力方面的不足，这些基准主要关注日常图像的语义匹配。ARK提出一个双轴基准，从知识领域（五个领域，共17个子类）和推理能力（六个类别）两个互补视角评估多模态检索。该基准涵盖16种异构视觉数据类型，并采用针对性的困难负样本以避免捷径匹配。实验结果显示，知识密集型与推理密集型检索之间存在显著性能差距，细粒度视觉和空间推理成为持续存在的瓶颈。简单的改进措施如重排序和重写可带来一致的提升，但仍存在较大的优化空间。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-10T14:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v2">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近期在多模态大语言模型（MLLMs）中进行空间推理的研究越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这常常导致语义与几何信息的错位以及冗余信号。我们提出GeoThinker框架，将整合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力机制选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控机制进行校准，该机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面取得了新的最先进水平，在VSI-Bench上达到了72.6的峰值得分。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力，显著提升了空间感知能力，包括具身指称和自动驾驶等任务。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), where geometric information is often misaligned with semantic content and leads to redundant signals. The proposed GeoThinker framework introduces an active perception approach by enabling the model to selectively retrieve geometric evidence based on its internal reasoning needs. It employs Spatial-Grounded Fusion at specific Vision-Language Model (VLM) layers, using frame-strict cross-attention and Importance Gating to focus on task-relevant geometric structures. Experimental results on the VSI-Bench show that GeoThinker achieves a state-of-the-art score of 72.6 and demonstrates strong performance in complex downstream tasks such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）在空间推理任务中被动整合几何信息的局限性，指出其常导致语义与几何信息的错位以及冗余信号的问题。提出GeoThinker框架，通过主动感知方法，使模型能够根据内部推理需求选择性地检索几何证据。该方法在特定VLM层应用空间基础融合，结合帧严格交叉注意力和重要性门控机制，以聚焦任务相关的几何结构。在VSI-Bench上的实验结果表明，GeoThinker取得72.6的峰值得分，优于现有方法，并在复杂场景如具身指称和自动驾驶中展现出强大的泛化能力与空间感知提升。</div>
</details>
</div>
<div class="card">
<div class="title">From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki</div>
<div class="meta-line">First: 2026-02-09T14:39:43+00:00 · Latest: 2026-02-10T08:48:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08735v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08735v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从对应关系到行动：多模态大语言模型中类人多图像空间推理</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在单图像空间推理方面取得了显著进展，但需要整合多个视角信息的多图像空间推理仍具有挑战性。认知研究指出，人类通过两种机制解决此类任务：跨视角对应关系，用于识别不同视角中对应同一物理位置的区域；以及逐步视角转换，通过按顺序组合相对视角变化来完成推理。然而，现有研究仅部分且通常隐式地引入了这些机制，缺乏对两者的显式监督。我们提出了一种名为HATCH（用于跨视角对应关系和视角变化的人类感知训练）的训练框架，包含两个互补的目标：（1）块级空间对齐，鼓励块表示在空间对应区域中对齐；（2）行动后回答推理，要求模型在预测最终答案前生成显式的视角转换动作。在三个基准测试上的实验表明，HATCH在保持单图像推理能力的同时，显著优于同等规模的基线模型，并且在性能上与更大规模的模型竞争。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-image spatial reasoning in multimodal large language models (MLLMs), which is less developed than single-image reasoning. The authors propose HATCH, a training framework that explicitly incorporates two human-like mechanisms: cross-view correspondence and stepwise viewpoint transformation. By introducing Patch-Level Spatial Alignment and Action-then-Answer Reasoning as complementary objectives, HATCH enables the model to align spatial information across views and generate sequential viewpoint transitions. Experimental results on three benchmarks show that HATCH significantly outperforms existing baselines of similar size and achieves performance comparable to much larger models, while maintaining strong single-image reasoning abilities.</div>
<div class="mono" style="margin-top:8px">本文针对多图像空间推理在多模态大语言模型（MLLMs）中的挑战，指出其相较于单图像推理更为复杂，因为需要整合多个视角的信息。作者提出HATCH框架，显式地引入了两种人类推理机制：跨视角对应和逐步视角转换。HATCH包含两个目标——图像块级空间对齐，旨在对齐不同视角中对应物理位置的图像块表示；以及动作-答案推理，要求模型在生成最终答案前显式生成视角转换动作。在三个基准测试上的实验结果表明，HATCH在性能上显著优于同等规模的现有基线模型，并且在表现上可与更大规模的模型相媲美，同时保留了单图像推理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs</div>
<div class="meta-line">Authors: Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-23T09:17:07+00:00 · Latest: 2026-02-10T07:17:25+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17653v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17653v2">PDF</a> · <a href="https://github.com/LiAuto-DSR/GeoGramBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored. In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning. To evaluate this capability, we present GeoGramBench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity. Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level. These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoGramBench：现代大语言模型几何程序推理基准测试</div>
<div class="mono" style="margin-top:8px">几何空间推理是许多人工智能应用的基础，但大语言模型（LLMs）在处理以过程代码形式表达的几何空间信息方面的能力仍处于探索阶段。本文通过形式化程序到几何任务来解决这一问题，该任务挑战模型将程序绘图代码转化为准确且抽象的几何推理。为了评估这一能力，我们提出了GeoGramBench，这是一个包含500个精心优化问题的基准，这些问题按照一个定制的三级分类法组织，该分类法考虑的是几何复杂性而非传统的数学推理复杂性。我们对17个前沿LLM的全面评估揭示了其在最高抽象层级上存在一致且显著的不足：即使最先进的模型在该层级上的准确率也低于50%。这些结果突显了程序驱动空间推理所带来的独特挑战，并确立了GeoGramBench作为推进符号到空间几何推理研究的宝贵资源。项目页面：https://github.com/LiAuto-DSR/GeoGramBench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces GeoGramBench, a benchmark designed to assess the geometric program reasoning capabilities of modern large language models (LLMs). The motivation stems from the underexplored ability of LLMs to process and reason about geometric spatial information encoded in procedural code. The benchmark consists of 500 carefully curated problems organized into a three-level taxonomy based on geometric complexity rather than traditional mathematical reasoning. Evaluation of 17 state-of-the-art LLMs shows significant performance gaps, with even the most advanced models achieving less than 50% accuracy at the highest abstraction level.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLMs）在处理以程序代码形式表达的几何空间推理能力这一尚未充分研究的领域展开探讨。作者提出了GeoGramBench基准测试，包含500个精心整理的问题，并按照几何复杂度构建了三级分类体系。通过对17个前沿LLM的全面评估，发现即使最先进的模型在最高抽象层级的推理任务中也仅达到低于50%的准确率，揭示了程序驱动空间推理所面临的独特挑战，并确立了GeoGramBench作为推进符号到空间几何推理研究的重要资源。</div>
</details>
</div>
<div class="card">
<div class="title">AgentCgroup: Understanding and Controlling OS Resources of AI Agents</div>
<div class="meta-line">Authors: Yusheng Zheng, Jiakun Fan, Quanzhi Fu, Yiwei Yang, Wei Zhang, Andi Quinn</div>
<div class="meta-line">First: 2026-02-10T02:37:42+00:00 · Latest: 2026-02-10T02:37:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09345v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09345v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed in multi-tenant cloud environments, where they execute diverse tool calls within sandboxed containers, each call with distinct resource demands and rapid fluctuations. We present a systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Our measurements reveal that (1) OS-level execution (tool calls, container and agent initialization) accounts for 56-74% of end-to-end task latency; (2) memory, not CPU, is the concurrency bottleneck; (3) memory spikes are tool-call-driven with a up to 15.4x peak-to-average ratio; and (4) resource demands are highly unpredictable across tasks, runs, and models. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three mismatches in existing resource controls: a granularity mismatch (container-level policies vs. tool-call-level dynamics), a responsiveness mismatch (user-space reaction vs. sub-second unpredictable bursts), and an adaptability mismatch (history-based prediction vs. non-deterministic stateful execution). We propose AgentCgroup , an eBPF-based resource controller that addresses these mismatches through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies driven by in-kernel monitoring. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentCgroup：理解与控制AI代理的OS资源</div>
<div class="mono" style="margin-top:8px">AI代理越来越多地部署在多租户云环境中，它们在沙箱容器中执行多样化的工具调用，每个调用具有不同的资源需求和快速波动。我们对沙箱AI编码代理的OS级资源动态进行了系统性分析，研究了SWE-rebench基准中两个LLM模型的144个软件工程任务。我们的测量结果表明：(1) OS级执行（工具调用、容器和代理初始化）占端到端任务延迟的56-74%；(2) 内存而非CPU是并发瓶颈；(3) 内存峰值由工具调用驱动，其峰值与平均值的比值高达15.4倍；(4) 资源需求在任务、运行和模型之间高度不可预测。通过将这些特性与无服务器、微服务和批处理工作负载进行比较，我们识别出现有资源控制存在三个不匹配：粒度不匹配（容器级策略与工具调用级动态）、响应性不匹配（用户空间反应与亚秒级不可预测的峰值）以及适应性不匹配（基于历史的预测与非确定性状态执行）。我们提出了AgentCgroup，一种基于eBPF的资源控制器，通过与工具调用边界对齐的分层cgroup结构、内核级强制执行（通过sched_ext和memcg_bpf_ops）以及由内核监控驱动的运行时自适应策略来解决这些问题。初步评估表明，该方案提升了多租户隔离性并减少了资源浪费。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing deployment of AI agents in multi-tenant cloud environments necessitates a deeper understanding of their OS-level resource usage patterns. This study characterizes the resource dynamics of sandboxed AI coding agents by analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. The findings show that OS-level operations account for a significant portion of task latency, memory is the primary concurrency bottleneck, and resource demands are highly unpredictable. The proposed AgentCgroup system, based on eBPF, introduces hierarchical cgroup structures aligned with tool-call boundaries and in-kernel enforcement mechanisms to address these challenges, achieving improved multi-tenant isolation and reduced resource waste.</div>
<div class="mono" style="margin-top:8px">随着AI代理在多租户云环境中的广泛应用，对其操作系统级资源使用模式的理解变得尤为重要。本研究通过分析SWE-rebench基准中的144个软件工程任务，针对两个大语言模型的沙箱AI编码代理进行资源动态特征刻画。研究结果表明，操作系统级执行占任务端到端延迟的56%-74%，内存是并发瓶颈，且资源需求在任务、运行和模型间具有高度不确定性。为此，本文提出AgentCgroup系统，基于eBPF构建分层cgroup结构，并通过内核级执行和内存控制接口实现资源管理，有效提升了多租户隔离性并减少了资源浪费。</div>
</details>
</div>
<div class="card">
<div class="title">Automated QoR improvement in OpenROAD with coding agents</div>
<div class="meta-line">Authors: Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee</div>
<div class="meta-line">First: 2026-01-09T19:30:02+00:00 · Latest: 2026-02-09T22:54:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06268v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06268v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, effective clock period reductions of up to 10.0%, and power reductions of up to 19.4%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在OpenROAD中使用编码代理实现自动化QoR改进</div>
<div class="mono" style="margin-top:8px">EDA开发和创新一直受到专家工程资源稀缺的限制。尽管领先的LLMs在编码和科学推理任务中表现出色，但它们在推动EDA技术发展方面的潜力尚未经过充分验证。我们提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主、基于仓库的编码系统，能够读取OpenROAD，提出研究方向，将其扩展为实施步骤，并提交可执行的代码差异。我们的贡献包括：(i) 一个用于EDA代码修改的闭环LLM框架；(ii) 针对PPA导向改进的OpenROAD任务套件和评估协议；以及 (iii) 需要最少人工监督的端到端演示。在OpenROAD上的实验实现了最高达5.9%的布线线长减少、10.0%的有效时钟周期减少和19.4%的功耗降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of expert engineering resources in EDA development and explore the potential of large language models (LLMs) in advancing EDA technology. The authors introduce AuDoPEDA, an autonomous coding system that leverages OpenAI models and a Codex-class agent to analyze OpenROAD, suggest research directions, generate implementation steps, and submit executable code changes. Experimental results show that AuDoPEDA achieves up to 5.9% reduction in routed wirelength, 10.0% improvement in effective clock period, and 19.4% decrease in power consumption, demonstrating its effectiveness in PPA-oriented improvements.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决EDA开发中专家工程资源稀缺的问题，并探索大语言模型在推动EDA技术发展方面的潜力。作者提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主编码系统，能够分析OpenROAD，提出研究方向，生成实现步骤，并提交可执行的代码修改。在OpenROAD上的实验结果显示，该系统在PPA（功耗、性能、面积）优化方面取得了显著成效，包括布线长度减少5.9%、有效时钟周期缩短10.0%以及功耗降低19.4%。</div>
</details>
</div>
<div class="card">
<div class="title">AIDev: Studying AI Coding Agents on GitHub</div>
<div class="meta-line">Authors: Hao Li, Haoxiang Zhang, Ahmed E. Hassan</div>
<div class="meta-line">First: 2026-02-09T20:45:58+00:00 · Latest: 2026-02-09T20:45:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.
  &gt; AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIDev: 在 GitHub 上研究 AI 编码代理</div>
<div class="mono" style="margin-top:8px">AI 编码代理正在迅速改变软件工程，执行诸如功能开发、调试和测试等任务。尽管其影响日益扩大，研究界仍缺乏一个全面的数据集，用于记录这些代理在实际项目中的使用情况。为了解决这一问题，我们引入了 AIDev，这是一个大规模的数据集，专注于实际 GitHub 仓库中由代理生成的拉取请求（Agentic-PRs）。AIDev 整合了由五个代理（OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code）生成的 932,791 个 Agentic-PRs，这些 PRs 涉及 116,211 个仓库和 72,189 名开发者。此外，AIDev 还包含一个精选的 33,596 个 Agentic-PRs 子集，来自拥有超过 100 星标评分的 2,807 个仓库，提供了诸如评论、审查、提交和相关问题等更多信息。该数据集为未来关于 AI 应用、开发者生产力和人机协作的研究提供了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid development of AI coding agents has significantly influenced software engineering, yet there is a lack of comprehensive data on their real-world usage. To address this, AIDev was created as a large-scale dataset containing 932,791 agent-authored pull requests from five AI coding agents, including OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These pull requests span 116,211 repositories and involve 72,189 developers, with a curated subset of 33,596 PRs from popular repositories with over 100 stars, including additional metadata such as comments and related issues. This dataset provides a valuable resource for studying AI adoption, developer productivity, and human-AI collaboration in software engineering.</div>
<div class="mono" style="margin-top:8px">随着AI编码代理在软件工程领域的快速发展，研究其实际应用情况的需求日益增长。AIDev作为一个大规模数据集，收录了由五个AI编码代理（包括OpenAI Codex、Devin、GitHub Copilot、Cursor和Claude Code）生成的932,791个代理拉取请求（Agentic-PRs），覆盖了116,211个仓库，涉及72,189名开发者。此外，该数据集还包含一个精选的33,596个Agentic-PRs子集，来自拥有超过100颗星标的2,807个仓库，提供了评论、审查、提交和相关问题等详细信息。AIDev为研究AI采纳、开发者生产力以及人机协作提供了重要的基础数据。</div>
</details>
</div>
<div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-09T15:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v2">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在问题级别的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念导向任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ProjDevBench is to address the limitations of current coding agent evaluations, which primarily focus on bug fixing rather than end-to-end project development. The benchmark introduces a comprehensive evaluation framework that combines Online Judge testing with LLM-assisted code review to assess coding agents on system architecture design, functional correctness, and iterative solution refinement. It includes 20 programming problems across 8 categories, evaluating six agents based on different LLM backends. The main experimental results show that while agents perform well on basic functionality and data structures, they struggle with complex system design, time complexity optimization, and resource management, leading to an overall acceptance rate of 27.38%.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，现有评估主要集中在单个问题的修复上，而忽视了端到端项目开发的能力。ProjDevBench提出一个全面的基准，为编码代理提供项目需求并评估生成的代码库，涵盖系统架构设计、功能正确性以及迭代解决方案优化三个方面。该基准包含8个类别下的20个编程问题，评估了基于不同LLM后端构建的六个编码代理。实验结果显示，尽管代理在基础功能和数据结构方面表现良好，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难，整体接受率为27.38%。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-09T14:24:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展收益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，用于代码代理的性能扩展，通过多样性而非原始数据量来提升表现。在固定训练预算下，增加轨迹多样性带来的性能提升比增加轨迹数量更大，从而优化代理训练的性能成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，捕捉真实服务中的逻辑依赖关系；（2）蓝图驱动的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙箱环境中的代码工具，缓解内在编码能力的灾难性遗忘。在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得显著提升。我们计划在发表后释放完整的代码库和合成数据集（包含30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of code large language models (LLMs) as they transition into tool-interactive agents, where performance is constrained by low-quality synthetic data and the diminishing returns of increasing data quantity. To overcome these challenges, the authors introduce TDScaling, a data synthesis framework that enhances performance through trajectory diversity rather than volume. The framework incorporates four key innovations: capturing logical dependencies with a Business Cluster mechanism, ensuring trajectory coherence via a blueprint-driven multi-agent paradigm, preventing mode collapse with an adaptive evolution mechanism based on entropy metrics, and maintaining coding proficiency with a sandboxed code tool. Experimental results on multiple benchmarks and code agent tasks show that TDScaling significantly improves both tool-use generalization and coding capabilities under a fixed training budget.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）在演变为工具交互代理时面临的挑战，如合成数据质量低和数量扩展带来的边际效益递减问题。提出TDScaling框架，通过提升轨迹多样性而非单纯增加数据量来增强性能。该框架包含四项创新：业务聚类机制以捕捉实际服务的逻辑依赖，蓝图驱动的多代理范式确保轨迹一致性，基于熵度量的自适应演化机制引导合成向长尾场景发展以防止模式坍塌，以及沙盒环境中的代码工具以保持内在编码能力。实验结果表明，TDScaling在多个基准测试中有效提升了工具使用泛化能力和编码能力，实现了性能与成本之间的更优平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models</div>
<div class="meta-line">Authors: Yueyan Li, Chenggong Zhao, Zeyuan Zang, Caixia Yuan, Xiaojie Wang</div>
<div class="meta-line">First: 2025-09-23T16:07:18+00:00 · Latest: 2026-02-09T10:18:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19191v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19191v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the &quot;what&quot; and &quot;where&quot; pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model&#x27;s perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像阅读文本一样阅读图像：视觉-语言模型中的序列化图像理解</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多种现实任务中表现出色。然而，现有VLMs通常通过序列化图像处理视觉信息，这种方法与人类视觉的并行特性存在显著差异。此外，其内部机制不透明，阻碍了更深入的理解和架构创新。受人类视觉双流假说的启发，我们区分了“what”（物体识别）和“where”（空间感知）路径，将VLMs中的视觉处理分解为这两个部分进行独立研究。对于物体识别，我们将图像转换为文本标记图，并发现模型对图像内容的感知是一个从浅层到深层的两阶段过程，始于属性识别，最终实现语义消歧。对于空间感知，我们从理论上推导并实证验证了VLMs中位置表示的几何结构。基于这些发现，我们提出了一种基于即插即用视觉解码器的指令无关标记压缩算法以提高解码效率，并引入了一种RoPE缩放技术以增强空间推理能力。通过严谨的实验，我们的工作验证了这些分析，提供了对VLM内部机制更深入的理解，并为设计更强大的未来架构提供了清晰的指导原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Vision-Language Models (VLMs) by analyzing their visual processing mechanisms, which differ from the parallel nature of human vision. Inspired by the dual-stream hypothesis, the authors decompose visual understanding into object recognition and spatial perception, revealing that object recognition progresses through a two-stage process from shallow to deep layers, starting with attribute detection and ending with semantic disambiguation. They also uncover the geometric structure behind positional representations in VLMs. Based on these insights, they propose an instruction-agnostic token compression algorithm and a RoPE scaling technique to improve decoding efficiency and spatial reasoning, respectively. Experimental results confirm the validity of their analysis and provide foundational knowledge for future architectural improvements in VLMs.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言模型（VLMs）在视觉处理上偏离人类并行机制以及内部机制不透明的问题展开研究。受双流假说启发，作者将视觉处理分解为物体识别与空间感知两个部分，发现物体理解在模型中呈现从浅层到深层的两阶段过程，先进行属性识别，最终实现语义消歧。同时，他们通过理论推导和实验证实了VLMs中位置表示的几何结构，并提出了基于可插拔视觉解码器的指令无关的token压缩算法和RoPE缩放技术，以提升解码效率和空间推理能力。实验结果验证了这些分析，加深了对VLM内部机制的理解，并为未来更强大的架构设计提供了明确的指导原则。</div>
</details>
</div>
<div class="card">
<div class="title">BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models</div>
<div class="meta-line">Authors: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li</div>
<div class="meta-line">First: 2026-02-09T08:47:14+00:00 · Latest: 2026-02-09T08:47:14+00:00</div>
<div class="meta-line">Comments: 38 pages, 9 figures. Project page:https://bimanibench.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08392v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bimanibench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiManiBench：一种用于评估多模态大语言模型双臂协调的分层基准</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）显著推动了具身人工智能的发展，使用它们来评估机器人智能已成为关键趋势。然而，现有框架主要局限于单臂操作，无法捕捉如提起重锅等双臂任务所需的时空协调。为此，我们引入BiManiBench，这是一个分层基准，从三个层面评估MLLMs：基础空间推理、高级动作规划和低级末端执行器控制。我们的框架隔离了双臂任务中的独特挑战，如手臂可达性与运动学约束，从而区分感知幻觉与规划失败。对超过30种最先进的模型的分析表明，尽管在高级推理方面表现优异，MLLMs在双臂空间定位和控制上仍存在困难，常导致相互干扰和顺序错误。这些发现表明当前范式缺乏对双臂运动学约束的深入理解，强调未来研究应关注双臂碰撞规避和精细时间序列控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the bimanual coordination capabilities of Multimodal Large Language Models (MLLMs) in the context of embodied AI, as current benchmarks are limited to single-arm manipulation. BiManiBench is introduced as a hierarchical benchmark that assesses MLLMs across three levels: spatial reasoning, action planning, and end-effector control. Experimental results show that while MLLMs perform well in high-level reasoning, they face significant challenges in dual-arm spatial grounding and control, often leading to mutual interference and sequencing errors.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估多模态大语言模型（MLLMs）在具身AI和机器人智能中的双臂协调能力。为此，我们提出了BiManiBench框架，这是一个分层次的基准测试，用于评估MLLMs在空间推理、动作规划和末端执行器控制三个层面的表现。实验结果表明，尽管MLLMs在高层推理方面表现良好，但在双臂空间定位和控制方面仍存在显著困难，常导致手臂相互干扰和动作顺序错误。这表明当前范式对双臂运动学约束的理解尚不深入，未来研究需要更加关注手臂间碰撞避免和精细的时间序列控制。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science</div>
<div class="meta-line">Authors: Jie Zhang, Xingtong Yu, Yuan Fang, Rudi Stouffs, Zdravko Trivic</div>
<div class="meta-line">First: 2026-02-09T07:28:49+00:00 · Latest: 2026-02-09T07:28:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08342v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanGraphEmbeddings: 学习和评估具有空间基础的多模态嵌入以用于城市科学</div>
<div class="mono" style="margin-top:8px">在城市环境中学习可迁移的多模态嵌入具有挑战性，因为城市理解本质上是空间的，但现有的数据集和基准缺乏街道视图图像与城市结构之间的显式对齐。我们引入了UGData，这是一个具有空间基础的数据集，将街道视图图像锚定到结构化的空间图上，并通过空间推理路径和空间上下文描述提供图对齐的监督，揭示超出图像内容的距离、方向性、连通性和邻近上下文。基于UGData，我们提出了UGE，这是一种两阶段的训练策略，通过结合指令引导的对比学习与基于图的空间编码，逐步且稳定地对齐图像、文本和空间结构。最后，我们引入了UGBench，这是一个全面的基准，用于评估具有空间基础的嵌入如何支持多样化的城市理解任务，包括地理定位排序、图像检索、城市感知和空间锚定。我们在多个最先进的视觉语言模型（VLM）主干网络上开发了UGE，包括Qwen2-VL、Qwen2.5-VL、Phi-3-Vision和LLaVA1.6-Mistral，并通过LoRA调优训练固定维度的空间嵌入。基于Qwen2.5-VL-7B主干网络构建的UGE在训练城市中实现了图像检索44%的提升和地理定位排序30%的提升，在未见城市中分别实现了超过30%和22%的提升，证明了显式空间锚定在空间密集型城市任务中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of learning transferable multimodal embeddings for urban environments, where understanding is deeply spatial but current datasets lack explicit alignment between images and spatial structures. The authors introduce UGData, a dataset that links street-view images to spatial graphs and provides supervision through spatial reasoning paths and captions. Based on this, they propose UGE, a two-stage training method that combines instruction-guided contrastive learning with graph-based spatial encoding to align images, text, and spatial data. Experimental results show that UGE achieves significant improvements in image retrieval and geolocation ranking, with up to 44% and 30% gains on training cities, and over 30% and 22% on held-out cities, highlighting the effectiveness of spatially grounded embeddings for urban tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决城市环境中学习可迁移的多模态嵌入的挑战，因为城市理解本质上具有空间特性，但现有数据集缺乏街道视图图像与空间结构之间的显式对齐。为此，作者提出了UGData数据集，该数据集将街道视图图像锚定到结构化空间图上，并通过空间推理路径和上下文描述提供图对齐的监督。他们进一步提出了UGE，一种两阶段训练策略，结合了指令引导的对比学习和基于图的空间编码，以逐步对齐图像、文本和空间结构。实验结果表明，基于Qwen2.5-VL-7B主干的UGE在训练城市中实现了图像检索和地理定位排名分别提高44%和30%，在未见过的城市中分别提高了30%和22%，证明了显式空间对齐在空间密集型城市任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</div>
<div class="meta-line">Authors: Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal</div>
<div class="meta-line">First: 2026-02-09T03:21:48+00:00 · Latest: 2026-02-09T03:21:48+00:00</div>
<div class="meta-line">Comments: the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adaptive-visual-tts.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时以及多少需要想象：基于世界模型的自适应测试时缩放用于视觉空间推理</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）取得了快速进展，但当正确答案依赖于未见过或替代视角下的场景外观时，视觉空间推理仍然不可靠。最近的研究通过引入世界模型来增强视觉想象以解决这一问题，但关于何时需要想象、想象多少是有益的，以及何时会变得有害等问题仍缺乏深入理解。在实践中，无差别地使用想象可能会增加计算量，甚至通过引入误导性证据而降低性能。在本文中，我们深入分析了测试时的视觉想象作为一种可控资源在空间推理中的作用。我们研究了静态视觉证据何时足够、何时能提升推理能力，以及过多或不必要的想象如何影响准确性和效率。为支持这一分析，我们提出了AVIC，一个基于世界模型的自适应测试时框架，它在选择性调用和缩放视觉想象之前，会显式地推理当前视觉证据是否足够。在空间推理基准（SAT、MMSI）和一个具身导航基准（R2R）上的实验结果表明，存在明确的场景，其中想象是关键、边缘或有害的，而选择性控制策略可以在大幅减少世界模型调用和语言标记的情况下，达到或超越固定想象策略的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reliable visual spatial reasoning in Multimodal Large Language Models (MLLMs), where correct answers often depend on unseen or alternative viewpoints. The authors propose AVIC, an adaptive test-time framework that uses world models to selectively control the use of visual imagination based on the sufficiency of current visual evidence. Their experiments on spatial reasoning benchmarks and an embodied navigation task demonstrate that imagination can be critical, marginal, or even detrimental depending on the scenario, and that selective control leads to better performance with fewer computational resources.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态大语言模型（MLLMs）在需要未见视角的视觉空间推理中可靠性不足的问题。作者提出了AVIC框架，通过世界模型在推理过程中判断何时以及需要多少视觉想象，从而提升推理的准确性和效率。在空间推理基准（SAT、MMSI）和具身导航任务（R2R）上的实验结果表明，选择性控制视觉想象的策略在性能上可匹配或超越固定策略，同时显著减少世界模型调用和语言标记的使用。</div>
</details>
</div>
<div class="card">
<div class="title">ViT-5: Vision Transformers for The Mid-2020s</div>
<div class="meta-line">Authors: Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad, Cihang Xie, Alan Yuille</div>
<div class="meta-line">First: 2026-02-08T18:03:44+00:00 · Latest: 2026-02-08T18:03:44+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/wangf3014/ViT-5</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08071v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08071v1">PDF</a> · <a href="https://github.com/wangf3014/ViT-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViT-5：面向2020年代中期的视觉Transformer</div>
<div class="mono" style="margin-top:8px">本工作系统地探讨了通过利用过去五年架构进展来现代化视觉Transformer主干的方法。在保留经典注意力-前馈网络结构的基础上，我们对各个组件进行了细致优化，包括归一化、激活函数、位置编码、门控机制以及可学习标记。这些改进构成了新一代的视觉Transformer，我们称之为ViT-5。大量实验表明，ViT-5在理解和生成基准测试中均优于当前最先进的普通视觉Transformer。在ImageNet-1k分类任务中，ViT-5-Base在计算资源相当的情况下达到84.2%的top-1准确率，超过了DeiT-III-Base的83.8%。ViT-5还作为生成建模的更强主干：当将其嵌入到SiT扩散框架中时，其FID为1.84，而使用普通ViT主干时为2.06。除了主要指标外，ViT-5在表示学习和空间推理方面也有所提升，并且在不同任务间具有良好的迁移能力。其设计与当前基础模型实践相一致，为2020年代中期的视觉主干提供了一个简单的即插即用升级方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces ViT-5, an updated Vision Transformer model that incorporates architectural improvements from the past five years while maintaining the canonical Attention-FFN structure. The model undergoes component-wise refinements in normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. Experimental results show that ViT-5 outperforms previous Vision Transformer models on both classification and generative tasks, achieving 84.2% top-1 accuracy on ImageNet-1k and a lower FID score in diffusion models. It also demonstrates enhanced representation learning and reliable task transfer, making it a suitable backbone for modern vision applications.</div>
<div class="mono" style="margin-top:8px">本文提出了ViT-5，这是一种通过引入过去五年中架构改进来现代化原始视觉Transformer主干的模型。该模型保留了经典的Attention-FFN结构，但对归一化、激活函数、位置编码、门控机制等关键组件进行了优化。实验结果显示，ViT-5在ImageNet-1k分类任务中以84.2%的top-1准确率超越了DeiT-III-Base，并在SiT扩散框架中实现更低的FID得分，显示出在理解和生成任务中的优越性能。此外，ViT-5在表示学习和空间推理方面表现更佳，且具备良好的任务迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">View-Centric Multi-Object Tracking with Homographic Matching in Moving UAV</div>
<div class="meta-line">Authors: Deyi Ji, Lanyun Zhu, Siqi Gao, Qi Zhu, Yiru Zhao, Peng Xu, Yue Ding, Hongtao Lu, Jieping Ye, Feng Wu, Feng Zhao</div>
<div class="meta-line">First: 2024-03-16T06:48:33+00:00 · Latest: 2026-02-08T10:40:20+00:00</div>
<div class="meta-line">Comments: TGRS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.10830v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.10830v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we address the challenge of Multi-Object Tracking (MOT) in moving Unmanned Aerial Vehicle (UAV) scenarios, where irregular flight trajectories, such as hovering, turning left/right, and moving up/down, lead to significantly greater complexity compared to fixed-camera MOT. Specifically, changes in the scene background not only render traditional frame-to-frame object IoU association methods ineffective but also introduce significant view shifts in the objects, which complicates tracking. To overcome these issues, we propose a novel HomView-MOT framework, which for the first time, harnesses the view homography inherent in changing scenes to solve MOT challenges in moving environments, incorporating homographic matching and view-centric concepts. We introduce a Fast Homography Estimation (FHE) algorithm for rapid computation of homography matrices between video frames, enabling object View-Centric ID Learning (VCIL) and leveraging multi-view homography to learn cross-view ID features. Concurrently, our Homographic Matching Filter (HMF) maps object bounding boxes from different frames onto a common view plane for a more realistic physical IoU association. Extensive experiments have proven that these innovations allow HomView-MOT to achieve state-of-the-art performance on prominent UAV MOT datasets VisDrone and UAVDT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于同构匹配的移动无人机多目标跟踪</div>
<div class="mono" style="margin-top:8px">本文针对移动无人机场景下的多目标跟踪（MOT）挑战，提出了一种新颖的HomView-MOT框架。该框架首次利用变化场景中的视图同构性来解决移动环境中的MOT问题，结合了同构匹配和视图中心的概念。我们引入了快速同构估计（FHE）算法，用于快速计算视频帧之间的同构矩阵，从而实现目标视图中心ID学习（VCIL）并利用多视图同构学习跨视图ID特征。同时，我们的同构匹配滤波器（HMF）将不同帧中的目标边界框映射到同一视图平面上，以实现更真实的物理IoU关联。大量实验表明，这些创新使HomView-MOT在VisDrone和UAVDT等主流无人机MOT数据集上达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-object tracking (MOT) in moving UAV scenarios, where irregular flight paths and changing backgrounds complicate traditional frame-to-frame association methods. The authors propose the HomView-MOT framework, which utilizes homographic matching and view-centric concepts to handle view shifts and improve tracking accuracy. A Fast Homography Estimation (FHE) algorithm is introduced to compute homography matrices efficiently, enabling View-Centric ID Learning (VCIL) and cross-view feature learning. The Homographic Matching Filter (HMF) further enhances object association by mapping bounding boxes to a common view plane. Experimental results on VisDrone and UAVDT datasets demonstrate that HomView-MOT achieves state-of-the-art performance in moving UAV MOT tasks.</div>
<div class="mono" style="margin-top:8px">本文针对移动无人机场景下的多目标跟踪问题，提出了一种新的HomView-MOT框架，以应对不规则飞行轨迹和场景背景变化带来的挑战。该方法首次利用场景中的视图仿射变换特性，结合视图中心概念和仿射匹配技术，提升目标跟踪的鲁棒性。通过快速仿射估计算法，实现视频帧间仿射矩阵的高效计算，支持视图中心ID学习和跨视图特征学习。同时，Homographic Matching Filter将不同帧中的目标边界框映射到统一视图平面，以更准确地进行物理IoU关联。在VisDrone和UAVDT等无人机多目标跟踪数据集上的实验表明，HomView-MOT在动态环境下实现了最先进的跟踪性能。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents</div>
<div class="meta-line">Authors: Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, Lingxiao Jiang</div>
<div class="meta-line">First: 2026-02-08T10:26:31+00:00 · Latest: 2026-02-08T10:26:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07900v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考基于LLM的软件工程代理生成测试的价值</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）代码代理通过迭代编辑代码、调用工具和验证候选补丁来越来越多地解决仓库级别的问题。在这些工作流中，代理经常即兴编写测试，这一范式被许多SWE-bench排行榜上的高排名代理所采用。然而，我们观察到，GPT-5.2几乎不编写新测试，却仍能实现与顶级代理相当的性能。这引发了关键问题：这些测试是否真正有助于问题解决，还是仅仅模仿了人类的测试实践，同时消耗了大量交互预算？
为了揭示代理编写测试的影响，我们进行了一项实证研究，分析了在SWE-bench Verified上六个最先进的LLM代理的轨迹。我们的结果显示，尽管测试编写被广泛采用，但同一模型中解决和未解决的任务在测试编写频率上表现出相似性。此外，这些测试通常作为观察反馈渠道，代理更倾向于使用能揭示价值的打印语句，而不是正式的断言检查。基于这些见解，我们通过修改四个代理的提示来增加或减少测试编写量，进行了一项受控实验。结果表明，代理编写测试的数量变化不会显著影响最终结果。综上所述，我们的研究揭示了当前测试编写实践在自主软件工程任务中可能仅提供有限的效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the value of agent-generated tests in LLM-based software engineering agents, motivated by the observation that high-performing agents on the SWE-bench leaderboard often write tests on the fly, yet models like GPT-5.2, which generate minimal tests, can achieve comparable performance. The research analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified, revealing that test writing is common but not strongly correlated with task resolution. The findings suggest that agent-written tests primarily function as observational feedback mechanisms, with agents favoring print statements over formal assertions. A controlled experiment manipulating test generation in four agents showed that altering the volume of tests did not significantly affect final outcomes, indicating that current test-writing practices may offer limited utility in autonomous software engineering tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于大语言模型（LLM）的软件工程代理中自动生成测试的作用，其动机源于观察到即使几乎不编写新测试的代理也能达到顶级代理的性能水平。通过对六个先进LLM在SWE-bench Verified数据集上的代理轨迹进行分析，研究发现测试编写虽然普遍，但与任务解决与否无明显关联。实验结果表明，测试通常作为观察反馈机制，代理更倾向于使用值揭示型打印语句而非正式断言检查。进一步的受控实验显示，调整测试编写提示以增加或减少测试数量并未显著影响最终结果，说明当前的测试编写实践在自主软件工程任务中可能仅提供有限的效用。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds</div>
<div class="meta-line">Authors: Chen Yang, Guanxin Lin, Youquan He, Peiyao Chen, Guanghe Liu, Yufan Mo, Zhouyuan Xu, Linhao Wang, Guohui Zhang, Zihang Zhang, Shenxiang Zeng, Chen Wang, Jiansheng Fan</div>
<div class="meta-line">First: 2026-02-08T08:29:38+00:00 · Latest: 2026-02-08T08:29:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ssi-bench.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构思维：通过受限流形上的推理评估空间智能</div>
<div class="mono" style="margin-top:8px">空间智能对于在物理世界中运行的视觉-语言模型（VLMs）至关重要，但许多基准测试主要评估不受限的场景，使得模型可以利用2D捷径。我们引入了SSI-Bench，这是一个针对受限流形上空间推理的VQA基准测试，基于复杂的真实世界3D结构，其可行配置严格受几何、拓扑和物理约束的控制。SSI-Bench包含1,000个排名问题，涵盖几何和拓扑推理，并需要多样化的组合空间操作，如心理旋转、截面推理、遮挡推理和力-路径推理。该基准测试通过完全以人类为中心的流程创建：十位研究人员花费超过400小时整理图像、标注结构组件并设计问题，以最小化像素级线索。评估31个广泛使用的VLMs显示与人类存在显著差距：最佳开源模型准确率为22.2%，最强闭源模型准确率为33.6%，而人类准确率为91.6%。鼓励模型进行思考仅带来边际提升，错误分析表明模型在结构锚定和约束一致的3D推理方面存在失败。项目页面：https://ssi-bench.github.io.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SSI-Bench, a new VQA benchmark designed to evaluate spatial intelligence in vision--language models (VLMs) by focusing on constrained manifolds derived from complex real-world 3D structures. The motivation stems from the observation that existing benchmarks often assess models on unconstrained scenes, allowing them to rely on 2D shortcuts rather than true 3D reasoning. The benchmark includes 1,000 ranking questions that require geometric and topological reasoning, as well as compositional spatial operations like mental rotation and occlusion reasoning. Evaluations on 31 popular VLMs show significant performance gaps compared to humans, with the best open-source model achieving 22.2% accuracy and the strongest closed-source model reaching 33.6%, while humans score 91.6%. The results highlight the need for better structural grounding and constraint-aware 3D reasoning in VLMs.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的视觉问答基准测试SSI-Bench，旨在通过评估受限流形上的空间推理能力来检验视觉语言模型（VLMs）的空间智能。研究动机源于现有基准测试多评估无约束场景，使模型能够依赖2D捷径而非真正的3D理解。该基准测试包含1000个排名问题，涵盖几何和拓扑推理，以及如心理旋转、截面推理、遮挡推理和力-路径推理等组合空间操作。实验结果显示，当前VLMs与人类存在显著性能差距，最佳开源模型准确率为22.2%，最强闭源模型为33.6%，而人类得分高达91.6%。错误分析表明，模型在结构锚定和约束一致的3D推理方面存在困难。</div>
</details>
</div>
<div class="card">
<div class="title">rePIRL: Learn PRM with Inverse RL for LLM Reasoning</div>
<div class="meta-line">Authors: Xian Wu, Kaijie Zhu, Ying Zhang, Lun Wang, Wenbo Guo</div>
<div class="meta-line">First: 2026-02-08T05:47:27+00:00 · Latest: 2026-02-08T05:47:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07832v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>rePIRL: 通过逆强化学习学习LLM推理中的过程奖励模型</div>
<div class="mono" style="margin-top:8px">过程奖励在深度强化学习中被广泛用于提高训练效率、减少方差并防止奖励黑客行为。在LLM推理中，现有工作也探索了各种学习有效过程奖励模型（PRM）的方法，无论是否借助专家策略。然而，现有方法要么依赖于对专家策略的强假设（例如需要其奖励函数），要么存在内在限制（例如熵崩溃），导致PRM效果较弱或泛化能力有限。本文提出rePIRL，这是一种受逆强化学习启发的框架，能够在对专家策略的最小假设下学习有效的PRM。具体而言，我们设计了一种双学习过程，交替更新策略和PRM。我们的学习算法采用了定制化技术，以应对将传统逆强化学习扩展到LLM时的挑战。我们从理论上证明，所提出的框架能够统一在线和离线PRM学习方法，从而证明rePIRL可以在最小假设下学习PRM。我们在标准化的数学和编程推理数据集上的实证评估展示了rePIRL相较于现有方法的有效性。我们进一步展示了所训练的PRM在测试时训练、测试时扩展以及为训练困难问题提供早期信号方面的应用。最后，我们通过详细消融实验验证了我们的训练方案和关键设计选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of learning effective process reward models (PRMs) for large language model (LLM) reasoning, where existing methods either depend on expert policies or face limitations like entropy collapse. The proposed rePIRL framework is inspired by inverse reinforcement learning and employs a dual learning process to iteratively update the policy and the PRM. Theoretical analysis shows that rePIRL can unify online and offline PRM learning, while empirical results on math and coding datasets demonstrate its superior performance. Additionally, the framework supports test-time training, scaling, and early signal detection for difficult tasks, validated through an ablation study.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLM）推理中学习有效过程奖励模型（PRM）的挑战，指出现有方法要么依赖专家奖励函数，要么面临如熵坍塌等问题。提出rePIRL框架，该框架受逆强化学习启发，通过交替更新策略和PRM的双学习过程，减少对专家策略的假设。理论分析表明，rePIRL能够统一在线与离线PRM学习方法，实验结果在标准化数学和编程推理数据集上验证了其有效性。此外，该框架支持测试时训练和扩展，并通过消融实验验证了其训练方案和关键设计。</div>
</details>
</div>
<div class="card">
<div class="title">Towards an Understanding of Context Utilization in Code Intelligence</div>
<div class="meta-line">Authors: Yanlin Wang, Kefeng Duan, Dewu Zheng, Ensheng Shi, Fengji Zhang, Yanli Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Hongyu Zhang, Qianxiang Wang, Zibin Zheng</div>
<div class="meta-line">First: 2025-04-11T17:59:53+00:00 · Latest: 2026-02-07T19:37:03+00:00</div>
<div class="meta-line">Comments: Accepted by ACM Computing Surveys</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.08734v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.08734v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Code intelligence is an emerging domain in software engineering, aiming to improve the effectiveness and efficiency of various code-related tasks. Recent research suggests that incorporating contextual information beyond the basic original task inputs (i.e., source code) can substantially enhance model performance. Such contextual signals may be obtained directly or indirectly from sources such as API documentation or intermediate representations like abstract syntax trees can significantly improve the effectiveness of code intelligence. Despite growing academic interest, there is a lack of systematic analysis of context in code intelligence. To address this gap, we conduct an extensive literature review of 146 relevant studies published between September 2007 and August 2024. Our investigation yields four main contributions. (1) A quantitative analysis of the research landscape, including publication trends, venues, and the explored domains; (2) A novel taxonomy of context types used in code intelligence; (3) A task-oriented analysis investigating context integration strategies across diverse code intelligence tasks; (4) A critical evaluation of evaluation methodologies for context-aware methods. Based on these findings, we identify fundamental challenges in context utilization in current code intelligence systems and propose a research roadmap that outlines key opportunities for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代码智能中上下文利用的理解</div>
<div class="mono" style="margin-top:8px">代码智能是软件工程中的新兴领域，旨在提高各种代码相关任务的效果和效率。最近的研究表明，结合超出基本任务输入（即源代码）的上下文信息可以显著提升模型性能。这些上下文信号可能直接或间接地从API文档或抽象语法树等中间表示中获取，从而显著提高代码智能的效果。尽管学术界对此兴趣日益增长，但对代码智能中上下文的系统性分析仍显不足。为解决这一问题，我们对2007年9月至2024年8月间发表的146项相关研究进行了广泛的文献综述。我们的调查得出四个主要贡献：(1) 对研究领域进行定量分析，包括出版趋势、发表平台和研究领域；(2) 提出一种用于代码智能的新型上下文类型分类法；(3) 任务导向的分析，探讨不同代码智能任务中的上下文整合策略；(4) 对上下文感知方法的评估方法进行批判性评估。基于这些发现，我们识别了当前代码智能系统中上下文利用的基本挑战，并提出了一个研究路线图，概述了未来研究的关键机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of context in code intelligence, a field focused on enhancing code-related tasks through advanced techniques. The motivation stems from the observation that incorporating contextual information beyond source code can significantly improve model performance. The authors conducted a comprehensive review of 146 studies from 2007 to 2024, leading to four key contributions: a quantitative analysis of the research landscape, a new taxonomy of context types, a task-oriented analysis of context integration strategies, and a critical evaluation of evaluation methods. The main findings highlight the challenges in effectively utilizing context within current systems and suggest a roadmap for future research.</div>
<div class="mono" style="margin-top:8px">本文探讨了上下文在代码智能中的作用，该领域旨在通过提升有效性与效率来优化代码相关任务。研究指出，除了源代码之外，引入如API文档或抽象语法树等上下文信息可以显著增强模型性能。通过对2007年至2024年间发表的146篇相关文献进行系统综述，作者提供了研究现状的定量分析，提出了新的上下文类型分类体系，分析了不同任务中上下文整合策略，并评估了当前上下文感知方法的评估方法。研究结果揭示了现有代码智能系统在上下文利用方面的主要挑战，并提出了未来研究的关键方向。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning</div>
<div class="meta-line">Authors: Yancheng Long, Yankai Yang, Hongyang Wei, Wei Chen, Tianke Zhang, Haonan fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</div>
<div class="meta-line">First: 2026-02-07T09:23:34+00:00 · Latest: 2026-02-07T09:23:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term &quot;Attention Collapse,&quot; where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialReward：通过显式空间推理弥合图像编辑在线强化学习中的感知鸿沟</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）为复杂的图像编辑提供了有前景的途径，但目前受到可靠且细粒度奖励信号稀缺的限制。现有评估器经常面临我们称之为『注意力崩溃』的关键感知鸿沟问题，即模型忽略了跨图像比较，无法捕捉细粒度细节，导致感知不准确和评分失调。为了解决这些限制，我们提出了SpatialReward，这是一种通过显式空间推理强制进行精确验证的奖励模型。通过将推理锚定在预测的编辑区域，SpatialReward在像素级证据基础上进行语义判断，显著提升了评估的准确性。我们的模型在精心挑选的26万条空间感知数据集上进行训练，在MMRB2和EditReward-Bench上取得了最先进的性能，并在我们提出的MultiEditReward-Bench上优于专有评估器。此外，SpatialReward在在线RL中作为强大的信号，使OmniGen2在GEdit-Bench上的表现提升+0.90，超越了领先的判别模型，并且GPT-4.1的提升幅度翻倍（+0.45）。这些结果表明，空间推理对于解锁图像编辑中的有效对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of online reinforcement learning (RL) in image editing, particularly the lack of reliable and fine-grained reward signals. The proposed method, SpatialReward, introduces explicit spatial reasoning to bridge the perception gap by anchoring semantic judgments to pixel-level evidence from predicted edit regions. Experimental results show that SpatialReward achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on MultiEditReward-Bench. It also significantly improves the performance of OmniGen2 on GEdit-Bench, surpassing the leading discriminative model and doubling the gain of GPT-4.1.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决奖励信号不可靠的问题来提升在线强化学习在图像编辑中的效果。提出的方法SpatialReward引入了显式的空间推理，以弥合感知差距，特别是针对模型在跨图像比较和细节捕捉上的不足，即所谓的&quot;注意力崩溃&quot;。实验结果表明，SpatialReward在MMRB2和EditReward-Bench上达到了最先进的性能，并在MultiEditReward-Bench上优于专有评估器。它还显著提升了OmniGen2在GEdit-Bench上的表现，超越了领先的判别模型，并将GPT-4.1的改进效果翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-07T01:15:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使大型语言模型在推荐系统中首次具备扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这一问题可能源于之前持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，表明其在学习可泛化的用户偏好模式方面具有优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具备稳健的幂律扩展特性。我们的实验表明，在多种合成数据模式下，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of developing Large Language Models (LLMs) for recommender systems by proposing a principled synthetic data generation framework. The motivation stems from the limitations of real user interaction data, which often contain noise, bias, and incompleteness, hindering the establishment of reliable scaling laws. The method introduces a curated, pedagogical curriculum to generate high-quality synthetic data, enabling more effective continual pre-training. The main experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, with a $+130\%$ improvement in recall@100 for SasRec. Additionally, the study demonstrates robust power-law scaling for LLMs trained on the synthetic data, indicating consistent perplexity reduction across different modalities.</div>
<div class="mono" style="margin-top:8px">本文旨在解决推荐系统中大型语言模型（LLMs）开发面临的挑战，其中缺乏可预测的扩展定律阻碍了研究进展。作者提出了一种基于原则的合成数据生成框架，通过构建一个精心设计的、教学性的课程来克服真实用户交互数据中的噪声、偏差和不完整性。实验表明，使用该合成数据训练的模型在排序任务中显著优于使用真实数据训练的模型，SasRec在recall@100指标上提升了130%。此外，他们首次实验证明了在推荐领域持续预训练的LLMs在合成数据上表现出稳健的幂律扩展特性，多种合成数据模态下均实现了可预测的困惑度下降，为LLMs在推荐系统中的可靠扩展奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</div>
<div class="meta-line">Authors: Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</div>
<div class="meta-line">First: 2026-02-06T18:59:59+00:00 · Latest: 2026-02-06T18:59:59+00:00</div>
<div class="meta-line">Comments: 21 pages, 6 figures and 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://genmilab.github.io/MedMO-Page">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO&#x27;s broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedMO：面向医学图像的多模态大语言模型的定位与理解</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）迅速发展，但其在医学领域的应用仍受限于领域覆盖不足、模态对齐和基于上下文的推理能力的差距。本文提出MedMO，这是一个基于通用MLLM架构并仅在大规模、领域特定数据上训练的医学基础模型。MedMO采用多阶段训练方案：(i) 跨模态预训练，将异构视觉编码器与医学语言主干对齐；(ii) 在涵盖图像描述、视觉问答（VQA）、报告生成、检索和基于边界框的疾病定位等多任务监督下进行指令调优；(iii) 采用可验证奖励的强化学习，结合事实性检查和基于边界框的GIoU奖励，以增强复杂临床场景中的空间定位和逐步推理能力。MedMO在多个模态和任务上均优于其他强大的开源医学MLLMs。在VQA基准测试中，MedMO在基线基础上平均准确率提升了13.7%，且性能接近SOTA模型Fleming-VL（仅差1.9%）。在基于文本的问答任务中，其准确率分别比基线提升6.9%和比Fleming-VL提升14.5%。在医学报告生成任务中，MedMO在语义和临床准确性方面均取得显著提升。此外，它展现出强大的定位能力，IoU指标分别比基线提升40.4%和比Fleming-VL提升37.0%，突显其在空间推理和定位方面的稳健性能。在放射学、眼科和病理学显微镜领域的评估验证了MedMO在跨模态任务中的广泛泛化能力。我们发布了两个版本的MedMO：4B和8B。项目地址：https://genmilab.github.io/MedMO-Page</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing multimodal large language models (MLLMs) in medical applications, such as insufficient domain coverage, poor modality alignment, and weak grounded reasoning. MedMO is a medical foundation model developed using a generalized MLLM architecture and trained on large-scale, domain-specific data. It employs a three-stage training process: cross-modal pretraining to align visual and language modalities, instruction tuning on diverse medical tasks including captioning, VQA, report generation, and disease localization, and reinforcement learning with factuality checks and a box-level GIoU reward to enhance spatial reasoning. The model demonstrates significant improvements over existing medical MLLMs, achieving an average accuracy increase of +13.7% on VQA tasks and +14.5% on text-based QA. It also shows substantial gains in medical report generation and spatial grounding, with IoU improvements of +40.4 and +37.0% respectively, across multiple medical modalities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有多模态大语言模型（MLLMs）在医学领域中的不足，包括领域覆盖、模态对齐和基于上下文的推理能力。MedMO 是一个基于通用 MLLM 架构的医学基础模型，专门使用大规模、领域相关的数据进行训练。该模型采用三阶段训练方法：跨模态预训练以对齐视觉编码器与医学语言主干，指令调优以处理包括图像描述、视觉问答、报告生成、检索和基于边界框的疾病定位等多任务，以及结合事实性检查和边界框级 GIoU 奖励的强化学习策略。实验结果表明，MedMO 在多个任务和模态上均优于现有模型，尤其在准确率和空间推理能力方面取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Beyond Redundancy: Task Complexity&#x27;s Role in Vision Token Specialization in VLLMs</div>
<div class="meta-line">Authors: Darryl Hannan, John Cooper, Dylan White, Yijing Watkins</div>
<div class="meta-line">First: 2026-02-06T18:13:01+00:00 · Latest: 2026-02-06T18:13:01+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06914v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越冗余：VLLMs中视觉标记专业化与任务复杂度的关系</div>
<div class="mono" style="margin-top:8px">视觉大语言模型（VLLMs）的视觉能力一直落后于其语言能力。特别是，许多基准研究已经表明，当需要细粒度的视觉信息或空间推理时，VLLMs表现不佳。然而，我们尚未完全理解为什么VLLMs在这些任务上相较于其他模型表现如此困难。一些研究将视觉冗余作为解释，认为高级视觉信息均匀分布在多个标记中，而具体的细粒度视觉信息则被舍弃。在本研究中，我们更深入地探讨这一前提，旨在更好地理解模型如何处理不同类型视觉信息以及哪些信息被舍弃。为此，我们引入了一个简单的人工合成基准数据集，专门用于探测各种视觉特征，并设计了一组衡量视觉冗余的指标，使我们能够更深入地理解它们之间的关系。随后，我们探索了在多个复杂视觉任务上微调VLLMs，以更好地理解冗余和压缩如何随着训练数据复杂度的变化而变化。我们发现任务复杂度与视觉压缩之间存在联系，这意味着拥有足够比例的高复杂度视觉数据对于改变VLLMs的视觉表示分布方式至关重要，从而提高其在复杂视觉任务上的表现。我们希望这项研究能为下一代VLLMs的训练提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why vision large language models (VLLMs) perform poorly on complex visual tasks compared to linguistic ones. It challenges the common assumption that visual redundancy is the main cause by introducing a synthetic benchmark dataset and metrics to analyze visual information distribution. The findings suggest that task complexity influences visual compression, and incorporating high-complexity visual data during training can enhance the model&#x27;s ability to specialize in visual features, thereby improving performance on challenging visual tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了为何视觉大语言模型（VLLMs）在复杂视觉任务上的表现不如语言任务。它质疑了视觉冗余是主要原因的常见假设，通过引入一个合成基准数据集和测量视觉冗余的指标进行深入分析。研究结果表明，任务复杂度与视觉压缩之间存在关联，提示在训练中引入高复杂度的视觉数据对于改善视觉表示的分布和提升复杂视觉任务的表现至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCode: A Benchmark for Evaluating Software Engineering Agents</div>
<div class="meta-line">Authors: Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Simon Alford, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta</div>
<div class="meta-line">First: 2026-02-02T16:04:10+00:00 · Latest: 2026-02-06T15:49:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02262v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02262v2">PDF</a> · <a href="https://github.com/seal-research/OmniCode">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCode：评估软件工程代理的基准</div>
<div class="mono" style="margin-top:8px">LLM驱动的编码代理正在重新定义现实世界软件的开发方式。为了推动对更优编码代理的研究，我们需要能够严格评估此类代理执行各种软件工程任务能力的挑战性基准。然而，像HumanEval和SWE-Bench这样的流行编码基准主要关注狭窄范围的任务，如编程竞赛和补丁生成。实际上，软件工程师在现实世界软件开发中需要处理更广泛的任务。为了解决这一差距，我们提出了OmniCode，一个包含超越代码或补丁生成的更广泛和多样化任务类别的新型软件工程基准。总体而言，OmniCode包含1794个任务，涵盖三种编程语言（Python、Java和C++）以及四个关键类别：错误修复、测试生成、代码审查修复和风格修复。与以往的软件工程基准相比，OmniCode的任务（1）经过人工验证以消除定义不清的问题，（2）合成构建或最近整理以避免数据泄露问题，提供了一种从有限现实数据中合成生成多样化软件任务的新框架。我们使用流行的代理框架如SWE-Agent对OmniCode进行了评估，结果显示虽然它们在Python的错误修复任务中表现良好，但在测试生成任务以及C++和Java等语言中表现不足。例如，在Java测试生成任务中，SWE-Agent使用DeepSeek-V3.1最多仅达到20.9%。OmniCode旨在成为一种稳健的基准，推动开发能够在软件开发不同方面表现优异的代理。代码和数据可在https://github.com/seal-research/OmniCode获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind OmniCode is to provide a comprehensive benchmark for evaluating the capabilities of LLM-powered software engineering agents beyond narrow tasks like code or patch generation. The proposed benchmark includes 1794 tasks across Python, Java, and C++, covering four categories: bug fixing, test generation, code review fixing, and style fixing. These tasks are manually validated and synthetically generated to avoid data leakage, offering a more realistic assessment of agent performance. Evaluation with existing frameworks such as SWE-Agent shows that while they excel in Python bug fixing, they struggle with test generation in Java and C++.</div>
<div class="mono" style="margin-top:8px">OmniCode的提出旨在为评估LLM驱动的编码代理提供更全面的基准，以应对现实软件开发中更广泛的工程任务。与现有专注于竞赛编程或补丁生成的基准不同，OmniCode涵盖了四个关键任务类别：错误修复、测试生成、代码审查修复和风格调整，并支持Python、Java和C++三种编程语言，共包含1794个任务。该数据集通过人工验证和合成生成方式构建，以避免数据泄露问题，为评估编码代理的多样性提供新框架。实验结果显示，当前编码代理如SWE-Agent在Python错误修复方面表现良好，但在Java测试生成等任务上表现欠佳，仅达到20.9%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation</div>
<div class="meta-line">Authors: Biao Xiong, Zhen Peng, Ping Wang, Qiegen Liu, Xian Zhong</div>
<div class="meta-line">First: 2026-02-06T15:36:50+00:00 · Latest: 2026-02-06T15:36:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07100v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07100v1">PDF</a> · <a href="https://github.com/rosolose/TLC-PLAN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TLC-Plan：一种基于两级码本的网络用于端到端向量平面图生成</div>
<div class="mono" style="margin-top:8px">自动化平面图生成旨在通过联合建模全局空间组织和精确几何细节来提高设计质量、建筑效率和可持续性。然而，现有方法在光栅空间中操作，并依赖后处理向量化，这会引入结构不一致并阻碍端到端学习。受组合空间推理的启发，我们提出了TLC-Plan，这是一种分层生成模型，可直接从输入边界合成向量平面图，基于模块化和可重用的模式与人类建筑工作流程对齐。TLC-Plan采用两级VQ-VAE，将全局布局编码为语义标记的房间边界框，并使用多边形级码细化局部几何。这种层次结构通过CodeTree表示统一，同时，一个自回归Transformer根据边界采样码以生成多样且拓扑有效的设计，无需显式的房间拓扑或尺寸先验。大量实验表明，在RPLAN数据集上取得了最先进的性能（FID = 1.84，MSE = 2.06），并在LIFULL数据集上取得了领先结果。所提出的框架推进了面向现实建筑应用的约束感知和可扩展的向量平面图生成。源代码和训练模型已发布在https://github.com/rosolose/TLC-PLAN。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automated floorplan generation seeks to enhance design quality, architectural efficiency, and sustainability by integrating global spatial organization with precise geometric details. Existing methods operate in raster space and require post hoc vectorization, leading to structural inconsistencies and limiting end-to-end learning. To address these issues, TLC-Plan introduces a hierarchical generative model based on a two-level VQ-VAE, which encodes global layouts as semantically labeled room bounding boxes and refines local geometries using polygon-level codes. The model uses a CodeTree representation to unify the hierarchy and an autoregressive transformer to sample codes conditioned on input boundaries, enabling the generation of diverse and topologically valid designs. Experimental results on the RPLAN and LIFULL datasets demonstrate state-of-the-art performance, with FID scores of 1.84 and MSE of 2.06, respectively, highlighting its effectiveness in constraint-aware and scalable vector floorplan generation.</div>
<div class="mono" style="margin-top:8px">自动平面图生成旨在通过整合全局空间组织与精确几何细节来提升设计质量、建筑效率和可持续性。然而，现有方法在栅格空间中操作，并依赖后处理向量化，导致结构不一致并限制端到端学习。为解决这些问题，TLC-Plan 提出了一种基于双级 VQ-VAE 的分层生成模型，将全局布局编码为语义标记的房间边界框，并利用多边形级代码细化局部几何。该模型通过 CodeTree 表示统一这两个层级，并采用自回归 Transformer 根据边界条件采样代码，生成多样且拓扑有效的设计。在 RPLAN 和 LIFULL 数据集上的实验结果表明，该框架在 RPLAN 上取得 FID 1.84 和 MSE 2.06 的最先进性能，在 LIFULL 上也表现出领先结果，展示了其在约束感知和可扩展的矢量平面图生成中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks</div>
<div class="meta-line">Authors: Junxian Li, Kai Liu, Leyang Chen, Weida Wang, Zhixin Wang, Jiaqi Xu, Fan Li, Renjing Pei, Linghe Kong, Yulun Zhang</div>
<div class="meta-line">First: 2026-02-06T12:47:16+00:00 · Latest: 2026-02-06T12:47:16+00:00</div>
<div class="meta-line">Comments: The main part of our paper: PlanViz Code is at: https://github.com/lijunxian111/PlanViz Supplementary material is at: https://github.com/lijunxian111/PlanViz/releases/tag/v1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06663v1">PDF</a> · <a href="https://github.com/lijunxian111/PlanViz">Code1</a> · <a href="https://github.com/lijunxian111/PlanViz/releases/tag/v1">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&amp;UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PlanViz：评估面向计算机使用任务的规划导向图像生成与编辑</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在生成自然图像和多模态推理方面展现出令人印象深刻的能力。然而，它们在支持与日常生活密切相关的计算机使用规划任务方面的潜力仍被低估。在计算机使用任务中，图像生成和编辑需要空间推理和程序理解等能力，目前尚不清楚UMMs是否具备完成这些任务的能力。因此，我们提出了PlanViz，一个专门用于评估计算机使用任务中图像生成与编辑的新基准。为了实现我们的评估目标，我们关注那些频繁出现在日常生活中并需要规划步骤的子任务。具体来说，设计了三个新的子任务：路线规划、工作流程图绘制和网页与用户界面展示。我们通过整理人工标注的问题和参考图像，并实施质量控制流程，来应对数据质量方面的挑战。为了解决全面且精确评估的难题，我们提出了一种任务自适应评分方法PlanScore，该评分有助于理解生成图像的正确性、视觉质量和效率。通过实验，我们突出了该领域研究的关键局限性与未来研究机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces PlanViz, a benchmark designed to evaluate the performance of unified multimodal models (UMMs) in planning-oriented image generation and editing tasks relevant to computer use. The motivation stems from the underexplored potential of UMMs in supporting tasks that require spatial reasoning and procedural understanding, such as route planning, work diagramming, and web&amp;UI displaying. The authors curate human-annotated questions and reference images, implement a quality control process, and propose a task-adaptive evaluation metric called PlanScore to assess correctness, visual quality, and efficiency. Experimental results highlight the current limitations of UMMs in these tasks and identify opportunities for future research.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索统一多模态模型（UMMs）在支持计算机使用任务中的规划导向图像生成与编辑方面的潜力，这些任务与日常生活密切相关。作者提出了PlanViz，一个包含三个子任务的新基准：路线规划、工作流程图绘制和网页与用户界面展示，这些任务需要空间推理和过程理解能力。为确保数据质量，他们整理了人工标注的问题和参考图像，并实施了质量控制流程。此外，他们提出了一种任务自适应的评估指标PlanScore，用于衡量生成图像的正确性、视觉质量和效率。实验结果揭示了当前UMMs在这些规划任务中的关键局限性，并指出了未来研究的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</div>
<div class="meta-line">Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</div>
<div class="meta-line">First: 2026-02-05T18:59:55+00:00 · Latest: 2026-02-06T06:50:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06041v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06041v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视角描述预测相机姿态以实现空间推理</div>
<div class="mono" style="margin-top:8px">多图像空间推理对于当前的多模态大语言模型（MLLMs）仍然是一个具有挑战性的任务。虽然单视角感知本质上是2D的，但跨视角推理需要在不同视角之间建立一致的场景理解。特别地，我们研究视角转换，即模型必须从多视角观察中构建一致的3D理解，并据此在新的语言指定视角下进行推理。我们提出了CAMCUE，这是一个具有姿态感知的多图像框架，它将相机姿态作为跨视角融合和新视角推理的显式几何锚点。CAMCUE将每个视角的姿态注入到视觉标记中，将自然语言视角描述锚定到目标相机姿态，并合成基于姿态的想象目标视角以支持回答。为了支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例配对了多视角图像和姿态，并与多样化的目标视角描述和视角转换问题相关联。我们还在测试集中包含了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确率提高了9.06%，并且能够从自然语言视角描述中预测目标姿态，其旋转准确率超过90%（在20°以内），平移准确率在0.5误差阈值内。这种直接的锚定避免了昂贵的测试时搜索和匹配过程，将推理时间从每个实例256.6秒减少到1.45秒，从而实现了快速、交互式的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of multi-image spatial reasoning in current multimodal large language models (MLLMs), which struggle to build coherent 3D scene understanding across different viewpoints. The proposed method, CAMCUE, introduces a pose-aware framework that explicitly uses camera poses as geometric anchors for cross-view fusion and novel-view reasoning. It integrates per-view camera poses into visual tokens, maps natural language viewpoint descriptions to target poses, and synthesizes imagined target views to facilitate reasoning. The CAMCUE-DATA dataset, containing 27,668 training and 508 test instances, is used to evaluate the model&#x27;s performance. Experimental results show that CAMCUE improves overall accuracy by 9.06%, achieving over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This approach enables efficient inference, reducing time per example from 256.6s to 1.45s, and supports real-time, interactive applications.</div>
<div class="mono" style="margin-top:8px">该研究针对当前多模态大语言模型（MLLMs）在多图像空间推理中的挑战，特别是如何在不同视角间建立连贯的3D场景理解。提出的方法CAMCUE是一个基于相机姿态的框架，通过将相机姿态作为跨视角融合的几何锚点，实现对新视角的推理。该方法将每视角的相机姿态注入视觉标记中，将自然语言视角描述映射到目标姿态，并合成基于姿态的想象目标视角以支持推理。在新构建的CAMCUE-DATA数据集上，实验结果显示整体准确率提升了9.06%，旋转准确率超过90%（误差在20°以内），平移准确率在0.5误差阈值内。这种方法避免了昂贵的测试时搜索匹配过程，将推理时间从256.6秒降至1.45秒每例，从而支持实时交互应用。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
