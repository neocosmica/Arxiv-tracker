<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-21 04:01</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260221_0401</div>
    <div class="row"><div class="card">
<div class="title">HiMAP: History-aware Map-occupancy Prediction with Fallback</div>
<div class="meta-line">Authors: Yiming Xu, Yi Yang, Hao Cheng, Monika Sester</div>
<div class="meta-line">First: 2026-02-19T10:24:02+00:00 · Latest: 2026-02-19T10:24:02+00:00</div>
<div class="meta-line">Comments: Accepted in 2026 IEEE International Conference on Robotics and Automation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17231v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17231v1">PDF</a> · <a href="https://github.com/XuYiMing83/HiMAP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiMAP：具有回退机制的历史感知地图占用预测</div>
<div class="mono" style="margin-top:8px">准确的运动预测对于自动驾驶至关重要，但大多数预测方法依赖于基于身份关联的多目标跟踪（MOT），假设目标能够被正确且连续地跟踪。当由于遮挡、身份切换或检测遗漏等原因导致跟踪失败时，预测质量会下降，安全风险增加。我们提出了\textbf{HiMAP}，一种无需跟踪的轨迹预测框架，在MOT失败时仍能保持可靠性。HiMAP将过去的检测转换为时空不变的历史地图占用表示，并引入一个历史查询模块，根据当前智能体状态从无标签的占用表示中迭代检索特定智能体的历史信息。检索到的历史信息通过时间映射嵌入进行总结，并与最终查询和地图上下文一起驱动一种类似DETR的解码器，生成多模态的未来轨迹。这种设计消除了对身份的依赖，支持通过可重用编码进行流式推理，并在跟踪不可用时作为稳健的回退方案。在Argoverse~2数据集上，HiMAP在不使用身份信息的情况下实现了与基于跟踪方法相当的性能，并在无跟踪设置下显著优于强大的基线模型，相较于微调后的QCNet，在FDE上相对提升11\%，在ADE上相对提升12\%，MR降低4\%。除了整体指标外，HiMAP能够在不等待跟踪恢复的情况下同时为所有智能体提供稳定的预测，突显了其在安全关键型自动驾驶中的实用价值。代码可在以下链接获取：https://github.com/XuYiMing83/HiMAP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">HiMAP is proposed to address the limitations of motion prediction in autonomous driving systems, particularly when multi-object tracking (MOT) fails due to occlusion, identity switches, or missed detections. Unlike traditional methods that rely on object identities, HiMAP introduces a tracking-free framework that converts past detections into spatiotemporally invariant historical occupancy maps. It employs a historical query module to retrieve agent-specific history from unlabeled occupancy data, which is then summarized into a temporal map embedding. This embedding, combined with the current agent state and map context, guides a DETR-style decoder to generate multi-modal future trajectories. HiMAP achieves performance comparable to tracking-based methods on Argoverse~2 without using object IDs and significantly outperforms baselines in the no-tracking setting, with improvements in FDE, ADE, and MR.</div>
<div class="mono" style="margin-top:8px">HiMAP旨在解决自动驾驶中多目标跟踪（MOT）失效时运动预测的局限性，例如遮挡或身份切换等情况。与依赖目标身份的传统方法不同，HiMAP采用无跟踪方法，通过将历史检测转换为时空不变的历史占用地图来实现。它引入了一个历史查询模块，从无标签的占用表示中检索特定代理的历史信息，并利用时间占用嵌入引导一个DETR风格的解码器预测多模态的未来轨迹。在Argoverse~2上的实验结果表明，HiMAP在不使用目标ID的情况下实现了与基于跟踪方法相当的性能，并在无跟踪设置下显著优于基线模型，FDE、ADE指标分别提升11%和12%，MR降低4%。</div>
</details>
</div>
<div class="card">
<div class="title">Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering</div>
<div class="meta-line">Authors: Kishan Maharaj, Nandakishore Menon, Ashita Saxena, Srikanth Tamilselvam</div>
<div class="meta-line">First: 2026-02-19T09:05:03+00:00 · Latest: 2026-02-19T09:05:03+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 Figures, 5 Tables, Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17183v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17183v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型在长上下文代码问答中的鲁棒性与推理保真度</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）越来越多地用于需要对长代码上下文进行推理的软件工程任务，但其在不同输入条件下的鲁棒性仍不明确。我们通过受控的消融实验系统地研究了长上下文代码问答的敏感性，包括答案格式、干扰项和上下文规模。我们扩展了LongCodeBench Python数据集，新增了COBOL和Java的问答集，并在三种设置下评估了最先进的模型：(i) 混淆的多项选择题，(ii) 开放式问题，以及(iii) 包含相关和对抗性无关信息的“大海捞针”式上下文。结果表明，在混淆的多项选择题和开放式问题中，模型性能显著下降，并且在存在无关提示的情况下表现出脆弱的行为。我们的研究突显了当前长上下文评估的局限性，并为评估传统和现代系统中的代码推理提供了更广泛的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the robustness and reasoning fidelity of large language models in long-context code question answering, motivated by the increasing use of LLMs in software engineering tasks. The authors evaluate state-of-the-art models using controlled ablations across three settings: shuffled multiple-choice options, open-ended questions, and contexts with irrelevant information. Their results reveal significant performance degradation in the presence of shuffled options and open-ended queries, as well as fragile responses when exposed to adversarial irrelevant cues, indicating limitations in current long-context evaluation methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨大型语言模型在长上下文代码问答任务中的鲁棒性和推理可信度，鉴于LLMs在软件工程任务中的广泛应用。作者通过三种设置评估最先进的模型：打乱的多项选择选项、开放性问题以及包含相关和对抗性无关信息的上下文。实验结果表明，在打乱选项和开放性问题中模型性能显著下降，并在存在无关提示时表现出脆弱的行为，揭示了当前长上下文评估方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses</div>
<div class="meta-line">Authors: Kan Watanabe, Rikuto Tsuchida, Takahiro Monno, Bin Huang, Kazuma Yamasaki, Youmei Fan, Kazumasa Shimari, Kenichi Matsumoto</div>
<div class="meta-line">First: 2026-02-19T05:06:31+00:00 · Latest: 2026-02-19T05:06:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17084v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17084v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何沟通：对拉取请求描述特征和人类审查响应的研究</div>
<div class="mono" style="margin-top:8px">大型语言模型的快速采用促使了AI编码代理在GitHub上自主创建拉取请求的出现。然而，这些代理在拉取请求描述特征上的差异，以及人类审查者如何响应它们，仍缺乏深入研究。在本研究中，我们使用AIDev数据集对五个AI编码代理创建的拉取请求进行了实证分析。我们分析了代理在拉取请求描述特征上的差异，包括结构特征，并从审查活动、响应时间、情感和合并结果等方面考察了人类审查者的响应。我们发现，AI编码代理表现出不同的拉取请求描述风格，这些风格与审查者的参与度、响应时间和合并结果存在关联。我们观察到，代理在审查者互动指标和合并率上存在显著差异。这些发现突显了拉取请求呈现方式和审查者互动动态在人机协作软件开发中的作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents generate pull request descriptions and how human reviewers respond to them. With the increasing use of large language models in software development, understanding the characteristics of AI-generated pull requests and their impact on human review processes is crucial. By analyzing pull requests from five AI coding agents using the AIDev dataset, the research identifies differences in description styles and their correlations with reviewer engagement, response time, and merge outcomes. The findings reveal that the way AI agents present their changes significantly influences human reviewer behavior and the likelihood of successful merges.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理生成拉取请求描述的特点以及人类审阅者对此的反应。随着大型语言模型在软件开发中的广泛应用，理解AI生成的拉取请求特征及其对人类审阅流程的影响变得尤为重要。利用AIDev数据集，研究比较了五个AI编码代理，分析了它们的拉取请求描述风格，并考察了由此引发的审阅者参与度、响应时间、情感倾向和合并结果。研究结果表明，不同的代理生成的拉取请求描述存在显著差异，这些差异影响了审阅者的互动行为和合并成功率，突显了有效沟通在人机协作软件开发中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">Wink: Recovering from Misbehaviors in Coding Agents</div>
<div class="meta-line">Authors: Rahul Nanda, Chandra Maddila, Smriti Jha, Euna Mehnaz Khan, Matteo Paltenghi, Satish Chandra</div>
<div class="meta-line">First: 2026-02-19T03:15:00+00:00 · Latest: 2026-02-19T03:15:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user&#x27;s instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wink：在编码代理中恢复误行为</div>
<div class="mono" style="margin-top:8px">基于大型语言模型（LLMs）的自主编码代理正越来越多地被软件行业采用，以自动化复杂的工程任务。然而，这些代理容易出现各种误行为，例如偏离用户指令、陷入重复循环，或未能正确使用工具。这些失败会干扰开发流程，通常需要耗费大量资源的人工干预。在本文中，我们提出了一种大规模自动恢复代理误行为的系统。我们首先基于生产流量分析，引入了一种误行为分类体系，识别出三种主要类别：规范漂移、推理问题和工具调用失败，我们发现这些误行为约占所有代理轨迹的30%。
为了解决这些问题，我们开发了一种轻量级、异步的自我干预系统，名为Wink。Wink观察代理轨迹，并提供有针对性的纠偏指导，以引导代理回到高效的工作路径。我们在超过10,000条真实代理轨迹上评估了该系统，发现它成功解决了90%需要单次干预的误行为。此外，在我们的生产环境中进行的实时A/B测试表明，该系统在工具调用失败、每会话的标记数和每会话的工程师干预次数方面均实现了统计显著的减少。我们分享了设计和部署该系统的经验，提供了在大规模构建弹性代理系统时面临的挑战的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing adoption of autonomous coding agents in the software industry has highlighted the need for robust recovery mechanisms due to frequent misbehaviors such as specification drift, reasoning problems, and tool call failures, which occur in about 30% of agent trajectories. To address these issues, the authors developed Wink, a lightweight and asynchronous self-intervention system that observes agent behavior and provides targeted guidance to correct deviations. Evaluation on over 10,000 real-world trajectories showed Wink resolves 90% of single-intervention misbehaviors, and a live A/B test demonstrated significant reductions in tool call failures, tokens per session, and engineer interventions per session.</div>
<div class="mono" style="margin-top:8px">随着自主编码代理在软件行业中的广泛应用，其行为偏差、推理错误和工具调用失败等问题日益突出，这些故障会干扰开发流程并需要大量的人工干预。本文提出Wink，一个轻量级的异步自我干预系统，用于观察代理行为并提供针对性的纠正指导，使其回归有效工作状态。在超过10,000条真实轨迹上的评估显示，Wink能够成功解决90%需要单次干预的行为问题，并在生产环境的A/B测试中显著降低了工具调用失败、每会话的标记数以及工程师干预次数。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Multiagent Learning Algorithms with Large Language Models</div>
<div class="meta-line">Authors: Zun Li, John Schultz, Daniel Hennes, Marc Lanctot</div>
<div class="meta-line">First: 2026-02-18T22:41:00+00:00 · Latest: 2026-02-18T22:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16928v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16928v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型发现多智能体学习算法</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）在不完美信息博弈中的进展历史上很大程度上依赖于手动迭代优化基线。尽管基础方法家族如反事实遗憾最小化（CFR）和策略空间响应预言机（PSRO）建立在坚实的理论基础上，但它们最有效的变体设计往往依赖于人类直觉来探索庞大的算法设计空间。在本工作中，我们提出使用AlphaEvolve，一种由大语言模型驱动的进化编码智能体，以自动发现新的多智能体学习算法。我们通过在两种不同的博弈论学习范式中演化出新颖的变体，展示了该框架的通用性。首先，在迭代遗憾最小化领域，我们演化出控制遗憾累积和策略推导的逻辑，发现了一种新的算法：波动自适应折扣（VAD-CFR）。VAD-CFR采用新颖且非直观的机制，包括波动敏感的折扣、一致性强化的乐观策略以及硬式预热策略累积计划，从而超越了诸如折扣预测CFR+等最先进的基线。其次，在基于种群的训练算法领域，我们演化出PSRO的训练时和评估时元策略求解器，发现了一种新的变体：平滑混合乐观遗憾（SHOR-PSRO）。SHOR-PSRO引入了一种混合元策略求解器，通过线性融合乐观遗憾匹配与平滑、温度控制的最佳纯策略分布来实现。通过在训练过程中动态调整该融合因子和平滑多样性奖励，该算法实现了从种群多样性到严格均衡寻找的自动化过渡，相较于标准静态元策略求解器表现出更优的实证收敛性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to address the challenge of manually designing effective multiagent learning algorithms in imperfect-information games. The authors propose using AlphaEvolve, an evolutionary coding agent based on large language models, to automatically discover new algorithms. They apply this framework to two game-theoretic learning paradigms, resulting in two novel algorithms: VAD-CFR for iterative regret minimization and SHOR-PSRO for population-based training. VAD-CFR introduces mechanisms such as volatility-sensitive discounting and consistency-enforced optimism, achieving better performance than existing baselines. SHOR-PSRO combines Optimistic Regret Matching with a smoothed strategy distribution, dynamically adjusting parameters to improve convergence and equilibrium finding.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在不完全信息博弈中手动设计有效多智能体学习算法的挑战，传统方法如CFR和PSRO往往依赖人类直觉。作者提出使用基于大语言模型的进化编码代理AlphaEvolve来自动发现新算法。他们将该框架应用于两个范式：迭代遗憾最小化和基于种群的训练。在第一个范式中，他们演化出VAD-CFR算法，引入了波动敏感折扣和一致性强化乐观机制，优于现有基线。在第二个范式中，他们开发了SHOR-PSRO，一种在训练过程中动态融合策略的混合元策略求解器，表现出优于标准静态求解器的收敛性能。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid-Gym: Training Coding Agents to Generalize Across Tasks</div>
<div class="meta-line">Authors: Yiqing Xie, Emmy Liu, Gaokai Zhang, Nachiket Kotalwar, Shubham Gandhi, Sathwik Acharya, Xingyao Wang, Carolyn Rose, Graham Neubig, Daniel Fried</div>
<div class="meta-line">First: 2026-02-18T19:30:55+00:00 · Latest: 2026-02-18T19:30:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16819v1">PDF</a> · <a href="https://github.com/yiqingxyq/Hybrid-Gym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Hybrid-Gym: 训练编码代理以跨任务泛化</div>
<div class="mono" style="margin-top:8px">在评估编码代理的质量时，主流基准主要关注在GitHub上解决单一问题，例如SWE-Bench。相反，在实际使用中，这些代理需要解决更多样化和复杂的任务，这些任务涉及探索代码库、测试软件和设计架构等其他技能。在本文中，我们首先通过将轨迹分解为细粒度组件，来表征一些跨多样化任务共享的可迁移技能，并推导出一套设计辅助训练任务的原则，以教授语言模型这些技能。基于这些原则，我们提出了一种训练环境Hybrid-Gym，包含一系列可扩展的合成任务，如函数定位和依赖搜索。实验表明，使用我们合成任务训练的代理能够有效泛化到训练中未出现的多样化现实任务，在SWE-Bench Verified上提升25.4%，在SWT-Bench Verified上提升7.9%，在Commit-0 Lite上提升5.1%。Hybrid-Gym还能够补充下游任务构建的数据集（例如，在SWT-Bench Verified上提升SWE-Play 4.9%）。代码可在：https://github.com/yiqingxyq/Hybrid-Gym 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing coding agent benchmarks, which primarily evaluate models on solving single, specific issues on GitHub. The authors propose Hybrid-Gym, a training environment designed to teach language models transferable skills such as code exploration, testing, and architecture design through scalable synthetic tasks. Experimental results demonstrate that agents trained on Hybrid-Gym show significant improvements in generalization across diverse real-world tasks, achieving absolute gains of 25.4% on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Additionally, Hybrid-Gym enhances the performance of existing datasets for downstream tasks.</div>
<div class="mono" style="margin-top:8px">本文针对现有编码代理基准测试的局限性，指出这些测试主要评估模型解决单一特定编码任务的能力。作者提出Hybrid-Gym，这是一个通过合成任务（如函数定位和依赖搜索）来训练语言模型掌握可迁移技能的环境。实验结果显示，使用Hybrid-Gym训练的代理在多样化的真实任务上表现出显著的泛化能力，分别在SWE-Bench Verified、SWT-Bench Verified和Commit-0 Lite上取得25.4%、7.9%和5.1%的绝对提升。此外，Hybrid-Gym还能提升现有下游任务数据集的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Situated Awareness in the Real World</div>
<div class="meta-line">Authors: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan, Jiajun Wu, Ming-Hsuan Yang, Xin Eric Wang</div>
<div class="meta-line">First: 2026-02-18T18:22:52+00:00 · Latest: 2026-02-18T18:22:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16682v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent&#x27;s viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model&#x27;s observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在现实世界中学习情境意识</div>
<div class="mono" style="margin-top:8px">人类感知的一个核心方面是情境意识，即能够将自己与周围的物理环境建立联系，并在特定情境下推理可能的动作。然而，大多数现有的多模态基础模型（MFMs）基准测试强调以环境为中心的空间关系（场景中物体之间的关系），而忽视了需要以观察者为中心进行推理的关系，例如相对于智能体视角、姿态和运动的空间推理。为弥合这一差距，我们引入了SAW-Bench（现实世界中的情境意识），这是一个全新的基准测试，用于评估以观察者为中心的情境意识，使用现实世界的视频进行测试。SAW-Bench包含786段使用Ray-Ban Meta（Gen 2）智能眼镜自行录制的视频，涵盖多样化的室内外环境，并包含超过2,071对人工标注的问题-答案对。它通过六个不同的意识任务来探测模型对以观察者为中心的理解能力。我们的全面评估显示，即使使用表现最好的多模态基础模型Gemini 3 Flash，人类与模型之间仍存在37.66%的性能差距。此外，我们的深入分析揭示了几个值得注意的发现；例如，虽然模型可以在以观察者为中心的视频中利用部分几何线索，但它们常常无法推断出连贯的摄像机几何结构，导致系统性的空间推理错误。我们将SAW-Bench定位为情境空间智能的基准测试，超越被动观察，转向对物理基础、以观察者为中心动态的理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the limitation of existing multimodal foundation models (MFMs) in capturing observer-centric spatial relationships, which are essential for situated awareness. SAW-Bench, a new benchmark using real-world videos from Ray-Ban Meta smart glasses, evaluates models&#x27; ability to understand and reason about the environment from an agent&#x27;s perspective. The benchmark includes 786 videos and over 2,071 annotated question-answer pairs, testing six awareness tasks. Evaluation shows a significant performance gap between humans and models, with the best MFM, Gemini 3 Flash, achieving only 37.66% accuracy, highlighting challenges in coherent spatial reasoning from first-person viewpoints.</div>
<div class="mono" style="margin-top:8px">本文旨在评估多模态基础模型（MFMs）中的具身意识，这是实现类人感知和行动推理的关键。作者提出了SAW-Bench，一个基于真实世界视频的新基准，用于检验以观察者为中心的理解能力。该数据集包含786段使用智能眼镜录制的视频和超过2071个人工标注的问题-答案对，涵盖多种环境和六个意识任务。评估结果显示，即使使用表现最好的MFMs——Gemini 3 Flash，人类与模型之间仍存在37.66%的性能差距。分析表明，模型在第一视角视频中难以准确推断相机几何结构，导致系统性的空间推理错误。</div>
</details>
</div>
<div class="card">
<div class="title">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</div>
<div class="meta-line">Authors: Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield</div>
<div class="meta-line">Venue: CVPR 2025 Oral</div>
<div class="meta-line">First: 2024-11-25T16:21:34+00:00 · Latest: 2026-02-18T04:26:35+00:00</div>
<div class="meta-line">Comments: CVPR 2025 (Oral); Project Website: https://chanh.ee/RoboSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.16537v5">Abs</a> · <a href="https://arxiv.org/pdf/2411.16537v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image datasets that often lack sophisticated spatial understanding. For example, datasets frequently do not capture reference frame comprehension, yet effective spatial reasoning requires understanding whether to reason from ego-, world-, or object-centric perspectives. To address this issue, we introduce RoboSpatial, a large-scale dataset for spatial understanding in robotics. It consists of real indoor and tabletop scenes, captured as 3D scans and egocentric images, and annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial relationships, and the pairing of 2D egocentric images with 3D scans makes it both 2D- and 3D- ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robot manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSpatial：为机器人赋予空间理解能力的2D和3D视觉-语言模型</div>
<div class="mono" style="margin-top:8px">空间理解是使机器人能够感知环境、推理环境并有意义地与之交互的关键能力。在现代机器人技术中，这些能力越来越多地由视觉-语言模型提供。然而，这些模型在空间推理任务中面临重大挑战，因为它们的训练数据基于通用图像数据集，通常缺乏复杂的空间理解。例如，数据集经常无法捕捉参考系的理解，而有效的空间推理需要明确是从自体中心、世界中心还是物体中心视角进行推理。为了解决这一问题，我们引入了RoboSpatial，这是一个用于机器人空间理解的大规模数据集。它包含真实室内外和桌面场景，以3D扫描和第一视角图像形式采集，并标注了与机器人相关的丰富空间信息。该数据集包含100万张图像、5000个3D扫描和300万个标注的空间关系，且2D第一视角图像与3D扫描的配对使其同时适用于2D和3D任务。我们的实验表明，使用RoboSpatial训练的模型在空间能力预测、空间关系预测和机器人操作等下游任务中均优于基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboSpatial addresses the challenge of spatial reasoning in vision-language models used for robotics by introducing a large-scale dataset that captures real indoor and tabletop scenes through 3D scans and egocentric images, annotated with rich spatial information. The dataset includes 1M images, 5k 3D scans, and 3M spatial relationships, designed to support both 2D and 3D models. Experimental results demonstrate that models trained on RoboSpatial achieve better performance on tasks like spatial affordance prediction, spatial relationship prediction, and robot manipulation compared to existing baselines.</div>
<div class="mono" style="margin-top:8px">RoboSpatial旨在解决现有视觉-语言模型在空间推理任务中的不足，通过提供专门针对机器人应用的大规模数据集来提升其空间理解能力。该数据集包含真实场景的室内和桌面环境，结合2D视角图像与3D扫描，并标注了丰富的空间关系信息。实验结果表明，使用RoboSpatial训练的模型在空间能力预测、空间关系预测和机器人操作等下游任务中，表现优于传统基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis</div>
<div class="meta-line">Authors: Tianwei Lin, Zhongwei Qiu, Wenqiao Zhang, Jiang Liu, Yihan Xie, Mingjian Gao, Zhenxuan Fan, Zhaocheng Li, Sijing Li, Zhongle Xie, Peng LU, Yueting Zhuang, Yingda Xia, Ling Zhang, Beng Chin Ooi</div>
<div class="meta-line">First: 2026-02-18T00:42:41+00:00 · Latest: 2026-02-18T00:42:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16110v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16110v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCT：面向全面CT分析的统一切片-体积LVLM</div>
<div class="mono" style="margin-top:8px">计算机断层扫描（CT）是最广泛使用且诊断信息密集的成像方式之一，涵盖心脏、肺、肝脏和结肠等关键器官。临床解释依赖于切片驱动的局部特征（如亚厘米结节、病灶边界）和体积驱动的空间表示（如肿瘤浸润、器官间解剖关系）。然而，现有的大型视觉-语言模型（LVLMs）在CT切片与体积理解方面仍存在碎片化问题：切片驱动的LVLMs具有较强的泛化能力，但缺乏跨切片的空间一致性；而体积驱动的LVLMs能够显式捕捉体积语义，但存在粒度粗糙且与切片输入兼容性差的问题。缺乏统一的建模范式是医学LVLMs临床转化的主要瓶颈。我们提出了OmniCT，一种强大的统一切片-体积LVLM，其贡献包括：(i) 空间一致性增强（SCE）：结合体积切片组合与三轴位置嵌入以引入体积一致性，并通过MoE混合投影实现高效的切片-体积适应；(ii) 器官级语义增强（OSE）：分割和感兴趣区域（ROI）定位显式对齐解剖区域，强调病灶和器官级语义；(iii) MedEval-CT：最大的切片-体积CT数据集和混合基准，整合了全面的评估指标。OmniCT在多种临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理需求。更重要的是，它为跨模态医学影像理解建立了一个新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">OmniCT is developed to address the limitations of existing Large Vision-Language Models (LVLMs) in handling both slice and volumetric CT data. It introduces a unified modeling approach that enhances spatial consistency through volumetric slice composition and tri-axial positional embedding, while also improving semantic understanding at the organ level via explicit segmentation and ROI localization. The model is evaluated on the newly proposed MedEval-CT dataset, which includes both slice and volume-based tasks, and demonstrates significant performance improvements across various clinical applications, effectively bridging the gap between local and global CT analysis.</div>
<div class="mono" style="margin-top:8px">OmniCT的研究动机是解决现有大型视觉-语言模型（LVLMs）在处理CT切片和体积数据时的不足。为此，OmniCT提出了一种统一的切片-体积LVLM框架，包含三个主要贡献：空间一致性增强（SCE），通过体积组合和三轴位置嵌入提升跨切片的空间一致性，并利用MoE混合投影实现高效的切片-体积适配；器官级语义增强（OSE），通过分割和感兴趣区域（ROI）定位对解剖区域进行显式对齐，强调器官和病灶级别的语义；以及MedEval-CT，这是目前最大的切片-体积CT数据集和混合基准，支持统一评估。实验结果表明，OmniCT在多种临床任务中显著优于现有方法，展现出对微细节检测和宏观空间推理的强大能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-17T22:24:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到政策：为内容感知布局设计增强语言模型的空间推理</div>
<div class="mono" style="margin-top:8px">我们引入了LaySPA，这是一个强化学习框架，为大型语言模型（LLMs）赋予明确且可解释的空间推理能力，以实现内容感知的图形布局设计。LaySPA解决了两个关键挑战：LLMs在空间推理方面的局限性以及设计决策过程中的不透明性。我们不直接在像素层面操作，而是将布局设计重新表述为一个在结构化文本空间环境上的策略学习问题，该环境明确编码了画布几何、元素属性和元素间关系。LaySPA生成包含可解释推理轨迹和结构化布局规范的双层级输出，从而实现透明且可控的设计决策。布局设计策略通过多目标空间批评进行优化，将布局质量分解为几何有效性、关系一致性以及审美一致性，并采用相对群体优化方法在开放性设计空间中稳定学习。实验表明，LaySPA在结构有效性与视觉质量方面均有提升，优于更大的专有LLMs，并在使用更少标注样本和更低延迟的情况下，其性能与专门化的SOTA布局生成器相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces LaySPA, a reinforcement learning framework designed to enhance the spatial reasoning capabilities of large language models (LLMs) for content-aware graphic layout design. The motivation stems from the limitations of current LLMs in understanding spatial relationships and the opacity of design decisions. LaySPA reformulates layout design as a policy learning problem in a structured textual spatial environment, encoding canvas geometry, element attributes, and inter-element relationships. The framework produces interpretable reasoning traces and structured layout specifications, allowing for transparent and controllable design. It optimizes layout policies using a multi-objective spatial critique that evaluates geometric validity, relational coherence, and aesthetic consistency, and employs relative group optimization to stabilize training. Experimental results show that LaySPA improves structural validity and visual quality, surpassing larger proprietary LLMs and matching the performance of specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">本文提出LaySPA，一种强化学习框架，旨在提升大型语言模型（LLMs）在内容感知图形布局设计中的空间推理能力。研究动机源于当前LLMs在空间理解方面的不足以及设计决策过程的不透明性。LaySPA将布局设计问题重新表述为在结构化文本空间环境中的策略学习任务，该环境显式编码了画布几何、元素属性及元素间关系。框架生成双层级输出，包括可解释的推理轨迹和结构化布局规范，以实现透明和可控的设计决策。实验结果表明，LaySPA在结构有效性与视觉质量方面表现优异，超越了更大规模的专有LLMs，并在标注样本更少、延迟更低的情况下达到了与专用顶级布局生成器相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families</div>
<div class="meta-line">Authors: Yuval Levental</div>
<div class="meta-line">First: 2026-02-17T19:06:19+00:00 · Latest: 2026-02-17T19:06:19+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, 2 tables. Workshop-length paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15950v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15950v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型能识别正方形吗？文本识别在三种模型家族中中介了空间推理</div>
<div class="mono" style="margin-top:8px">我们提出一个简单的实验，揭示了视觉-语言模型（VLMs）的一个基本限制：当单元格没有文本标识时，无法准确定位二进制网格中的填充单元格。我们生成了十五个15x15的网格，填充密度从10.7%到41.8%不等，并将每个网格渲染为两种图像类型——文本符号（. 和 #）和无网格线的填充正方形。然后我们要求三个前沿的VLMs（Claude Opus、ChatGPT 5.2和Gemini 3 Thinking）进行转录。在文本符号条件下，Claude和ChatGPT分别达到约91%的单元格准确率和84%的F1分数，而Gemini达到84%准确率和63% F1分数。在填充正方形条件下，所有三个模型的准确率都降至60-73%，F1分数降至29-39%。关键的是，所有条件都通过相同的视觉编码器——文本符号是图像，而非标记化文本。文本与正方形之间的F1分数差距在模型间从34到54分不等，这表明VLMs似乎具备一种高保真度的文本识别路径，用于空间推理，其表现远超其原生的视觉路径。每个模型在正方形条件下都表现出不同的失败模式——系统性低估（Claude）、严重高估（ChatGPT）以及模板幻觉（Gemini）——但它们都共享相同的根本缺陷：对非文本视觉元素的空间定位严重退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the spatial reasoning capabilities of vision-language models (VLMs) by testing their ability to transcribe filled cells in binary grids. The experiment uses two image representations: text symbols (e.g., . and #) and filled squares without gridlines. While VLMs perform well with text symbols, achieving high accuracy and F1 scores, their performance drops significantly with filled squares, indicating a fundamental limitation in spatial localization for non-textual visual elements. The results show that the F1 score gap between text and square conditions ranges from 34 to 54 points across models, suggesting that VLMs rely heavily on text-recognition pathways for spatial tasks, which are less effective for visual elements without textual identity.</div>
<div class="mono" style="margin-top:8px">本研究探讨了视觉语言模型（VLMs）在空间推理中的一个基本限制，即无法准确定位二进制网格中的非文本视觉元素，如填充的正方形。研究人员设计了一个实验，使用15x15的网格，填充单元格密度不同，并将其渲染为文本符号（如.和#）或无网格线的填充正方形。三个先进的VLMs在文本符号条件下表现优于填充正方形条件。实验结果表明，VLMs在空间推理中严重依赖文本识别能力，其原生的视觉处理能力明显不足，导致各模型在填充正方形条件下表现出不同的但相关的失败模式。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Gradients for Cumulative Prospect Theory in Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Lepel, Anas Barakat</div>
<div class="meta-line">First: 2024-10-03T15:45:39+00:00 · Latest: 2026-02-17T17:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.02605v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.02605v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive a policy gradient theorem for Cumulative Prospect Theory (CPT) objectives in finite-horizon Reinforcement Learning (RL), generalizing the standard policy gradient theorem and encompassing distortion-based risk objectives as special cases. Motivated by behavioral economics, CPT combines an asymmetric utility transformation around a reference point with probability distortion. Building on our theorem, we design a first-order policy gradient algorithm for CPT-RL using a Monte Carlo gradient estimator based on order statistics. We establish statistical guarantees for the estimator and prove asymptotic convergence of the resulting algorithm to first-order stationary points of the (generally non-convex) CPT objective. Simulations illustrate qualitative behaviors induced by CPT and compare our first-order approach to existing zeroth-order methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中基于累积前景理论的策略梯度</div>
<div class="mono" style="margin-top:8px">我们在有限时间范围的强化学习（RL）中推导出一个针对累积前景理论（CPT）目标的策略梯度定理，该定理推广了标准的策略梯度定理，并涵盖了基于概率扭曲的风险目标作为特例。受行为经济学启发，CPT结合了围绕参考点的非对称效用变换和概率扭曲。基于我们的定理，我们设计了一个基于顺序统计量的蒙特卡洛梯度估计器的一阶策略梯度算法用于CPT-RL。我们建立了该估计器的统计保证，并证明了所得到的算法在（通常非凸的）CPT目标上渐近收敛于一阶平稳点。仿真展示了CPT引发的定性行为，并将我们的第一阶方法与现有的零阶方法进行了比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to incorporate Cumulative Prospect Theory (CPT) into reinforcement learning by deriving a policy gradient theorem for CPT objectives in finite-horizon settings. The method extends standard policy gradients by integrating an asymmetric utility function and probability distortion, which are key components of CPT. The authors propose a first-order policy gradient algorithm using Monte Carlo estimators based on order statistics and provide theoretical guarantees for its statistical properties and asymptotic convergence. Experimental results demonstrate the qualitative behavior of CPT in RL and show that the first-order approach outperforms existing zeroth-order methods in certain scenarios.</div>
<div class="mono" style="margin-top:8px">本文旨在将累积前景理论（CPT）应用于强化学习，通过推导有限时间范围内CPT目标的策略梯度定理来解决这一问题。研究动机来源于行为经济学，认为决策者在决策时会基于参考点的不对称效用函数和概率扭曲进行判断。作者基于该定理设计了一种一阶策略梯度算法，使用基于顺序统计量的蒙特卡洛梯度估计器。他们为估计器提供了统计保证，并证明了该算法在一般非凸CPT目标下能够渐近收敛到一阶平稳点。实验模拟展示了CPT在强化学习中的定性行为，并比较了一阶方法与现有零阶方法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery</div>
<div class="meta-line">Authors: Zelin Xu, Yupu Zhang, Saugat Adhikari, Saiful Islam, Tingsong Xiao, Zibo Liu, Shigang Chen, Da Yan, Zhe Jiang</div>
<div class="meta-line">First: 2026-02-17T06:08:43+00:00 · Latest: 2026-02-17T06:08:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15918v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EarthSpatialBench：评估多模态大语言模型在地球影像中的空间推理能力</div>
<div class="mono" style="margin-top:8px">评估多模态大语言模型（MLLMs）中的空间推理能力在计算机视觉领域引起了越来越多的关注，因其对具身AI及其他需要与物理世界进行精确交互的代理系统至关重要。然而，地球影像中的空间推理仍相对滞后，因为其独特地涉及在地理参考图像中定位物体，并利用视觉线索和向量几何坐标（如2D边界框、折线和多边形）进行距离、方向和拓扑关系的定量推理。现有的地球影像基准测试主要关注二维空间定位、图像描述和粗略空间关系（如简单的方向或邻近线索），缺乏对定量方向和距离推理、系统性拓扑关系以及超出边界框的复杂物体几何结构的支持。为填补这一空白，我们提出了\textbf{EarthSpatialBench}，这是一个全面的基准测试，用于评估MLLMs在地球影像中的空间推理能力。该基准包含超过325,000个问答对，涵盖以下方面：(1) 关于空间距离和方向的定性和定量推理；(2) 系统性拓扑关系；(3) 单个物体查询、物体对查询和组合聚合组查询；(4) 通过文本描述、视觉叠加和显式几何坐标（包括2D边界框、折线和多边形）表达的物体引用。我们在开源和专有模型上进行了广泛的实验，以识别MLLMs在空间推理方面的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to address the lack of comprehensive benchmarks for evaluating spatial reasoning capabilities in multimodal large language models (MLLMs) when dealing with Earth imagery. The proposed EarthSpatialBench introduces a benchmark that covers qualitative and quantitative spatial reasoning, topological relations, and complex object geometries beyond simple bounding boxes. The benchmark includes over 325K question-answer pairs and supports various types of spatial queries, including those involving textual descriptions, visual overlays, and explicit geometry coordinates. Extensive experiments on both open-source and proprietary models revealed significant limitations in the spatial reasoning abilities of current MLLMs.</div>
<div class="mono" style="margin-top:8px">该研究旨在评估多模态大语言模型（MLLMs）在处理地球影像时的空间推理能力，这对于具身AI和需要与物理世界精确交互的代理系统至关重要。为填补现有研究的不足，提出了EarthSpatialBench这一全面的基准测试，包含超过325,000个问答对，涵盖空间距离和方向的定性与定量推理、系统性的拓扑关系以及超出简单边界框的复杂物体几何形状。该基准支持通过文本描述、视觉叠加和显式几何坐标（如2D边界框、折线和多边形）表达的物体引用。在开源和专有模型上进行的广泛实验揭示了这些模型在进行精确定量空间推理和处理复杂拓扑关系方面存在显著局限。</div>
</details>
</div>
<div class="card">
<div class="title">Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery</div>
<div class="meta-line">Authors: Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Abhay Yadav, Ananth Vishnu Bhaskar, Hardik Prajapati, Cheng Peng, Rama Chellappa, Shivkumar Chandrasekaran, B. S. Manjunath</div>
<div class="meta-line">First: 2026-02-16T17:06:54+00:00 · Latest: 2026-02-16T17:06:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14929v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14929v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wrivinder：面向地面图像在卫星影像上地理定位的空间智能方法</div>
<div class="mono" style="margin-top:8px">将地面图像与地理注册的卫星地图对齐对于地图绘制、导航和态势感知至关重要，但在视角差异较大或GPS不可靠的情况下仍具挑战性。我们提出了Wrivinder，这是一个零样本、基于几何的框架，通过聚合多张地面照片来重建一致的3D场景，并将其与上方卫星影像对齐。Wrivinder结合了SfM重建、3D高斯点云、语义定位和基于单目深度的度量线索，生成稳定的顶视渲染，可直接与卫星上下文匹配，实现度量准确的相机地理定位。为支持该任务的系统性评估（该任务缺乏合适的基准数据集），我们还发布了MC-Sat，这是一个经过整理的数据集，连接了多视角地面图像与地理注册的卫星瓦片，涵盖多种户外环境。Wrivinder和MC-Sat共同提供了首个全面的基线和测试平台，用于研究无需成对监督的几何中心跨视角对齐。在零样本实验中，Wrivinder在密集和大范围场景中均实现了低于30米的地理定位精度，突显了基于几何的聚合方法在稳健地面到卫星定位中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of aligning ground-level images with satellite maps, especially when GPS is unavailable or there are significant viewpoint differences. Wrivinder, a zero-shot framework, leverages geometry-driven methods such as SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth estimation to create a consistent 3D scene from multiple ground photographs, enabling accurate geo-localization of the camera on satellite imagery. The main experimental results show that Wrivinder achieves sub-30 meter geolocation accuracy in both dense and large-area scenes, demonstrating the effectiveness of geometry-based approaches for cross-view alignment without paired supervision.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在大视角差异或GPS不可靠的情况下，将地面图像与卫星地图对齐的挑战。提出了一种零样本、基于几何的框架Wrivinder，通过多张地面照片重建一致的3D场景，并将其与高空卫星图像对齐。该框架结合了SfM重建、3D高斯点云、语义定位和单目深度度量线索，生成稳定的顶视渲染以实现精确的相机地理定位。实验结果显示，Wrivinder在密集和大范围场景中均实现了亚30米的地理定位精度，突显了基于几何的聚合方法在地面到卫星定位中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Haichao Zhu, Zhaorui Yang, Qian Zhang</div>
<div class="meta-line">First: 2026-02-16T02:26:59+00:00 · Latest: 2026-02-16T02:26:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14409v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14409v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习提出，几何处理：一种用于高效空间推理的模块化框架</div>
<div class="mono" style="margin-top:8px">空间感知旨在从视觉观测中估计相机运动和场景结构，这一问题传统上通过几何建模和物理一致性约束来解决。近年来，基于学习的方法在几何感知中展现出强大的表示能力，并在实践中越来越多地用于增强以几何为中心的经典系统。然而，学习组件是否应直接替代几何估计，还是应作为此类流程中的中间模块，仍是一个开放性问题。
在本工作中，我们针对这一问题，研究了一种端到端的模块化框架，以实现有效的空间推理，其中学习部分提出几何假设，而几何算法则处理估计决策。具体而言，我们在RGB-D序列的相对相机姿态估计背景下研究了这一原则。使用VGGT作为代表性学习模型，我们评估了在不同运动幅度和场景动态下基于学习的姿态和深度提案，并随后采用经典点对平面RGB-D ICP作为几何后端。我们在TUM RGB-D基准上的实验揭示了三个一致的发现：(1) 仅依靠基于学习的姿态提案是不可靠的；(2) 当学习提出的几何未正确对齐相机内参时，性能会下降；(3) 当学习提出的深度在几何上对齐并随后经过几何处理阶段时，在中等挑战性的刚性场景中会出现一致的性能提升。
这些结果表明，几何不仅仅是优化组件，而是验证和吸收基于学习的几何观测的关键仲裁者。我们的研究强调了模块化、几何感知系统设计在实现鲁棒空间感知中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of learning-based components in spatial perception systems, specifically focusing on whether they should replace or complement geometric estimation. The authors propose a modular framework where learning models generate geometric hypotheses, and classical geometric algorithms are used to refine and validate these proposals. Using VGGT as a learning model for pose and depth estimation on RGB-D sequences, they evaluate the effectiveness of this approach through experiments on the TUM RGB-D benchmark. The results show that learning-based pose proposals alone are unreliable, improper alignment with camera intrinsics can degrade performance, but when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements are observed in moderately challenging rigid settings.</div>
<div class="mono" style="margin-top:8px">本文旨在探讨学习方法与经典几何算法在空间推理任务中的结合，动机源于对相机运动和场景结构高效准确估计的需求。提出了一种模块化框架，其中学习部分生成几何假设，几何算法负责验证和优化。使用VGGT生成姿态和深度建议，并结合RGB-D ICP作为几何后端进行验证。在TUM RGB-D基准测试中发现，仅依靠学习生成的姿态估计不可靠，若学习生成的几何信息未正确对齐相机内参会导致性能下降，但在适当对齐并经过几何处理阶段后，可在中等挑战的刚性场景中实现一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines</div>
<div class="meta-line">Authors: Yifan Wu, Yiran Peng, Yiyu Chen, Jianhao Ruan, Zijie Zhuang, Cheng Yang, Jiayi Zhang, Man Chen, Yenchi Tseng, Zhaoyang Yu, Liang Chen, Yuyao Zhai, Bang Liu, Chenglin Wu, Yuyu Luo</div>
<div class="meta-line">First: 2026-02-15T20:03:19+00:00 · Latest: 2026-02-15T20:03:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14296v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoWebWorld: 通过有限状态机合成无限可验证的网络环境</div>
<div class="mono" style="margin-top:8px">自主网络GUI代理的性能严重依赖于其训练数据的质量和数量。然而，一个根本性的瓶颈仍然存在：从真实网站中收集交互轨迹既昂贵又难以验证。底层状态转换是隐藏的，导致需要依赖不一致且成本高昂的外部验证器来评估每一步的正确性。为了解决这一问题，我们提出了AutoWebWorld，这是一个新颖的框架，通过将网络环境建模为有限状态机（FSMs）来合成可控且可验证的网络环境，并利用编码代理将FSMs转换为可交互的网站。与真实网站中隐式的状态转换不同，AutoWebWorld明确定义了所有状态、动作和转换规则。这使得可以进行程序化验证：动作的正确性通过预定义的规则进行检查，任务成功则通过在FSM图中达到目标状态来确认。AutoWebWorld实现了完全自动化的搜索与验证流程，仅以每轨迹0.04美元的成本，从29个多样化的网络环境中生成超过11,663条验证轨迹。在这些合成数据上进行训练显著提升了实际应用中的性能。我们的70亿参数Web GUI代理在WebVoyager上仅需15步就超越了所有基线。此外，我们观察到一个明确的扩展定律：随着合成数据量的增加，AutoWebWorld在WebVoyager和Online-Mind2Web上的性能持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of AutoWebWorld is to overcome the challenges of collecting and verifying interaction trajectories for training autonomous Web GUI agents. The framework models web environments as Finite State Machines (FSMs), explicitly defining states, actions, and transition rules, which allows for programmatic verification. By using coding agents to translate FSMs into interactive websites, AutoWebWorld generates a large number of verified trajectories at a low cost, achieving over 11,663 verified trajectories from 29 diverse environments at $0.04 per trajectory. Training on this synthetic data significantly improves real-world performance, with the 7B Web GUI agent outperforming all baselines within 15 steps on WebVoyager and showing consistent performance improvements as synthetic data volume increases.</div>
<div class="mono" style="margin-top:8px">AutoWebWorld的动机是解决自主Web GUI代理训练中交互轨迹收集与验证的困难。该框架通过将Web环境建模为有限状态机（FSMs），显式定义状态、动作和转移规则，从而实现程序化验证。利用编码代理将FSMs转换为可交互的网站，AutoWebWorld以低成本生成大量验证轨迹，显著提升了真实环境中的代理性能。实验结果表明，基于该合成数据训练的7B参数模型在WebVoyager上15步内优于所有基线，并且随着合成数据量的增加，其在WebVoyager和Online-Mind2Web上的表现持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-15T19:48:43+00:00 · Latest: 2026-02-15T19:48:43+00:00</div>
<div class="meta-line">Comments: 15 pages, 33 pages with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KernelBlaster: 通过记忆增强的上下文强化学习实现持续跨任务CUDA优化</div>
<div class="mono" style="margin-top:8px">在多个GPU架构世代上优化CUDA代码具有挑战性，因为实现最佳性能需要在日益复杂且特定于硬件的优化空间中进行广泛探索。传统编译器受限于固定的启发式方法，而对大型语言模型（LLMs）进行微调成本较高。然而，CUDA代码优化的代理工作流在整合先前探索的知识方面能力有限，导致采样偏差和次优解。我们提出KernelBlaster，这是一个基于记忆增强的上下文强化学习（MAIC-RL）框架，旨在提升基于LLM的GPU编码代理的CUDA优化搜索能力。KernelBlaster通过将知识积累到可检索的持久CUDA知识库中，使代理能够从经验中学习，并在未来的任务中做出系统性的决策。我们提出了一种新颖的基于配置文件引导和文本梯度的代理流程，用于CUDA生成和优化，以在不同世代的GPU架构上实现高性能。KernelBlaster引导LLM代理系统地探索超越简单重写的高潜力优化策略。与PyTorch基线相比，我们的方法在KernelBench Level 1、2、3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。我们发布了KernelBlaster作为一个开源代理框架，附带测试框架、验证组件和可复现的评估流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of KernelBlaster is to address the challenges of optimizing CUDA code across different GPU architectures by overcoming the limitations of traditional compilers and the inefficiencies of fine-tuning Large Language Models (LLMs). The proposed method, Memory-Augmented In-context Reinforcement Learning (MAIC-RL), enables LLM-based agents to accumulate and retrieve optimization knowledge through a Persistent CUDA Knowledge Base, allowing them to make informed decisions on future tasks. KernelBlaster introduces a profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization, leading to significant performance improvements. Experimental results show that compared to the PyTorch baseline, KernelBlaster achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively.</div>
<div class="mono" style="margin-top:8px">KernelBlaster的研究动机是解决在不同代GPU架构上优化CUDA代码的挑战，这需要在日益复杂的硬件特定优化空间中进行广泛探索。该方法采用Memory-Augmented In-context Reinforcement Learning（MAIC-RL）框架，通过构建可检索的Persistent CUDA Knowledge Base，使基于LLM的GPU编码代理能够积累经验并做出系统性决策。KernelBlaster引入了一种基于配置文件和文本梯度的代理流程，用于CUDA生成和优化，从而实现跨多代GPU架构的高性能。实验结果表明，与PyTorch基线相比，KernelBlaster在KernelBench Levels 1、2和3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。</div>
</details>
</div>
<div class="card">
<div class="title">SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement</div>
<div class="meta-line">Authors: Xiaojun Jia, Jie Liao, Simeng Qin, Jindong Gu, Wenqi Ren, Xiaochun Cao, Yang Liu, Philip Torr</div>
<div class="meta-line">First: 2026-02-15T16:09:48+00:00 · Latest: 2026-02-15T16:09:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14211v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkillJect：面向编码代理的基于技能的隐蔽提示注入自动化框架</div>
<div class="mono" style="margin-top:8px">代理技能正逐渐成为编码代理的核心抽象，通过封装长文本指令和辅助脚本扩展工具增强行为。这种抽象引入了一个未被充分衡量的攻击面：基于技能的提示注入，其中被污染的技能可能引导代理偏离用户的意图和安全策略。在实践中，由于恶意意图过于明显或与原始技能偏离过大，简单的注入通常会失败，导致代理忽略或拒绝这些注入；现有的攻击方法也大多是手工构建的。我们提出了首个针对代理技能的隐蔽提示注入自动化框架。该框架与三个代理形成闭环：一个攻击代理在显式的隐蔽约束下合成注入技能，一个代码代理在现实的工具环境中使用注入的技能执行任务，一个评估代理记录操作轨迹（如工具调用和文件操作）并验证是否发生了目标恶意行为。我们还提出了一种恶意负载隐藏策略，通过在辅助脚本中隐藏对抗性操作，同时注入优化的诱导提示以触发工具执行。在多种编码代理设置和现实世界软件工程任务上的广泛实验表明，我们的方法在现实环境中能够持续实现高攻击成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of skill-based abstractions in coding agents creates a new vulnerability: skill-based prompt injection, where malicious skills can mislead agents from their intended behavior. To address this, the authors introduce SkillJect, an automated framework that generates stealthy injection skills through a closed-loop system involving three agents: an Attack Agent that creates injection prompts under stealth constraints, a Code Agent that executes tasks in a realistic environment, and an Evaluate Agent that monitors and verifies the occurrence of malicious actions. Their method includes a payload hiding strategy that embeds adversarial operations within auxiliary scripts, making the attacks harder to detect. Experimental results across various coding-agent scenarios and real-world software tasks demonstrate that SkillJect achieves high attack success rates in realistic conditions.</div>
<div class="mono" style="margin-top:8px">随着编码代理中技能抽象的广泛应用，这种结构带来了新的安全隐患，因为恶意攻击者可以通过提示注入操控这些技能，使其偏离用户意图和安全策略。为了解决这一问题，作者提出了SkillJect，这是一个专门用于技能型提示注入的自动化框架。该框架采用闭环系统，包含三个代理：攻击代理在隐秘性约束下生成注入技能，代码代理在真实环境中执行任务，评估代理则记录行为轨迹并验证是否发生了目标恶意行为。此外，该方法还引入了一种恶意负载隐藏策略，将对抗性操作嵌入辅助脚本中，从而提升攻击的有效性和隐蔽性。在多种编码代理设置和现实中的软件工程任务中进行的广泛实验表明，SkillJect在现实条件下能够实现较高的攻击成功率。</div>
</details>
</div>
<div class="card">
<div class="title">EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming</div>
<div class="meta-line">Authors: Sen Fang, Weiyuan Ding, Mengshi Zhang, Zihao Chen, Bowen Xu</div>
<div class="meta-line">First: 2025-05-18T01:02:33+00:00 · Latest: 2026-02-15T05:28:25+00:00</div>
<div class="meta-line">Comments: 27 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12185v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.12185v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI&#x27;s o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVALOOOP：一种以自一致性为中心的框架，用于评估大型语言模型在编程中的鲁棒性</div>
<div class="mono" style="margin-top:8px">评估大型语言模型（LLMs）在编程中的鲁棒性对于确保其在基于人工智能的软件开发中的可靠性至关重要。然而，对抗性攻击存在根本性局限，这会损害公平的鲁棒性评估：它们在不同攻击策略下会产生矛盾的评估结果，并且更关键的是，它们仅通过外部扰动进行操作，无法捕捉到自主编码代理所需的内在稳定性，因为后续输入是由模型自身生成的。我们引入了EVALOOOP，这是一种新颖的评估框架，从自一致性角度评估鲁棒性，利用软件工程任务中固有的自然二元性（例如代码生成和代码摘要）。EVALOOOP建立了一个自包含的反馈循环，其中LLM在代码和自然语言之间反复转换，直到功能失效。鲁棒性通过一种新的平均可持续循环（ASL）指标进行量化——即在基准任务中保持功能正确性的迭代次数的平均值。这种循环策略无需依赖外部攻击配置，内在地评估鲁棒性，提供了一个统一的指标，揭示了LLMs在持续自指转换中如何有效保持语义完整性。我们在EVALOOOP上评估了96个流行的LLMs，参数范围从0.5B到685B，并发现EVALOOOP通常在十次循环内导致pass@1准确率绝对下降2.65%-47.62%。有趣的是，鲁棒性并不总是与初始性能（即单次查询）一致；例如，Qwen3-235B-A22B-Instruct-2507，尽管其初始代码生成能力不如OpenAI的o系列模型和DeepSeek-V3，但其鲁棒性（ASL得分）却更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind EVALOOOP is to address the limitations of adversarial attacks in assessing the robustness of large language models (LLMs) for programming tasks, as these attacks fail to capture intrinsic model stability. The framework introduces a self-consistency-based approach, where an LLM iteratively transforms between code and natural language in a self-contained feedback loop until functional failure occurs. The robustness is measured using the Average Sustainable Loops (ASL) metric, which calculates the mean number of iterations maintaining correctness. Experimental results on 96 LLMs with varying parameter sizes show that EVALOOOP typically leads to a 2.65%-47.62% drop in pass@1 accuracy within ten loops, highlighting that robustness does not always correlate with initial performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有对抗攻击在评估编程任务中大语言模型（LLMs）鲁棒性时的局限性，因为这些攻击往往无法反映模型的内在稳定性。EVALOOOP 是一种以自一致性为中心的评估框架，通过建立一个反馈循环来评估模型的鲁棒性，该循环中 LLM 会反复在代码和自然语言之间转换，直到功能失效。该框架引入了一种新的指标 Average Sustainable Loops（ASL），用于衡量模型在保持功能正确性下的平均迭代次数。在 MBPP Plus 数据集上对 96 个 LLM 进行的实验表明，EVALOOOP 通常会导致 pass@1 准确率下降 2.65%-47.62%，说明鲁棒性并不总是与初始性能相关。例如，Qwen3-235B-A22B-Instruct-2507 虽然在初始代码生成方面不如 OpenAI 的 o 系列模型和 DeepSeek-V3，但其鲁棒性（ASL 分数）却更优。</div>
</details>
</div>
<div class="card">
<div class="title">Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Xiaoyu Li, Yitao Wu, Xian Wu, Haolin Zhuo, Lijun Zhao, Lining Sun</div>
<div class="meta-line">First: 2026-02-14T13:34:21+00:00 · Latest: 2026-02-14T13:34:21+00:00</div>
<div class="meta-line">Comments: Based on this work, we achieved 1st place on the KITTI tracking leaderboard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13772v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13772v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Offline-Poly：面向离线三维多目标跟踪的多面体框架</div>
<div class="mono" style="margin-top:8px">离线三维多目标跟踪（MOT）是4D自动标注（4DAL）过程中的关键组成部分。它通过引入时间上下文来增强高性能检测器生成的伪标签。然而，现有的离线三维MOT方法通常是在线框架的直接扩展，未能充分利用离线设置的优势。此外，这些方法往往依赖于固定的上游检测器和定制化架构，限制了其适应性。为了解决这些问题，我们提出了Offline-Poly，一种基于以跟踪为中心设计的通用离线三维MOT方法。我们引入了一种标准化范式，称为Tracking-by-Tracking（TBT），该范式仅依赖于任意现成的跟踪输出，并生成离线优化的轨迹片段。这种设计将离线跟踪器与特定的上游检测器或跟踪器解耦。在TBT范式下，Offline-Poly接受一个或多个粗略的跟踪结果，并通过包含预处理、分层匹配与融合以及轨迹片段优化的结构化流程进行处理。每个模块都旨在利用离线跟踪的两个基本特性：资源不受限性，允许在实时限制之外进行全局优化；以及未来可观测性，使得可以在完整的时间范围内进行轨迹推理。Offline-Poly首先利用全局场景上下文消除短期鬼轨迹，并通过重新识别碎片化片段进行轨迹恢复。然后，它构建场景级相似性以关联多个输入源的轨迹片段。最后，Offline-Poly通过联合利用局部和全局运动模式来优化轨迹片段。在nuScenes数据集上，我们实现了77.6%的AMOTA最优性能；在KITTI数据集上，取得了83.00%的HOTA领先结果。全面的实验进一步验证了Offline-Poly的灵活性、通用性和模块化有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Offline-Poly is proposed to address the limitations of existing offline 3D multi-object tracking methods, which are often direct extensions of online frameworks and lack adaptability. The method introduces a tracking-centric design based on a standardized paradigm called Tracking-by-Tracking (TBT), which operates on arbitrary off-the-shelf tracking outputs to generate refined tracklets. Offline-Poly utilizes the advantages of the offline setting, such as resource unconstrainedness and future observability, through a structured pipeline involving pre-processing, hierarchical matching and fusion, and tracklet refinement. It achieves state-of-the-art performance on nuScenes with 77.6% AMOTA and leading results on KITTI with 83.00% HOTA, demonstrating its flexibility and generalizability.</div>
<div class="mono" style="margin-top:8px">Offline-Poly 是为了解决现有离线三维多目标跟踪方法的局限性而提出的，这些方法通常是对在线框架的直接扩展，缺乏适应性。该方法引入了一种以跟踪为中心的设计，称为 Tracking-by-Tracking（TBT），它基于任意现成的跟踪输出，通过结构化的流程生成优化后的轨迹片段。该流程包括预处理、分层匹配与融合以及轨迹片段优化，充分利用了离线跟踪中全局优化和完整时间观测的特性。在 nuScenes 数据集上，Offline-Poly 达到了 77.6% 的 AMOTA，而在 KITTI 数据集上取得了 83.00% 的 HOTA，展示了其灵活性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird&#x27;s-Eye View with Deformable Attention and Sparse Goal Proposals</div>
<div class="meta-line">Authors: Minsang Kong, Myeongjun Kim, Sang Gu Kang, Hejiu Lu, Yupeng Zhong, Sang Hun Lee</div>
<div class="meta-line">First: 2025-09-12T09:17:54+00:00 · Latest: 2026-02-14T08:37:57+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Intelligent Transportation Systems (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10080v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10080v2">PDF</a> · <a href="https://github.com/Kongminsang/bevtraj">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In autonomous driving, trajectory prediction is essential for safe and efficient navigation. While recent methods often rely on high-definition (HD) maps to provide structured environmental priors, such maps are costly to maintain, geographically limited, and unreliable in dynamic or unmapped scenarios. Directly leveraging raw sensor data in Bird&#x27;s-Eye View (BEV) space offers greater flexibility, but BEV features are dense and unstructured, making agent-centric spatial reasoning challenging and computationally inefficient. To address this, we propose Bird&#x27;s-Eye View Trajectory Prediction (BEVTraj), a map-free framework that employs deformable attention to adaptively aggregate task-relevant context from sparse locations in dense BEV features. We further introduce a Sparse Goal Candidate Proposal (SGCP) module that predicts a small set of realistic goals, enabling fully end-to-end multimodal forecasting without heuristic post-processing. Extensive experiments show that BEVTraj achieves performance comparable to state-of-the-art HD map-based methods while providing greater robustness and flexibility without relying on pre-built maps. The source code is available at https://github.com/Kongminsang/bevtraj.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BEVTraj: 无需地图的鸟瞰视角端到端轨迹预测方法，结合可变形注意力与稀疏目标提案</div>
<div class="mono" style="margin-top:8px">在自动驾驶中，轨迹预测对于安全和高效导航至关重要。尽管近期方法常依赖高精度（HD）地图来提供结构化的环境先验，但此类地图维护成本高、地理范围有限，并且在动态或未映射场景中不可靠。直接利用鸟瞰视角（BEV）空间中的原始传感器数据提供了更大的灵活性，但BEV特征密集且无结构，使得以代理为中心的空间推理具有挑战性且计算效率低下。为了解决这一问题，我们提出了鸟瞰视角轨迹预测（BEVTraj），这是一种无需地图的框架，通过可变形注意力机制从密集的BEV特征中自适应地聚合任务相关的上下文信息。我们进一步引入了稀疏目标候选提案（SGCP）模块，用于预测一组现实的目标，从而实现完全端到端的多模态预测，无需启发式后处理。大量实验表明，BEVTraj在性能上与基于HD地图的最先进方法相当，同时在不依赖预构建地图的情况下提供了更高的鲁棒性和灵活性。源代码可在 https://github.com/Kongminsang/bevtraj 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trajectory prediction is crucial for autonomous driving systems to navigate safely and efficiently. Traditional approaches often depend on high-definition maps, which are expensive, limited in scope, and unreliable in dynamic environments. BEVTraj introduces a map-free framework that uses deformable attention to selectively gather relevant contextual information from dense Bird&#x27;s-Eye View features, improving spatial reasoning efficiency. It also incorporates a Sparse Goal Candidate Proposal module to generate realistic future goals, allowing for end-to-end multimodal forecasting. Experimental results demonstrate that BEVTraj matches the performance of HD map-based methods while being more robust and adaptable in diverse scenarios.</div>
<div class="mono" style="margin-top:8px">轨迹预测对于自动驾驶系统的安全和高效导航至关重要。传统方法通常依赖高精度地图，但这些地图成本高、地理范围有限且在动态环境中不可靠。BEVTraj提出了一种无需地图的框架，利用可变形注意力机制从密集的鸟瞰图特征中选择性地聚合任务相关上下文，从而实现更灵活和高效的空问推理。此外，该框架引入了稀疏目标候选提案模块，用于生成现实的目标预测，支持端到端的多模态预测。实验结果表明，BEVTraj在无需地图的情况下，其性能与基于高精度地图的方法相当，且更具鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories</div>
<div class="meta-line">Authors: Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen</div>
<div class="meta-line">First: 2025-04-29T22:22:44+00:00 · Latest: 2026-02-14T01:32:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21205v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.21205v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 29 standalone LLMs and 15 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SecRepoBench：在现实仓库中评估代码代理的安全代码补全基准</div>
<div class="mono" style="margin-top:8px">本文介绍了SecRepoBench，这是一个用于评估代码代理在现实仓库中进行安全代码补全的基准。SecRepoBench包含27个C/C++仓库中的318个代码补全任务，涵盖15种CWE漏洞。我们使用该基准评估了29个独立的LLMs和15个代码代理，覆盖了三种最先进的代理框架。我们发现，最先进的LLMs在生成正确且安全的代码补全方面存在困难。然而，代码代理显著优于独立LLMs。我们展示了SecRepoBench比之前的最先进的基准更具挑战性。最后，我们的全面分析为增强代码代理在现实仓库中编写正确且安全代码的能力提供了潜在方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents SecRepoBench, a benchmark designed to assess code agents&#x27; performance in secure code completion within real-world repositories. The benchmark includes 318 tasks across 27 C/C++ repositories, addressing 15 common CWE vulnerabilities. Evaluating 29 standalone LLMs and 15 code agents using three advanced agent frameworks, the study reveals that while state-of-the-art LLMs perform poorly in generating secure code, code agents significantly outperform them. Additionally, SecRepoBench is demonstrated to be more challenging than existing benchmarks, offering valuable insights for improving code agent security capabilities in practical settings.</div>
<div class="mono" style="margin-top:8px">本文提出了SecRepoBench，这是一个用于评估代码代理在真实仓库中进行安全代码补全的基准测试。该基准包含27个C/C++仓库中的318个代码补全任务，涵盖15种常见的软件漏洞。通过评估29个独立的大型语言模型（LLMs）和15个代码代理，使用三种最先进的代理框架，研究发现LLMs在生成正确且安全的代码补全方面表现不佳，而代码代理则显著优于独立模型。结果还表明，SecRepoBench比之前的基准更具挑战性，突显了在实际环境中提升代码代理生成安全代码能力的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Spear-Phishing Website Generation in Large Language Model Coding Agents</div>
<div class="meta-line">Authors: Tailia Malloy, Tegawende F. Bissyande</div>
<div class="meta-line">First: 2026-02-13T12:12:53+00:00 · Latest: 2026-02-13T12:12:53+00:00</div>
<div class="meta-line">Comments: 18 Pages, 7 Figures, 1 Table. Accepted to the conference Human Computer Interaction International</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are expanding beyond being a tool humans use and into independent agents that can observe an environment, reason about solutions to problems, make changes that impact those environments, and understand how their actions impacted their environment. One of the most common applications of these LLM Agents is in computer programming, where agents can successfully work alongside humans to generate code while controlling programming environments or networking systems. However, with the increasing ability and complexity of these agents comes dangers about the potential for their misuse. A concerning application of LLM agents is in the domain cybersecurity, where they have the potential to greatly expand the threat imposed by attacks such as social engineering. This is due to the fact that LLM Agents can work autonomously and perform many tasks that would normally require time and effort from skilled human programmers. While this threat is concerning, little attention has been given to assessments of the capabilities of LLM coding agents in generating code for social engineering attacks. In this work we compare different LLMs in their ability and willingness to produce potentially dangerous code bases that could be misused by cyberattackers. The result is a dataset of 200 website code bases and logs from 40 different LLM coding agents. Analysis of models shows which metrics of LLMs are more and less correlated with performance in generating spear-phishing sites. Our analysis and the dataset we present will be of interest to researchers and practitioners concerned in defending against the potential misuse of LLMs in spear-phishing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型编码代理生成钓鱼网站的能力</div>
<div class="mono" style="margin-top:8px">大型语言模型正在从人类使用的工具扩展为能够观察环境、推理解决问题、对环境产生影响并理解其行为后果的独立代理。这些LLM代理最常见的应用之一是计算机编程，它们可以与人类协作生成代码，同时控制编程环境或网络系统。然而，随着这些代理的能力和复杂性不断增加，其被滥用的潜在风险也随之而来。在网络安全领域，LLM代理的一个令人担忧的应用是它们可能大大扩展社会工程攻击的威胁。这是因为LLM代理可以自主工作，并执行通常需要熟练人类程序员投入时间和精力的任务。尽管这种威胁令人担忧，但对LLM编码代理在生成社会工程攻击代码方面的能力评估却很少受到关注。在本研究中，我们比较了不同LLM在生成可能被网络攻击者滥用的危险代码库方面的能力和意愿。结果是一个包含200个网站代码库和40个不同LLM编码代理日志的数据集。模型分析展示了哪些LLM指标与生成定向钓鱼网站的性能更为或更不相关。我们的分析和所展示的数据集将对关注如何防范LLM在定向钓鱼中被滥用的研究人员和实践者具有参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential misuse of large language models (LLMs) in generating spear-phishing websites, a critical concern in cybersecurity. The research compares the ability and willingness of different LLM coding agents to produce malicious code bases that could be exploited by attackers. Through an extensive analysis, the study generates a dataset comprising 200 website code bases and logs from 40 LLM agents, identifying which model metrics are most closely associated with successful spear-phishing site generation. The findings provide valuable insights for researchers and practitioners aiming to mitigate risks posed by LLMs in security contexts.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLM）编码代理在生成钓鱼网站方面的潜在滥用问题，这是网络安全领域的重要关注点。研究人员评估了多种LLM，以考察其生成可能被攻击者利用的恶意代码库的能力和意愿。他们构建了一个包含200个网站代码库和40个不同LLM编码代理日志的数据集。分析结果揭示了哪些LLM指标与生成精准钓鱼网站的效果更为相关或不相关，为研究人员和从业者提供了应对LLM在安全领域潜在风险的参考。</div>
</details>
</div>
<div class="card">
<div class="title">3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset</div>
<div class="meta-line">Authors: Mehran Advand, Zahra Dehghanian, Navid Faraji, Reza Barati, Seyed Amir Ahmad Safavi-Naini, Hamid R. Rabiee</div>
<div class="meta-line">First: 2026-02-13T11:08:15+00:00 · Latest: 2026-02-13T11:08:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mehrn79.github.io/3DLAND">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DLAND：腹部异常病变三维定位数据集</div>
<div class="mono" style="margin-top:8px">现有的腹部CT医学影像数据集通常缺乏三维标注、多器官覆盖或精确的病灶-器官关联，阻碍了稳健的表示学习和临床应用。为解决这一问题，我们引入了3DLAND，这是一个大规模基准数据集，包含超过6,000个增强CT体数据，配有超过20,000个高保真度的三维病灶标注，涵盖七个腹部器官：肝脏、肾脏、胰腺、脾脏、胃和胆囊。我们的三阶段简化流程集成了自动化空间推理、优化提示的2D分割以及记忆引导的3D传播，经专家放射科医生验证，表面Dice评分超过0.75。通过提供多样化的病灶类型和患者人口统计信息，3DLAND支持医疗AI在异常检测、定位和跨器官迁移学习方面的可扩展评估。我们的数据集为评估器官感知的三维分割模型建立了新的基准，为面向医疗的AI发展铺平了道路。为促进可重复性和进一步研究，3DLAND数据集及实现代码已公开在https://mehrn79.github.io/3DLAND。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the 3DLAND dataset is to address the limitations of existing abdominal CT datasets, which often lack comprehensive 3D annotations, multi-organ coverage, and precise lesion-to-organ associations. The dataset introduces a large-scale benchmark with over 6,000 contrast-enhanced CT volumes and more than 20,000 high-fidelity 3D lesion annotations across seven abdominal organs. The method employs a three-phase pipeline that combines automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores above 0.75. The main experimental results demonstrate that 3DLAND supports scalable evaluation of anomaly detection, localization, and cross-organ transfer learning, offering a valuable resource for improving organ-aware 3D segmentation models in medical AI.</div>
<div class="mono" style="margin-top:8px">本研究提出3DLAND数据集，旨在解决现有腹部CT数据集中缺乏三维标注、多器官覆盖以及精确病灶-器官关联的问题。该数据集包含超过6000例增强CT影像，覆盖肝脏、肾脏、胰腺、脾脏、胃和胆囊等七个腹部器官，提供了超过20000个高保真度的三维病灶标注。通过一个三阶段的流程，结合自动化空间推理、提示优化的2D分割和记忆引导的3D传播，数据集经放射科专家验证，表面Dice评分超过0.75。3DLAND支持对异常检测、定位和跨器官迁移学习的可扩展评估，为提升面向医疗的三维分割模型提供了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Easy-Poly: An Easy Polyhedral Framework For 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Peng Zhang, Xin Li, Xin Lin, Liang He</div>
<div class="meta-line">First: 2025-02-25T04:01:25+00:00 · Latest: 2026-02-13T08:05:15+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17822v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.17822v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent 3D multi-object tracking (3D MOT) methods mainly follow tracking-by-detection pipelines, but often suffer from high false positives, missed detections, and identity switches, especially in crowded and small-object scenarios. To address these challenges, we propose Easy-Poly, a filter-based 3D MOT framework with four key innovations: (1) CNMSMM, a novel Camera-LiDAR fusion detection method combining multi-modal augmentation and an efficient NMS with a new loss function to improve small target detection; (2) Dynamic Track-Oriented (DTO) data association that robustly handles uncertainties and occlusions via class-aware optimal assignment and parallel processing strategies; (3) Dynamic Motion Modeling (DMM) using a confidence-weighted Kalman filter with adaptive noise covariance to enhance tracking accuracy; and (4) an extended life-cycle management system reducing identity switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 65.65% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 75.6%), while also running in real-time. Our framework advances robustness and adaptability in complex driving environments, paving the way for safer autonomous driving perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Easy-Poly：一种用于3D多目标跟踪的简单多面体框架</div>
<div class="mono" style="margin-top:8px">最近的3D多目标跟踪(3D MOT)方法主要采用基于检测的跟踪流程，但在密集和小目标场景中常面临高误检率、漏检和身份切换的问题。为了解决这些挑战，我们提出了Easy-Poly，一个基于滤波的3D MOT框架，包含四项关键创新：(1) CNMSMM，一种结合多模态增强和高效NMS的新损失函数的相机-激光雷达融合检测方法，以提升小目标检测性能；(2) 动态目标导向(DTO)数据关联，通过类感知最优分配和平行处理策略，鲁棒地处理不确定性与遮挡；(3) 动态运动建模(DMM)，使用带有自适应噪声协方差的置信度加权卡尔曼滤波以提高跟踪精度；(4) 扩展生命周期管理系统，减少身份切换和误终止。实验结果表明，Easy-Poly在mAP（如LargeKernel3D中从63.30%提升至65.65%）和AMOTA（如从73.1%提升至75.6%）方面优于Poly-MOT和Fast-Poly等先进方法，同时支持实时运行。我们的框架提升了复杂驾驶环境下的鲁棒性和适应性，为更安全的自动驾驶感知铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the performance of 3D multi-object tracking in challenging scenarios such as crowded environments and small-object detection. Easy-Poly introduces a filter-based framework with four key innovations: a novel Camera-LiDAR fusion detection method (CNMSMM) that enhances small target detection through multi-modal augmentation and a new loss function; Dynamic Track-Oriented data association that uses class-aware optimal assignment and parallel processing to handle occlusions and uncertainties; Dynamic Motion Modeling with a confidence-weighted Kalman filter and adaptive noise covariance to improve tracking accuracy; and an extended life-cycle management system to reduce identity switches and false terminations. Experimental results demonstrate that Easy-Poly outperforms existing methods like Poly-MOT and Fast-Poly, achieving higher mAP and AMOTA scores while maintaining real-time performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D多目标跟踪在拥挤场景和小目标检测中的性能。Easy-Poly提出了一种基于滤波的框架，包含四项关键创新：一种新颖的相机-激光雷达融合检测方法（CNMSMM）、动态目标导向数据关联、使用置信度加权卡尔曼滤波的动态运动建模，以及扩展的生命管理机制。实验结果表明，Easy-Poly在mAP和AMOTA指标上优于现有方法如Poly-MOT和Fast-Poly，同时保持实时运行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models</div>
<div class="meta-line">Authors: Joshua Ong Jun Leang, Yu Zhao, Mihaela Cătălina Stoian, Wenda Li, Shay B. Cohen, Eleonora Giunchiglia</div>
<div class="meta-line">First: 2026-02-13T03:56:22+00:00 · Latest: 2026-02-13T03:56:22+00:00</div>
<div class="meta-line">Comments: 8 pages, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12586v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我可以点单吗？扩散语言模型中用于槽填充顺序的蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">尽管在掩码扩散模型（MDMs）中，计划-填充解码在数学和代码推理方面表现出潜力，但其性能仍高度依赖于槽填充顺序，常常导致输出方差显著。我们引入了McDiffuSE框架，将槽选择建模为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。McDiffuSE利用前瞻模拟，在承诺之前评估部分完成情况，系统地探索生成顺序的组合空间。实验表明，与自回归基线相比，平均提升3.2%，与基线计划-填充方法相比提升8.0%，在MBPP和MATH500上分别取得19.5%和4.9%的显著提升。我们的分析表明，尽管McDiffuSE主要遵循顺序填充，但引入非顺序生成对于最大化性能是必要的。我们观察到，为了克服模型置信度偏差并发现有效的填充顺序，需要更大的探索常数，而不是增加模拟次数。这些发现确立了基于MCTS的规划作为提升MDMs生成质量的有效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of output variance in Masked Diffusion Models (MDMs) due to the sensitivity of slot infilling order during plan-and-infill decoding. It proposes McDiffuSE, a framework that treats slot selection as a decision-making process and optimizes the infilling order using Monte Carlo Tree Search (MCTS). By employing look-ahead simulations, McDiffuSE evaluates partial completions before finalizing the order, enabling systematic exploration of the combinatorial generation space. Experimental results demonstrate that McDiffuSE improves performance by 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill methods, with significant gains of 19.5% on MBPP and 4.9% on MATH500. The analysis indicates that while sequential ordering is common, non-sequential generation is crucial for optimal performance, and larger exploration constants are more effective than increased simulation counts in mitigating model confidence biases.</div>
<div class="mono" style="margin-top:8px">该论文针对扩散语言模型中槽填充顺序对输出方差的影响问题，提出McDiffuSE框架，将槽选择视为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。该方法利用前瞻模拟在最终确定顺序前评估部分填充结果，系统地探索生成顺序的组合空间。实验结果显示，McDiffuSE在自回归基线模型上平均提升3.2%，在基线计划填充模型上提升8.0%，在MBPP和MATH500数据集上分别取得19.5%和4.9%的显著提升。分析表明，尽管多数情况下采用顺序填充，但引入非顺序生成对性能提升至关重要，且更大的探索常数比增加模拟次数更有效，有助于克服模型置信度偏差并发现有效的填充顺序。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-12T21:47:09+00:00</div>
<div class="meta-line">Comments: added more results on scaling law analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使LLMs在推荐系统中首次实现扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这可能是由于先前持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性所致。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，展示了其在学习可泛化的用户偏好模式方面的优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具有稳健的幂律扩展特性。我们的实验表明，在多种合成数据模式下，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of developing Large Language Models (LLMs) for recommender systems by proposing a principled synthetic data generation framework. The motivation stems from the lack of predictable scaling laws in previous continual pre-training approaches, which were hindered by noisy, biased, and incomplete real user interaction data. The method introduces a curated, pedagogical curriculum to generate high-quality synthetic data, enabling more effective learning of user preference patterns. Experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, with a $+130\%$ improvement in recall@100 for SasRec. Furthermore, the study demonstrates robust power-law scaling for LLMs trained on this structured data, highlighting consistent perplexity reduction across different modalities.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在推荐系统中的应用受到缺乏可预测的扩展定律的阻碍，这限制了研究方向和资源优化。本文提出了一种基于原则的合成数据生成框架，通过构建结构化的教学课程来解决原始用户交互数据中的噪声、偏差和不完整性问题。实验结果表明，使用该合成数据训练的模型在排序任务中显著优于基于真实数据训练的模型，SasRec在recall@100指标上提升了130%。此外，研究首次实验证明了LLMs在持续预训练于高质量推荐数据时表现出稳健的幂律扩展特性，不同合成数据模态下均呈现出一致且可预测的困惑度下降，为推荐领域中LLMs能力的可靠扩展奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study</div>
<div class="meta-line">Authors: Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim</div>
<div class="meta-line">First: 2025-10-26T16:49:06+00:00 · Latest: 2026-02-12T21:11:14+00:00</div>
<div class="meta-line">Comments: Accepted at LREC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22747v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Québec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Québec French LLMs on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模语言模型的低资源方言适应：以法语方言为例</div>
<div class="mono" style="margin-top:8px">尽管大规模语言模型（LLMs）已被广泛采用，但其最强的能力仍主要局限于少数高资源语言，这些语言拥有大量训练数据。最近，持续预训练（CPT）作为一种方法，被用于微调这些模型以适应低资源的区域方言。本文研究了在数据和计算资源受限的情况下使用CPT进行方言学习的效果。我们利用低秩适应（LoRA）和计算高效的持续预训练技术，使用一个非常小的数据集将三个LLMs适应为魁北克法语方言，并在COLE数据集上进行基准测试。实验结果表明，在不到1%的模型参数更新的情况下，对少数方言基准测试有显著提升，而对标准语言基准测试的退化则非常有限。结果分析表明，提升效果高度依赖于语料库的构成。这些发现表明，结合参数高效微调（PEFT）的持续预训练可以有效缩小方言差距，通过提供成本效益高且可持续的语言资源创建方式，使高质量LLM能够扩展到少数语言群体。我们发布了首个魁北克法语LLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting large language models to low-resource dialects, focusing on Québec French. The study explores continual pre-training (CPT) combined with low-rank adaptation (LoRA) to fine-tune three LLMs using minimal data and computational resources. Experimental results on the COLE suite show significant improvements in performance for the Québec French dialect with little impact on the original high-resource French language benchmarks, highlighting the effectiveness of parameter-efficient fine-tuning (PEFT) in dialect adaptation. The findings suggest that CPT with PEFT can provide a cost-effective solution for expanding LLM capabilities to minority dialects.</div>
<div class="mono" style="margin-top:8px">本文探讨了在低资源环境下对大型语言模型进行方言适配的挑战，以魁北克法语为例。研究动机源于观察到当前大型语言模型在高资源语言上表现优异，而少数方言则缺乏足够的训练数据。作者采用持续预训练（CPT）结合低秩适应（LoRA）的方法，使用极小的数据集对三个大型语言模型进行适配，并在COLE数据集上进行基准测试。实验结果表明，适配后的模型在魁北克法语基准上取得提升，同时在标准语言基准上几乎没有性能下降，仅更新不到1%的模型参数。研究分析指出，语料库的构成对方言适配效果有显著影响，表明参数高效微调（PEFT）方法能够以低成本和可持续的方式提升少数语言的模型表现。</div>
</details>
</div>
<div class="card">
<div class="title">LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens</div>
<div class="meta-line">Authors: Zekun Li, Sizhe An, Chengcheng Tang, Chuan Guo, Ivan Shugurov, Linguang Zhang, Amy Zhao, Srinath Sridhar, Lingling Tao, Abhay Mittal</div>
<div class="meta-line">First: 2026-02-12T20:02:21+00:00 · Latest: 2026-02-12T20:02:21+00:00</div>
<div class="meta-line">Comments: Project page: https://kunkun0w0.github.io/project/LLaMo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kunkun0w0.github.io/project/LLaMo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (&gt;30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaMo：通过连续自回归标记统一运动理解与生成的预训练语言模型扩展</div>
<div class="mono" style="margin-top:8px">近年来，大模型在统一多模态生成与理解方面取得了显著进展。然而，将运动与语言生成和理解统一的模型开发仍处于初步探索阶段。现有方法通常在配对的运动-文本数据上微调大型语言模型（LLMs），由于可用的文本-运动对规模有限，这可能导致语言能力的灾难性遗忘。此外，先前的方法通常通过量化将运动转换为离散表示，以与语言模型集成，从而引入了显著的抖动伪影。为了解决这些挑战，我们提出了LLaMo，一个统一框架，通过一种特定模态的混合变换器（MoT）架构扩展预训练的LLMs。该设计在保留基础模型语言理解能力的同时，实现了可扩展的多模态适应。我们将人类运动编码为因果连续的潜在空间，并通过一个轻量级的流匹配头在解码器主干中保持下一个标记预测范式，从而实现实时流式运动生成（&gt;30 FPS）。借助预训练LLMs的全面语言理解和大规模运动-文本预训练，我们的实验表明，LLaMo在一般场景下实现了高保真度的文本到运动生成和运动到文本描述，尤其是在零样本运动生成方面，标志着向通用统一运动-语言大模型迈出的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop a unified model that can understand and generate motion while preserving strong language capabilities. LLaMo introduces a modality-specific Mixture-of-Transformers (MoT) architecture to extend pretrained language models, enabling scalable multimodal adaptation without catastrophic forgetting. By encoding human motion into a continuous latent space and using a lightweight flow-matching head in the decoder-only backbone, LLaMo supports real-time motion generation with over 30 FPS. Experimental results show that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, particularly in zero-shot scenarios, demonstrating its effectiveness in bridging motion and language understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的模型，用于运动理解和生成，以解决现有方法在灾难性遗忘和抖动伪影方面的不足。LLaMo提出了一种模态特定的Mixture-of-Transformers（MoT）架构，扩展了预训练语言模型，同时保留其语言理解能力，并实现可扩展的多模态适应。该模型将人体运动编码为连续的潜在空间，并通过轻量级的流匹配头保持下一个词预测范式，支持实时运动生成（&gt;30 FPS）。实验结果表明，LLaMo在零样本运动生成等一般场景中实现了高质量的文本到运动生成和运动到文本描述，展示了其在统一运动语言理解方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态通常在语言语义上缺乏足够的关联性，影响了跨模态对齐的效果，尤其是在需要跨远距离区域或多个图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，这是一种新的框架，将视觉操作重新定义为语言引导的特征调节。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重新编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们通过ViLaVT这一新型LVLM实例化了该范式，其配备了一个专门用于此类交互式视觉推理的动态视觉编码器，并采用结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the visual reasoning capabilities of large vision-language models (LVLMs) by addressing the limitations of single-pass visual encoding that leads to loss of fine-grained visual details. The proposed method, &#x27;chatting with images,&#x27; introduces a framework where visual manipulation is guided by language prompts, allowing the model to dynamically re-encode multiple image regions in a joint manner. This approach improves the alignment between linguistic and visual modalities. Experimental results across eight benchmarks show that the model, ViLaVT, achieves significant improvements, especially in complex spatial reasoning tasks involving multiple images and videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机是改进大型视觉-语言模型（LVLMs）的性能，解决单次视觉编码导致的细粒度视觉信息丢失问题。提出的方法&#x27;与图像对话&#x27;引入了一种框架，通过语言提示引导视觉操作，实现对多个图像区域的动态联合重新编码。这种方法增强了语言与视觉模态之间的对齐，特别是在需要跨远距离区域或多图像进行推理的任务中。在八个基准测试中的实验结果表明，新模型ViLaVT在复杂的空间推理任务中，尤其是涉及多图像和视频的任务中，取得了显著的提升。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260220_0359.html">20260220_0359</a>
<a href="archive/20260219_0408.html">20260219_0408</a>
<a href="archive/20260218_0406.html">20260218_0406</a>
<a href="archive/20260217_0354.html">20260217_0354</a>
<a href="archive/20260216_0344.html">20260216_0344</a>
<a href="archive/20260215_0344.html">20260215_0344</a>
<a href="archive/20260213_0409.html">20260213_0409</a>
<a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
