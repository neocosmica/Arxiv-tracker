<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 04:09</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0409</div>
    <div class="row"><div class="card">
<div class="title">GameDevBench: Evaluating Agentic Capabilities Through Game Development</div>
<div class="meta-line">Authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten, Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar, Chris Donahue</div>
<div class="meta-line">First: 2026-02-11T18:15:11+00:00 · Latest: 2026-02-11T18:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5&#x27;s performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GameDevBench：通过游戏开发评估代理能力</div>
<div class="mono" style="margin-top:8px">尽管在代码代理方面取得了快速进展，但其多模态对应物的进展却相对滞后。一个关键挑战是缺乏结合软件开发复杂性与深度多模态理解需求的评估测试平台。游戏开发提供了这样的测试平台，因为代理必须在视觉游戏场景中处理诸如着色器、精灵和动画等本质上多模态的资产，同时应对庞大的代码库。我们提出了GameDevBench，这是首个用于评估代理在游戏开发任务上的基准。GameDevBench包含132个任务，来源于网络和视频教程。这些任务需要显著的多模态理解，并且复杂度较高——平均解决方案所需的代码行数和文件更改量是之前软件开发基准的三倍以上。代理在游戏开发任务上仍存在困难，最佳代理仅能完成54.5%的任务。我们发现任务难度与多模态复杂度之间存在强相关性，成功率从以游戏玩法为导向的任务的46.9%下降到2D图形任务的31.6%。为了提升多模态能力，我们引入了两种基于图像和视频的简单反馈机制。尽管这些方法简单，但它们持续提升了代理的性能，其中最大的提升是Claude Sonnet 4.5的性能从33.3%提升到47.7%。我们公开发布GameDevBench，以支持进一步的代理游戏开发研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GameDevBench is to address the lack of comprehensive evaluation frameworks for multimodal coding agents, which are essential for tasks requiring both code generation and deep understanding of visual elements. The benchmark is designed to assess agents&#x27; abilities in game development, a domain that involves complex codebases and the manipulation of multimodal assets such as shaders, sprites, and animations. GameDevBench includes 132 tasks from web and video tutorials, with the average solution requiring over three times more code and file changes than previous benchmarks. Experimental results show that even the best agent only solves 54.5% of the tasks, with success rates dropping significantly to 31.6% for 2D graphics tasks. The study also introduces two simple feedback mechanisms based on images and videos, which improve agent performance, notably increasing Claude Sonnet 4.5&#x27;s success rate from 33.3% to 47.7%.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多模态编码代理缺乏全面评估框架的问题，这对于需要文本与视觉理解相结合的任务至关重要。作者提出了GameDevBench，这是一个由网络和视频教程中提取的132个任务组成的基准，用于评估代理在游戏开发中的能力，该游戏开发涉及复杂的多模态交互。实验结果显示，即使是最优代理也只能完成54.5%的任务，对于涉及2D图形的任务，成功率显著下降。研究还引入了两种基于图像和视频的简单反馈机制，这些机制有效提升了代理性能，特别是对Claude Sonnet 4.5，其成功率从33.3%提升至47.7%。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tienie Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-11T17:42:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态往往在语言语义上缺乏充分的关联，影响了跨模态对齐的效果，尤其是在需要跨远距离区域或多张图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，这是一种新的框架，将视觉操作重新定义为语言引导的特征调制。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们在ViLaVT中实现了这一范式，ViLaVT是一种新型的LVLM，配备了专门用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT在复杂多图像和基于视频的空间推理任务中取得了显著且一致的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current large vision-language models (LVLMs) that rely on single-pass visual encoding, leading to the loss of fine-grained visual information. The proposed method, &#x27;chatting with images,&#x27; introduces a framework where visual manipulation is guided by language prompts, enabling dynamic joint re-encoding across multiple image regions. This approach enhances cross-modal alignment by tightly coupling linguistic reasoning with visual state updates. The authors implement this paradigm in ViLaVT, a new LVLM with a dynamic vision encoder, and train it using a two-stage curriculum of supervised fine-tuning and reinforcement learning. Experimental results across eight benchmarks show that ViLaVT achieves significant improvements, especially in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升大型视觉-语言模型（LVLMs）的视觉推理能力，解决单次视觉编码导致的细粒度视觉信息丢失问题。提出的方法&#x27;与图像对话&#x27;将视觉操作重新定义为语言引导的特征调整，通过语言提示动态地对多个图像区域进行联合重编码，从而加强语言推理与视觉状态更新之间的耦合。该方法在ViLaVT模型中得到实现，ViLaVT是一个专门用于交互式视觉推理的新型LVLM，其动态视觉编码器经过监督微调和强化学习的双阶段课程训练。在八个基准测试中，实验结果表明ViLaVT表现出显著且一致的性能提升，尤其在涉及多图像和视频的复杂空间推理任务中效果更为突出。</div>
</details>
</div>
<div class="card">
<div class="title">CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning</div>
<div class="meta-line">Authors: Hang Wu, Yujun Cai, Zehao Li, Haonan Ge, Bowen Sun, Junsong Yuan, Yiwei Wang</div>
<div class="meta-line">First: 2026-01-30T04:45:43+00:00 · Latest: 2026-02-11T17:26:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00181v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00181v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamReasoner：通过结构化空间推理强化摄像机运动理解</div>
<div class="mono" style="margin-top:8px">理解摄像机动态是视频空间智能的基本支柱。然而，现有的多模态模型大多将此任务视为黑箱分类，常常依赖表面视觉模式而非几何线索来混淆物理上不同的运动。我们提出了CamReasoner框架，将摄像机运动理解重新表述为一个结构化推理过程，以弥合感知与电影逻辑之间的差距。我们的方法以观察-思考-回答（O-T-A）范式为核心，迫使模型在一个显式的推理模块中解码轨迹和视锥等时空线索。为了赋予模型这一能力，我们构建了一个大规模推理轨迹套件，包含18,000个SFT推理链和38,000个RL反馈样本。值得注意的是，我们是首个在该领域使用强化学习进行逻辑对齐的团队，确保运动推理基于物理几何而非情境猜测。通过将强化学习应用于观察-思考-回答（O-T-A）推理范式，CamReasoner有效抑制了幻觉现象，并在多个基准测试中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of CamReasoner is to improve the understanding of camera movements in videos by moving beyond superficial visual patterns and incorporating geometric reasoning. The framework introduces an Observation-Thinking-Answer (O-T-A) paradigm, where the model explicitly reasons about spatio-temporal cues like trajectories and view frustums. The main experimental results show that CamReasoner achieves state-of-the-art performance on multiple benchmarks by using reinforcement learning to align motion inferences with physical geometry, thereby reducing hallucinations and enhancing accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过引入几何推理来提升视频中摄像机运动的理解，突破以往仅依赖表面视觉模式的局限。CamReasoner提出了一种基于观察-思考-回答（O-T-A）范式的结构化推理框架，要求模型在显式的推理模块中处理轨迹和视锥等时空线索。该框架通过包含18,000个SFT推理链和38,000个RL反馈样本的大规模推理轨迹集进行训练，并首次在该领域应用强化学习实现逻辑对齐。实验结果表明，CamReasoner有效抑制了幻觉现象，在多个基准测试中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting</div>
<div class="meta-line">Authors: Rishikesh Bhyri, Brian R Quaranto, Philip J Seger, Kaity Tung, Brendan Fox, Gene Yang, Steven D. Schwaitzberg, Junsong Yuan, Nan Xi, Peter C W Kim</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-11T16:49:37+00:00 · Latest: 2026-02-11T16:49:37+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026. This version includes additional authors who contributed during the rebuttal phase</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11024v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于密集手术器械计数的链式观察空间推理</div>
<div class="mono" style="margin-top:8px">准确计数手术室（OR）中的手术器械是确保手术期间患者安全的关键前提。尽管大型视觉-语言模型和代理AI取得了近期进展，但准确计数这些器械仍然极具挑战性，尤其是在器械密集排列的场景中。为了解决这一问题，我们引入了Chain-of-Look，这是一种新颖的视觉推理框架，通过强制执行结构化的视觉链来模拟人类的顺序计数过程，而不是依赖传统的无序目标检测方法。这种视觉链引导模型沿着连贯的空间轨迹进行计数，从而在复杂场景中提高计数准确性。为了进一步增强视觉链的物理合理性，我们引入了邻近损失函数，该函数显式建模了密集排列手术器械所固有的空间约束。我们还提出了SurgCount-HD，一个包含1,464张高密度手术器械图像的新数据集。大量实验表明，我们的方法在密集手术器械计数这一具有挑战性的任务中，优于现有的计数方法（如CountGD、REC）以及多模态大语言模型（如Qwen、ChatGPT）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate counting of surgical instruments in operating rooms is essential for patient safety, yet remains challenging in dense scenarios due to the tight clustering of objects. The Chain-of-Look framework addresses this by simulating the sequential human counting process through a structured visual chain, which allows the model to follow a coherent spatial trajectory for improved accuracy. A neighboring loss function is introduced to enforce spatial constraints, enhancing the physical plausibility of the counting process. The proposed method achieves superior performance compared to existing approaches such as CountGD, REC, and large language models like Qwen and ChatGPT on the SurgCount-HD dataset.</div>
<div class="mono" style="margin-top:8px">手术器械在手术室中的准确计数对于保障患者安全至关重要，但在密集场景下由于器械紧密排列，仍面临较大挑战。Chain-of-Look框架通过模拟人类按顺序计数的过程，引入结构化的视觉链，区别于传统的无序目标检测方法。为增强视觉链的物理合理性，提出邻近损失函数以显式建模密集排列器械的空间约束。在SurgCount-HD数据集上的实验表明，该方法在密集手术器械计数任务中优于现有最先进的计数方法和大型语言模型。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning GPT-5 for GPU Kernel Generation</div>
<div class="meta-line">Authors: Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah</div>
<div class="meta-line">First: 2026-02-11T16:22:54+00:00 · Latest: 2026-02-11T16:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora&#x27;s environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在GPU内核生成中微调GPT-5</div>
<div class="mono" style="margin-top:8px">开发高效的GPU内核对于扩展现代AI系统至关重要，但由于复杂的硬件架构和对专门优化知识的需求，这一任务仍然具有挑战性。尽管大型语言模型（LLMs）在通用顺序代码生成方面表现出强大的能力，但在GPU代码生成方面却面临显著挑战，原因包括高质量标记训练数据的稀缺、合成解决方案生成时的编译器偏差，以及在不同硬件世代间的泛化能力有限。这使得监督微调（SFT）难以作为可扩展的方法来改进当前的LLMs。相比之下，强化学习（RL）提供了一种数据高效且适应性强的替代方案，但需要访问相关工具、仔细选择训练问题，并具备强大的评估环境。我们介绍了Makora的环境和工具，用于前沿模型的强化学习微调，并报告了我们对GPT-5进行微调以生成Triton代码的结果。在单次尝试设置下，我们的微调模型将内核正确性从43.7%提升至77.0%（+33.3个百分点），并使在KernelBench基准测试中优于TorchInductor的问题比例从14.8%提升至21.8%（+7个百分点）。与基线GPT-5相比，我们的模型在KernelBench上超过了先前的最先进模型。当集成到完整的编码代理中时，它能够在扩展后的KernelBench套件中解决高达97.4%的问题，并在72.9%的问题上优于PyTorch的TorchInductor编译器，几何平均速度提升2.12倍。我们的工作表明，针对特定领域的强化学习后训练可以释放LLMs在高度专业化的技术领域中的能力，而传统监督学习则受限于数据的可获得性，从而为AI辅助的加速器编程开辟了新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenges in generating efficient GPU kernels using Large Language Models (LLMs), as supervised fine-tuning is limited by the lack of high-quality training data. The authors propose a reinforcement learning (RL) approach, leveraging Makora&#x27;s environment and tools, to fine-tune GPT-5 for Triton code generation. Their experiments show that the fine-tuned model significantly improves kernel correctness, achieving 77.0% compared to 43.7% in the baseline, and outperforms TorchInductor on 21.8% of problems. When integrated into a coding agent, it solves 97.4% of problems in an expanded KernelBench suite, with a geometric mean speedup of 2.12x over PyTorch&#x27;s compiler.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决使用大语言模型（LLMs）生成高效GPU内核的挑战，特别是由于高质量训练数据不足和监督微调的局限性。作者提出了一种基于强化学习（RL）的方法，利用Makora的环境和工具对GPT-5进行微调，以实现Triton代码生成。实验结果显示，微调后的模型在内核正确性方面显著提升，从43.7%提高到77.0%，并在与TorchInductor的对比中，解决问题的比例提高了7个百分点。当集成到编码代理中时，该模型能够在扩展版的KernelBench测试套件中解决97.4%的问题，优于TorchInductor在72.9%的问题上，并实现2.12倍的几何平均加速。</div>
</details>
</div>
<div class="card">
<div class="title">City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</div>
<div class="meta-line">Authors: Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</div>
<div class="meta-line">First: 2025-12-17T19:59:31+00:00 · Latest: 2026-02-11T06:31:53+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026 (ORAL)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15933v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.15933v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dwipddalal.github.io/AgentNav/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environment. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs, reasoning techniques (e.g., GEPA, chain-of-thought, reflection) and competitive baseline PReP significantly underperform in this challenging setting. To address this, we propose Verbalization of Path(VoP), which explicitly grounds the agent&#x27;s internal reasoning by probing city-scale cognitive maps (key landmarks and directions toward the destination) from the MLLM, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外城市导航：从大规模知识中探索MLLMs的涌现导航</div>
<div class="mono" style="margin-top:8px">利用多模态大语言模型（MLLMs）开发具身智能体为解决复杂的现实世界任务提供了巨大潜力。然而，当前的评估基准主要以语言为中心或严重依赖模拟环境，很少涉及实际现实场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们引入了稀疏锚定视觉导航任务，专门设计用于评估MLLMs在具有挑战性的、知识密集型的现实环境中的顺序决策能力。我们通过CityNav这一涵盖四个不同全球城市的综合性基准来实现该任务，专门用于评估原始的MLLM驱动智能体在城市导航中的表现。智能体仅需依赖视觉输入和内部多模态推理，无需额外的环境标注或专门的架构修改，即可在50多个决策点中进行顺序导航。关键的是，智能体必须通过解读城市特定线索和识别地标自主实现定位，进行空间推理，并战略性地规划和执行前往目标的路线。通过广泛的评估，我们表明当前最先进的MLLMs、推理技术（如GEPA、思维链、反思）以及具有竞争力的基线PReP在这一具有挑战性的场景中表现显著不足。为了解决这一问题，我们提出了路径语言化（VoP）方法，通过从MLLM中探查城市级认知地图（关键地标和朝向目标的方向）来显式锚定智能体的内部推理，从而显著提升导航成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of evaluating multimodal large language models (MLLMs) in real-world navigation tasks, where current benchmarks are insufficient. The authors propose the Sparsely Grounded Visual Navigation task, implemented through the CityNav benchmark, which tests MLLMs&#x27; ability to navigate complex urban environments using only visual inputs and internal reasoning. Their experiments show that existing methods, including state-of-the-art MLLMs and techniques like GEPA, chain-of-thought, and reflection, perform poorly in this setting. To improve performance, they introduce the Verbalization of Path (VoP) method, which explicitly grounds the agent&#x27;s reasoning by extracting city-scale cognitive maps, leading to better navigation outcomes.</div>
<div class="mono" style="margin-top:8px">本文旨在解决当前评估多模态大语言模型（MLLMs）在现实世界导航任务中的不足，这些任务在现有基准中常被忽视。作者提出了稀疏视觉导航任务，并通过CityNav基准进行实现，以评估MLLMs在复杂城市环境中仅依靠视觉输入和内部推理进行导航的能力。实验结果表明，现有的方法，包括最先进的MLLMs和推理技术如GEPA、思维链和反思，在该任务中表现不佳。为此，他们提出了VoP方法，通过提取城市级认知地图来显式地锚定代理的推理过程，从而显著提升导航成功率。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Federica Sarro, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-11T04:58:49+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：编码代理中上下文检索的基准测试</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准测试中表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估框架。ContextBench包含来自8种编程语言、66个仓库的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四种前沿的LLM和五个编码代理。我们的结果表明，复杂的代理结构在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有的端到端基准测试，揭示了问题解决过程中的关键环节。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of current evaluations of coding agents, which primarily focus on final task success rather than the process of retrieving and utilizing code context. ContextBench introduces a process-oriented benchmark that includes 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each with human-annotated gold contexts. The main experimental results reveal that advanced agent scaffolding provides only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between the context explored and the context actually used. These findings highlight the importance of incorporating intermediate context metrics into the evaluation of coding agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估编码代理在问题解决过程中如何检索和使用代码上下文，而不仅仅关注最终任务的成功率。ContextBench 提供了一个面向过程的基准，包含来自66个仓库、涵盖八种编程语言的1136个问题解决任务，每个任务都附有人工标注的黄金上下文。作者开发了一个自动化评估框架，用于追踪代理的行为并测量上下文的召回率、精确率和效率。实验结果表明，高级代理结构在上下文检索方面仅带来小幅提升，LLMs倾向于优先召回而非精确，且存在显著的探索与使用上下文之间的差距。这些发现强调了在基准测试中引入中间上下文指标的重要性，以更深入地理解并改进编码代理的推理过程。</div>
</details>
</div>
<div class="card">
<div class="title">MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps</div>
<div class="meta-line">Authors: Sharat Bhat, Harshita Khandelwal, Tushar Kataria, Vivek Gupta</div>
<div class="meta-line">First: 2026-02-11T04:36:14+00:00 · Latest: 2026-02-11T04:36:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10518v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10518v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapVerse：一个用于多样化真实地图的地理问答基准</div>
<div class="mono" style="margin-top:8px">地图是结构化和上下文知识的强大载体，涵盖地理、人口、基础设施和环境模式。对这类知识的推理需要模型能够整合空间关系、视觉线索、现实世界背景和领域专业知识，而当前的大语言模型（LLMs）和视觉-语言模型（VLMs）仍难以一致地展现这些能力。然而，目前用于评估VLMs在地图推理上的数据集范围狭窄，局限于特定领域，并且严重依赖人工生成内容（如LLMs或基于流水线的方法的输出），难以深入评估真实的地理推理能力。为了解决这一问题，我们提出了MapVerse，一个基于真实地图的大规模基准数据集。该数据集包含1,025张地图上的11,837对人工撰写的问答对，涵盖十种多样的地图类别，以及每种地图类别下的多种问题类型。该数据集为评估地图阅读、解释和多模态推理提供了丰富的场景。我们对十种最先进的模型进行了评估，以建立基准并量化推理能力的差距。除了整体表现外，我们还进行了细粒度的类别分析，以评估模型推理在多个维度上的表现，并探讨影响推理结果的视觉因素。我们的研究发现，尽管当前的VLMs在分类式任务上表现具有竞争力，但在需要复杂空间推理的高级任务上，无论是开源还是闭源模型都存在不足。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing datasets for evaluating geospatial reasoning in vision-language models (VLMs), which are often narrow in scope and rely on artificial content. MapVerse is introduced as a large-scale benchmark built on real-world maps, containing 11,837 human-authored question-answer pairs across 1,025 maps and ten diverse map categories. The main experimental results show that while current VLMs perform well on classification tasks, they struggle with more complex spatial reasoning, highlighting significant gaps in their capabilities.</div>
<div class="mono" style="margin-top:8px">MapVerse的提出是为了弥补现有地图推理基准数据集的不足，这些数据集通常范围狭窄且依赖人工生成内容。该基准基于真实世界地图构建，包含1,025张地图上的11,837对人工撰写的问答对，涵盖十个不同的地图类别。实验结果表明，尽管当前的视觉语言模型在分类任务上表现良好，但在需要复杂空间推理的高级任务上仍存在明显不足。</div>
</details>
</div>
<div class="card">
<div class="title">CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</div>
<div class="meta-line">Authors: Tianjiao Yu, Xinzhuo Li, Yifan Shen, Yuanzhe Liu, Ismini Lourentzou</div>
<div class="meta-line">First: 2025-12-14T17:05:11+00:00 · Latest: 2026-02-10T22:27:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12768v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12768v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoRe3D：以协作推理为基础的三维智能</div>
<div class="mono" style="margin-top:8px">近期在大型多模态模型方面的进展表明，显式的推理机制在提高模型可靠性、可解释性和跨模态对齐方面起着关键作用。尽管这种以推理为中心的方法在语言和视觉任务中已被证明是有效的，但其在三维领域的扩展仍不成熟。CoRe3D引入了一个统一的三维理解和生成推理框架，该框架在语义和空间抽象上联合运作，使从语言中推断出的高层意图能够直接指导低层三维内容的生成。该设计的核心是一个基于空间的推理表示，将三维潜在空间分解为局部区域，使模型能够以组合和程序化的方式进行几何推理。通过紧密耦合语义链式推理与结构化空间推理，CoRe3D生成的三维输出表现出强局部一致性和与语言描述的忠实对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of CoRe3D stems from the need to enhance the reliability, interpretability, and cross-modal alignment of models in 3D tasks, which has not been sufficiently addressed by existing reasoning-centric approaches. The method proposes a unified framework for 3D understanding and generation that integrates semantic and spatial abstractions, enabling high-level language intent to guide low-level 3D content creation. Key experimental results demonstrate that CoRe3D achieves strong local consistency in generated 3D outputs and maintains faithful alignment with linguistic descriptions through its spatially grounded reasoning representation.</div>
<div class="mono" style="margin-top:8px">CoRe3D的研究动机是通过引入显式的推理机制来提升3D智能系统的可靠性、可解释性和跨模态对齐能力。其方法提出了一种统一的3D理解和生成推理框架，该框架同时作用于语义和空间抽象层面，使语言中的高层意图能够直接指导3D内容的生成。关键实验结果表明，CoRe3D通过其基于空间的推理表示，生成的3D输出在局部一致性方面表现优异，并且能够忠实反映语言描述的内容。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning</div>
<div class="meta-line">Authors: Yancheng Long, Yankai Yang, Hongyang Wei, Wei Chen, Tianke Zhang, Haonan fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</div>
<div class="meta-line">First: 2026-02-07T09:23:34+00:00 · Latest: 2026-02-10T19:38:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07458v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term &quot;Attention Collapse,&quot; where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialReward：通过显式空间推理弥合在线强化学习在图像编辑中的感知鸿沟</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）为复杂的图像编辑提供了有前景的途径，但目前受到可靠且细粒度奖励信号稀缺的限制。现有评估器经常面临我们称之为『注意力崩溃』的关键感知鸿沟，其中模型忽略了跨图像比较，无法捕捉细粒度细节，导致感知不准确和评分失调。为了解决这些限制，我们提出了SpatialReward，这是一种通过显式空间推理强制进行精确验证的奖励模型。通过将推理锚定在预测的编辑区域，SpatialReward在像素级证据基础上进行语义判断，显著提升了评估的准确性。该模型在精心挑选的26万条空间感知数据集上进行训练，在MMRB2和EditReward-Bench上取得了最先进的性能，并在我们提出的MultiEditReward-Bench上优于专有评估器。此外，SpatialReward在在线RL中作为稳健的信号，使OmniGen2在GEdit-Bench上提升+0.90，超越了领先的判别模型，并将GPT-4.1的提升幅度翻倍（+0.45）。这些结果表明，空间推理对于在图像编辑中实现有效的对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the performance of online reinforcement learning (RL) in image editing by addressing the issue of unreliable reward signals. The proposed method, SpatialReward, introduces explicit spatial reasoning to bridge the perception gap, specifically tackling the problem of &quot;Attention Collapse&quot; where models fail to compare across images and miss fine-grained details. The model is trained on a large spatial-aware dataset and demonstrates state-of-the-art results on multiple benchmarks, significantly outperforming existing evaluators and enhancing the performance of OmniGen2 in online RL settings.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在线强化学习（RL）在图像编辑中的局限性，尤其是缺乏可靠且精细的奖励信号。提出的方法SpatialReward通过引入显式的空间推理来弥合感知差距，其重点在于预测的编辑区域，并将语义判断基于像素级证据。实验结果表明，SpatialReward在MMRB2和EditReward-Bench上达到了最先进的性能，并在MultiEditReward-Bench上优于现有的专有评估器。它还显著提升了OmniGen2在GEdit-Bench上的表现，超越了领先的判别模型，并将GPT-4.1的提升效果翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到动作：在空间基础先验中构建视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-动作（VLA）模型在三维真实世界中进行操作，但通常基于二维编码器，这导致了空间推理的缺失，限制了其泛化能力和适应性。近期的VLA三维集成技术要么需要专用传感器且跨模态迁移效果差，要么注入的线索较弱，缺乏几何信息并损害了视觉-语言对齐。在本工作中，我们引入了FALCON（从空间到动作），一种新颖范式，它将丰富的三维空间标记注入到动作头中。FALCON利用空间基础模型，仅通过RGB图像即可提供强大的几何先验，并包含一个可选的具身空间模型，能够融合深度或姿态信息以提高保真度，而无需重新训练或架构修改。为了保持语言推理能力，空间标记被输入到空间增强的动作头中，而不是简单地连接到视觉-语言主干网络中。这些设计使FALCON能够解决空间表示、模态迁移性和对齐方面的局限性。在三个模拟基准和十一项真实世界任务的全面评估中，我们提出的FALCON取得了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化下保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing vision-language-action (VLA) models that operate in 3D environments but rely on 2D encoders, leading to a spatial reasoning gap. The proposed FALCON model introduces a novel approach by injecting rich 3D spatial tokens into the action head, utilizing spatial foundation models to derive strong geometric priors from RGB data. It also incorporates an Embodied Spatial Model that can optionally integrate depth or pose information for enhanced accuracy. The Spatial-Enhanced Action Head preserves language reasoning without altering the vision-language backbone. Experimental results across three simulation benchmarks and eleven real-world tasks show that FALCON achieves state-of-the-art performance, outperforms baselines, and maintains robustness in challenging conditions such as clutter and varying object scales.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言-动作（VLA）模型在3D环境中运行但依赖2D编码器导致的空间推理不足问题。FALCON模型通过在动作头中引入丰富的3D空间标记，利用空间基础模型从RGB数据中提取强几何先验。它还包含一个可选的具身空间模型，能够融合深度或姿态信息以提高精度，而无需重新训练或改变架构。空间增强动作头独立处理这些空间标记，以保持语言推理能力。在三个模拟基准和十一项现实任务的全面评估中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化等挑战性条件下表现出鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Mindset: Reasoning with Adaptive Cognitive Modes</div>
<div class="meta-line">Authors: Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen</div>
<div class="meta-line">First: 2026-02-10T18:31:47+00:00 · Latest: 2026-02-10T18:31:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10063v1">PDF</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset">Code1</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维模式链：基于自适应认知模式的推理</div>
<div class="mono" style="margin-top:8px">人类解决问题的过程从不是单一思维模式的重复，这里的思维模式指的是不同的认知处理方式。在解决特定任务时，我们并不依赖单一的思维模式，而是将多种思维模式整合到同一个解决方案过程中。然而，现有的LLM推理方法陷入了一个普遍的误区：它们在所有步骤中都应用相同的固定思维模式，忽视了解决同一问题的不同阶段需要根本不同的思维模式。这种单一假设阻碍了模型向更高层次智能的发展。为了解决这一局限性，我们提出了Chain of Mindset（CoM），一个无需训练的代理框架，能够实现步骤级的自适应思维模式协调。CoM将推理分解为四种功能上异质的思维模式：空间思维、收敛思维、发散思维和算法思维。一个元代理根据推理状态的演变动态选择最优的思维模式，而双向上下文门控机制则过滤跨模块的信息流动，以保持推理的有效性和效率。我们在涵盖数学、代码生成、科学问答和空间推理的六个具有挑战性的基准上进行了实验，结果表明CoM在整体准确率上分别比最强基线模型Qwen3-VL-32B-Instruct和Gemini-2.0-Flash高出4.96\%和4.72\%，同时保持推理效率。我们的代码可在\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitation of existing large language models (LLMs) in reasoning tasks, where they often apply a fixed mindset across all steps, failing to adapt to the varying cognitive requirements of different problem-solving stages. The proposed Chain of Mindset (CoM) framework introduces a training-free agentic approach that dynamically orchestrates four distinct cognitive mindsets—Spatial, Convergent, Divergent, and Algorithmic—at the step level. A Meta-Agent selects the optimal mindset based on the current reasoning state, and a bidirectional Context Gate manages information flow between modules. Experimental results across six benchmarks in mathematics, code generation, scientific QA, and spatial reasoning show that CoM achieves state-of-the-art performance, improving overall accuracy by 4.96% and 4.72% on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash respectively, while maintaining reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">该研究针对现有大语言模型在推理任务中的局限性，即它们通常在整个推理过程中使用固定的心态，未能适应不同问题解决阶段所需的认知模式变化。提出的Chain of Mindset（CoM）框架采用无训练的代理方法，根据推理状态动态协调四种功能不同的认知模式：空间、收敛、发散和算法模式。Meta-Agent负责选择最优的认知模式，而双向上下文门控机制则管理模块间的信息流动。实验结果在六个涵盖数学、代码生成、科学问答和空间推理的基准测试中表明，CoM实现了最先进的性能，分别在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上将整体准确率提升了4.96\%和4.72\%，同时保持推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge</div>
<div class="meta-line">Authors: Yijie Lin, Guofeng Ding, Haochen Zhou, Haobin Li, Mouxing Yang, Xi Peng</div>
<div class="meta-line">First: 2026-02-10T14:45:02+00:00 · Latest: 2026-02-10T14:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARK：一个结合推理与知识的双轴多模态检索基准</div>
<div class="mono" style="margin-top:8px">现有的多模态检索基准主要强调日常图像的语义匹配，并对专业领域知识和复杂推理的诊断能力有限。为解决这一问题，我们引入了ARK，一个从两个互补视角分析多模态检索的基准：(i) 知识领域（五个领域，17个子类型），这些领域描述了检索所依赖的内容和专业知识；(ii) 推理技能（六个类别），这些类别描述了在识别正确候选时所需进行的多模态证据推理类型。具体而言，ARK评估了单模态和多模态查询与候选的检索，涵盖16种异构视觉数据类型。为了避免评估中的捷径匹配，大多数查询都与需要多步推理的针对性难负样本配对。我们在ARK上评估了23种代表性的基于文本和多模态的检索器，并观察到知识密集型与推理密集型检索之间存在显著差距，其中细粒度视觉和空间推理成为持续存在的瓶颈。我们进一步表明，简单的增强方法如重排序和重写能够带来一致的改进，但仍存在大量提升空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing multimodal retrieval benchmarks, which focus primarily on semantic matching in daily-life images and neglect professional knowledge and complex reasoning. ARK introduces a dual-axis benchmark that evaluates retrieval based on knowledge domains (five domains with 17 subtypes) and reasoning skills (six categories). The benchmark includes 16 heterogeneous visual data types and employs targeted hard negatives to prevent shortcut matching. Experimental results show a significant performance gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning being major bottlenecks. Simple techniques like re-ranking and rewriting provide consistent improvements, but substantial potential for enhancement still exists.</div>
<div class="mono" style="margin-top:8px">提出ARK基准的动机是解决现有多模态检索系统在处理专业领域知识和复杂推理任务方面的不足。ARK引入了一个双轴评估框架，分别从知识领域和推理技能两个互补的视角进行分析，涵盖五个知识领域和六种推理类别。该基准包含16种异构的视觉数据类型，并采用针对性的难例来避免捷径匹配。实验结果显示，知识密集型与推理密集型检索之间存在显著性能差距，细粒度的视觉和空间推理成为主要瓶颈。简单的改进方法如重排序和重写能够带来一致的提升，但仍存在较大的优化空间。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-10T14:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v2">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近期在多模态大语言模型（MLLMs）中进行空间推理的研究越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这常常导致语义与几何信息的错位以及冗余信号。我们提出GeoThinker框架，将整合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力机制选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控机制进行校准，该机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面设立了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力和显著提升的空间感知能力，包括具身指称和自动驾驶。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current spatial reasoning approaches in Multimodal Large Language Models (MLLMs), which often suffer from semantic-geometry misalignment and redundant signals due to passive integration of geometric information. The proposed GeoThinker framework introduces an active perception paradigm, enabling the model to selectively retrieve geometric evidence based on its internal reasoning needs. It employs Spatial-Grounded Fusion at specific VLM layers, where semantic visual priors query and integrate task-relevant geometry through frame-strict cross-attention, further refined by Importance Gating. Experimental results on VSI-Bench show that GeoThinker achieves a peak score of 72.6, outperforming existing methods, and demonstrates strong generalization and improved spatial perception in complex tasks such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对当前多模态大语言模型（MLLMs）在空间推理中存在的问题，即几何信息通常被动融合导致语义与几何不匹配及冗余。提出GeoThinker框架，通过主动感知策略，使模型能够根据内部推理需求选择性地检索几何证据。该方法在特定视觉语言模型（VLM）层应用空间基础融合，结合帧严格交叉注意力和重要性门控机制，以聚焦任务相关的结构。实验结果表明，GeoThinker在VSI-Bench上取得72.6的最高峰值成绩，并在复杂下游任务如具身指称和自动驾驶中展现出强大的泛化能力和空间感知提升。</div>
</details>
</div>
<div class="card">
<div class="title">EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems</div>
<div class="meta-line">Authors: Wentao Zhang, Jianfeng Wang, Liheng Liang, Yilei Zhao, HaiBin Wen, Zhe Zhao</div>
<div class="meta-line">First: 2026-02-10T14:04:22+00:00 · Latest: 2026-02-10T14:04:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10171v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvoCodeBench：一种用于自进化的LLM驱动编码系统的类人表现基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在编程任务中的持续进步，LLM驱动的编码系统已从单次代码生成演进为能够在推理过程中进行迭代改进的复杂系统。然而，现有的代码基准主要强调静态正确性，并隐式地假设推理过程中模型能力是固定的。因此，它们无法捕捉推理时的自我进化特性，例如代理在迭代优化解决方案时准确性与效率是否提升。此外，这些基准对资源成本的考量有限，并且很少将模型性能与人类程序员进行校准。而且，许多基准主要针对高资源语言，忽略了跨语言鲁棒性和长尾语言的稳定性。因此，我们提出了EvoCodeBench，这是一个用于评估自进化的LLM驱动编码系统在多种编程语言中的表现基准，并与人类表现进行直接比较。EvoCodeBench追踪性能动态，测量解决方案的正确性以及解决时间、内存消耗等效率指标，并在重复问题解决尝试中评估算法设计的改进。为了将评估建立在以人类为中心的参考框架中，我们直接在相同任务上将模型性能与人类程序员进行比较，从而在人类能力分布中实现相对性能评估。此外，EvoCodeBench支持多种编程语言，能够在统一协议下进行系统性的跨语言和长尾语言稳定性分析。我们的结果表明，自进化系统在时间上表现出可衡量的效率提升，而与人类相对的多语言分析提供了仅通过准确性无法获得的见解。EvoCodeBench为评估不断进化的LLM驱动编码智能奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind EvoCodeBench is to address the limitations of existing code benchmarks that focus on static correctness and ignore the dynamic self-evolution of LLM-driven coding systems during inference. The benchmark evaluates the performance of self-evolving systems across multiple programming languages by tracking correctness, solving time, memory consumption, and algorithmic improvements over repeated attempts. Experimental results show that these systems achieve measurable gains in efficiency over time and that comparing their performance to human programmers provides deeper insights into their capabilities.</div>
<div class="mono" style="margin-top:8px">EvoCodeBench的提出旨在解决现有代码基准测试主要关注静态正确性而忽视LLM驱动编码系统在推理过程中自我演进能力的问题。该基准通过跟踪重复问题解决过程中的正确性、解决时间、内存消耗和算法改进，评估系统性能的动态变化。同时，它直接将模型表现与人类程序员进行对比，实现以人类为中心的相对性能评估。实验结果表明，自我演进系统在时间推移中可实现可衡量的效率提升，且多语言评估提供了仅凭准确率无法获得的深入见解。</div>
</details>
</div>
<div class="card">
<div class="title">From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki</div>
<div class="meta-line">First: 2026-02-09T14:39:43+00:00 · Latest: 2026-02-10T08:48:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08735v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08735v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从对应关系到行动：多模态大语言模型中类人多图像空间推理</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在单图像空间推理方面取得了显著进展，但需要整合多个视角信息的多图像空间推理仍然具有挑战性。认知研究指出，人类通过两种机制解决此类任务：跨视角对应关系，用于识别不同视角中对应同一物理位置的区域；以及逐步视角转换，通过依次组合相对视角变化来完成推理。然而，现有研究仅部分且通常隐式地引入了这些机制，缺乏对两者的显式监督。我们提出了一种名为HATCH（跨视角对应关系和视角变化的人类感知训练）的训练框架，包含两个互补的目标：（1）块级空间对齐，鼓励块表示在空间对应区域中对齐；（2）行动后回答推理，要求模型在预测最终答案前生成显式的视角转换动作。在三个基准测试上的实验表明，HATCH在保持单图像推理能力的同时，显著优于同等规模的基线模型，并且在性能上与更大规模的模型竞争。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-image spatial reasoning in multimodal large language models (MLLMs), which is less developed than single-image reasoning. The authors propose HATCH, a training framework that explicitly incorporates two human-like mechanisms: cross-view correspondence and stepwise viewpoint transformation. By introducing Patch-Level Spatial Alignment and Action-then-Answer Reasoning as complementary objectives, HATCH enables the model to align spatial information across views and generate explicit viewpoint transitions. Experimental results on three benchmarks show that HATCH significantly outperforms existing baselines of similar size and achieves performance comparable to much larger models, while maintaining strong single-image reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文针对多图像空间推理在多模态大语言模型（MLLMs）中的挑战，提出了一种新的训练框架HATCH，以模拟人类的两种推理机制：跨视图对应和逐步视角转换。HATCH包含两个互补的目标：基于图像块的空间对齐和先生成视角转换动作再进行回答的推理方式。实验结果表明，HATCH在三个基准测试中显著优于同类规模的基线模型，并在性能上接近更大规模的模型，同时保持了良好的单图像推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs</div>
<div class="meta-line">Authors: Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-23T09:17:07+00:00 · Latest: 2026-02-10T07:17:25+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17653v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17653v2">PDF</a> · <a href="https://github.com/LiAuto-DSR/GeoGramBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored. In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning. To evaluate this capability, we present GeoGramBench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity. Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level. These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoGramBench：对现代大语言模型几何程序推理能力的基准测试</div>
<div class="mono" style="margin-top:8px">几何空间推理是许多人工智能应用的基础，但大语言模型（LLMs）在处理以过程代码形式表达的几何空间信息方面的能力仍缺乏充分探索。本文通过形式化程序到几何任务来解决这一问题，该任务挑战模型将程序化绘图代码转化为准确且抽象的几何推理。为了评估这一能力，我们提出了GeoGramBench，这是一个包含500个精心优化问题的基准测试，这些问题按照一个专门设计的三级分类法组织，考虑的是几何复杂性而非传统的数学推理复杂性。我们对17个前沿LLM的全面评估揭示了其在最高抽象层级上存在一致且显著的不足：即使最先进的模型在该层级上的准确率也低于50%。这些结果突显了程序驱动空间推理所面临的独特挑战，并确立了GeoGramBench作为推进符号到空间几何推理研究的宝贵资源。项目页面：https://github.com/LiAuto-DSR/GeoGramBench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored capability of large language models (LLMs) in handling geometric spatial reasoning expressed through procedural code. The authors introduce GeoGramBench, a benchmark consisting of 500 carefully curated problems organized into a three-level taxonomy based on geometric complexity. Through comprehensive evaluation of 17 state-of-the-art LLMs, they find that even the most advanced models struggle significantly, achieving less than 50% accuracy at the highest abstraction level. These findings underscore the challenges of program-driven spatial reasoning and position GeoGramBench as a critical resource for future research in symbolic-to-spatial geometric reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLMs）在处理通过程序代码表达的几何空间推理能力这一尚未充分研究的领域展开探讨。作者提出了GeoGramBench基准测试，包含500个精心整理的问题，并按照几何复杂度而非传统数学推理复杂度构建了三级分类体系。对17个前沿LLM的全面评估显示，即使是最先进的模型在最高抽象层级的推理准确率也低于50%，揭示了程序驱动空间推理所面临的独特挑战，并确立了GeoGramBench作为推进符号到空间几何推理研究的重要资源。</div>
</details>
</div>
<div class="card">
<div class="title">AgentCgroup: Understanding and Controlling OS Resources of AI Agents</div>
<div class="meta-line">Authors: Yusheng Zheng, Jiakun Fan, Quanzhi Fu, Yiwei Yang, Wei Zhang, Andi Quinn</div>
<div class="meta-line">First: 2026-02-10T02:37:42+00:00 · Latest: 2026-02-10T02:37:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09345v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09345v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed in multi-tenant cloud environments, where they execute diverse tool calls within sandboxed containers, each call with distinct resource demands and rapid fluctuations. We present a systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Our measurements reveal that (1) OS-level execution (tool calls, container and agent initialization) accounts for 56-74% of end-to-end task latency; (2) memory, not CPU, is the concurrency bottleneck; (3) memory spikes are tool-call-driven with a up to 15.4x peak-to-average ratio; and (4) resource demands are highly unpredictable across tasks, runs, and models. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three mismatches in existing resource controls: a granularity mismatch (container-level policies vs. tool-call-level dynamics), a responsiveness mismatch (user-space reaction vs. sub-second unpredictable bursts), and an adaptability mismatch (history-based prediction vs. non-deterministic stateful execution). We propose AgentCgroup , an eBPF-based resource controller that addresses these mismatches through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies driven by in-kernel monitoring. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentCgroup：理解与控制AI代理的OS资源</div>
<div class="mono" style="margin-top:8px">AI代理越来越多地部署在多租户云环境中，在沙箱容器中执行多样化的工具调用，每个调用具有不同的资源需求和快速波动。我们对沙箱AI编码代理的OS级资源动态进行了系统性表征，分析了来自SWE-rebench基准的144个软件工程任务，涉及两个LLM模型。我们的测量结果表明：(1) OS级执行（工具调用、容器和代理初始化）占端到端任务延迟的56-74%；(2) 内存而非CPU是并发瓶颈；(3) 内存峰值由工具调用驱动，其峰值与平均值的比值高达15.4倍；(4) 资源需求在任务、运行和模型之间高度不可预测。通过将这些特征与无服务器、微服务和批处理工作负载进行比较，我们识别出现有资源控制存在三个不匹配：粒度不匹配（容器级策略与工具调用级动态）、响应性不匹配（用户空间反应与亚秒级不可预测的突发）、以及适应性不匹配（基于历史的预测与非确定性状态执行）。我们提出了AgentCgroup，一种基于eBPF的资源控制器，通过与工具调用边界对齐的分层cgroup结构、内核级强制执行（通过sched_ext和memcg_bpf_ops）以及由内核监控驱动的运行时自适应策略来解决这些问题。初步评估表明，该方案提升了多租户隔离性并减少了资源浪费。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing deployment of AI agents in multi-tenant cloud environments necessitates a deeper understanding of their OS-level resource usage patterns. This study characterizes the resource dynamics of sandboxed AI coding agents by analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Key findings include that OS-level operations account for a significant portion of task latency, memory is the primary concurrency bottleneck, and resource demands are highly unpredictable. The research identifies three mismatches between current resource control mechanisms and AI agent behavior and proposes AgentCgroup, an eBPF-based controller that uses hierarchical cgroup structures and in-kernel enforcement to address these issues. Preliminary results show enhanced multi-tenant isolation and reduced resource waste.</div>
<div class="mono" style="margin-top:8px">随着AI代理在多租户云环境中的广泛应用，对其操作系统级资源使用模式的理解变得尤为重要。本文通过分析SWE-rebench基准中的144个软件工程任务，研究了沙箱环境中AI编码代理的资源动态。研究发现，操作系统级执行占任务端到端延迟的56-74%，内存而非CPU是并发瓶颈，内存峰值可达平均值的15.4倍，且资源需求在任务、运行和模型间高度不可预测。为解决现有资源控制机制的不足，本文提出AgentCgroup，一种基于eBPF的资源控制器，通过与工具调用边界对齐的分层cgroup结构、内核级执行和运行时自适应策略进行优化。初步评估表明，该方法提升了多租户隔离性并减少了资源浪费。</div>
</details>
</div>
<div class="card">
<div class="title">Automated QoR improvement in OpenROAD with coding agents</div>
<div class="meta-line">Authors: Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee</div>
<div class="meta-line">First: 2026-01-09T19:30:02+00:00 · Latest: 2026-02-09T22:54:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06268v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06268v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, effective clock period reductions of up to 10.0%, and power reductions of up to 19.4%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在OpenROAD中使用编码代理实现自动化QoR改进</div>
<div class="mono" style="margin-top:8px">EDA开发和创新一直受到专家工程资源稀缺的限制。尽管领先的LLMs在编码和科学推理任务中表现出色，但它们在推动EDA技术进步方面的潜力尚未经过充分验证。我们提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主、基于仓库的编码系统，能够读取OpenROAD，提出研究方向，将其扩展为实施步骤，并提交可执行的代码差异。我们的贡献包括：(i) 一个用于EDA代码修改的闭环LLM框架；(ii) 针对PPA导向改进的OpenROAD任务套件和评估协议；以及 (iii) 需要最少人工监督的端到端演示。在OpenROAD上的实验实现了最高达5.9%的布线线长减少、10.0%的有效时钟周期减少和19.4%的功耗降低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of EDA development caused by the shortage of expert engineering resources. The authors introduce AuDoPEDA, an autonomous coding system that leverages OpenAI models and a Codex-class agent to analyze OpenROAD, suggest research directions, generate implementation steps, and submit executable code changes. Experimental results show that AuDoPEDA achieves up to 5.9% reduction in routed wirelength, 10.0% improvement in effective clock period, and 19.4% decrease in power consumption.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决EDA开发与创新中专家工程资源不足的问题。作者提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主编码系统，能够分析OpenROAD，提出研究方向，生成实现步骤并提交可执行的代码修改。实验结果表明，AuDoPEDA在布线长度、有效时钟周期和功耗方面分别实现了最高5.9%、10.0%和19.4%的改进。</div>
</details>
</div>
<div class="card">
<div class="title">AIDev: Studying AI Coding Agents on GitHub</div>
<div class="meta-line">Authors: Hao Li, Haoxiang Zhang, Ahmed E. Hassan</div>
<div class="meta-line">First: 2026-02-09T20:45:58+00:00 · Latest: 2026-02-09T20:45:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.
  &gt; AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIDev：在 GitHub 上研究 AI 编码代理</div>
<div class="mono" style="margin-top:8px">AI 编码代理正在迅速改变软件工程，执行诸如功能开发、调试和测试等任务。尽管其影响日益扩大，研究界仍缺乏一个全面的数据集，用于记录这些代理在实际项目中的使用情况。为了解决这一问题，我们引入了 AIDev，这是一个大规模的数据集，专注于实际 GitHub 仓库中由代理生成的拉取请求（Agentic-PRs）。AIDev 整合了由五个代理（OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code）生成的 932,791 个 Agentic-PRs，这些 PRs 涉及 116,211 个仓库和 72,189 名开发者。此外，AIDev 还包含一个精选的 33,596 个 Agentic-PRs 子集，来自拥有超过 100 星标评分的 2,807 个仓库，提供了诸如评论、审查、提交和相关问题等更多信息。该数据集为未来关于 AI 应用、开发者生产力和人机协作的研究提供了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid development of AI coding agents has significantly influenced software engineering, yet there is a lack of comprehensive datasets to study their real-world usage. To address this, AIDev was created as a large-scale dataset containing 932,791 agent-authored pull requests from five AI coding agents, including OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These pull requests span 116,211 repositories and involve 72,189 developers. AIDev also includes a curated subset of 33,596 pull requests from popular repositories with over 100 stars, offering additional context such as comments, reviews, and related issues. This dataset provides a valuable resource for analyzing AI adoption, developer productivity, and human-AI collaboration in modern software engineering practices.</div>
<div class="mono" style="margin-top:8px">随着AI编码代理在软件工程领域的快速发展，其对功能开发、调试和测试等任务的影响日益显著，但目前缺乏全面的数据集来研究其实际应用情况。为填补这一空白，AIDev被创建为一个大规模数据集，包含由五个AI编码代理（包括OpenAI Codex、Devin、GitHub Copilot、Cursor和Claude Code）生成的932,791个代理提交的拉取请求（Agentic-PRs）。这些拉取请求覆盖了116,211个仓库，涉及72,189名开发者，其中精选了33,596个来自拥有超过100颗星标的2,807个仓库的拉取请求，提供了包括评论、审查、提交和相关问题在内的详细信息。该数据集为未来研究AI采纳、开发者生产力和人机协作提供了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-09T15:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v2">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在单个问题的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念导向任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ProjDevBench is to address the gap in evaluating coding agents for end-to-end project development, as current benchmarks focus on individual bug fixes rather than comprehensive system building. The benchmark combines Online Judge testing with LLM-assisted code review to assess agents on system architecture design, functional correctness, and iterative solution refinement. It includes 20 programming problems across 8 categories, evaluating six coding agents based on different LLM backends. The main experimental results show an overall acceptance rate of 27.38%, indicating that while agents can handle basic functionality and data structures, they face significant challenges in complex system design, time complexity optimization, and resource management.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，现有评估主要集中在单个问题的修复上，而忽略了端到端项目开发的全面性。ProjDevBench提出一个基准，通过提供项目需求并评估生成的代码仓库，从系统架构设计、功能正确性和迭代解决方案优化三个方面对编码代理进行测试。该基准包含8个类别下的20个编程问题，涵盖概念性任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。实验结果显示，尽管代理在基础功能和数据结构方面表现良好，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难，整体接受率为27.38%。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-09T14:24:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展带来的边际效益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，通过提升多样性而非单纯增加数据量来扩展代码代理的性能。在固定训练预算下，增加轨迹多样性所带来的性能提升比增加轨迹数量更大，从而优化了代理训练的性能-成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，用于捕捉真实服务中的逻辑依赖关系；（2）蓝图驱动的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙箱环境中的代码工具，以减轻内在编码能力的灾难性遗忘。我们在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得显著提升。我们计划在论文发表后公开完整的代码库和合成数据集（包含30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of code large language models (LLMs) when evolving into tool-interactive agents, particularly the issues of low-quality synthetic data and the diminishing returns of increasing data quantity. It introduces TDScaling, a data synthesis framework that enhances performance by focusing on trajectory diversity rather than raw volume. The framework incorporates four innovations: capturing logical dependencies with a Business Cluster mechanism, ensuring trajectory coherence through a blueprint-driven multi-agent paradigm, preventing mode collapse with an adaptive evolution mechanism based on entropy metrics, and maintaining coding proficiency with a sandboxed code tool. Experimental results on various benchmarks and code agent tasks show that TDScaling significantly improves both tool-use generalization and coding capabilities under a fixed training budget.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）在转变为工具交互代理过程中遇到的性能瓶颈，指出其受限于低质量合成数据和数据量增长带来的边际效益递减。作者提出TDScaling，一种基于轨迹多样性提升的合成数据框架，通过增加轨迹多样性而非数据量来提高模型性能。该框架包含四项创新：利用业务聚类机制捕捉真实服务的逻辑依赖关系，通过蓝图驱动的多代理范式确保轨迹一致性，使用领域熵、推理模式熵和累积动作复杂度引导合成向长尾场景发展以防止模式坍缩，以及通过沙盒环境的代码工具缓解内在编码能力的灾难性遗忘。实验结果表明，在多个基准测试中，TDScaling在提升工具使用泛化能力和编码能力方面均优于传统的基于数据量的扩展方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models</div>
<div class="meta-line">Authors: Yueyan Li, Chenggong Zhao, Zeyuan Zang, Caixia Yuan, Xiaojie Wang</div>
<div class="meta-line">First: 2025-09-23T16:07:18+00:00 · Latest: 2026-02-09T10:18:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19191v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19191v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the &quot;what&quot; and &quot;where&quot; pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model&#x27;s perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像阅读文本一样阅读图像：视觉-语言模型中的序列化图像理解</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多种现实任务中表现出色。然而，现有VLMs通常通过序列化图像处理视觉信息，这种方法与人类视觉的并行特性存在显著差异。此外，其内部机制的不透明性阻碍了更深入的理解和架构创新。受人类视觉双流假说的启发，我们区分了“what”（物体识别）和“where”（空间感知）路径，将VLMs中的视觉处理分解为这两个部分进行独立研究。对于物体识别，我们将图像转换为文本标记图，并发现模型对图像内容的感知是一个从浅层到深层的两阶段过程，始于属性识别，最终实现语义消歧。对于空间感知，我们从理论上推导并实证验证了VLMs中位置表示的几何结构。基于这些发现，我们引入了一种基于即插即用视觉解码器的指令无关标记压缩算法以提高解码效率，并提出了一种RoPE缩放技术以增强空间推理能力。通过严谨的实验，我们的工作验证了这些分析，为深入理解VLMs内部机制提供了依据，并为设计更具能力的未来架构提供了清晰的指导原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing Vision-Language Models (VLMs) by highlighting their deviation from the parallel processing of human vision and their lack of transparency in internal mechanisms. The authors propose a dual-stream approach, decomposing visual processing into object recognition and spatial perception, inspired by the human visual system&#x27;s &#x27;what&#x27; and &#x27;where&#x27; pathways. They analyze how VLMs perceive image content through a two-stage process involving attribute recognition and semantic disambiguation, and uncover the geometric structure behind positional representations. Based on these insights, they introduce a token compression algorithm and a RoPE scaling technique to improve decoding efficiency and spatial reasoning, which are validated through extensive experiments.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言模型（VLMs）在视觉处理上偏离人类并行机制以及内部机制不透明的问题展开研究。受双流假说启发，作者将视觉处理分解为物体识别与空间感知两个部分，发现物体理解在模型中呈现从浅层到深层的两阶段过程，先进行属性识别，最终实现语义消歧。同时，他们通过理论推导和实验证实了位置表示背后的几何结构，并提出了基于可插拔视觉解码器的指令无关的token压缩算法和RoPE缩放技术以提升空间推理能力。实验结果验证了这些分析，为深入理解VLM内部机制和未来架构设计提供了清晰的指导原则。</div>
</details>
</div>
<div class="card">
<div class="title">BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models</div>
<div class="meta-line">Authors: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li</div>
<div class="meta-line">First: 2026-02-09T08:47:14+00:00 · Latest: 2026-02-09T08:47:14+00:00</div>
<div class="meta-line">Comments: 38 pages, 9 figures. Project page:https://bimanibench.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08392v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bimanibench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiManiBench：一种用于评估多模态大语言模型双臂协调的分层基准</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）显著推动了具身人工智能的发展，使用它们对机器人智能进行基准测试已成为关键趋势。然而，现有框架主要局限于单臂操作，无法捕捉像提起重锅这样的双臂任务所需的时空协调。为此，我们引入BiManiBench，这是一个分层基准，从三个层级评估MLLMs：基础空间推理、高级动作规划和低级末端执行器控制。我们的框架隔离了双臂任务中的独特挑战，如手臂可达性与运动学约束，从而区分感知幻觉与规划失败。对超过30种最先进的模型的分析表明，尽管在高级推理方面表现优异，MLLMs在双臂空间定位和控制方面仍存在困难，经常导致相互干扰和顺序错误。这些发现表明当前范式缺乏对相互运动学约束的深入理解，强调未来研究应关注双臂碰撞避免和精细的时间序列控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the bimanual coordination capabilities of Multimodal Large Language Models (MLLMs) in the context of embodied AI and robotic intelligence. To achieve this, the authors propose BiManiBench, a hierarchical benchmark that assesses MLLMs across three levels: spatial reasoning, action planning, and end-effector control. The main experimental results show that while many MLLMs perform well in high-level reasoning, they struggle with dual-arm spatial grounding and control, often leading to mutual interference and sequencing errors, indicating a gap in understanding inter-arm kinematic constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估多模态大语言模型（MLLMs）在具身AI中的双臂协调能力，因为现有基准主要局限于单臂操作。为此，提出了BiManiBench这一分层基准，用于评估MLLMs在空间推理、动作规划和末端执行器控制三个层面的表现。实验结果表明，尽管MLLMs在高层推理方面表现良好，但在双臂空间定位和控制上存在困难，常导致手臂相互干扰和顺序错误，反映出当前范式对双臂运动学约束理解不足，提示未来研究应关注手臂间碰撞避免和精细时间序列控制。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science</div>
<div class="meta-line">Authors: Jie Zhang, Xingtong Yu, Yuan Fang, Rudi Stouffs, Zdravko Trivic</div>
<div class="meta-line">First: 2026-02-09T07:28:49+00:00 · Latest: 2026-02-09T07:28:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08342v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanGraphEmbeddings：为城市科学学习和评估空间锚定的多模态嵌入</div>
<div class="mono" style="margin-top:8px">在城市环境中学习可迁移的多模态嵌入具有挑战性，因为城市理解本质上是空间的，但现有的数据集和基准缺乏街道视图图像与城市结构之间的显式对齐。我们引入了UGData，这是一个空间锚定的数据集，将街道视图图像锚定到结构化的空间图上，并通过空间推理路径和空间上下文描述提供图对齐的监督，揭示超出图像内容的距离、方向性、连通性和邻里上下文。基于UGData，我们提出了UGE，这是一种两阶段的训练策略，通过结合指令引导的对比学习与基于图的空间编码，逐步且稳定地对齐图像、文本和空间结构。最后，我们引入了UGBenchmark，这是一个全面的基准，用于评估空间锚定嵌入如何支持多样化的城市理解任务——包括地理定位排序、图像检索、城市感知和空间锚定。我们在多个最先进的视觉语言模型（VLM）主干网络上开发了UGE，包括Qwen2-VL、Qwen2.5-VL、Phi-3-Vision和LLaVA1.6-Mistral，并使用LoRA调优训练固定维度的空间嵌入。基于Qwen2.5-VL-7B主干网络的UGE在训练城市中实现了图像检索44%的提升和地理定位排序30%的提升，在保留城市中分别实现了超过30%和22%的提升，证明了显式空间锚定在空间密集型城市任务中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of learning transferable multimodal embeddings for urban environments, where understanding is inherently spatial but existing datasets lack explicit alignment between street-view images and urban structure. To tackle this, the authors introduce UGData, a dataset that links street-view images to spatial graphs and provides supervision through spatial reasoning paths and context captions. They propose UGE, a two-stage training strategy that combines instruction-guided contrastive learning with graph-based spatial encoding to align images, text, and spatial structures. Experimental results show that UGE achieves significant improvements in image retrieval and geolocation ranking on both training and held-out cities, demonstrating the effectiveness of spatially grounded embeddings for urban understanding tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决城市环境中学习可迁移的多模态嵌入的挑战，因为城市理解本质上具有空间属性，但现有数据集缺乏街道视图图像与空间结构之间的显式对齐。为此，作者提出了UGData数据集，该数据集将街道视图图像锚定到结构化空间图上，并通过空间推理路径和上下文描述提供图对齐的监督。他们提出了一种两阶段的训练策略UGE，结合了指令引导的对比学习和基于图的空间编码，以对齐图像、文本和空间结构。实验结果表明，基于Qwen2.5-VL-7B骨干网络的UGE在训练城市中分别实现了图像检索和地理定位排名44%和30%的提升，在未见城市中分别达到30%和22%的提升，证明了显式空间对齐在空间密集型城市任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</div>
<div class="meta-line">Authors: Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal</div>
<div class="meta-line">First: 2026-02-09T03:21:48+00:00 · Latest: 2026-02-09T03:21:48+00:00</div>
<div class="meta-line">Comments: the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adaptive-visual-tts.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时以及想象多少：使用世界模型进行视觉空间推理的自适应测试时缩放</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）取得了快速进展，但当正确答案依赖于未见过或替代视角下的场景外观时，视觉空间推理仍然不可靠。最近的研究通过引入世界模型来增强推理过程中的视觉想象能力，但关于何时需要想象、想象多少是有益的，以及何时会带来负面影响等问题仍不明确。在实践中，无差别地使用想象可能会增加计算量，甚至通过引入误导性证据而降低性能。在本文中，我们深入分析了测试时的视觉想象作为一种可控资源在空间推理中的作用。我们研究了静态视觉证据何时足够、何时能提升推理能力，以及过度或不必要的想象如何影响准确性和效率。为支持这一分析，我们提出了AVIC，一个基于世界模型的自适应测试时框架，该框架在选择性调用和缩放视觉想象之前，会显式地推理当前视觉证据是否足够。在空间推理基准（SAT、MMSI）和一个具身导航基准（R2R）上的实验结果表明，存在一些明确的场景，其中想象是关键、边缘或有害的，同时证明了选择性控制策略可以在大幅减少世界模型调用和语言标记数量的情况下，匹配或超越固定想象策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of visual spatial reasoning in Multimodal Large Language Models (MLLMs), where reliability is compromised when answers depend on unseen or alternative viewpoints. The authors propose AVIC, an adaptive test-time framework that leverages world models to selectively control the use of visual imagination based on the sufficiency of existing visual evidence. Their experiments on spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation task (R2R) demonstrate that imagination can be critical, marginal, or even detrimental depending on the context, and that AVIC achieves comparable or better performance than fixed imagination strategies with fewer world-model calls and language tokens.</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在视觉空间推理方面取得了快速进展，但当正确答案依赖于未见过或替代视角的场景时，其可靠性仍存在问题。本文提出AVIC框架，通过世界模型在测试时选择性地控制视觉想象的使用，以判断当前视觉证据是否足够。在SAT、MMSI等空间推理基准以及R2R导航任务上的实验表明，视觉想象在不同情境下可能具有关键性、边际性或有害性。结果表明，AVIC在减少世界模型调用和语言标记数量的同时，能够达到或超越固定想象策略的性能。</div>
</details>
</div>
<div class="card">
<div class="title">ViT-5: Vision Transformers for The Mid-2020s</div>
<div class="meta-line">Authors: Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad, Cihang Xie, Alan Yuille</div>
<div class="meta-line">First: 2026-02-08T18:03:44+00:00 · Latest: 2026-02-08T18:03:44+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/wangf3014/ViT-5</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08071v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08071v1">PDF</a> · <a href="https://github.com/wangf3014/ViT-5">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViT-5：面向2020年代中期的视觉Transformer</div>
<div class="mono" style="margin-top:8px">本工作系统地探讨了通过利用过去五年架构进展来现代化视觉Transformer主干的方法。在保留经典注意力-前馈网络结构的基础上，我们对各个组件进行了细致优化，包括归一化、激活函数、位置编码、门控机制以及可学习标记。这些改进构成了新一代的视觉Transformer，我们称之为ViT-5。大量实验表明，ViT-5在理解和生成基准测试中均优于当前最先进的普通视觉Transformer。在ImageNet-1k分类任务中，ViT-5-Base在计算资源相当的情况下达到84.2%的top-1准确率，超过了DeiT-III-Base的83.8%。此外，ViT-5也作为生成建模的更强主干：当将其嵌入到SiT扩散框架中时，其FID为1.84，而使用普通ViT主干时为2.06。除了主要指标外，ViT-5在表示学习和空间推理方面也有所提升，并且在任务间具有良好的迁移能力。其设计与当前基础模型实践相一致，为2020年代中期的视觉主干提供了一个简单的替换升级方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces ViT-5, an updated Vision Transformer architecture that incorporates recent architectural improvements from the past five years while retaining the canonical Attention-FFN structure. The modifications include refinements to normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. Experimental results show that ViT-5 outperforms previous Vision Transformer models on both classification and generative tasks, achieving 84.2% top-1 accuracy on ImageNet-1k and a lower FID score in diffusion models. These enhancements lead to better representation learning and task transfer capabilities, making ViT-5 a more effective backbone for modern vision tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在通过引入过去五年中变压器架构的最新进展，对视觉变换器（ViT）进行现代化改进。作者在保留经典注意力-前馈网络结构的基础上，对归一化、激活函数、位置编码和门控机制等关键组件进行了优化，提出了新的ViT-5模型。实验结果显示，ViT-5在图像分类和生成任务中均优于现有模型。在ImageNet-1k分类任务中，ViT-5-Base在计算资源相当的情况下达到84.2%的top-1准确率，超越了DeiT-III-Base。在基于扩散模型的生成任务中，ViT-5的FID得分为1.84，优于标准ViT模型的2.06，表明其生成质量更高。</div>
</details>
</div>
<div class="card">
<div class="title">View-Centric Multi-Object Tracking with Homographic Matching in Moving UAV</div>
<div class="meta-line">Authors: Deyi Ji, Lanyun Zhu, Siqi Gao, Qi Zhu, Yiru Zhao, Peng Xu, Yue Ding, Hongtao Lu, Jieping Ye, Feng Wu, Feng Zhao</div>
<div class="meta-line">First: 2024-03-16T06:48:33+00:00 · Latest: 2026-02-08T10:40:20+00:00</div>
<div class="meta-line">Comments: TGRS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.10830v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.10830v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we address the challenge of Multi-Object Tracking (MOT) in moving Unmanned Aerial Vehicle (UAV) scenarios, where irregular flight trajectories, such as hovering, turning left/right, and moving up/down, lead to significantly greater complexity compared to fixed-camera MOT. Specifically, changes in the scene background not only render traditional frame-to-frame object IoU association methods ineffective but also introduce significant view shifts in the objects, which complicates tracking. To overcome these issues, we propose a novel HomView-MOT framework, which for the first time, harnesses the view homography inherent in changing scenes to solve MOT challenges in moving environments, incorporating homographic matching and view-centric concepts. We introduce a Fast Homography Estimation (FHE) algorithm for rapid computation of homography matrices between video frames, enabling object View-Centric ID Learning (VCIL) and leveraging multi-view homography to learn cross-view ID features. Concurrently, our Homographic Matching Filter (HMF) maps object bounding boxes from different frames onto a common view plane for a more realistic physical IoU association. Extensive experiments have proven that these innovations allow HomView-MOT to achieve state-of-the-art performance on prominent UAV MOT datasets VisDrone and UAVDT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于同构匹配的移动无人机多目标跟踪</div>
<div class="mono" style="margin-top:8px">本文针对移动无人机场景下的多目标跟踪（MOT）挑战，提出了一种新颖的HomView-MOT框架。该框架首次利用变化场景中固有的视图同构性，解决移动环境中的MOT问题，结合了同构匹配和以视图为中心的概念。我们引入了快速同构估计（FHE）算法，用于快速计算视频帧之间的同构矩阵，从而实现目标视图中心ID学习（VCIL）并利用多视图同构学习跨视图ID特征。同时，我们的同构匹配滤波器（HMF）将不同帧中的目标边界框映射到统一的视图平面上，以更真实地进行物理IoU关联。大量实验表明，这些创新使HomView-MOT在VisDrone和UAVDT等主流无人机MOT数据集上达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-object tracking (MOT) in moving UAV scenarios, where irregular flight paths and changing backgrounds complicate traditional frame-to-frame association methods. The authors propose the HomView-MOT framework, which utilizes homographic matching and view-centric concepts to improve tracking accuracy by leveraging the inherent view homography in dynamic scenes. A Fast Homography Estimation (FHE) algorithm enables efficient computation of homography matrices, supporting View-Centric ID Learning (VCIL) and cross-view feature learning. The Homographic Matching Filter (HMF) further enhances object association by mapping bounding boxes to a common view plane. Experimental results on the VisDrone and UAVDT datasets demonstrate that HomView-MOT achieves state-of-the-art performance in moving UAV MOT tasks.</div>
<div class="mono" style="margin-top:8px">本文针对移动无人机（UAV）场景下的多目标跟踪（MOT）问题，探讨了不规则飞行轨迹和变化背景带来的挑战。作者提出HomView-MOT框架，首次利用场景中的视图仿射变换特性，结合视图中心概念和仿射匹配方法来提升跟踪性能。该框架引入了快速仿射估计（FHE）算法，用于高效计算视频帧间的仿射矩阵，支持视图中心ID学习（VCIL）和跨视图特征学习。同时，Homographic Matching Filter（HMF）通过将不同帧中的目标边界框映射到统一视图平面，提高目标关联的准确性。在VisDrone和UAVDT等主流无人机MOT数据集上的实验表明，HomView-MOT在动态无人机环境中实现了最先进的跟踪效果。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents</div>
<div class="meta-line">Authors: Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, Lingxiao Jiang</div>
<div class="meta-line">First: 2026-02-08T10:26:31+00:00 · Latest: 2026-02-08T10:26:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07900v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考基于大语言模型的软件工程代理生成测试的价值</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）代码代理正越来越多地通过迭代编辑代码、调用工具和验证候选补丁来解决仓库级别的问题。在这些工作流中，代理经常临时编写测试，这一范式被许多排名靠前的代理采用。然而，我们观察到，GPT-5.2 几乎不编写新测试，却仍能实现与顶级代理相当的性能。这引发了关键问题：这些测试是否真正有助于问题解决，还是仅仅模仿了人类的测试实践，同时消耗了大量交互预算？
  为了揭示代理编写测试的影响，我们进行了一项实证研究，分析了在 SWE-bench Verified 上六个最先进的 LLM 代理的轨迹。我们的结果显示，尽管测试编写是常见的做法，但同一模型中已解决和未解决的任务在测试编写频率上表现出相似性。此外，这些测试通常作为观察反馈渠道，代理更倾向于使用揭示价值的打印语句，而不是正式的断言检查。基于这些见解，我们通过修改四个代理的提示来增加或减少测试编写量，并进行了受控实验。结果表明，代理编写测试的数量变化不会显著影响最终结果。综上所述，我们的研究揭示了当前测试编写实践在自主软件工程任务中可能提供的边际效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the role of agent-generated tests in LLM-based software engineering agents, motivated by the observation that agents with minimal test writing can perform comparably to top-ranked ones on the SWE-bench leaderboard. The research analyzes agent behavior across six advanced LLMs on the SWE-bench Verified dataset, revealing that test writing is common but not strongly correlated with task resolution. The findings indicate that tests often function as observational feedback mechanisms, with agents favoring print statements over formal assertions. A controlled experiment modifying test writing prompts showed no significant impact on final outcomes, suggesting that current test-writing practices may offer limited utility in autonomous software engineering tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于大语言模型（LLM）的软件工程代理中自动生成测试的价值，动机源于观察到SWE-bench排行榜上的高表现代理通常会即时编写测试，但像GPT-5.2这样几乎不生成新测试的模型却能达到相似的性能。通过分析六种先进LLM在SWE-bench Verified上的代理轨迹，研究发现测试编写虽普遍，但与任务解决与否无明显关联。进一步分析表明，代理更倾向于使用能提供反馈的打印语句而非正式断言，且控制实验显示测试编写量的变化对最终结果影响不大，表明当前的测试编写实践可能在自主软件工程任务中提供有限的效用。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds</div>
<div class="meta-line">Authors: Chen Yang, Guanxin Lin, Youquan He, Peiyao Chen, Guanghe Liu, Yufan Mo, Zhouyuan Xu, Linhao Wang, Guohui Zhang, Zihang Zhang, Shenxiang Zeng, Chen Wang, Jiansheng Fan</div>
<div class="meta-line">First: 2026-02-08T08:29:38+00:00 · Latest: 2026-02-08T08:29:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.07864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ssi-bench.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构思维：通过受限流形上的推理评估空间智能</div>
<div class="mono" style="margin-top:8px">空间智能对于视觉-语言模型（VLMs）在物理世界中的表现至关重要，但许多基准测试主要评估未受约束的场景，使得模型可以利用2D捷径。我们引入了SSI-Bench，这是一个针对受限流形上空间推理的VQA基准测试，基于复杂的真实世界3D结构，其可行配置严格受几何、拓扑和物理约束的控制。SSI-Bench包含1,000个排序问题，涵盖几何和拓扑推理，并需要多样化的组合空间操作，如心理旋转、截面推理、遮挡推理和力-路径推理。该基准测试通过完全以人类为中心的流程创建：十位研究人员花费超过400小时筛选图像、标注结构组件并设计问题，以最小化像素级线索。评估31个广泛使用的VLMs显示，与人类相比存在显著差距：最佳开源模型准确率为22.2%，最强闭源模型准确率为33.6%，而人类准确率为91.6%。鼓励模型进行思考仅带来边际提升，错误分析表明模型在结构锚定和约束一致的3D推理方面存在缺陷。项目页面：https://ssi-bench.github.io.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial intelligence is essential for vision-language models (VLMs) to interact effectively with the physical world, but existing benchmarks often evaluate unconstrained scenes that allow models to rely on 2D shortcuts. To address this, the authors introduce SSI-Bench, a VQA benchmark designed to assess spatial reasoning on constrained manifolds derived from complex real-world 3D structures governed by geometric, topological, and physical constraints. The benchmark includes 1,000 ranking questions that require compositional spatial operations such as mental rotation and occlusion reasoning. Evaluation of 31 popular VLMs shows significant performance gaps compared to humans, with the best open-source model achieving 22.2% accuracy and the strongest closed-source model reaching 33.6%, while humans score 91.6%. Error analysis indicates that models struggle with structural grounding and maintaining constraint consistency in 3D reasoning.</div>
<div class="mono" style="margin-top:8px">空间智能对于视觉-语言模型（VLMs）在物理世界中的有效交互至关重要，但现有基准测试多评估无约束场景，允许模型依赖2D捷径。为解决这一问题，作者提出了SSI-Bench，这是一个用于评估受限流形上空间推理的VQA基准测试，基于受几何、拓扑和物理约束严格控制的复杂现实3D结构。该基准包含1000个排序问题，要求多种组合性空间操作。对31个广泛使用的VLMs进行评估显示，其性能与人类存在显著差距，最佳开源模型准确率为22.2%，最强闭源模型为33.6%，而人类得分高达91.6%。错误分析表明，模型在结构锚定和约束一致的3D推理方面存在困难。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
