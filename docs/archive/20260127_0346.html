<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-27 03:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260127_0346</div>
    <div class="row"><div class="card">
<div class="title">Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</div>
<div class="meta-line">Authors: Riyang Bao, Cheng Yang, Dazhou Yu, Zhexiang Tang, Gengchen Mai, Liang Zhao</div>
<div class="meta-line">First: 2026-01-23T18:33:45+00:00 · Latest: 2026-01-23T18:33:45+00:00</div>
<div class="meta-line">Comments: 15pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16965v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spatial-Agent: 基于科学核心概念的代理式地理空间推理</div>
<div class="mono" style="margin-top:8px">地理空间推理对于城市分析、交通规划和灾害响应等现实世界应用至关重要。然而，现有的基于大语言模型（LLM）的代理通常无法进行真实的地理计算，而是依赖网络搜索或模式匹配，并在空间关系上产生幻觉。我们提出了Spatial-Agent，这是一种基于地理空间信息科学基础理论的AI代理。我们的方法将地理分析问答形式化为概念转换问题，其中自然语言问题被解析为可执行的工作流，表示为GeoFlow图——一种具有空间概念节点和转换边的有向无环图。基于空间信息理论，Spatial-Agent通过基于模板的生成方式提取空间概念，分配功能角色并遵循原则性的顺序约束，进而组合转换序列。在MapEval-API和MapQA基准上的大量实验表明，Spatial-Agent显著优于现有的基线方法，包括ReAct和Reflexion，同时生成可解释且可执行的地理空间工作流。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial-Agent was developed to address the limitations of current LLM-based agents in performing genuine geospatial computations. These agents often rely on web search or pattern matching, leading to hallucinations in spatial relationships. The proposed method integrates foundational theories of spatial information science, transforming natural language questions into executable workflows through GeoFlow Graphs, which are directed acyclic graphs representing spatial concepts and their transformations. Experimental results on MapEval-API and MapQA benchmarks show that Spatial-Agent significantly outperforms existing baselines like ReAct and Reflexion, generating interpretable and executable geospatial workflows.</div>
<div class="mono" style="margin-top:8px">Spatial-Agent 是为了解决当前基于 LLM 的代理在进行准确地理空间推理时的局限性而开发的。现有代理常依赖网络搜索或模式匹配，导致空间关系的错误假设。该方法融合了空间信息科学的基础理论，将自然语言问题转化为可执行的工作流程，通过 GeoFlow 图（有向无环图）表示空间概念及其转换。在 MapEval-API 和 MapQA 基准上的实验结果表明，Spatial-Agent 显著优于 ReAct 和 Reflexion 等现有方法，能够生成可解释且可执行的地理空间解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents</div>
<div class="meta-line">Authors: Xinze Li, Ziyue Zhu, Siyuan Liu, Yubo Ma, Yuhang Zang, Yixin Cao, Aixin Sun</div>
<div class="meta-line">First: 2026-01-23T12:09:59+00:00 · Latest: 2026-01-23T12:09:59+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent&#x27;s own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMemBench：面向VLM代理的事件记忆交互基准测试</div>
<div class="mono" style="margin-top:8px">我们引入了EMemBench，这是一个通过交互游戏评估代理长期记忆的程序化基准。与使用固定问题集不同，EMemBench从每个代理自身的轨迹中生成问题，涵盖文本和视觉游戏环境。每个模板通过底层游戏信号计算可验证的基准答案，具有可控的可回答性，并在记忆技能上实现均衡覆盖：单跳/多跳回忆、归纳、时间、空间、逻辑和对抗性技能。我们使用上下文提示作为基线，评估具有强大语言模型/VLM作为骨干的记忆代理。在15个文本游戏和多个视觉种子上，结果远未饱和：归纳和空间推理仍然是持续的瓶颈，尤其是在视觉环境中。持久记忆在文本游戏中为开放骨干模型带来了明显提升，但对VLM代理的改进则不够一致，这表明基于视觉的事件记忆仍然是一个开放性挑战。一项人类研究进一步验证了EMemBench的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EMemBench is introduced as a programmatic benchmark to evaluate the long-term memory capabilities of agents in interactive environments. It generates questions based on each agent&#x27;s trajectory, encompassing both text and visual game settings, and computes verifiable ground truth from game signals. The benchmark covers various memory skills such as single/multi-hop recall, induction, temporal, spatial, logical, and adversarial reasoning. Evaluation results show that induction and spatial reasoning remain significant challenges, particularly in visual settings, and that while persistent memory benefits open backbones in text games, improvements for VLM agents are less consistent, highlighting the difficulty of visually grounded episodic memory.</div>
<div class="mono" style="margin-top:8px">EMemBench 是一个用于评估智能体长期记忆能力的程序化基准，通过交互式游戏进行测试。与传统依赖固定问题的基准不同，EMemBench 根据每个智能体的轨迹动态生成问题，涵盖文本和视觉环境。该基准包含模板，从游戏信号中推导可验证的基准答案，确保答案可控且覆盖多种记忆技能，如单跳/多跳回忆、归纳、时间、空间、逻辑和对抗性推理。在15个文本游戏和多个视觉种子上进行的实验表明，归纳和空间推理是持续存在的挑战，尤其是在视觉环境中。尽管持久记忆在文本游戏中显著提升了开放架构的表现，但VLM智能体的改进则不够一致，表明基于视觉的事件记忆仍是一个开放性问题。</div>
</details>
</div>
<div class="card">
<div class="title">TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning</div>
<div class="meta-line">Authors: Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin, Haoyu Cao, Xing Sun, Ying Shen, Hai-Tao Zheng, Liang Lin, Philip S. Yu</div>
<div class="meta-line">First: 2026-01-23T07:35:05+00:00 · Latest: 2026-01-23T07:35:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16520v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramPuzzle：通过组合空间推理评估多模态大语言模型</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在视觉识别和语义理解方面取得了显著进展。然而，它们在精确的组合空间推理方面的能力仍 largely unexplored。现有基准测试通常涉及相对简单的任务，并依赖语义近似或粗略的相对位置，而其评估指标通常有限且缺乏严谨的数学定义。为弥合这一差距，我们引入 TangramPuzzle，这是一个基于几何的基准测试，旨在通过经典的 Tangram 游戏视角评估组合空间推理。我们提出了 Tangram Construction Expression（TCE），一种符号几何框架，通过精确的、可由机器验证的坐标规范来定义 Tangram 组装，以减少视觉近似带来的歧义。我们设计了两个互补的任务：轮廓预测，要求从局部组件推断整体形状；以及端到端代码生成，要求解决逆向几何组装问题。我们在多个先进的开源和专有模型上进行了广泛评估实验，发现了一个有趣的见解：MLLMs 倾向于优先匹配目标轮廓，而忽视几何约束，导致组件发生形变或扭曲。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of rigorous evaluation of compositional spatial reasoning in Multimodal Large Language Models (MLLMs). The authors introduce TangramPuzzle, a geometry-based benchmark inspired by the classic Tangram game, to assess MLLMs&#x27; ability to reason about spatial compositions. They propose Tangram Construction Expression (TCE), a symbolic geometric framework that allows precise, machine-verifiable coordinate-based reasoning. Two tasks, Outline Prediction and End-to-End Code Generation, are designed to test different aspects of spatial reasoning. Experimental results show that MLLMs tend to focus on matching the target silhouette rather than adhering to geometric constraints, resulting in distorted or incorrect piece arrangements.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前评估多模态大语言模型（MLLMs）组合空间推理能力的基准测试存在的不足。作者提出了TangramPuzzle，这是一个基于几何的经典七巧板游戏启发的基准测试，用于通过精确的坐标规范来评估组合空间推理能力。他们设计了Tangram Construction Expression（TCE），一种符号化框架，使几何推理能够被机器验证。两个互补任务——轮廓预测和端到端代码生成——用于测试空间推理的不同方面。实验结果表明，MLLMs往往更关注匹配目标轮廓，而忽视几何约束，导致拼图块的变形或错误排列。</div>
</details>
</div>
<div class="card">
<div class="title">Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models</div>
<div class="meta-line">Authors: Bridget Leonard, Scott O. Murray</div>
<div class="meta-line">First: 2026-01-23T00:21:27+00:00 · Latest: 2026-01-23T00:21:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent&#x27;s visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于认知的标记克服多模态模型中的自我中心偏差</div>
<div class="mono" style="margin-top:8px">多模态语言模型（MLMs）在语义视觉-语言任务上表现良好，但在需要采用其他代理视觉视角的空间推理任务上表现不佳。这些错误反映了持续存在的自我中心偏差，并引发了关于当前模型是否支持 allocentric 推理的疑问。受人类空间认知启发，我们引入了视角标记，这些专门的嵌入通过（1）具身身体关键点线索或（2）支持心理旋转的抽象表示来编码方向。将这些标记整合到 LLaVA-1.5-13B 中，使其在二级视觉视角任务上表现出性能提升。在合成和自然基准（Isle Bricks V2、COCO、3DSRBench）中，视角标记提高了准确性，基于旋转的标记能够泛化到非人类参考代理。表示分析表明，微调增强了基础模型中已有的潜在方向敏感性，这表明 MLMs 包含 allocentric 推理的前身，但缺乏适当的内部结构。总体而言，将基于认知的空间结构直接嵌入到标记空间中，为视角采取和更类人空间推理提供了一种轻量且模型无关的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the egocentric bias in multimodal language models (MLMs), which limits their ability to perform spatial reasoning from another agent&#x27;s visual perspective. To overcome this limitation, the authors introduce perspective tokens—specialized embeddings that encode spatial orientation using either embodied body-keypoint cues or abstract mental rotation representations. By integrating these tokens into LLaVA-1.5-13B, the model achieves improved performance on level-2 visual perspective-taking tasks. Experimental results on synthetic and naturalistic benchmarks such as Isle Bricks V2, COCO, and 3DSRBench show significant accuracy gains, with rotation-based tokens demonstrating better generalization to non-human reference agents. The findings suggest that while MLMs may already possess latent orientation sensitivity, they lack the internal structure necessary for allocentric reasoning, and embedding cognitively inspired spatial information into the token space offers a lightweight, model-agnostic solution to enhance perspective-taking capabilities.</div>
<div class="mono" style="margin-top:8px">本文探讨了多模态语言模型（MLMs）中存在的视角偏见问题，即它们难以从其他代理的视角进行空间推理。为解决这一问题，作者引入了视角标记——一种专门编码空间方向的嵌入，通过身体关键点提示或抽象的心理旋转表示实现。将这些标记整合到LLaVA-1.5-13B模型中，使其在二级视觉视角任务中表现提升。在合成和自然基准测试（如Isle Bricks V2、COCO、3DSRBench）中，视角标记显著提高了准确性，其中基于旋转的标记在非人类参考代理中表现出更强的泛化能力。研究结果表明，尽管MLMs可能已具备潜在的方向敏感性，但缺乏支持非自我中心推理的适当内部结构。</div>
</details>
</div>
<div class="card">
<div class="title">VOCAL: Visual Odometry via ContrAstive Learning</div>
<div class="meta-line">Authors: Chi-Yao Huang, Zeel Bhatt, Yezhou Yang</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-06-30T20:26:13+00:00 · Latest: 2026-01-22T22:04:57+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.00243v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.00243v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Breakthroughs in visual odometry (VO) have fundamentally reshaped the landscape of robotics, enabling ultra-precise camera state estimation that is crucial for modern autonomous systems. Despite these advances, many learning-based VO techniques rely on rigid geometric assumptions, which often fall short in interpretability and lack a solid theoretical basis within fully data-driven frameworks. To overcome these limitations, we introduce VOCAL (Visual Odometry via ContrAstive Learning), a novel framework that reimagines VO as a label ranking challenge. By integrating Bayesian inference with a representation learning framework, VOCAL organizes visual features to mirror camera states. The ranking mechanism compels similar camera states to converge into consistent and spatially coherent representations within the latent space. This strategic alignment not only bolsters the interpretability of the learned features but also ensures compatibility with multimodal data sources. Extensive evaluations on the KITTI dataset highlight VOCAL&#x27;s enhanced interpretability and flexibility, pushing VO toward more general and explainable spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOCAL: 通过对比学习实现的视觉里程计</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）领域的突破彻底改变了机器人学的格局，使超精确的相机状态估计成为现代自主系统的关键。尽管取得了这些进展，许多基于学习的VO技术仍依赖于刚性的几何假设，这在可解释性和完全数据驱动框架中的理论基础方面往往存在不足。为克服这些局限性，我们提出了VOCAL（通过对比学习实现的视觉里程计），一个新颖的框架，将VO重新构想为一个标签排序问题。通过将贝叶斯推断与表示学习框架相结合，VOCAL将视觉特征组织为与相机状态相对应的形式。排序机制迫使相似的相机状态在潜在空间中收敛为一致且空间上连贯的表示。这种策略性的对齐不仅增强了所学特征的可解释性，还确保了其与多模态数据源的兼容性。在KITTI数据集上的广泛评估突显了VOCAL在可解释性和灵活性方面的提升，推动视觉里程计向更通用和可解释的空间智能发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces VOCAL, a novel framework for visual odometry that addresses the limitations of existing learning-based methods by moving away from rigid geometric assumptions. VOCAL reinterprets visual odometry as a label ranking problem and integrates Bayesian inference with representation learning to align visual features with camera states. The ranking mechanism ensures spatial coherence and consistency in the latent space, improving interpretability and enabling compatibility with multimodal data. Experimental results on the KITTI dataset demonstrate that VOCAL achieves better interpretability and flexibility compared to traditional approaches, advancing the field toward more general and explainable spatial intelligence.</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）是自主机器人系统中的关键模块，但许多基于学习的方法由于依赖刚性几何假设而存在可解释性差和理论基础薄弱的问题。VOCAL通过将VO重新定义为标签排序问题，结合贝叶斯推理与表征学习，使视觉特征与相机状态对齐。该框架通过在潜在空间中强制相似相机状态形成一致的表征，增强了空间一致性。在KITTI数据集上的广泛评估表明，VOCAL提升了可解释性和灵活性，为更通用和可解释的空间智能提供了新的方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-22T19:05:41+00:00</div>
<div class="meta-line">Comments: Work done as part of the EleutherAI SOAR Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲区</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲区。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，训练过程往往将图像扁平化为1D的patch序列，丢弃了空间推理所需的2D结构。我们认为，这种缺乏空间感知能力是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the spatial reasoning limitations in vision-language models (VLMs), which are typically trained using contrastive language-image pretraining (CLIP) style image encoders that flatten images into 1D sequences, thereby losing essential 2D spatial structure. The authors propose two approaches to enhance spatial awareness: training image encoders with alternative objectives and incorporating 2D positional encodings. Their experiments demonstrate that these modifications improve spatial reasoning performance across multiple benchmarks, highlighting the importance of spatial structure in VLM design for applications like robotics and embodied AI.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在空间推理方面的不足，指出当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，将图像扁平化为1D序列，丢失了对空间结构至关重要的2D信息。作者提出了两种改进方法：使用替代目标训练图像编码器以及引入2D位置编码。实验结果显示，这些架构上的改进显著提升了模型在多个基准测试中的空间推理能力，强调了空间结构在VLM设计中的重要性，特别是在机器人和具身AI等应用中。</div>
</details>
</div>
<div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知</div>
<div class="mono" style="margin-top:8px">大型音频语言模型（LALMs）在语音识别、音频描述和听觉问答任务中最近取得了显著进展。然而，这些模型是否能够感知空间动态，特别是声源的运动，仍不清楚。在本研究中，我们发现当前的ALLMs存在系统性的运动感知缺陷。为了解决这一问题，我们引入了AudioMotionBench，这是首个专门设计用于评估听觉运动理解的基准测试。AudioMotionBench引入了一个受控的问答基准测试，用于评估音频语言模型（LALMs）能否从双耳音频中推断出移动声源的方向和轨迹。全面的定量和定性分析表明，当前模型在可靠识别运动线索或区分方向模式方面存在困难。平均准确率仍低于50\%，突显了听觉空间推理的基本局限性。我们的研究指出了人类与模型在听觉空间推理之间的根本差距，为未来增强音频语言模型的空间认知能力提供了诊断工具和新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of Large Audio-Language Models (LALMs) to perceive auditory motion, specifically the direction and trajectory of moving sound sources. The researchers developed AudioMotionBench, a novel benchmark designed to evaluate this capability using binaural audio inputs. Their findings show that current models perform poorly in recognizing motion cues and distinguishing directional patterns, with average accuracy below 50\%, indicating a significant limitation in their auditory spatial reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型音频语言模型（LALMs）在感知听觉运动方面的能力，特别是对移动声源方向和轨迹的识别。研究人员提出了AudioMotionBench，这是首个专门用于评估听觉运动理解能力的基准测试。通过全面的定量和定性分析，他们发现当前模型在识别运动线索或区分方向模式方面表现不佳，平均准确率低于50\%，表明在听觉空间推理方面存在根本性缺陷。这些结果揭示了人类听觉空间推理与现有模型之间的显著差距，为未来提升音频语言模型的空间认知能力提供了诊断工具和新见解。</div>
</details>
</div>
<div class="card">
<div class="title">SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</div>
<div class="meta-line">Authors: Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-10T20:04:08+00:00 · Latest: 2026-01-22T14:26:01+00:00</div>
<div class="meta-line">Comments: Conference: NeurIPS 2025 (main)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10046v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimWorld-Robotics：为多模态机器人导航与协作合成逼真且动态的城市环境</div>
<div class="mono" style="margin-top:8px">近年来，基础模型在开发能够根据多模态输入在开放场景中执行多样化任务的通用型机器人方面取得了令人鼓舞的成果。然而，当前的研究主要集中在室内和家庭场景。在本工作中，我们提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境的具身AI模拟平台。SWR基于Unreal Engine 5构建，能够程序化生成包含行人和交通系统等动态元素的无限逼真城市场景，其在真实感、复杂性和可扩展性方面超越了以往的城市模拟。它还支持多机器人控制与通信。借助这些关键特性，我们构建了两个具有挑战性的机器人基准测试：(1) 多模态指令跟随任务，其中机器人必须根据视觉-语言导航指令在存在行人和交通的情况下到达目标；(2) 多智能体搜索任务，其中两个机器人必须通过通信协作定位并相遇。与现有基准不同，这两个新基准在现实场景中全面评估了多种关键机器人能力，包括(1) 多模态指令的语义对齐，(2) 大规模环境中的三维空间推理，(3) 在行人和交通中的安全长距离导航，(4) 多机器人协作，以及(5) 基于环境的通信。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SimWorld-Robotics (SWR), a simulation platform designed to support the development of generalist robotics in complex, large-scale urban environments. Motivated by the need for more realistic and dynamic training environments for robots, SWR leverages Unreal Engine 5 to generate photorealistic and procedurally infinite urban scenes with dynamic elements such as pedestrians and traffic systems. The platform enables multi-robot control and communication, and is used to create two challenging benchmarks: a multimodal instruction-following task and a multi-agent search task. Experimental results show that state-of-the-art models, including vision-language models, struggle with these tasks due to insufficient perception, reasoning, and planning capabilities required for real-world urban navigation.</div>
<div class="mono" style="margin-top:8px">本文介绍了SimWorld-Robotics（SWR），一个用于在大规模、逼真的城市环境中开发通用机器人系统的模拟平台。研究动机源于对更真实且复杂的基准测试的需求，以评估机器人在开放场景中的能力。SWR基于Unreal Engine 5构建，能够程序化生成包含行人和交通系统的动态城市场景，支持多机器人控制与通信，从而实现对多模态指令跟随和多机器人协作的测试。实验结果表明，当前最先进的模型，包括视觉-语言模型，在这些任务中表现不佳，凸显了其在城市环境中感知、推理和规划能力的不足。</div>
</details>
</div>
<div class="card">
<div class="title">Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</div>
<div class="meta-line">Authors: Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</div>
<div class="meta-line">First: 2025-05-19T16:56:31+00:00 · Latest: 2026-01-22T14:25:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13353v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.13353v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code&#x27;s operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs&#x27; accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval&#x27;s 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知与敏感性：探讨语义回忆对长上下文代码推理的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用于理解大型代码库，但它们是通过理解长代码上下文的操作语义还是依赖模式匹配捷径仍不清楚。我们区分了词汇回忆（逐字检索代码）和语义回忆（理解操作语义）。通过评估10个最先进的LLMs，我们发现虽然前沿模型能够实现接近完美的、位置无关的词汇回忆，但当代码位于长上下文的中心位置时，语义回忆会严重退化。我们引入了语义回忆敏感性这一指标，用于衡量任务是否需要理解代码的操作语义，还是允许使用模式匹配捷径。通过一种新颖的反事实测量方法，我们表明模型在解决现有代码理解基准时严重依赖模式匹配捷径。我们提出了一项新任务SemTrace，其通过不可预测的操作实现高语义回忆敏感性；LLMs的准确性表现出严重的位移效应，当相关代码片段接近输入代码上下文的中间位置时，中位数准确率下降了92.73%，而CRUXEval则下降了53.36%。我们的研究结果表明，当前的评估方法在长上下文代码理解中严重低估了语义回忆失败的情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how large language models (LLMs) handle long context code reasoning, focusing on whether they rely on pattern matching or truly understand the operational semantics of code. The research differentiates between lexical recall, which involves retrieving code verbatim, and semantic recall, which requires comprehension of code meaning. By evaluating ten state-of-the-art LLMs, the authors found that while models perform well in lexical recall, their semantic recall significantly declines when code is located in the central part of long contexts. They introduce a metric called semantic recall sensitivity to assess the necessity of semantic understanding in tasks. Using a counterfactual measurement approach, they demonstrate that models heavily depend on pattern matching shortcuts for existing benchmarks. The proposed task SemTrace shows that LLMs&#x27; accuracy drops sharply as code snippets move towards the middle of the input, with a median accuracy decrease of 92.73% compared to CRUXEval&#x27;s 53.36%. These results indicate that current evaluations may not fully capture the challenges of semantic recall in long context code understanding.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在处理长代码上下文时是否依赖模式匹配还是真正理解代码的操作语义。研究区分了字面回溯（直接检索代码）和语义回溯（理解代码含义），并评估了十种最先进的LLMs。结果显示，尽管模型在字面回溯上表现接近完美，但当代码位于长上下文的中间位置时，语义回溯能力严重下降。作者引入了语义回溯敏感性这一新指标，用于评估任务是否需要语义理解。通过一种新颖的反事实评估方法，他们表明模型在解决现有代码理解基准时高度依赖模式匹配。他们提出了一项新的任务SemTrace，通过不可预测的操作实现高语义回溯敏感性。实验结果表明，当代码片段接近输入的中间位置时，LLMs的准确率下降高达92.73%，揭示了当前代码理解基准对语义回溯失败的低估问题。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</div>
<div class="meta-line">Authors: Pascal Benschop, Justin Dauwels, Jan van Gemert</div>
<div class="meta-line">First: 2026-01-22T09:14:11+00:00 · Latest: 2026-01-22T09:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用合成生成视频评估视觉语言模型的情境与空间感知</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在空间推理方面仍存在脆弱性，当语义依赖于细微的时间或几何线索时。我们引入了一个合成基准测试，用于探测两种互补能力：情境感知（识别互动是否具有伤害性或无害性）和空间感知（追踪谁对谁做了什么，以及推理相对位置和运动）。通过最小化的视频对，我们测试了三个挑战：区分暴力行为与无害活动、跨视角绑定攻击者角色、以及判断细粒度轨迹对齐。尽管我们在无训练设置下评估了近期的VLMs，但该基准适用于任何视频分类模型。结果显示，模型在各项任务中的表现仅略高于随机猜测。一个简单的辅助手段——稳定的颜色线索——部分缓解了攻击者角色的混淆，但并未解决其根本弱点。通过发布数据和代码，我们旨在提供可复现的诊断工具，并为补充大规模预训练而探索轻量级空间先验知识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to evaluate the situational and spatial awareness capabilities of vision language models (VLMs) by introducing a synthetic video benchmark. The benchmark tests three key challenges: distinguishing between violent and benign interactions, binding assailant roles across different viewpoints, and assessing fine-grained trajectory alignment. The results indicate that current VLMs perform only slightly better than chance on these tasks, suggesting limitations in their ability to process subtle temporal and geometric information. The use of stable color cues provides some improvement in reducing role confusion but does not fully address the core issues.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLMs）在空间推理方面的不足，提出一个合成基准测试，用于评估情境意识和空间意识。该基准测试检验了区分暴力与良性互动、跨视角绑定角色以及评估细粒度轨迹对齐的能力。结果显示，当前VLMs在这些任务上的表现仅略高于随机猜测，表明其在理解细微的时间和几何线索方面仍存在脆弱性。简单的修改，如稳定颜色提示，有助于减少部分角色混淆，但无法根本解决这一问题。</div>
</details>
</div>
<div class="card">
<div class="title">AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</div>
<div class="meta-line">Authors: Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Yingheng Wang, Bram Hoex, Zhicheng Zhong, Tong Xie</div>
<div class="meta-line">First: 2025-10-06T11:17:56+00:00 · Latest: 2026-01-22T05:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04704v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.04704v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomWorld：评估大型语言模型在晶体材料中空间推理能力的基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在文本推理方面表现出色，并开始发展空间理解能力，这引发了是否可以将这些能力结合用于复杂、领域特定任务的疑问。这一问题在材料科学等需要深入理解三维原子结构的领域尤为重要。尽管初步研究已成功将LLMs应用于纯晶体生成或坐标理解任务，但缺乏一个标准化的基准来系统评估其在多样化原子结构中的核心推理能力。为解决这一问题，我们引入了AtomWorld基准，用于基于晶体学信息文件（CIFs）的LLMs任务评估。这些任务包括结构编辑、CIF感知和性质引导建模，揭示了一个关键限制：尽管当前模型已建立有希望的基线，但在结构理解和空间推理方面仍持续失败。我们的实验表明，这些模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析和材料洞察中的累积性错误。通过定义这些标准化任务，AtomWorld为推动LLMs向稳健的原子尺度建模发展奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to assess the spatial reasoning capabilities of large language models (LLMs) in the context of crystalline materials, which is vital for materials science. The authors introduce AtomWorld, a benchmark based on Crystallographic Information Files (CIFs), to evaluate LLMs on tasks such as structural editing, CIF perception, and property-guided modeling. The main experimental results indicate that current models, although showing promise, struggle with accurate structural understanding and spatial reasoning, often making frequent errors in modifying crystal structures and interpreting CIF data, which can lead to significant issues in downstream analysis and material insights.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型（LLMs）在晶体材料领域的空间推理能力，这对于材料科学至关重要。作者提出了AtomWorld基准，基于晶体学信息文件（CIFs），用于评估LLMs在结构编辑、CIF感知和性质引导建模等任务中的表现。实验结果表明，当前模型尽管有良好表现，但在结构理解与空间推理方面仍存在明显不足，经常在修改晶体结构任务中出错，甚至在基本的CIF格式理解上也存在问题，这可能会影响后续分析和材料洞察的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">VibeTensor: System Software for Deep Learning, Fully Generated by AI Agents</div>
<div class="meta-line">Authors: Bing Xu, Terry Chen, Fengzhe Zhou, Tianqi Chen, Yangqing Jia, Vinod Grover, Haicheng Wu, Wei Liu, Craig Wittenbrink, Wen-mei Hwu, Roger Bringmann, Ming-Yu Liu, Luis Ceze, Michael Lightstone, Humphrey Shi</div>
<div class="meta-line">First: 2026-01-21T19:29:00+00:00 · Latest: 2026-01-21T19:29:00+00:00</div>
<div class="meta-line">Comments: Open-source: https://github.com/NVLabs/vibetensor</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16238v1">PDF</a> · <a href="https://github.com/NVLabs/vibetensor">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">VIBETENSOR is an open-source research system software stack for deep learning, generated by LLM-powered coding agents under high-level human guidance. In this paper, &quot;fully generated&quot; refers to code provenance: implementation changes were produced and applied as agent-proposed diffs; validation relied on agent-run builds, tests, and differential checks, without per-change manual diff review. It implements a PyTorch-style eager tensor library with a C++20 core (CPU+CUDA), a torch-like Python overlay via nanobind, and an experimental Node.js/TypeScript interface. Unlike thin bindings, VIBETENSOR includes its own tensor/storage system, schema-lite dispatcher, reverse-mode autograd, CUDA runtime (streams/events/graphs), a stream-ordered caching allocator with diagnostics, and a stable C ABI for dynamically loaded operator plugins. We view this release as a milestone for AI-assisted software engineering: it shows coding agents can generate a coherent deep learning runtime spanning language bindings down to CUDA memory management, validated primarily by builds and tests. We describe the architecture, summarize the workflow used to produce and validate the system, and evaluate the artifact. We report repository scale and test-suite composition, and summarize reproducible microbenchmarks from an accompanying AI-generated kernel suite, including fused attention versus PyTorch SDPA/FlashAttention. We also report end-to-end training sanity checks on 3 small workloads (sequence reversal, ViT, miniGPT) on NVIDIA H100 (Hopper, SM90) and Blackwell-class GPUs; multi-GPU results are Blackwell-only and use an optional CUTLASS-based ring-allreduce plugin gated on CUDA 13+ and sm103a toolchain support. Finally, we discuss failure modes in generated system software, including a &quot;Frankenstein&quot; composition effect where locally correct subsystems interact to yield globally suboptimal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VibeTensor：由AI代理完全生成的深度学习系统软件</div>
<div class="mono" style="margin-top:8px">VIBETENSOR是一个开源的深度学习研究系统软件栈，由LLM驱动的编码代理在高水平人类指导下生成。在本文中，&quot;完全生成&quot;指的是代码溯源：实现的更改由代理提议并应用；验证依赖于代理运行的构建、测试和差异检查，而无需逐项人工审查差异。它实现了一个PyTorch风格的即时张量库，包含C++20核心（CPU + CUDA），通过nanobind实现了一个类似Torch的Python封装，并提供了一个实验性的Node.js/TypeScript接口。与薄绑定不同，VIBETENSOR包含自己的张量/存储系统、schema-lite调度器、反向模式自动微分、CUDA运行时（流/事件/图），一个流顺序的缓存分配器及其诊断功能，以及一个稳定的C ABI用于动态加载的操作符插件。我们认为此次发布是AI辅助软件工程的一个里程碑：它表明编码代理可以生成一个连贯的深度学习运行时，从语言绑定一直到CUDA内存管理，主要通过构建和测试进行验证。我们描述了该系统的架构，总结了用于生成和验证系统的流程，并评估了该系统。我们报告了仓库规模和测试套件组成，并总结了从配套的AI生成内核套件中获得的可复现的微基准测试结果，包括融合注意力与PyTorch SDPA/FlashAttention的对比。我们还报告了在NVIDIA H100（Hopper，SM90）和Blackwell系列GPU上对三个小型工作负载（序列反转、ViT、miniGPT）进行的端到端训练合理性检查；多GPU结果仅限于Blackwell，并使用基于CUTLASS的可选环形all-reduce插件，该插件依赖于CUDA 13+和sm103a工具链支持。最后，我们讨论了生成系统软件中的失败模式，包括一种&quot;弗兰肯斯坦&quot;组合效应，其中局部正确的子系统相互作用导致全局性能不佳。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind VIBETENSOR is to explore the potential of AI agents in generating complex deep learning system software with minimal human intervention. The system is developed using LLM-powered coding agents guided by high-level human input, with implementation changes proposed and applied as diffs, validated through automated builds, tests, and differential checks. The main experimental results demonstrate that VIBETENSOR achieves comparable performance to PyTorch in microbenchmarks such as fused attention, and passes end-to-end training sanity checks on small workloads across different GPU architectures, including NVIDIA H100 and Blackwell-class GPUs. However, the study also highlights challenges in system software generation, such as the &quot;Frankenstein&quot; composition effect, where individual subsystems perform well but interact poorly to result in suboptimal overall performance.</div>
<div class="mono" style="margin-top:8px">本文介绍了VibeTensor，这是一个由AI代理在人类高层指导下完全生成的开源深度学习系统软件栈。该系统包含C++20核心、通过nanobind实现的Python覆盖层以及实验性的Node.js/TypeScript接口，具备自主的张量/存储系统、反向模式自动微分和CUDA运行时组件。主要实验结果表明，VibeTensor能够生成一个连贯的深度学习运行时环境，通过自动构建、测试和差异检查进行验证，其在微基准测试（如融合注意力）中表现与PyTorch相当。系统还支持在多种工作负载和GPU架构上的端到端训练，尽管在某些生成系统软件的失败模式中，如&quot;Frankenstein&quot;组合效应，局部正确的子系统可能因交互导致整体性能不佳。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理为何失败？对GitHub中失败代理拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编码代理现在正在向软件项目提交拉取请求（PRs），不仅作为助手，还作为自主贡献者。随着这些代理贡献在真实仓库中迅速增加，人们对它们在实际中的行为以及为何许多未能被合并仍知之甚少。在本文中，我们对GitHub上五个编码代理提交的33000个代理撰写的PR进行了大规模研究。（RQ1）我们首先从四个广泛维度定量描述了已合并和未合并的PR：1）不同任务类型的合并结果，2）代码更改，3）CI构建结果，以及4）评审动态。我们观察到，与文档、CI和构建更新相关的任务合并成功率最高，而性能和错误修复任务表现最差。未被合并的PR往往涉及更大的代码更改，修改更多文件，并且通常无法通过项目的CI/CD流水线验证。（RQ2）为进一步探讨为何某些代理PR未被合并，我们对600个PR进行了定性分析，以构建拒绝模式的分层分类法。该分析通过揭示定量指标未涵盖的拒绝原因，如缺乏有意义的评审互动、重复PR、不受欢迎的功能实现以及代理与项目目标的不一致，补充了RQ1的定量发现。我们的研究结果突显了关键的社会技术因素和人机协作因素，这些因素对于提高未来代理工作流的成功率至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of AI coding agents in contributing to software projects through pull requests (PRs) on GitHub. By analyzing 33,000 PRs from five coding agents, the research identifies patterns in merge success across different task types, revealing that documentation, CI, and build-related tasks have the highest success rates, while performance and bug-fix tasks are the least successful. Not-merged PRs are associated with larger code changes, more files modified, and frequent CI/CD pipeline failures. A qualitative analysis of 600 PRs further uncovers common rejection reasons such as lack of reviewer interaction, duplicate submissions, and misaligned agent behavior, providing deeper insights into the socio-technical challenges of AI-assisted software development.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在通过GitHub拉取请求（PR）为软件项目做出贡献时的失败情况。通过对五个编码代理提交的33,000个PR进行分析，发现与文档、CI和构建相关的任务合并成功率最高，而性能和错误修复任务的成功率最低。未被合并的PR通常涉及较大的代码改动、修改更多文件，并且往往无法通过项目的CI/CD流水线验证。对600个PR的定性分析进一步揭示了常见的拒绝模式，如缺乏审阅者的参与、重复提交、不受欢迎的功能实现以及代理与项目目标的不一致，为AI辅助开发的社会技术挑战提供了深入见解。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</div>
<div class="meta-line">Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</div>
<div class="meta-line">First: 2025-07-07T18:17:06+00:00 · Latest: 2026-01-21T13:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05386v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.05386v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model&#x27;s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT&#x27;s gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化微调自然缓解持续微调中的遗忘</div>
<div class="mono" style="margin-top:8px">持续微调（CPT）是一种流行且有效的技术，用于将基础模型（如多模态大语言模型）适应于特定且不断演化的下游任务。尽管现有研究主要集中在数据回放、模型扩展或参数正则化等方法上，但CPT中学习范式的根本作用仍被广泛忽视。本文对两种核心的微调范式——监督微调（SFT）和强化微调（RFT）进行了比较分析，探讨它们在持续微调过程中对知识保留的影响。我们的实验基于包含七个多样化多模态任务的基准数据集，使用Qwen2.5-VL-7B-Instruct作为持续微调的基础模型。研究得出两个重要发现：（1）在持续学习下游任务时，SFT会导致先前学习任务的灾难性遗忘，而RFT则能自然保留先前知识，其性能与多任务训练相当。（2）RFT成功保护并甚至增强了模型在标准基准（如MMMU和MMLU-Pro）上的通用知识，而SFT则严重损害了模型的通用能力。进一步分析表明，这种稳定性并非主要归因于显式的机制，如KL惩罚或思维链推理，而是源于RFT中隐含的正则化机制。我们的理论分析表明，RFT的梯度更新自然受到奖励方差的缩放，作为一种数据依赖的正则化器，其内在机制能够保护先前获得的知识。最后，我们提出了一种基于rollout的实例过滤算法，以提升RFT的稳定性和效率。我们的全面研究展示了RFT作为持续微调的稳健范式的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the issue of catastrophic forgetting in continual post-training (CPT) of foundation models, particularly focusing on the effectiveness of reinforcement fine-tuning (RFT) compared to supervised fine-tuning (SFT). The study highlights that RFT naturally preserves prior knowledge and maintains performance levels similar to multi-task training, while SFT leads to significant degradation in performance on previously learned tasks. Through experiments on seven multimodal tasks using Qwen2.5-VL-7B-Instruct, the authors find that the stability of RFT is attributed to an implicit regularization mechanism, rather than explicit techniques like KL penalty. They further propose a rollout-based instance filtering algorithm to improve the efficiency and robustness of RFT in continual learning scenarios.</div>
<div class="mono" style="margin-top:8px">本文探讨了在持续微调（CPT）过程中基础模型出现灾难性遗忘的问题，重点比较了监督微调（SFT）与强化微调（RFT）两种方法的效果。研究发现，当模型持续学习下游任务时，SFT会导致先前学习知识的严重丢失，而RFT则能自然保留这些知识，其性能与多任务训练相当。在七个不同的多模态任务上使用Qwen2.5-VL-7B-Instruct进行实验，结果表明RFT不仅能防止遗忘，还能提升模型在标准基准上的通用知识。作者认为这种稳定性源于RFT内部的隐式正则化机制，该机制由奖励方差驱动，而非显式的KL惩罚等方法。最后，他们提出了一种基于rollout的实例过滤算法，以提高RFT在持续学习中的效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</div>
<div class="meta-line">Authors: Xinyi Zheng, Yunze Liu, Chi-Hao Wu, Fan Zhang, Hao Zheng, Wenqi Zhou, Walterio W. Mayol-Cuevas, Junxiao Shen</div>
<div class="meta-line">First: 2026-01-21T11:32:24+00:00 · Latest: 2026-01-21T11:32:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialMem：基于度量锚定和快速检索的统一三维记忆</div>
<div class="mono" style="margin-top:8px">我们提出了SpatialMem，这是一个以记忆为中心的系统，将三维几何、语义和语言统一为一个可查询的表示。从随意捕捉的视角RGB视频出发，SpatialMem重建度量缩放的室内环境，检测结构化的三维锚点（墙壁、门、窗）作为第一层框架，并填充一个分层记忆，其中包含开放词汇的对象节点——将证据片段、视觉嵌入和两层文本描述与三维坐标相链接——以实现紧凑存储和快速检索。这种设计支持对空间关系（如距离、方向、可视性）的可解释推理，并且无需专用传感器即可支持下游任务，如语言引导的导航和对象检索。在三个真实室内场景上的实验表明，SpatialMem在增加杂乱和遮挡的情况下仍能保持强大的锚点-描述级导航完成度和分层检索准确性，为具身空间智能提供了一个高效且可扩展的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpatialMem is motivated by the need for a unified representation of 3D geometry, semantics, and language in indoor environments. The system reconstructs metrically scaled spaces from egocentric RGB videos and uses structural 3D anchors as a foundational scaffold. It then builds a hierarchical memory by linking object evidence patches, visual embeddings, and textual descriptions to 3D coordinates, enabling compact storage and fast retrieval. Experimental results across three real-life indoor scenes show that SpatialMem maintains high navigation completion and retrieval accuracy even under increasing clutter and occlusion, demonstrating its effectiveness for embodied spatial intelligence tasks.</div>
<div class="mono" style="margin-top:8px">SpatialMem 的研究动机是构建一个统一的 3D 几何、语义和语言表示，以支持高效的环境推理和任务执行。该系统从第一视角 RGB 视频出发，重建可量化的室内环境，并以结构化的 3D 锚点（如墙壁、门、窗）作为基础框架。随后，它通过将开放词汇的对象节点与视觉证据片段、视觉嵌入以及两层文本描述关联，构建了一个分层记忆结构。实验结果表明，在三个真实室内场景中，SpatialMem 在增加杂乱和遮挡的情况下仍能保持良好的导航完成度和检索准确性，展示了其在具身空间智能任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">3D Space as a Scratchpad for Editable Text-to-Image Generation</div>
<div class="meta-line">Authors: Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</div>
<div class="meta-line">First: 2026-01-21T02:40:19+00:00 · Latest: 2026-01-21T02:40:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://oindrilasaha.github.io/3DScratchpad/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D空间作为可编辑文本到图像生成的草稿纸</div>
<div class="mono" style="margin-top:8px">近期在大语言模型（LLMs）方面的进展表明，当将中间思维外部化为显式的操作空间（如思维链追踪或工具增强推理）时，推理能力会得到提升。然而，视觉语言模型（VLMs）缺乏类似的机制来进行空间推理，这限制了它们生成准确反映几何关系、物体身份和构图意图的图像的能力。我们引入了空间草稿纸的概念——一种连接语言意图与图像合成的3D推理基底。给定一个文本提示，我们的框架解析主体和背景元素，将它们实例化为可编辑的3D网格，并采用代理场景规划来确定位置、方向和视角选择。最终的3D布局通过保留身份的提示被渲染回图像域，使VLM能够生成空间一致且视觉连贯的输出。与以往基于2D布局的方法不同，我们的方法支持直观的3D编辑，这些编辑能够可靠地传播到最终图像中。在GenAI-Bench上，我们的方法在文本对齐方面实现了32%的提升，证明了显式的3D推理对精确可控图像生成的优势。我们的结果凸显了一种新的视觉-语言模型范式，即不仅在语言层面进行推理，也在空间层面进行推理。代码和可视化结果见 https://oindrilasaha.github.io/3DScratchpad/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of visual language models (VLMs) in generating images that accurately reflect spatial relationships and compositional intent. The authors propose a 3D spatial scratchpad as a novel mechanism to externalize reasoning, allowing VLMs to translate text prompts into editable 3D meshes and perform scene planning for placement, orientation, and viewpoint selection. The resulting 3D structure is then rendered back into the image domain with identity-preserving cues, leading to spatially consistent and visually coherent outputs. Experimental results on GenAI-Bench show a 32% improvement in text alignment compared to prior 2D layout-based methods, demonstrating the effectiveness of explicit 3D reasoning in enhancing controllability and precision in text-to-image generation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉语言模型（VLMs）在准确生成反映几何关系和构图意图的图像方面的不足，提出了一种3D空间草稿纸机制。该框架将文本提示解析为可编辑的3D网格，并通过代理场景规划确定物体的位置、方向和视角。随后，将3D布局渲染回图像域，并保留身份信息，从而生成空间一致且视觉连贯的图像。在GenAI-Bench上的实验结果显示，与传统的2D布局方法相比，文本对齐度提高了32%，证明了显式的3D推理在提升图像生成的可控性和精确性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics</div>
<div class="meta-line">Authors: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li</div>
<div class="meta-line">First: 2026-01-20T14:51:45+00:00 · Latest: 2026-01-20T14:51:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14027v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14027v1">PDF</a> · <a href="https://github.com/project-numina/numina-lean-agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Numina-Lean-Agent：一个开放且通用的代理推理系统用于形式化数学</div>
<div class="mono" style="margin-top:8px">代理系统最近已成为形式化定理证明的主导范式，通过协调多个模型和工具实现了强大的性能。然而，现有方法通常依赖于任务特定的流水线和训练过的形式化证明器，限制了其灵活性和可复现性。在本文中，我们提出了一种范式，即直接使用通用编码代理作为形式化数学推理器。该范式受到以下三点动机的驱动：(1) 通用编码代理为超越证明的多样化推理任务提供了自然的接口；(2) 仅通过替换底层基础模型即可提升性能，而无需训练；(3) MCP支持灵活扩展和自主调用专用工具，避免了复杂的设计。基于这一范式，我们引入了Numina-Lean-Agent，它结合了Claude Code和Numina-Lean-MCP，以实现与Lean的自主交互、相关定理检索、非形式化证明以及辅助推理工具。使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent解决了Putnam 2025中的所有问题（12/12），与最佳闭源系统表现相当。除了基准评估，我们还通过与数学家互动，成功地将Brascamp-Lieb定理形式化，进一步展示了其通用性。我们将在https://github.com/project-numina/numina-lean-agent发布Numina-Lean-Agent及其所有解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to develop a more flexible and reproducible agentic reasoning system for formal mathematics by moving away from task-specific pipelines and trained formal provers. The proposed method utilizes a general coding agent as a formal math reasoner, leveraging its ability to handle diverse reasoning tasks and improve performance through model replacement without additional training. The key experimental results show that Numina-Lean-Agent successfully solves all 12 problems in the Putnam 2025 competition using Claude Opus 4.5 as the base model, matching the performance of the best closed-source systems. Additionally, it demonstrates its generality by collaborating with mathematicians to formalize the Brascamp-Lieb theorem.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过摒弃任务特定的流水线和训练过的定理证明器，开发一个更灵活且可复现的数学形式化推理系统。提出的方法利用通用编码代理，结合Claude Code与Numina-Lean-MCP，实现与Lean定理证明器的自主交互、相关定理检索以及非形式化证明和辅助推理工具的使用。主要实验结果表明，Numina-Lean-Agent成功解决了Putnam 2025竞赛中的全部12道题目，性能与最佳闭源系统相当，并通过与数学家的互动成功形式化了Brascamp-Lieb定理。</div>
</details>
</div>
<div class="card">
<div class="title">CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments</div>
<div class="meta-line">Authors: Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li</div>
<div class="meta-line">First: 2026-01-20T13:44:02+00:00 · Latest: 2026-01-20T13:44:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14339v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CityCube：在城市环境中评估视觉-语言模型的跨视角空间推理能力</div>
<div class="mono" style="margin-top:8px">跨视角空间推理对于具身AI至关重要，是空间理解、心理模拟和复杂环境中规划的基础。现有基准主要关注室内或街道场景，忽略了开放城市空间中独特的挑战，这些场景具有丰富的语义、复杂的几何结构和视角变化。为了解决这一问题，我们引入CityCube，这是一个系统性的基准，旨在评估当前视觉-语言模型（VLMs）在城市环境中的跨视角推理能力。CityCube整合了四种视角动态，以模拟相机运动，并涵盖了来自多种平台（如车辆、无人机和卫星）的广泛视角。为了全面评估，它包含5,022个精心标注的多视角问答对，分为五个认知维度和三种空间关系表达。对33个VLMs的全面评估表明，其表现与人类存在显著差距：即使是大规模模型也难以超过54.1%的准确率，仍比人类表现低34.2%。相比之下，小型模型经过微调后可达到超过60.0%的准确率，突显了我们基准的重要性。进一步分析表明，VLMs与类人推理之间存在任务相关性和根本性的认知差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CityCube, a benchmark designed to evaluate cross-view spatial reasoning in vision-language models (VLMs) within urban environments. Motivated by the limitations of existing benchmarks that focus on indoor or street scenes, CityCube incorporates four viewpoint dynamics and multiple perspectives from different platforms to simulate real-world urban scenarios. The benchmark includes 5,022 annotated multi-view QA pairs organized into five cognitive dimensions and three spatial relation expressions. Evaluations on 33 VLMs show that even large models achieve only 54.1% accuracy, which is 34.2% lower than human performance, while small-scale fine-tuned models surpass this threshold, underscoring the importance of the benchmark for advancing spatial reasoning capabilities in VLMs.</div>
<div class="mono" style="margin-top:8px">本研究提出了CityCube，一个用于评估视觉语言模型（VLM）在城市环境中跨视角空间推理能力的基准。由于现有基准主要关注室内或街道场景，未能充分反映开放城市空间中丰富的语义、复杂的几何结构和视角变化带来的挑战，因此本研究构建了该基准。CityCube整合了四种视角动态，并涵盖车辆、无人机和卫星等多种平台视角，包含5,022个精心标注的多视角问答对，按五个认知维度和三种空间关系表达分类。对33个VLM的全面评估显示，即使大型模型也只能达到54.1%的准确率，远低于人类表现，而经过微调的小型模型则超过这一水平，突显了该基准的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍存在分裂：视觉-语言模型（如CLIP）在全局语义对齐方面表现优异，但缺乏空间精度；而自监督方法（如MAE、DINO）能够捕捉复杂的局部结构，但在高层次语义上下文方面存在困难。我们认为这些范式本质上是互补的，可以整合到一个有原则的多任务框架中，并通过密集空间监督进一步增强。我们提出了MTV，一个联合优化视觉-语言对比、自监督和密集空间目标的多任务视觉预训练框架。为减少对人工标注的依赖，我们利用高容量的&quot;专家&quot;模型，如Depth Anything V2和OWLv2，大规模合成密集且结构化的伪标签。除了框架本身，我们还系统地研究了多任务视觉学习的机制，分析了：(i) 每个目标的边际增益，(ii) 任务间的协同效应与干扰，以及(iii) 在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了&quot;兼顾两者优势&quot;的性能，在不损害全局语义理解的前提下，显著提升了细粒度空间推理能力。我们的发现表明，借助高质量的伪监督，多任务学习是一条通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current visual representation learning approaches by proposing MTV, a multi-task framework that integrates vision-language contrastive learning, self-supervised learning, and dense spatial supervision. The motivation stems from the observation that vision-language models excel at global semantics but lack spatial precision, while self-supervised methods capture local structures but struggle with high-level understanding. The study introduces a shared backbone optimized across these tasks, using expert models to generate pseudo-labels for dense spatial supervision. Experimental results show that MTV achieves superior performance in both fine-grained spatial reasoning and global semantic understanding, demonstrating the effectiveness of combining these paradigms through principled multi-task learning.</div>
<div class="mono" style="margin-top:8px">本文针对当前视觉表征学习方法的局限性，提出了MTV框架，该框架整合了视觉-语言对比学习、自监督学习和密集空间监督任务。研究动机源于视觉-语言模型与自监督方法在全局语义和局部结构上的互补性。框架利用专家模型生成伪标签，减少对人工标注的依赖。实验结果表明，MTV在细粒度空间推理方面表现优异，同时保持了对全局语义的强理解能力，说明以高质量伪监督驱动的多任务学习是构建更通用视觉编码器的可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories</div>
<div class="meta-line">Authors: Aditya Bharat Soni, Rajat Ghosh, Vaishnavi Bhargava, Valerie Chen, Debojyoti Dutta</div>
<div class="meta-line">First: 2026-01-20T08:10:56+00:00 · Latest: 2026-01-20T08:10:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- &quot;test first, write code later&quot;, and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Tester：在真实仓库中使用开源LLM进行问题复现测试的训练</div>
<div class="mono" style="margin-top:8px">软件测试对于确保软件系统的正确性和可靠性至关重要。从自然语言问题描述中自动生成问题复现测试可以简化根本原因分析，提高开发人员的生产力，促进测试驱动开发——&quot;先测试后编写代码&quot;，并且可用于提升自动化问题解决系统（如编码代理）的效果。现有的方法主要依赖于闭源LLM，对开源模型的探索有限。为了解决这一问题，我们提出了SWE-Tester——一种新颖的训练流程，用于训练开源LLM生成问题复现测试。首先，我们从2600个开源GitHub仓库中精选出41000个高质量训练实例，并用其训练不同规模和家族的LLM。微调后的模型在SWT-Bench Verified上实现了高达10\%的成功率提升和21\%的变更覆盖率提升。进一步分析表明，随着推理时的计算量增加、数据量增多和模型规模变大，性能有持续提升。这些结果突显了我们框架在推动该领域开源LLM发展方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the need for effective automated testing methods that can enhance software reliability and developer productivity. SWE-Tester introduces a novel pipeline for training open-source large language models (LLMs) to generate issue reproduction tests from natural language descriptions. The framework uses a curated dataset of 41K instances from 2.6K GitHub repositories to fine-tune various LLMs, achieving up to 10% improvement in success rate and 21% in change coverage on the SWT-Bench Verified benchmark. The results demonstrate that increasing model size, data volume, and inference-time compute consistently improves performance, underscoring the framework&#x27;s potential for advancing open-source LLMs in software testing.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升开源大语言模型（LLMs）在从自然语言描述生成问题复现测试方面的能力，这对于软件测试和调试至关重要。提出的方法SWE-Tester引入了一个训练开源LLMs的流水线，利用从2.6K个GitHub仓库中精选的41K个实例构建高质量训练数据集。在SWT-Bench Verified上的实验结果显示，经过微调的模型在成功率和变更覆盖率方面分别提升了10\%和21\%，表明该方法在提升真实软件系统测试生成效果方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis</div>
<div class="meta-line">Authors: Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang, Zhongxue Gan</div>
<div class="meta-line">First: 2026-01-20T06:50:40+00:00 · Latest: 2026-01-20T06:50:40+00:00</div>
<div class="meta-line">Comments: This study has been accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为前时空解耦：面向多模态情感分析的解耦表征学习</div>
<div class="mono" style="margin-top:8px">多模态情感分析融合语言、视觉和听觉模态。主流方法基于模态不变和模态特异性分解或复杂融合，仍然依赖于时空混合建模。这忽略了时空异质性，导致时空信息不对称，从而限制了性能。因此，我们提出TSDA（行为前时空解耦），在任何模态交互之前，显式地将每个模态解耦为时间动态和空间结构上下文。对于每个模态，时间编码器和空间编码器将信号分别映射到时间空间体。因子一致的跨模态对齐随后仅将时间特征与跨模态的时间对应特征对齐，空间特征仅与跨模态的空间对应特征对齐。因子特异性监督和去相关正则化减少了跨因子泄漏，同时保留了互补性。随后，一个门控再耦合模块将对齐的流重新耦合以执行任务。大量实验表明，TSDA优于基线方法。消融分析验证了设计的必要性和可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of existing multimodal sentiment analysis approaches that fail to account for spatiotemporal heterogeneity by integrating linguistic, visual, and acoustic modalities. The proposed method, TSDA, decouples each modality into temporal and spatial components before any interaction, using separate encoders for each. Temporal features are aligned across modalities based on their temporal counterparts, while spatial features are aligned based on their spatial counterparts. Factor-specific supervision and decorrelation regularization are introduced to minimize cross-factor leakage and maintain feature complementarity. A gated recouple module then combines the aligned streams for the final task. Experimental results demonstrate that TSDA achieves superior performance compared to existing baselines, and ablation studies validate the effectiveness and interpretability of its design.</div>
<div class="mono" style="margin-top:8px">本研究针对现有多模态情感分析方法未能考虑时空异质性的问题，提出TSDA方法，通过将语言、视觉和听觉模态分别解耦为时序动态和空间结构，在交互前进行独立建模。该方法使用时序编码器和空间编码器将信号映射到各自的时序和空间空间中，并通过因子一致的跨模态对齐对齐不同模态的时序和空间特征。因子特定监督和去相关正则化被引入以减少跨因子信息泄露并保持特征互补性。实验结果表明，TSDA在多个基准数据集上均优于现有方法，消融实验进一步验证了其设计的必要性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</div>
<div class="meta-line">Authors: Peter A. Massih, Eric Cosatto</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-19T21:14:34+00:00 · Latest: 2026-01-19T21:14:34+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素级精确推理：用于定量地理空间分析的QVLM架构和SQuID数据集</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLMs）在定量空间推理方面表现不佳，因为其架构会破坏进行计数和测量所需的像素级信息。视觉编码器通过图像块嵌入压缩图像，导致空间索引的丢失，从而无法实现精确的像素级追踪。我们提出了两个贡献以解决这一根本性限制。首先，我们引入了SQuID（卫星定量智能数据集），这是一个包含2000对卫星图像问答对的基准数据集，具有数值范围和分类答案，旨在评估定量空间推理能力。该数据集跨越三个难度层级，其标注由人类标签及其学习到的变异性自动生成。其次，我们提出了QVLM（定量视觉-语言模型），这是一种代码生成架构，通过将语言理解与视觉分析解耦，保持像素精度。QVLM不将图像编码为嵌入，而是生成可执行代码，首先调用分割模型获取像素级掩码，然后直接在这些掩码上进行操作，从而在整个推理过程中保留空间索引。我们的实验表明，使用GPT-5作为编码器的QVLM在SQuID数据集上达到了42.0%的准确率，而使用图像-问题对提示的VLM仅达到28.1%。我们的工作表明，对于定量空间推理任务，架构解耦能够提高定量任务的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of current Vision-Language Models (VLMs) in quantitative spatial reasoning, as they lose pixel-level information during image encoding. The authors introduce SQuID, a new dataset with 2,000 satellite image Question-Answer pairs, featuring numerical and categorical answers across three difficulty levels. They also propose QVLM, a model that decouples language understanding from visual analysis by generating executable code that uses segmentation masks for precise spatial reasoning. Experiments show that QVLM achieves 42.0% accuracy on SQuID, significantly outperforming traditional VLMs with 28.1% accuracy.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉语言模型（VLMs）在定量空间推理任务中的表现，特别是通过保留像素级信息来支持计数和测量。作者提出了QVLM架构，该架构通过将语言理解与视觉分析解耦，生成可执行代码以处理卫星图像，首先调用分割模型获取像素级掩码，然后直接操作这些掩码，保持空间索引的完整性。同时，他们引入了SQuID数据集，包含2000对卫星图像问答对，涵盖数值和分类答案，用于评估定量空间推理能力。实验结果显示，使用GPT-5作为代码生成器的QVLM在SQuID上达到42.0%的准确率，显著优于传统VLM在图像-问题对提示下的28.1%准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility</div>
<div class="meta-line">Authors: Nickil Maveli, Antonio Vergari, Shay B. Cohen</div>
<div class="meta-line">First: 2026-01-19T21:09:48+00:00 · Latest: 2026-01-19T21:09:48+00:00</div>
<div class="meta-line">Comments: 32 pages (preprint)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13398v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13398v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型能否压缩（并解压）？通过可逆性评估代码理解和执行</div>
<div class="mono" style="margin-top:8px">大语言模型在代码基准测试中表现出色，但代码的往返执行揭示了它们在正向和反向执行中保持一致推理能力的局限性。我们提出了RoundTripCodeEval（RTCE），一个包含四个不同代码执行推理任务的全面基准，旨在严格测试往返一致性。RTCE提供了一种无需执行的精确匹配评估方式，用于衡量双射保真度，评估模型在不同算法和方向上编码与解码操作之间是否保持一致的一一映射关系。我们系统地使用零样本提示、执行轨迹上的监督微调以及自我反思机制对最先进的代码大语言模型进行评估。每种方法都带来适度的改进，但均未完全弥合差距，表明当前的大语言模型在真正的往返一致性方面存在困难，这表明它们缺乏进行可信代码推理所需的内部一致性。RTCE揭示了若干新的、此前未被测量的见解，这些见解无法通过现有的输入输出预测、执行推理或往返自然语言基准测试所捕捉。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of large language models (LLMs) to maintain consistent reasoning during code compression and decompression, focusing on round-trip consistency. The authors introduce RoundTripCodeEval (RTCE), a benchmark that evaluates code understanding and execution through four distinct tasks designed to test the fidelity of bijection between encoding and decoding processes. They assess state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms, finding that while these methods yield modest improvements, none fully achieve round-trip consistency, suggesting a lack of internal coherence in current models for reliable code reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）在代码压缩与解压缩过程中保持一致推理能力的问题，这对可靠的代码理解和执行至关重要。作者提出了RoundTripCodeEval（RTCE）基准，通过四个不同的代码执行任务评估编码与解码过程之间的双向一致性，重点关注映射的保真度。实验结果显示，即使采用零样本提示、执行轨迹监督微调和自我反思机制，当前最先进的代码语言模型（Code-LLMs）也只能获得有限的提升，表明它们无法实现真正的双向一致性，暗示缺乏支持可信代码推理所需的内部连贯性。</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models</div>
<div class="meta-line">Authors: Samuel Cyrenius Anderson</div>
<div class="meta-line">First: 2026-01-19T19:53:37+00:00 · Latest: 2026-01-19T19:53:37+00:00</div>
<div class="meta-line">Comments: 34 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -&gt; 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -&gt; 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维的几何学：大规模语言模型中规模如何重构推理</div>
<div class="mono" style="margin-top:8px">规模并不均匀地提升推理能力，而是重构了推理。通过分析四个领域（法律、科学、代码、数学）和两个规模（80亿、700亿参数）下的25,000多个思维链轨迹，我们发现神经网络的扩展定律触发了领域特异性的相变，而非统一的能力提升。法律推理经历结晶化：表征维度减少了45%（d95: 501 -&gt; 274），轨迹对齐度增加了31%，并实现了10倍的语义分离。科学和数学推理保持液态——即使参数增加9倍，其几何结构仍保持不变。代码推理则形成了一种离散的战略模式格子（轮廓系数：0.13 -&gt; 0.42）。这种几何结构预示了可学习性。我们引入了神经推理算子——从初始到终端隐藏状态的学习映射。在结晶化的法律推理中，我们的算子通过探针解码在未见任务上达到了63.6%的准确率，无需遍历中间状态即可预测推理终点。我们进一步识别出一种跨领域和规模的普遍振荡特征（相干度 ~ -0.4），表明注意力层和前馈层通过相反的动力学驱动推理。这些发现表明，思维的成本并非由任务难度决定，而是由流形几何决定——这为推理加速提供了蓝图，只要拓扑结构允许。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how increasing the scale of large language models (LLMs) affects their reasoning capabilities across different domains. By analyzing over 25,000 chain-of-thought trajectories in Law, Science, Code, and Math with models of 8B and 70B parameters, the research reveals that scaling induces domain-specific phase transitions rather than uniform improvements. Legal reasoning exhibits crystallization, characterized by a significant reduction in representational dimensionality and increased trajectory alignment, while scientific and mathematical reasoning remains liquid, maintaining geometric invariance. Code reasoning transitions to a discrete lattice structure. The study introduces Neural Reasoning Operators, which map initial to terminal hidden states, achieving high accuracy in predicting reasoning outcomes without traversing intermediate steps. A universal oscillatory pattern is also identified, indicating that attention and feedforward layers contribute to reasoning through opposing dynamics.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）规模扩大对其在不同领域推理能力的影响。通过对法律、科学、代码和数学四个领域中超过25,000条思维链轨迹的分析，研究发现模型规模的增加会引发领域特异性的相变，而非整体能力的提升。法律推理转变为晶体态，表现出表征维度的减少和轨迹对齐度的增加，而科学和数学推理保持液态，即使参数增加9倍仍保持几何不变性。代码推理则形成具有策略模式的离散晶格结构。研究引入了神经推理算子，该算子通过探针解码在法律推理任务中达到63.6%的准确率，无需遍历中间状态即可预测推理终点。此外，研究还发现一种跨领域和规模的普遍振荡特征，表明注意力和前馈层通过相反的动力学机制驱动推理。这些发现表明，推理成本由表征流形的几何结构决定，而非任务难度，为推理加速提供了蓝图。</div>
</details>
</div>
<div class="card">
<div class="title">CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</div>
<div class="meta-line">Authors: Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</div>
<div class="meta-line">First: 2026-01-19T18:59:44+00:00 · Latest: 2026-01-19T18:59:44+00:00</div>
<div class="meta-line">Comments: Code is available: https://github.com/CausalSpatial/CausalSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13304v1">PDF</a> · <a href="https://github.com/CausalSpatial/CausalSpatial">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer &quot;what-if&quot; questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalSpatial：面向对象因果空间推理的基准测试</div>
<div class="mono" style="margin-top:8px">人类可以观察静态场景并瞬间预测接下来会发生什么——移动这个物体是否会导致碰撞？我们称这种能力为因果空间推理。然而，当前的多模态大语言模型（MLLMs）无法做到这一点，因为它们主要局限于静态空间感知，难以回答三维场景中的“假设情景”问题。我们引入了CausalSpatial，这是一个诊断基准，用于评估模型在碰撞、兼容性、遮挡和轨迹四个任务中是否能够预测物体运动的后果。结果揭示了一个严重差距：人类得分84%，而GPT-5仅达到54%。为什么MLLMs会失败？我们的分析发现了一个根本性缺陷：模型过度依赖文本链式推理，脱离了视觉证据，产生流畅但缺乏空间基础的幻觉。为了解决这一问题，我们提出了因果物体世界模型（COW），该框架通过生成假设动态的视频来外部化模拟过程。借助明确的因果视觉线索，COW使模型能够基于物理现实而非语言先验进行推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CausalSpatial, a benchmark designed to assess object-centric causal spatial reasoning in models. The motivation stems from the observation that humans can predict the consequences of object movements in a static scene, but current multimodal large language models (MLLMs) lack this capability due to their reliance on static spatial perception. The benchmark evaluates models across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Experimental results show a significant gap between human performance (84%) and that of GPT-5 (54%), highlighting the models&#x27; tendency to generate fluent yet spatially ungrounded hallucinations. To address this, the authors propose the Causal Object World model (COW), which simulates hypothetical dynamics through video generation, enabling models to base their reasoning on explicit visual cues of causality rather than linguistic patterns.</div>
<div class="mono" style="margin-top:8px">该研究提出了CausalSpatial基准，用于评估模型在物体中心因果空间推理方面的能力。研究动机源于观察到人类能够直观预测静态场景中物体运动的后果，如碰撞，而当前的多模态大语言模型（MLLMs）由于依赖静态空间感知，在此类任务上表现不佳。该基准包含四个任务：碰撞、兼容性、遮挡和轨迹。实验结果表明，人类表现达到84%，而GPT-5仅54%，揭示了现有模型的严重不足。作者提出Causal Object World（COW）框架，通过生成假设动态的视频来将推理根植于物理现实，而非语言模式。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-19T18:48:37+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能来寻找共同点并建立共识。随着AI代理越来越多地协作完成复杂工作，它们必须发展协调能力，才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的600多个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道被模糊、时机不当和不准确的信息堵塞；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the limitations of current coding agents in collaborative settings, particularly their inability to effectively coordinate with each other. The authors introduce CooperBench, a benchmark consisting of over 600 collaborative coding tasks across four programming languages and twelve libraries, designed to test how agents handle conflicting features that require coordination. The main experimental results show that state-of-the-art coding agents achieve on average 30% lower success rates when working together compared to when they perform tasks individually, highlighting the &#x27;curse of coordination.&#x27; The analysis identifies three key challenges: ineffective communication, commitment deviation, and incorrect expectations. The study also observes some emergent coordination behaviors, such as role division and negotiation, through large-scale simulations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨当前编码代理在协作环境中的局限性，尤其是它们在协调方面的能力不足。作者提出了CooperBench，这是一个包含超过600个跨四种编程语言和十二个库的协作编码任务的基准测试集，旨在评估代理的协调能力。主要实验结果显示，最先进的编码代理在协作时平均成功率比单独完成任务低30%，突显了AI代理中的&#x27;协调诅咒&#x27;，与人类团队通常因协作而提高生产力形成鲜明对比。分析指出三个关键问题：沟通不畅、承诺偏离以及对他人计划的错误预期。此外，通过大规模模拟，研究还观察到了一些罕见但有趣的协调行为，如角色分工和协商。</div>
</details>
</div>
<div class="card">
<div class="title">KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</div>
<div class="meta-line">Authors: Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong</div>
<div class="meta-line">First: 2026-01-19T17:20:16+00:00 · Latest: 2026-01-19T17:20:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13240v1">PDF</a> · <a href="https://github.com/jiangxxxue/KOCO-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&amp;A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KOCO-BENCH：大型语言模型能否在软件开发中利用领域知识？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在通用编程方面表现出色，但在领域特定的软件开发中表现不佳，因此需要领域专门化方法使LLMs能够学习和应用领域知识与数据。然而，现有的领域特定代码基准无法评估领域专门化方法的有效性，这些基准侧重于评估LLMs已有的知识，而非其获取和应用新知识的能力，且缺乏用于开发领域专门化方法的显式知识语料库。为此，我们提出了KOCO-BENCH，这是一个全新的基准，用于在现实世界软件开发中评估领域专门化方法。KOCO-BENCH包含6个新兴领域、11个软件框架和25个项目，配有精心整理的知识语料库，并包含多粒度评估任务，包括领域代码生成（从函数级到项目级，配有严格的测试套件）和领域知识理解（通过多项选择题问答）。与以往仅提供测试集供直接评估的基准不同，KOCO-BENCH要求从知识语料库中获取并应用多样化的领域知识（如API、规则、约束等）以解决评估任务。我们的评估结果表明，KOCO-BENCH对最先进的LLMs提出了重大挑战。即使应用了领域专门化方法（如SFT、RAG、kNN-LM），改进仍然有限。表现最好的编码代理Claude Code仅达到34.2%，突显了对更有效的领域专门化方法的迫切需求。我们发布了KOCO-BENCH、评估代码和基线模型，以促进进一步研究，详见https://github.com/jiangxxxue/KOCO-bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces KOCO-BENCH, a benchmark designed to evaluate how well large language models (LLMs) can leverage domain knowledge in software development. Existing benchmarks fail to assess the effectiveness of domain specialization methods, which focus on how LLMs acquire and apply new knowledge rather than what they already know. KOCO-BENCH includes six emerging domains, 11 frameworks, and 25 projects, with curated knowledge corpora and multi-granularity tasks such as code generation and knowledge understanding. Experimental results show that even with domain specialization techniques like SFT, RAG, and kNN-LM, LLMs perform poorly, with the best-performing model achieving only 34.2%, indicating the need for more advanced methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估大型语言模型（LLMs）在软件开发中利用领域知识的能力。现有基准测试主要评估模型已有的知识，而非其获取和应用新知识的能力。为此，作者提出了KOCO-BENCH，一个包含精心整理的知识语料库和多粒度任务的新基准，包括领域代码生成和知识理解。实验结果显示，即使应用了领域专业化方法如SFT、RAG和kNN-LM，LLMs的表现仍然有限，最佳编码代理Claude Code的准确率仅为34.2%。这表明需要更有效的领域知识整合方法。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-01-19T13:13:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v1">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D: 以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即在二维感知之外解释几何、视角和空间关系的能力。尽管最近的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知者，在真正的三维推理方面存在困难。我们引入了Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用三维重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上平均提升4.7%。我们进一步表明，对于在空间探索方面表现不佳的小型模型，通过强化学习策略使模型能够选择信息性视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是实现多模态代理中更灵活、更接近人类的三维推理的一种可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance spatial reasoning capabilities in vision large models (VLMs), which are currently limited to 2D perception. Think3D introduces a framework that enables VLM agents to interact with 3D space by integrating 3D reconstruction models to generate point clouds and camera poses from images or videos. This allows agents to perform camera-based operations and switch between ego and global views, facilitating a 3D chain-of-thought process. The framework significantly improves spatial reasoning performance on benchmark tasks such as BLINK Multi-view and MindCube, achieving average gains of +7.8% without additional training. Furthermore, reinforcement learning policies enhance smaller models&#x27; ability to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">Think3D的动机是通过让视觉大模型（VLMs）与三维空间互动，提升其空间推理能力。该框架利用三维重建模型从图像或视频中生成点云和相机姿态，使VLM代理能够通过基于相机的操作和视角切换来主动操控空间，从而将空间推理转化为交互式的三维思维过程。在无需额外训练的情况下，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，平均提升分别为BLINK Multi-view和MindCube上的+7.8%、VSI-Bench上的+4.7%。此外，通过强化学习策略，小型模型在选择信息性视角方面的能力得到显著增强，工具使用带来的性能提升从+0.7%提高到+6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">Knot So Simple: A Minimalistic Environment for Spatial Reasoning</div>
<div class="meta-line">Authors: Zizhao Chen, Yoav Artzi</div>
<div class="meta-line">First: 2025-05-23T15:34:08+00:00 · Latest: 2026-01-18T19:17:38+00:00</div>
<div class="meta-line">Comments: Fix camera ready footer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18028v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18028v3">PDF</a> · <a href="https://github.com/lil-lab/knotgym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Knot So Simple：一个用于空间推理的极简环境</div>
<div class="mono" style="margin-top:8px">我们提出了KnotGym，一个用于复杂空间推理和操作的交互式环境。KnotGym包含一系列以目标为导向的绳子操作任务，复杂度各不相同，所有任务均基于纯图像观察进行操作。任务的复杂度沿一个清晰且可量化的轴定义，基于绳结的交叉点数量，从而创建了一个自然的泛化测试。KnotGym具有简单的观察空间，便于可扩展开发，同时突出了在整合敏锐感知、空间推理和基于环境的操作中面临的核心挑战。我们评估了不同类别的方法，包括基于模型的强化学习、模型预测控制和链式思维推理，并展示了KnotGym所呈现的挑战。KnotGym可在https://github.com/lil-lab/knotgym获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces KnotGym, an interactive environment designed to evaluate spatial reasoning and manipulation through complex knot-tying tasks. The motivation stems from the need for a scalable and challenging platform to assess the integration of perception, reasoning, and action in robotic systems. KnotGym uses a simple observation space based on images and defines tasks along a complexity axis determined by the number of knot crossings, enabling a natural progression and generalization test. Experimental results show that existing methods, such as model-based reinforcement learning, model-predictive control, and chain-of-thought reasoning, face significant challenges in solving these tasks efficiently and accurately.</div>
<div class="mono" style="margin-top:8px">本文提出了KnotGym，一个用于评估空间推理与操作能力的最小化环境，通过复杂的绳结任务实现。研究动机源于对能够测试感知、推理与行动整合能力的基准环境的需求。KnotGym基于绳结交叉点数量定义任务复杂度，支持可扩展的任务开发和自然泛化。作者评估了多种方法，包括基于模型的强化学习、模型预测控制和链式思维推理，展示了这些任务对现有方法的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jihie Kim, Jean Oh</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-18T11:03:44+00:00</div>
<div class="meta-line">Comments: Corrected author order in metadata; manuscript unchanged</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未找到安全（404）：基于大语言模型的机器人决策中的隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键环境中，人工智能系统的一个错误可能导致生命损失。随着大语言模型（LLMs）在机器人决策中的应用日益广泛，风险的物理维度也在扩大；一个错误的指令可能直接危及人类安全。本文旨在系统评估LLMs在即使微小错误也可能导致灾难的场景中的表现。通过一个火灾疏散场景的定性评估，我们识别了基于LLM的决策中的关键失败案例。基于这些案例，我们设计了七个任务用于定量评估，分为：完整信息任务、不完整信息任务和安全导向空间推理（SOSR）任务。完整信息任务使用ASCII地图以减少解释歧义，并将空间推理与视觉处理分离。不完整信息任务要求模型推断缺失的上下文，测试其空间连续性与幻觉之间的区别。SOSR任务使用自然语言评估在生命威胁情境下的安全决策能力。我们对各种LLMs和视觉-语言模型（VLMs）在这些任务上的表现进行了基准测试。除了整体表现外，我们还分析了1%失败率的潜在影响，强调&quot;罕见&quot;错误如何演变为灾难性后果。结果揭示了严重的漏洞：一些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，模型指示机器人向危险区域移动而非紧急出口。我们的研究得出令人警醒的结论：当前的LLMs尚未准备好直接部署在安全关键系统中。在机器人领域，99%的准确率是危险误导的，因为它意味着每100次执行中可能有一次导致灾难性伤害。我们证明了即使是最先进的模型也无法保证安全，完全依赖它们会带来不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the critical issue of safety in robotics decision-making powered by Large Language Models (LLMs), emphasizing that even minor errors in such systems can lead to severe consequences. It introduces a framework for evaluating LLMs through three categories of tasks: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR), with the latter using natural language to assess safe behavior in life-threatening scenarios. The study reveals that several models failed completely in ASCII navigation tasks, and in simulated fire drills, some models directed robots toward hazardous areas instead of emergency exits, highlighting the risks of deploying LLMs in safety-critical applications without thorough validation.</div>
<div class="mono" style="margin-top:8px">本文探讨了在安全关键型机器人决策中使用大型语言模型（LLMs）所伴随的风险，指出即使是微小的错误也可能导致严重后果。作者设计了七个任务，包括完整信息、不完整信息和安全导向空间推理（SOSR），以系统评估LLMs和视觉语言模型（VLMs）的表现。实验结果显示，某些模型在ASCII导航任务中成功率仅为0%，并在模拟火灾演练中错误地指示机器人向危险区域移动而非安全出口。这些发现表明，当前的LLMs尚不适合部署在高风险环境中，其中99%的准确率可能误导人们，因为这意味着每100次执行中可能有一次导致灾难性伤害。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
