<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-08 03:40</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260208_0340</div>
    <div class="row"><div class="card">
<div class="title">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</div>
<div class="meta-line">Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</div>
<div class="meta-line">First: 2026-02-05T18:59:55+00:00 · Latest: 2026-02-05T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06041v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06041v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视角描述预测相机姿态以实现空间推理</div>
<div class="mono" style="margin-top:8px">多图像空间推理对于当前的多模态大语言模型（MLLMs）仍然是一个具有挑战性的任务。虽然单视角感知本质上是2D的，但跨视角推理需要在不同视角之间建立一致的场景理解。我们特别研究了视角转换（perspective taking）问题，其中模型必须从多视角观察中构建一致的3D理解，并据此在新的语言指定视角下进行推理。我们提出了CAMCUE，这是一个具有姿态感知的多图像框架，它将相机姿态作为跨视角融合和新视角推理的显式几何锚点。CAMCUE将每个视角的姿态注入视觉标记中，将自然语言视角描述锚定到目标相机姿态，并合成基于姿态的想象目标视角以支持回答。为支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例将多视角图像与姿态配对，并包含多样化的目标视角描述和视角转换问题。我们还在测试集中加入了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确率提升了9.06%，并且能够从自然语言视角描述中预测目标姿态，其旋转误差在20°以内准确率超过90%，平移误差在0.5阈值内。这种直接的锚定方式避免了昂贵的测试时搜索和匹配过程，将推理时间从每个实例256.6秒减少到1.45秒，从而实现了快速、交互式的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of multi-image spatial reasoning in current multimodal large language models (MLLMs), which struggle to build coherent 3D scene understanding from multiple views. The proposed method, CAMCUE, introduces a pose-aware framework that explicitly uses camera poses as geometric anchors for cross-view fusion and novel-view reasoning. It integrates per-view camera poses into visual tokens, maps natural language viewpoint descriptions to target poses, and synthesizes imagined target views to support reasoning. The CAMCUE-DATA dataset, containing 27,668 training and 508 test instances, is designed to evaluate the model&#x27;s ability to generalize to human language. Experimental results show that CAMCUE improves overall accuracy by 9.06% and achieves over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold, significantly reducing inference time from 256.6s to 1.45s per example.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升多模态大语言模型（MLLMs）在空间推理中的能力，特别是在从视角描述预测相机姿态方面。提出的方法CAMCUE是一个基于姿态的框架，通过将相机姿态作为显式的几何锚点，实现跨视角融合和新视角推理。该方法通过将自然语言视角描述映射到特定相机姿态，并合成基于姿态的想象目标视角，从而显著提升推理准确性，整体准确率提高了9.06%，旋转准确率超过90%（误差在20°以内），平移准确率在0.5误差阈值内。这种方法避免了耗时的测试时搜索匹配过程，将每个示例的推理时间从256.6秒降低至1.45秒，从而支持快速、交互式的实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-05T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v1">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型（MLLMs）在空间推理方面取得了显著进展，越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这常常导致语义与几何的不匹配以及冗余信号。我们提出GeoThinker框架，将整合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控进行校准，该门控机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面设立了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力，并显著提升了空间感知能力，包括具身指称和自动驾驶等任务。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), which often result in semantic-geometry misalignment and redundant signals. The proposed GeoThinker framework introduces an active perception approach by selectively retrieving geometric evidence based on internal reasoning needs. It employs Spatial-Grounded Fusion at specific VLM layers, combined with frame-strict cross-attention and Importance Gating to focus on task-relevant structures. Experimental results on VSI-Bench show that GeoThinker achieves a peak score of 72.6, outperforming existing methods and demonstrating strong generalization in complex scenarios like embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）在空间推理任务中被动整合几何信息的局限性进行了探讨，指出几何信息与语义之间的不匹配以及冗余信号的问题。提出GeoThinker框架，通过主动感知方法，根据模型内部推理需求选择性地检索几何证据。该框架在特定视觉语言模型（VLM）层应用空间基础融合，使语义视觉先验能够通过严格帧交叉注意力查询并整合相关几何信息，进一步由重要性门控机制优化。在VSI-Bench上的实验结果显示，GeoThinker取得了72.6的最高峰值成绩，并在复杂下游任务如具身指称和自动驾驶中表现出良好的泛化能力和空间感知提升。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-05T17:10:26+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cioutn.github.io/context-bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：面向编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估框架。ContextBench包含来自66个仓库、涵盖八种编程语言的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四种前沿LLM和五个编码代理。我们的结果表明，复杂的代理结构在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有的端到端基准，揭示了问题解决过程。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。数据和代码可在：https://cioutn.github.io/context-bench/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate how coding agents retrieve and utilize code context during issue resolution, rather than just focusing on final task success. ContextBench introduces a process-oriented benchmark with 1,136 tasks across eight programming languages, each annotated with human-verified code contexts. The authors developed an automated framework to track agent behavior and assess context recall, precision, and efficiency. Experimental results reveal that advanced agent architectures provide only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between the context explored and the context actually used.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估编码代理在问题解决过程中如何检索和使用代码上下文，而不仅仅关注最终任务的成功率。ContextBench 提供了一个面向过程的基准测试，包含来自66个仓库、涵盖八种编程语言的1136个问题解决任务，每个任务都附有人工标注的上下文信息。作者构建了一个自动化评估框架，用于追踪代理的行为并评估上下文的召回率、精确率和效率。实验结果表明，先进的代理架构在上下文检索方面仅带来小幅提升，LLMs倾向于优先召回而非精确，且存在显著的上下文探索与使用之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution</div>
<div class="meta-line">Authors: Philipp D. Siedler</div>
<div class="meta-line">First: 2025-05-21T22:00:20+00:00 · Latest: 2026-02-05T16:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16048v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.16048v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPhyR：基于拓扑优化的材料分布空间-物理推理基准数据集</div>
<div class="mono" style="margin-top:8px">我们引入了一个新颖的数据集，用于基于拓扑优化方法评估大型语言模型（LLM）的物理和空间推理能力。该方法用于在给定载荷和支撑条件下计算设计空间内的最优材料分布。在该数据集中，LLM会接收到诸如2D边界、施加的力和支撑等条件，并需推理出相应的最优材料分布。数据集包含多种任务，从填充部分结构中的掩码区域到预测完整的材料分布。解决这些任务需要理解力的流动以及在给定约束下的所需材料分布，而无需访问仿真工具或显式的物理模型，从而挑战模型对结构稳定性和空间组织的推理能力。我们的数据集旨在评估二维环境下的空间和物理推理能力，为传统的语言和逻辑基准提供补充视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SPhyR, a novel dataset aimed at evaluating the spatial and physical reasoning capabilities of Large Language Models (LLMs) in material distribution tasks. The dataset is based on topology optimization, where LLMs are given 2D boundary conditions, applied forces, and supports, and must infer the optimal material distribution without access to simulation tools or explicit physical models. The main experimental results show that models struggle with reasoning about structural stability and spatial organization, highlighting the need for improved physical understanding in LLMs.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为SPhyR的新数据集，旨在评估大型语言模型（LLM）在材料分布任务中的空间与物理推理能力。该数据集基于拓扑优化方法，为LLM提供二维边界条件、施加的力和支撑条件，要求其在没有仿真工具或显式物理模型的情况下推断出最优材料分布。任务包括预测完整分布和填充部分结构的掩码区域，需要模型理解力的流动和结构稳定性。实验结果表明，当前LLM在这些任务上表现不佳，凸显了提升其在物理和空间推理方面能力的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非自我中心感知者：通过帧实例化将非自我中心推理与自我中心视觉先验分离</div>
<div class="mono" style="margin-top:8px">随着对空间基础任务（如视觉-语言导航/动作）的需求增加，视觉-语言模型（VLMs）中的非自我中心感知能力正受到越来越多的关注。然而，VLMs在需要显式视角转换的非自我中心空间查询上仍表现脆弱，答案依赖于目标中心的推理框架，而非观察到的相机视角。因此，我们引入了Allocentric Perceiver，这是一种无需训练的策略，通过使用现成的几何专家从一个或多个图像中恢复度量3D状态，然后实例化一个与指令语义意图对齐的查询条件非自我中心参考框架。通过确定性地将重建的几何结构转换到目标框架，并使用结构化的、基于几何的表示提示主干VLM，Allocentric Perceiver将心理旋转从隐式推理转移到显式计算。我们在多个主干家族上评估Allocentric Perceiver在空间推理基准上的表现，观察到在非自我中心任务上取得了稳定且显著的提升（约10%），同时保持了强大的自我中心性能，并超越了空间感知微调模型以及最先进的开源和专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing demand for spatially grounded tasks like Vision-Language Navigation/Action has highlighted the limitations of current Vision-Language Models (VLMs) in handling allocentric spatial queries that require perspective shifts. To address this, the Allocentric Perceiver is introduced as a training-free method that reconstructs metric 3D states from images using geometric experts and then generates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s intent. This approach transforms the reconstructed geometry into the target frame explicitly, reducing the need for implicit mental rotation. Evaluations on spatial reasoning benchmarks show consistent and significant improvements ($\sim$10%) on allocentric tasks without compromising egocentric performance, outperforming both fine-tuned spatial-perception models and state-of-the-art alternatives.</div>
<div class="mono" style="margin-top:8px">随着对空间感知任务如视觉-语言导航和行动的需求增加，视觉-语言模型（VLMs）的非自我中心（allocentric）感知能力受到越来越多关注。然而，现有模型在需要显式视角转换的非自我中心空间查询中表现脆弱，因为它们依赖于自我中心的视觉先验。为此，本文提出了Allocentric Perceiver，这是一种无需训练的方法，通过使用现成的几何专家从图像中恢复度量3D状态，并构建与指令语义意图一致的查询条件非自我中心参考系。该方法通过确定性地将重建的几何信息转换为目标视角，并以结构化的几何表示提示主干VLM，将隐式推理中的心理旋转任务转为显式计算。实验结果表明，在非自我中心任务中取得了显著提升（约10%），同时保持了良好的自我中心任务表现，优于空间感知微调模型及当前最先进的开源和专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?</div>
<div class="meta-line">Authors: Yikun Zong, Cheston Tan</div>
<div class="meta-line">First: 2026-02-05T11:49:30+00:00 · Latest: 2026-02-05T11:49:30+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05570v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramSR：视觉-语言模型能否在连续几何空间中进行推理？</div>
<div class="mono" style="margin-top:8px">人类通过心理旋转、迭代优化和视觉反馈等认知过程，在空间推理任务如七巧板拼装中表现出色。受人类通过试错、观察和修正来解决七巧板谜题的启发，我们设计了一个框架，以建模这些人类认知机制。然而，对五种代表性视觉-语言模型（VLM）的全面实验揭示了在连续几何推理中的系统性失败：单块任务的平均IoU仅为0.41，两块组合任务则降至0.23，远低于人类表现，儿童能够成功完成七巧板任务。本文解决了一个自改进AI的基本挑战：模型能否在测试时通过迭代优化预测结果而无需参数更新？我们引入了一种测试时自优化框架，结合上下文学习（ICL）与奖励引导的反馈循环，灵感来源于人类认知过程。我们的无训练验证-优化智能体应用递归优化循环，基于几何一致性反馈迭代优化预测结果，在中等三角形案例中实现了IoU从0.63到0.932的显著提升。这表明，通过ICL和奖励循环引入人类启发的迭代优化机制，可以显著增强视觉-语言模型的几何推理能力，推动自改进AI在连续空间领域从理论走向实践。我们的工作可通过此匿名链接获取：https://anonymous.4open.science/r/TangramVLM-F582/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the ability of Vision-Language Models (VLMs) to perform continuous geometric reasoning, inspired by human spatial reasoning skills demonstrated in tasks like Tangram puzzle solving. The authors propose a test-time self-refinement framework that integrates in-context learning with reward-guided feedback loops to enable iterative prediction refinement without retraining. Experimental results across five VLMs show significant improvements in geometric reasoning performance, with IoU increasing from 0.63 to 0.932 on medium-triangle cases, highlighting the potential of human-inspired mechanisms in enhancing AI&#x27;s spatial reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在连续几何推理任务中的表现，受到人类在如七巧板拼图等空间推理任务中认知机制的启发。作者提出了一种测试时的自精炼框架，结合上下文学习与奖励引导的反馈循环，模拟人类的认知过程，如心理旋转和迭代优化。实验结果表明，该训练无关的验证-精炼代理在中等三角形案例中显著提升了几何推理性能，IoU从0.63提升至0.932，展示了人类启发机制在增强VLM空间能力方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Imagine a City: CityGenAgent for Procedural 3D City Generation</div>
<div class="meta-line">Authors: Zishan Liu, Zecong Tang, RuoCheng Wu, Xinzhe Zheng, Jingyu Hu, Ka-Hei Hui, Haoran Xie, Bo Dai, Zhengzhe Liu</div>
<div class="meta-line">First: 2026-02-05T06:36:03+00:00 · Latest: 2026-02-05T06:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05362v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05362v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models&#x27; generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>想象一座城市：用于程序化3D城市生成的CityGenAgent</div>
<div class="mono" style="margin-top:8px">交互式3D城市的自动化生成是自动驾驶、虚拟现实和具身智能等广泛应用中的关键挑战。尽管生成模型和程序化技术的最新进展提高了城市生成的逼真度，但现有方法在高保真资产创建、可控性和操作性方面仍存在困难。在本工作中，我们引入了CityGenAgent，这是一个由自然语言驱动的框架，用于高质量3D城市的分层程序化生成。我们的方法将城市生成分解为两个可解释的组件：Block Program和Building Program。为确保结构正确性和语义对齐，我们采用了一种两阶段学习策略：(1) 监督微调（SFT）。我们训练BlockGen和BuildingGen生成符合模式约束的有效程序，包括无自相交的多边形和完整的字段；(2) 强化学习（RL）。我们设计了空间对齐奖励（Spatial Alignment Reward）以增强空间推理能力，并设计了视觉一致性奖励（Visual Consistency Reward）以弥合文本描述与视觉模态之间的差距。得益于程序和模型的泛化能力，CityGenAgent支持自然语言编辑和操作。全面的评估表明，与现有方法相比，CityGenAgent在语义对齐、视觉质量和可控性方面具有显著优势，为可扩展的3D城市生成奠定了坚实的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to address the challenges in generating high-fidelity, controllable, and semantically aligned 3D cities for applications such as autonomous driving and virtual reality. The proposed method, CityGenAgent, is a natural language-driven framework that decomposes city generation into two hierarchical components: Block Program and Building Program. It employs a two-stage learning strategy, starting with Supervised Fine-Tuning to ensure schema compliance and then using Reinforcement Learning with custom rewards to improve spatial reasoning and visual consistency. Experimental results show that CityGenAgent outperforms existing methods in semantic alignment, visual quality, and controllability, making it a promising approach for scalable 3D city generation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有方法在生成高保真、可控的3D城市时存在的不足。CityGenAgent提出了一种基于自然语言驱动的框架，通过将城市生成任务分解为Block Program和Building Program两个可解释的组件实现分层过程生成。该框架采用两阶段学习策略，首先通过监督微调确保生成程序符合模式约束，然后利用强化学习结合空间对齐奖励和视觉一致性奖励提升空间推理能力。实验结果表明，CityGenAgent在语义对齐、视觉质量和可控性方面优于现有方法，为可扩展的3D城市生成提供了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking</div>
<div class="meta-line">Authors: Bishoy Galoaa, Xiangyu Bai, Utsav Nandi, Sai Siddhartha Vivek Dhir Rangoju, Somaieh Amraee, Sarah Ostadabbas</div>
<div class="meta-line">First: 2026-02-04T20:44:16+00:00 · Latest: 2026-02-04T20:44:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniTrack：用于多目标跟踪的可微分图表示学习</div>
<div class="mono" style="margin-top:8px">我们提出了UniTrack，这是一种即插即用的图论损失函数，旨在通过统一的可微分学习直接优化跟踪特定目标，从而显著提升多目标跟踪（MOT）性能。与之前重新设计跟踪架构的图基MOT方法不同，UniTrack提供了一个通用的训练目标，将检测精度、身份保持和时空一致性整合到一个端到端可训练的损失函数中，使得无需架构修改即可无缝集成到现有MOT系统中。通过可微分图表示学习，UniTrack使网络能够学习跨帧的运动连续性和身份关系的整体表示。我们在多种跟踪模型和多个具有挑战性的基准上验证了UniTrack，展示了在所有测试架构和数据集（包括Trackformer、MOTR、FairMOT、ByteTrack、GTR和MOTE）中的一致性提升。广泛评估表明，在具有挑战性的基准上，身份切换减少了最多53%，IDF1提高了12%，其中GTR在SportsMOT上实现了9.7%的MOTA性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniTrack introduces a differentiable graph-theoretic loss function aimed at improving multi-object tracking performance by directly optimizing tracking-specific objectives. It integrates detection accuracy, identity preservation, and spatiotemporal consistency into a unified end-to-end trainable framework, allowing for seamless integration with existing MOT systems without altering their architecture. The method enables networks to learn comprehensive motion and identity relationships across frames through differentiable graph representation learning. Experimental results across various MOT models and benchmarks show significant improvements, including up to 53% reduction in identity switches and 12% IDF1 gains, with GTR achieving a 9.7% increase in MOTA on SportsMOT.</div>
<div class="mono" style="margin-top:8px">UniTrack 提出了一种可微分的图论损失函数，旨在通过统一的学习方式直接优化多目标跟踪（MOT）的特定目标，从而显著提升跟踪性能。该方法将检测精度、身份保持和时空一致性整合为一个端到端可训练的损失函数，无需修改现有跟踪系统的架构即可无缝集成。通过可微分的图表示学习，网络能够学习跨帧的运动连续性和身份关系的整体表示。在多种跟踪模型和挑战性基准数据集上的实验表明，该方法带来了显著提升，包括身份切换减少高达53%，IDF1提升12%，其中GTR在SportsMOT数据集上实现了9.7%的MOTA提升。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：通过视觉语言模型将任务规划接地为空间感知的自中心动作以实现人形机器人应用</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人具有根本性挑战，因为其需要在部分信息观测和动态变化环境中紧密集成感知、移动和操作。此外，还需在不同类型子任务之间稳健过渡。为应对这些挑战，我们提出了一种新的任务——EgoActing，该任务要求将高层指令直接接地为各种精确且具有空间感知的人形机器人动作。我们进一步通过引入EgoActor，一个统一且可扩展的视觉语言模型（VLM），来实例化该任务。EgoActor能够预测移动基元（如行走、转向、侧移、高度变化）、头部运动、操作指令和人机交互，以实时协调感知与执行。我们利用来自现实世界演示的广泛监督，结合空间推理问答和模拟环境演示，使EgoActor能够做出稳健且上下文感知的决策，并在1秒内进行流畅的动作推理，适用于8B和4B参数模型。在模拟和现实环境中的大量评估表明，EgoActor有效连接了抽象的任务规划与具体的运动执行，同时在多样任务和未见过的环境中具有良好的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The deployment of humanoid robots in real-world environments is challenging due to the need for integrating perception, locomotion, and manipulation under partial observations and dynamic changes. To address this, the paper introduces EgoActing, a task that directly grounds high-level instructions into precise, spatially aware actions. The proposed method, EgoActor, is a unified and scalable vision-language model (VLM) capable of predicting various locomotion primitives, head movements, manipulation commands, and human-robot interactions. Evaluated in both simulated and real-world settings, EgoActor demonstrates robust context-aware decision-making and fluent action inference within under 1 second, showing strong generalization across different tasks and environments.</div>
<div class="mono" style="margin-top:8px">人形机器人在现实环境中的部署面临巨大挑战，因为需要在部分信息观测和动态变化的环境中整合感知、移动和操作能力。为此，本文提出了EgoActing任务，该任务直接将高层指令映射到精确且具有空间感知的机器人动作。所提出的EgoActor是一个统一的视觉-语言模型，能够预测包括移动、头部运动和操作指令在内的多种动作。它通过现实世界演示、空间推理问答和模拟环境数据进行广泛训练，使其能够做出上下文感知的决策并快速推理动作（小于1秒）。实验结果表明，EgoActor能够有效连接抽象任务规划与具体运动执行，并在多种任务和未见过的环境中表现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-02-04T12:38:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v2">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即超越二维感知，解释几何、视角和空间关系的能力。尽管近期的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用三维重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换，主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使其能够选择信息性视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是一种实现多模态代理更灵活、更接近人类的三维推理的可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of Think3D is to enhance spatial reasoning capabilities in vision large models (VLMs), which are currently limited to 2D perception and struggle with genuine 3D understanding. The framework introduces a method that enables VLM agents to interact with 3D space by using 3D reconstruction models to generate point clouds and camera poses from images or videos. This allows agents to manipulate space through camera-based operations and switch between ego and global views, thereby transforming spatial reasoning into an interactive 3D chain-of-thought process. The main experimental results show that Think3D significantly improves spatial reasoning performance without additional training, achieving average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. Additionally, reinforcement learning policies further enhance smaller models&#x27; performance by guiding them to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">Think3D的研究动机是提升视觉大模型（VLMs）的空间推理能力，因为现有模型主要依赖2D感知，难以实现真正的3D理解。该框架通过利用3D重建模型，从图像或视频中生成点云和相机姿态，使VLM代理能够通过基于相机的操作和视角切换来主动操控空间，从而模拟出3D的思维链过程。实验结果显示，无需额外训练，Think3D显著提升了GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理表现，分别在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上提升4.7%。此外，通过强化学习策略，使小型模型能够选择更有信息量的视角，从而将工具使用带来的性能提升从0.7%提高到6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</div>
<div class="meta-line">Authors: Mingcong Lei, Honghao Cai, Yuyuan Yang, Yimou Wu, Jinke Ren, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Shaohan Jiang, Ge Wang, Junyuan Tan, Zhenglin Wan, Zheng Li, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han</div>
<div class="meta-line">First: 2025-08-02T15:39:42+00:00 · Latest: 2026-02-04T12:10:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01415v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.01415v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied intelligence aims to enable robots to learn, reason, and generalize robustly across complex real-world environments. However, existing approaches often struggle with partial observability, fragmented spatial reasoning, and inefficient integration of heterogeneous memories, limiting their capacity for long-horizon adaptation. To address this, we introduce RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory within a parallelized architecture for efficient long-horizon planning and interactive learning. Its core innovations are a dynamic spatial knowledge graph for scalable, consistent memory updates and a closed-loop planner with a critic module for adaptive decision-making. Extensive experiments on EmbodiedBench show that RoboMemory, instantiated with Qwen2.5-VL-72B-Ins, improves the average success rate by 26.5% over its strong baseline and even surpasses the closed-source SOTA, Claude-3.5-Sonnet. Real-world trials further confirm its capability for cumulative learning, with performance consistently improving over repeated tasks. Our results position RoboMemory as a scalable foundation for memory-augmented embodied agents, bridging insights from cognitive neuroscience with practical robotic autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMemory：一种受大脑启发的多记忆代理框架，用于物理具身系统中的交互式环境学习</div>
<div class="mono" style="margin-top:8px">具身智能旨在使机器人能够在复杂的真实世界环境中学习、推理和泛化。然而，现有方法常面临部分可观测性、碎片化空间推理以及异质记忆整合效率低的问题，限制了其长时序适应能力。为了解决这些问题，我们引入了RoboMemory，这是一种受大脑启发的框架，通过并行化架构将空间、时间、事件和语义记忆统一起来，以实现高效的长时序规划和交互式学习。其核心创新包括用于可扩展、一致记忆更新的动态空间知识图谱，以及带有批评模块的闭环规划器，以实现自适应决策。在EmbodiedBench上的大量实验表明，RoboMemory在Qwen2.5-VL-72B-Ins实例化后，其平均成功率比其强基线提高了26.5%，甚至超过了闭源的SOTA模型Claude-3.5-Sonnet。现实世界测试进一步验证了其累积学习能力，性能在重复任务中持续提升。我们的结果表明，RoboMemory为记忆增强的具身代理提供了一个可扩展的基础，将认知神经科学的见解与实际的机器人自主性相结合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboMemory is introduced to enhance embodied intelligence by addressing challenges such as partial observability, fragmented spatial reasoning, and inefficient memory integration in physical robots. The framework integrates spatial, temporal, episodic, and semantic memory within a parallelized architecture, featuring a dynamic spatial knowledge graph for scalable memory updates and a closed-loop planner with a critic module for adaptive decision-making. Experimental results on EmbodiedBench demonstrate that RoboMemory, when implemented with Qwen2.5-VL-72B-Ins, achieves a 26.5% improvement in average success rate compared to a strong baseline and outperforms the closed-source state-of-the-art model, Claude-3.5-Sonnet. Real-world trials further validate its ability to support cumulative learning through repeated task execution.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决部分可观测性、空间推理碎片化以及异质记忆整合效率低等问题，提升机器人在复杂现实环境中的适应能力和学习能力。RoboMemory 是一个受大脑启发的框架，将空间、时间、事件和语义记忆统一在一个并行架构中，以支持长期规划和交互式学习。该框架引入了动态空间知识图，实现可扩展且一致的记忆更新，并采用带有批评模块的闭环规划器进行自适应决策。在 EmbodiedBench 上的实验表明，RoboMemory 在使用 Qwen2.5-VL-72B-Ins 实现时，平均成功率比强基线模型提高了 26.5%，并超越了闭源的最先进模型 Claude-3.5-Sonnet。现实世界测试进一步验证了其支持累积学习的能力，性能在重复任务中持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">MapCoder-Lite: Distilling Multi-Agent Coding into a Single Small LLM</div>
<div class="meta-line">Authors: Woongkyu Lee, Junhee Cho, Jungwook Choi</div>
<div class="meta-line">First: 2025-09-22T08:19:11+00:00 · Latest: 2026-02-04T07:25:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17489v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17489v2">PDF</a> · <a href="https://github.com/aiha-lab/MapCoder-Lite">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale (&gt;30B) models or collapse when downsized to small open-source models. We present MapCoder-Lite, a framework for distilling the complex reasoning of large, multi-agent coding systems into a single 7B model. Our contribution is a novel, three-pillar methodology that synergistically generates, refines, and encodes multi-agent knowledge: (i) pass-based trajectory distillation from strong LLMs fixes format fragility in retrieval and reduces failures in debugging, (ii) supervisor-guided correction with global feedback strengthens planning and coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests shows that MapCoder-Lite more than doubles xCodeEval accuracy (from 13.2% to 28.3%), eliminates all format failures, while reducing GPU memory and token-generation time by 4x compared to a 32B model. It also achieves over 10% gains on simpler coding benchmarks, demonstrating broad improvements beyond competitive programming. These results demonstrate that careful agent-wise fine-tuning unleashes high-quality multi-agent coding on a small language model. Our code is publicly available at https://github.com/aiha-lab/MapCoder-Lite.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the performance of code generation tasks using small-scale language models by distilling the capabilities of complex multi-agent coding systems. MapCoder-Lite introduces a three-pillar methodology that includes pass-based trajectory distillation, supervisor-guided correction with global feedback, and agent-wise LoRA fine-tuning to generate, refine, and encode multi-agent knowledge. Experimental results on xCodeEval, APPS, and CodeContests show that the framework significantly improves accuracy, achieving over 28% on xCodeEval, eliminates format failures, and reduces GPU memory and token-generation time by 4x compared to a 32B model.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将先进的多智能体编码系统适配到更小、更高效的语言模型的挑战。研究动机是实现高质量代码生成而无需依赖大型、昂贵的模型。MapCoder-Lite提出了一种三支柱方法：基于传递的轨迹蒸馏提升了检索和调试的可靠性，监督指导的修正增强了规划和编码能力，而基于智能体的LoRA微调则实现了内存高效的专门化。在xCodeEval、APPS和CodeContests等数据集上的实验结果表明，MapCoder-Lite在xCodeEval上准确率超过28%，消除了格式错误，并将GPU内存和生成令牌时间相比32B模型减少了4倍。</div>
</details>
</div>
<div class="card">
<div class="title">Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization</div>
<div class="meta-line">Authors: Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing</div>
<div class="meta-line">First: 2025-09-15T20:36:19+00:00 · Latest: 2026-02-04T05:16:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12434v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.12434v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues. A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs. While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS. Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents. To bridge this gap, we introduce EntroPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting. EntroPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses. We validate EntroPO by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches. On the SWEBENCH leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with EntroPO ranks 1st on SWEBENCH-LITE and 4th on SWEBENCH-VERIFIED on the open-weight leaderboard, surpassed only by models with over 10x more parameters(e.g., &gt;$350B).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过熵增强的多轮偏好优化构建编码代理</div>
<div class="mono" style="margin-top:8px">软件工程为大型语言模型（LLMs）提出了复杂且多步骤的挑战，需要在大型代码库上进行推理并协调使用工具。这些任务的难度在诸如SWE-bench等基准测试中得到了体现，其中当前的LLMs仍然难以解决现实问题。一种有前景的方法是测试时扩展（TTS），但其效果高度依赖于模型输出的多样性。虽然标准对齐方法如直接偏好优化（DPO）和卡尼曼-特弗斯基优化（KTO）在将模型输出与人类偏好对齐方面是有效的，但这一过程可能会以减少多样性为代价，从而限制TTS的效果。此外，现有的偏好优化算法通常设计用于单轮任务，无法充分应对交互式编码代理所需的多轮推理和工具集成的复杂性。为弥合这一差距，我们引入了EntroPO，这是一种熵增强框架，将现有的偏好优化算法适应到多轮、工具辅助的环境中。EntroPO通过显式保留策略熵来增强偏好目标，并将学习过程推广到多轮交互的优化，而非单轮响应。我们通过微调来自不同家族和规模（最高达106B参数）的多样化模型来验证EntroPO。为了最大化TTS带来的性能提升，我们进一步提出了一种混合最佳轨迹选择方案，结合了学习的验证器模型和无模型方法。在SWEBENCH排行榜上，我们的方法在开放权重模型中建立了新的最先进结果。使用EntroPO训练的30B参数模型在SWEBENCH-LITE上排名第一，在SWEBENCH-VERIFIED上排名第四，仅被参数量超过其10倍的模型（例如，&gt;350B参数）超越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Large Language Models (LLMs) in handling complex, multi-step coding tasks by introducing EntroPO, an entropy-enhanced framework for multi-turn preference optimization. The motivation stems from the challenges LLMs face in reasoning over large codebases and integrating tools, as seen in benchmarks like SWE-bench. EntroPO improves upon existing methods such as DPO and KTO by preserving policy entropy during alignment with human preferences, enabling more diverse model outputs that benefit from test-time scaling. The framework is validated through fine-tuning on a range of model sizes, achieving state-of-the-art results on SWEBENCH-LITE and SWEBENCH-VERIFIED among open-weight models.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在需要多步骤推理和工具集成的复杂软件工程任务中的表现问题。现有方法如直接偏好优化（DPO）和卡尼曼-特弗斯基优化（KTO）虽然能有效对齐模型输出与人类偏好，但会降低输出多样性，从而限制测试时扩展（TTS）的效果。为此，作者提出了EntroPO框架，通过增强熵来保持策略多样性，并优化多轮交互而非单轮响应。他们通过微调不同规模和家族的模型验证了该方法，并引入了一种混合轨迹选择方案以进一步提升性能。实验结果显示，使用EntroPO训练的30B参数模型在SWEBENCH基准测试中取得了优异成绩，在开放权重排行榜上位列第一（SWEBENCH-LITE）和第四（SWEBENCH-VERIFIED），仅低于参数量超过其10倍的模型（如超过350B参数的模型）</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-02-04T04:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot;-a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal audio scene analysis (ASA). To bridge this gap, we first introduce a hierarchical framework for audio scene analysis. Guided by this framework, we introduce a system that enables large audio-language models (LALMs) to understand and reason about the complex acoustic world.
  Our system endows LALMs with universal spatial understanding through four key innovations: (1) A scalable simulation pipeline that synthesizes high-quality First-Order-Ambisonics(FOA) data; (2) A unified model framework that integrates universal spatial encoding with a dense hybrid projection mechanism to bridge the modality gap; (3) A progressive training curriculum that evolves from representation alignment to reinforcement learning-based reasoning; and (4) A comprehensive benchmark for audio scene analysis (ASA) designed to rigorously evaluate atomic perception, relational integration, and cognitive reasoning capabilities, on which our model demonstrates comparatively strong capability for spatial understanding. Our work provides a clear pathway for leveraging the powerful reasoning abilities of LALMs towards holistic ASA, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：在大型音频语言模型中实现空间理解</div>
<div class="mono" style="margin-top:8px">现有的大型音频语言模型将世界视为&quot;单声道&quot;，即忽略关键的空间维度（&quot;在哪里&quot;）的单一音频流，这限制了其对通用音频场景分析（ASA）的能力。为弥合这一差距，我们首先提出了一种分层的音频场景分析框架。基于该框架，我们引入了一种系统，使大型音频语言模型（LALMs）能够理解和推理复杂的声学世界。
我们的系统通过四项关键创新赋予LALMs普遍的空间理解能力：(1) 一个可扩展的模拟管道，合成高质量的首阶全向声学（FOA）数据；(2) 一个统一的模型框架，将普遍空间编码与密集混合投影机制结合，以弥合模态差距；(3) 一个渐进式的训练课程，从表示对齐逐步发展到基于强化学习的推理；(4) 一个全面的音频场景分析（ASA）基准测试，旨在严格评估基本感知、关系整合和认知推理能力，我们的模型在该基准上展现出较强的空间理解能力。我们的工作为利用LALMs强大的推理能力实现全面的ASA提供了清晰的路径，从&quot;单声道&quot;语义识别推进到空间智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing large audio-language models (LALMs) that perceive the world as a single audio stream, neglecting the spatial dimension essential for universal audio scene analysis (ASA). The authors propose a hierarchical framework and a system that enhances LALMs with spatial understanding through four innovations: a scalable FOA data simulation pipeline, a unified model integrating spatial encoding with hybrid projection, a progressive training curriculum from alignment to reasoning, and a comprehensive ASA benchmark. The model demonstrates strong performance in spatial perception, relational integration, and cognitive reasoning, marking a significant step toward spatial intelligence in audio-language models.</div>
<div class="mono" style="margin-top:8px">本文针对现有大型音频语言模型（LALMs）将世界视为单一音频流、忽略空间维度这一限制，提出了一种分层框架和系统，以增强LALMs对空间的理解能力。该系统通过四项创新实现：可扩展的FOA数据模拟管道、整合空间编码与混合投影机制的统一模型框架、从表示对齐到基于强化学习的推理的渐进式训练课程，以及全面的音频场景分析（ASA）基准测试。实验结果表明，该模型在空间感知、关系整合和认知推理方面表现出较强的能力，标志着从单一语义识别向空间智能的迈进。</div>
</details>
</div>
<div class="card">
<div class="title">CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning</div>
<div class="meta-line">Authors: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le</div>
<div class="meta-line">First: 2025-05-31T23:32:01+00:00 · Latest: 2026-02-03T23:34:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00750v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00750v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codesense-bench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about code semantics is essential for enhancing code LLMs&#x27; abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limits models&#x27; capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CodeSense：面向代码语义推理的现实世界基准和数据集</div>
<div class="mono" style="margin-top:8px">理解并推理代码语义对于增强代码大语言模型（LLMs）解决现实世界软件工程（SE）任务的能力至关重要。尽管已有多个代码推理基准，但大多数依赖合成数据集或教育编程问题，且主要关注输入/输出预测等粗粒度推理任务，这限制了它们在实际软件工程场景中评估LLMs的有效性。为弥合这一差距，我们提出了CodeSense，这是首个提供一系列针对现实代码软件工程的细粒度代码推理任务的基准。我们从现实世界的代码仓库中收集了Python、C和Java软件项目，执行了这些仓库中的测试，收集了其执行轨迹，并构建了一个用于细粒度语义推理任务的基准数据集。随后，我们在最先进的LLMs上进行了全面评估。我们的结果表明，模型在处理细粒度推理任务时存在明显的性能差距。尽管链式推理和上下文学习等提示技术有所帮助，但LLMs中缺乏代码语义从根本上限制了其代码推理能力。除了数据集、基准和评估，我们的工作还提供了一个执行轨迹框架和工具集，使得收集细粒度软件工程推理任务的基准数据变得容易，为未来基准构建和模型微调提供了坚实的基础。我们的代码和数据位于https://codesense-bench.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the ability of code LLMs to perform real-world software engineering tasks by addressing the limitations of existing benchmarks that focus on coarse-grained reasoning. CodeSense introduces a new benchmark and dataset that includes fine-grained code reasoning tasks derived from real-world software projects in Python, C, and Java. The dataset is constructed by executing tests from real repositories and collecting execution traces to create a ground truth. Comprehensive evaluations on state-of-the-art LLMs reveal a significant performance gap in handling fine-grained reasoning, indicating that current models lack sufficient code semantics understanding. Prompting techniques like chain-of-thought and in-context learning provide some improvement, but the fundamental issue remains. The work also provides an execution tracing framework and toolset to facilitate future benchmarking and model training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升代码大语言模型（LLMs）在实际软件工程任务中的语义推理能力。作者提出了CodeSense，这是首个包含真实世界代码语义推理任务的基准和数据集。他们从真实代码仓库中收集了Python、C和Java项目，执行测试并获取执行轨迹，构建了用于细粒度语义推理的基准数据集。对当前最先进的LLMs进行评估发现，模型在处理细粒度推理任务时存在明显性能差距，表明其缺乏对代码语义的深入理解。尽管链式推理和上下文学习等提示技术有所改善，但LLMs本身对代码语义的缺失仍是根本限制。此外，该工作还提供了一个执行轨迹框架和工具集，便于收集细粒度软件工程推理任务的基准数据，为未来基准构建和模型训练奠定了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents</div>
<div class="meta-line">Authors: Bhanu Prakash Vangala, Ali Adibifar, Ashish Gehani, Tanu Malik</div>
<div class="meta-line">First: 2025-12-26T21:17:22+00:00 · Latest: 2026-02-03T22:46:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22387v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI生成的代码不可复现（尚且如此）：基于大语言模型编码代理的依赖缺口实证研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为编码代理的兴起承诺加速软件开发，但其对生成代码可复现性的影响仍鲜有研究。本文通过实证研究探讨LLM生成的代码是否可以在仅使用操作系统包和模型指定依赖的干净环境中成功执行。我们评估了三个最先进的LLM编码代理（Claude Code、OpenAI Codex和Gemini）在Python、JavaScript和Java三种语言中，由100个标准化提示生成的300个项目的表现。我们引入了一个三层依赖框架（区分声明依赖、工作依赖和运行时依赖），以量化代码执行的可复现性。研究结果表明，仅有68.3%的项目能够直接运行，不同语言之间存在显著差异（Python为89.2%，Java为44.0%）。我们还发现，从声明依赖到实际运行时依赖的平均扩展率为13.5倍，揭示了大量隐藏依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reproducibility of code generated by large language models (LLMs) when executed in a clean environment using only the dependencies specified by the model. The research evaluates three advanced LLM-based coding agents—Claude Code, OpenAI Codex, and Gemini—across 300 projects created from 100 standardized prompts in Python, JavaScript, and Java. A three-layer dependency framework is introduced to categorize claimed, working, and runtime dependencies, enabling a quantitative assessment of reproducibility. The findings reveal that only 68.3% of the projects can be executed successfully without additional dependencies, with notable differences across programming languages, and a significant average expansion of 13.5 times from declared to actual runtime dependencies.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLM）生成代码在干净环境中的可执行性问题。研究动机源于LLM编码代理在软件开发中的广泛应用，以及对其生成代码可重复性影响的了解不足。作者评估了三种先进的编码代理——Claude Code、OpenAI Codex和Gemini——在Python、JavaScript和Java三种语言中，基于100个标准化提示生成的300个项目。他们引入了一个三层依赖框架，用于区分声明依赖、工作依赖和运行时依赖。实验结果显示，仅有68.3%的项目能够无需额外配置直接运行，不同编程语言之间存在显著差异，其中Python项目的成功率最高（89.2%），而Java项目最低（44.0%）。此外，运行时依赖的数量平均比声明依赖多出13.5倍，揭示了大量隐藏依赖的存在，影响了代码的可重复性。</div>
</details>
</div>
<div class="card">
<div class="title">FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation</div>
<div class="meta-line">Authors: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Mingjie Zhan, Hongsheng Li</div>
<div class="meta-line">First: 2026-02-03T18:01:34+00:00 · Latest: 2026-02-03T18:01:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03798v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03798v1">PDF</a> · <a href="https://github.com/mnluzimu/FullStack-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FullStack-Agent：通过面向开发的测试和仓库反向翻译增强代理式全栈网页编码</div>
<div class="mono" style="margin-top:8px">帮助非专家用户开发复杂的交互式网站已成为LLM驱动代码代理的热门任务。然而，现有的代码代理往往仅生成前端网页，用华丽的视觉效果掩盖了实际全栈数据处理和存储能力的不足。值得注意的是，构建生产级的全栈网页应用远比仅生成前端网页更具挑战性，需要对数据流进行细致控制，全面理解不断更新的包和依赖关系，并准确定位代码库中的隐晦错误。为了解决这些困难，我们引入了FullStack-Agent，这是一个统一的代理系统，用于全栈代理式编码，包含三个部分：(1) FullStack-Dev，一个具备强大规划、代码编辑、代码库导航和错误定位能力的多代理框架；(2) FullStack-Learn，一种创新的数据扩展和自我提升方法，通过反向翻译爬取和合成的网站仓库来提升FullStack-Dev的核心LLM；(3) FullStack-Bench，一个全面的基准测试系统，系统性地测试生成网站的前端、后端和数据库功能。我们的FullStack-Dev在前端、后端和数据库测试用例上分别优于之前最先进的方法8.7%、38.2%和15.9%。此外，FullStack-Learn通过自我提升，使一个30B模型在三个测试用例集上的性能分别提升了9.7%、9.5%和2.8%，证明了我们方法的有效性。代码已发布在https://github.com/mnluzimu/FullStack-Agent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing code agents that focus only on frontend development, neglecting the complexities of full-stack web applications. FullStack-Agent introduces a unified agent system comprising three components: FullStack-Dev, which enables multi-agent collaboration with advanced planning, code editing, and bug localization; FullStack-Learn, a self-improving method that enhances the backbone LLM through repository back-translation; and FullStack-Bench, a benchmark for evaluating frontend, backend, and database functionalities. The results show that FullStack-Dev improves performance by 8.7%, 38.2%, and 15.9% on frontend, backend, and database tasks respectively, while FullStack-Learn boosts a 30B model&#x27;s performance by 9.7%, 9.5%, and 2.8% on the same tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有代码代理仅关注前端开发而忽视全栈数据处理与存储复杂性的不足。FullStack-Agent提出一个统一的代理系统，包含三个部分：FullStack-Dev，一个具备强大规划、代码编辑和代码库导航能力的多代理框架；FullStack-Learn，一种通过反向翻译爬取的代码库来提升基础大模型性能的自改进方法；以及FullStack-Bench，一个全面评估生成网站前端、后端和数据库功能的基准测试。实验结果显示，FullStack-Dev在前端、后端和数据库任务上的表现分别提升了8.7%、38.2%和15.9%，而FullStack-Learn则使30B模型在三个测试集上的性能分别提高了9.7%、9.5%和2.8%。</div>
</details>
</div>
<div class="card">
<div class="title">SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?</div>
<div class="meta-line">Authors: Azmine Toushik Wasi, Wahid Faisal, Abdur Rahman, Mahfuz Ahmed Anik, Munem Shahriar, Mohsin Mahmud Topu, Sadia Tasnim Meem, Rahatun Nesa Priti, Sabrina Afroz Mitu, Md. Iqramul Hoque, Shahriyar Zaman Ridoy, Mohammed Eunus Ali, Majd Hawasly, Mohammad Raza, Md Rizwan Parvez</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:52:02+00:00 · Latest: 2026-02-03T17:52:02+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. 92 Pages. 42 Figures and 29 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03916v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spatialab-reasoning.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs&#x27; spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth &amp; Occlusion, Orientation, Size &amp; Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs&#x27; spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatiaLab：视觉语言模型能否在真实环境中进行空间推理？</div>
<div class="mono" style="margin-top:8px">空间推理是人类认知的基本组成部分，但仍然是当前视觉语言模型（VLMs）的一大挑战。以往的工作主要依赖于合成或LLM生成的环境，任务设计和类似谜题的设置有限，未能捕捉到VLMs在现实世界中遇到的复杂性、视觉噪声和多样的空间关系。为了解决这一问题，我们引入了SpatiaLab，这是一个用于评估VLMs在现实、无约束情境下空间推理能力的全面基准。SpatiaLab包含六个主要类别：相对位置、深度与遮挡、方向、大小与比例、空间导航和3D几何，每个类别下有五个子类别，共计30种不同的任务类型。每个子类别至少包含25个问题，每个主类别至少包含200个问题，支持多项选择和开放式评估。在多种最先进的VLMs上进行的实验表明，其空间推理能力与人类存在显著差距。在多项选择设置中，InternVL3.5-72B的准确率为54.93%，而人类为87.57%。在开放式设置中，所有模型的性能下降约10-25%，其中GPT-5-mini得分最高，为40.93%，而人类为64.93%。这些结果突显了处理复杂空间关系、深度感知、导航和3D几何方面的关键局限性。通过提供一个多样化的现实评估框架，SpatiaLab揭示了推进VLMs空间推理能力的关键挑战和机遇，为未来研究提供了指导基准。SpatiaLab可在以下网址获取：https://spatialab-reasoning.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SpatiaLab, a benchmark designed to evaluate the spatial reasoning capabilities of vision-language models (VLMs) in realistic, unconstrained environments. The motivation stems from the observation that existing benchmarks rely on synthetic or LLM-generated data, which fail to reflect the complexity and diversity of real-world spatial relationships. SpatiaLab includes 1,400 visual question-answer pairs across six major categories and 30 subcategories, supporting both multiple-choice and open-ended evaluations. Experimental results show that even state-of-the-art models like InternVL3.5-72B achieve only 54.93% accuracy in multiple-choice tasks compared to human performance of 87.57%, while open-ended tasks see a significant performance drop, with GPT-5-mini achieving 40.93% versus 64.93% for humans. These findings underscore the limitations of current VLMs in handling complex spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文提出SpatiaLab，这是一个用于评估视觉语言模型（VLMs）在现实、无约束环境中空间推理能力的基准测试。研究动机源于现有评估主要依赖合成或LLM生成的环境，无法反映真实世界中复杂多样的空间关系。SpatiaLab包含1,400个视觉问答对，涵盖六个主要类别和30个子类别，支持多项选择和开放式评估。实验结果显示，即使是最先进的模型，如InternVL3.5-72B和GPT-5-mini，在多项选择设置下的准确率分别为54.93%和40.93%，远低于人类的87.57%。这些结果突显了VLMs在处理复杂空间关系、深度感知、导航和三维几何方面的重要局限性，强调了构建更强大且与人类认知一致的空间推理能力的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</div>
<div class="meta-line">Authors: Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao</div>
<div class="meta-line">First: 2025-03-17T09:24:26+00:00 · Latest: 2026-02-03T13:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12968v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiPMB：基于优化泊松多伯努利滤波的3D多目标跟踪增强方法</div>
<div class="mono" style="margin-top:8px">准确的3D多目标跟踪（MOT）对自动驾驶至关重要，因为它能够实现复杂环境下的鲁棒感知、导航和规划。尽管基于深度学习的解决方案在3D MOT任务中表现出色，但基于模型的方法因其简单性、可解释性和数据效率仍然具有吸引力。传统的基于模型的跟踪器通常依赖于随机向量贝叶斯滤波器，在基于检测的跟踪（TBD）框架中存在局限，主要由于启发式的数据关联和轨迹管理方案。相比之下，基于随机有限集（RFS）的贝叶斯滤波方法在理论上处理了目标的出生、生存和消亡过程，从而提高了可解释性和参数调整的灵活性。本文提出了一种新的基于RFS的3D MOT方法OptiPMB，该方法在基于检测的跟踪框架中采用优化的泊松多伯努利（PMB）滤波器，并结合了多项关键创新设计。具体而言，我们提出了一种测量驱动的混合自适应出生模型以提升轨迹初始化效果，采用自适应检测概率参数以有效维护被遮挡目标的轨迹，并优化了密度剪枝和轨迹提取模块以进一步提升整体跟踪性能。在nuScenes和KITTI数据集上的大量评估表明，OptiPMB在跟踪精度上优于现有最先进的方法，从而为基于模型的3D MOT建立了新的基准，并为未来基于RFS的跟踪器研究提供了有价值的参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the accuracy of 3D multi-object tracking (MOT) in autonomous driving by combining the advantages of model-based approaches with the performance of deep learning methods. OptiPMB introduces an optimized Poisson multi-Bernoulli (PMB) filter within the tracking-by-detection (TBD) framework, incorporating innovations such as a measurement-driven hybrid adaptive birth model, adaptive detection probability parameters, and optimized density pruning and track extraction modules. Experimental results on the nuScenes and KITTI datasets demonstrate that OptiPMB outperforms existing state-of-the-art methods in tracking accuracy, setting a new benchmark for model-based 3D MOT.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用模型化方法在自动驾驶中提升三维多目标跟踪（MOT）的准确性，相较于深度学习方法，模型化方法具有简单、可解释和数据效率高的优势。OptiPMB提出了一种在跟踪-检测（TBD）框架下优化的泊松多伯努利（PMB）滤波器，结合了测量驱动的混合自适应出生模型、自适应检测概率参数以及优化的密度剪枝和轨迹提取模块。在nuScenes和KITTI数据集上的大量实验表明，OptiPMB在跟踪精度上优于现有最先进的方法，为模型化三维MOT设定了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">CP-Agent: Agentic Constraint Programming</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2025-08-10T19:59:01+00:00 · Latest: 2026-02-03T12:13:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver&#x27;s feedback, and refining constraint models based on execution results.
  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CP-Agent：代理约束编程</div>
<div class="mono" style="margin-top:8px">将自然语言翻译为形式化约束模型需要对问题领域和建模框架有专业知识。为探索代理工作流的有效性，我们提出了CP-Agent，这是一个使用ReAct框架并带有持久IPython内核的Python编码代理。我们将相关的领域知识作为项目提示，长度不超过50行。该算法通过迭代执行代码、观察求解器的反馈，并根据执行结果优化约束模型来工作。我们在CP-Bench的101个约束编程问题上评估了CP-Agent。我们对基准进行了小幅修改，以解决问题说明中的系统性歧义和真实模型中的错误。在澄清后的基准上，CP-Agent在所有101个问题上均达到完美准确率。我们的实验表明，最小的指导优于详细的程序框架。我们的实验还表明，显式的任务管理工具对集中建模任务可能有正反两方面的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the effectiveness of agentic workflows in translating natural language into formal constraint models. CP-Agent, a Python coding agent, is proposed that integrates the ReAct framework with a persistent IPython kernel, using concise domain knowledge as a project prompt. The agent iteratively executes code, observes solver feedback, and refines constraint models based on execution outcomes. Experimental results on 101 constraint programming problems from CP-Bench show that CP-Agent achieves perfect accuracy on all problems after minor adjustments to the benchmark. The findings indicate that minimal guidance can outperform detailed procedural scaffolding, while explicit task management tools may have mixed impacts on modeling tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨代理工作流在将自然语言转化为形式化约束模型中的有效性。为此，提出了一种名为CP-Agent的Python编码代理，它结合了ReAct框架与持久化的IPython内核，并使用简短的领域知识作为项目提示。该代理通过迭代执行代码、观察求解器反馈并根据执行结果优化约束模型来工作。在对CP-Bench中的101个约束编程问题进行评估后，通过微调基准以解决问题描述中的系统性歧义和真实模型中的错误，CP-Agent在修正后的基准上实现了全部问题的完美准确率。实验结果表明，最小的指导可能优于详细的程序框架，而显式的任务管理工具对专注建模任务的影响可能是正负参半的。</div>
</details>
</div>
<div class="card">
<div class="title">InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation</div>
<div class="meta-line">Authors: Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu, Yanyong Zhang</div>
<div class="meta-line">First: 2026-02-03T08:22:13+00:00 · Latest: 2026-02-03T08:22:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03242v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://shanpoyang654.github.io/InstaDrive/page.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#x27;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InstaDrive：面向实例的驾驶世界模型用于真实且一致的视频生成</div>
<div class="mono" style="margin-top:8px">自动驾驶依赖于在高质量、大规模多视角驾驶视频上训练的鲁棒模型。虽然世界模型为生成真实驾驶视频提供了一种成本效益高的解决方案，但它们在保持实例级时间一致性和空间几何保真度方面存在困难。为了解决这些挑战，我们提出了InstaDrive，一个通过两项关键进展提升驾驶视频真实性的新框架：(1) 实例流引导器，从帧中提取并传播实例特征，以强制时间一致性，保持实例身份随时间不变。(2) 空间几何对齐器，提升空间推理能力，确保实例定位的精确性，并显式建模遮挡层次结构。通过引入这些实例感知机制，InstaDrive实现了最先进的视频生成质量，并在nuScenes数据集上增强了下游自动驾驶任务的性能。此外，我们利用CARLA的自动驾驶功能，在多样化的地图和区域上程序化和随机地模拟罕见但安全关键的驾驶场景，从而实现对自动驾驶系统的严格安全评估。我们的项目页面是https://shanpoyang654.github.io/InstaDrive/page.html。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InstaDrive was developed to improve the realism and consistency of driving videos generated by world models, which are essential for autonomous driving systems. The framework introduces two key components: the Instance Flow Guider, which maintains temporal consistency by propagating instance features across frames, and the Spatial Geometric Aligner, which enhances spatial accuracy and models occlusion hierarchies. These instance-aware mechanisms lead to state-of-the-art video generation quality and improve performance in downstream autonomous driving tasks on the nuScenes dataset. The approach also leverages CARLA&#x27;s autopilot to simulate rare and safety-critical driving scenarios for rigorous evaluation.</div>
<div class="mono" style="margin-top:8px">InstaDrive 是为提升驾驶视频生成的逼真度和一致性而设计，这对自动驾驶系统的训练至关重要。该框架引入了两个关键组件：实例流引导器，通过在帧间传播实例特征来保持时间一致性，并保留实例身份；以及空间几何对齐器，增强空间推理能力，确保实例定位精确并显式建模遮挡层次。这些实例感知机制使得 InstaDrive 在视频生成质量上达到最先进水平，并提升了在 nuScenes 数据集上的下游自动驾驶任务表现。此外，该框架利用 CARLA 的自动驾驶功能，对罕见但安全关键的驾驶场景进行程序化和随机化模拟，支持自动驾驶系统的严格安全评估。</div>
</details>
</div>
<div class="card">
<div class="title">GeoResponder: Towards Building Geospatial LLMs for Time-Critical Disaster Response</div>
<div class="meta-line">Authors: Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</div>
<div class="meta-line">First: 2025-09-18T09:46:55+00:00 · Latest: 2026-02-03T07:50:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19354v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19354v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models excel at linguistic tasks but lack the inner geospatial capabilities needed for time-critical disaster response, where reasoning about road networks, continuous coordinates, and access to essential infrastructure such as hospitals, shelters, and pharmacies is vital. We introduce GeoResponder, a framework that instills robust spatial reasoning through a scaffolded instruction-tuning curriculum. By stratifying geospatial learning into different cognitive layers, we effectively anchor semantic knowledge to the continuous coordinate manifold and enforce the internalization of spatial axioms. Extensive evaluations across four topologically distinct cities and diverse tasks demonstrate that GeoResponder significantly outperforms both state-of-the-art foundation models and domain-specific baselines. These results suggest that LLMs can begin to internalize and generalize geospatial structures, pointing toward the future development of language models capable of supporting disaster response needs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoResponder：构建用于时效性灾害响应的地理空间大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型在语言任务上表现出色，但在时效性灾害响应中缺乏必要的地理空间能力，其中对道路网络、连续坐标以及医院、避难所和药店等关键基础设施的推理至关重要。我们引入GeoResponder，这是一个通过分阶段指令微调课程来增强空间推理能力的框架。通过将地理空间学习分层为不同的认知层次，我们有效地将语义知识锚定在连续坐标流形上，并强制模型内化空间公理。在四个拓扑结构不同的城市和多种任务上的广泛评估表明，GeoResponder在性能上显著优于最先进的基础模型和领域特定基线。这些结果表明，大语言模型开始能够内化和泛化地理空间结构，预示着未来能够支持灾害响应需求的语言模型的发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GeoResponder addresses the limitations of large language models (LLMs) in handling geospatial tasks critical for disaster response. The framework introduces a scaffolded instruction-tuning curriculum to enhance spatial reasoning capabilities, enabling LLMs to understand and reason about road networks, coordinates, and essential infrastructure. Evaluations across four topologically distinct cities and various tasks show that GeoResponder outperforms both state-of-the-art foundation models and domain-specific baselines, indicating that LLMs can effectively internalize and generalize geospatial structures for real-world applications.</div>
<div class="mono" style="margin-top:8px">GeoResponder的研究动机是提升大型语言模型在时间敏感的灾害响应场景中的地理空间推理能力。该框架通过分层的指令微调课程，将强大的空间理解能力嵌入到LLMs中，使其能够处理道路网络、坐标和关键基础设施的访问问题。在四个拓扑结构不同的城市和多种任务上的实验结果表明，GeoResponder显著优于现有的基础模型和领域专用基线，表明LLMs能够有效内化并泛化地理空间结构。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-03T07:43:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展收益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，用于代码代理的性能扩展，通过多样性而非原始数据量来提升表现。在固定训练预算下，增加轨迹多样性所带来的性能提升比增加轨迹数量更大，从而改善代理训练的性能-成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，捕捉真实服务中的逻辑依赖关系；（2）蓝图驱动的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙盒环境中的代码工具，缓解内在编码能力的灾难性遗忘。我们在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得显著提升。我们计划在发表时释放完整的代码库和合成数据集（包括30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of code large language models (LLMs) when evolving into tool-interactive agents, particularly the diminishing returns of quantity scaling and the early bottlenecks caused by low-quality synthetic data. The authors introduce TDScaling, a data synthesis framework that enhances agent performance by increasing trajectory diversity rather than the volume of data. TDScaling incorporates four innovations: a Business Cluster mechanism to capture logical dependencies, a blueprint-driven multi-agent paradigm for trajectory coherence, an adaptive evolution mechanism using entropy and complexity metrics to avoid mode collapse, and a sandboxed code tool to preserve coding capabilities. Experimental results on multiple benchmarks and code agent tasks show that TDScaling significantly improves both tool-use generalization and coding proficiency.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLM）演进为工具交互代理时面临的合成数据质量低和数据量增长收益递减的问题，提出TDScaling框架，通过提升轨迹多样性而非单纯增加数据量来增强性能。TDScaling包含四项创新：业务聚类机制用于捕捉实际服务的逻辑依赖，蓝图驱动的多代理范式确保轨迹一致性，基于领域熵、推理模式熵和累积动作复杂度的自适应演化机制防止模式坍塌，以及一个沙盒环境的代码工具以保留内在编码能力。实验结果表明，TDScaling在多个基准测试中有效提升了工具使用泛化能力和编码熟练度，实现了更好的性能与成本平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</div>
<div class="meta-line">Authors: Sherman Wong, Zhenting Qi, Zhaodong Wang, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Wenlin Chen, Yilun Du, Minlan Yu, Ying Zhang</div>
<div class="meta-line">First: 2025-12-11T08:05:58+00:00 · Latest: 2026-02-03T05:01:47+00:00</div>
<div class="meta-line">Comments: The latest version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10398v6">Abs</a> · <a href="https://arxiv.org/pdf/2512.10398v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world software engineering tasks require coding agents that can operate on massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK supports a unified orchestrator with advanced context management for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the construction, evaluation, and refinement of agents through a build-test-improve cycle, enabling rapid agent development on new tasks and tool stacks. Instantiated on the Confucius SDK using the meta-agent, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a Resolve@1 of 59%, exceeding prior research baselines as well as commercial results, under identical repositories, model backends, and tool access.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>孔子代码代理：面向真实代码库的可扩展代理框架</div>
<div class="mono" style="margin-top:8px">现实中的软件工程任务需要能够处理大规模仓库、支持长周期会话并可靠协调复杂工具链的代码代理。现有的研究级代码代理虽然具有透明性，但在扩展到更重的生产级工作负载时表现不佳，而生产级系统虽然在实际性能上表现优异，但可扩展性、可解释性和可控性有限。我们引入了孔子代码代理（CCA），这是一个能够在大规模代码库中运行的软件工程代理。CCA基于孔子SDK构建，这是一个围绕三个互补视角（代理体验AX、用户体验UX和开发者体验DX）设计的代理开发平台。SDK支持统一的编排器，具备高级上下文管理以实现长上下文推理，支持跨会话的持续学习的持久笔记系统，以及用于可靠工具使用的模块化扩展系统。此外，我们还引入了一个元代理，通过构建-测试-改进的循环自动完成代理的构建、评估和优化，从而实现对新任务和工具栈的快速代理开发。在孔子SDK上使用元代理实例化后，CCA在现实中的软件工程任务中表现出色。在SWE-Bench-Pro上，CCA在相同仓库、模型后端和工具访问条件下，实现了59%的Resolve@1，超过了先前的研究基准和商业结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the Confucius Code Agent (CCA) is to address the limitations of existing coding agents, which either lack scalability for real-world codebases or offer limited flexibility and interpretability. CCA is built using the Confucius SDK, which provides a structured platform for agent development with features such as long-context reasoning, cross-session learning, and modular tool integration. The main experimental results show that CCA achieves a Resolve@1 score of 59% on SWE-Bench-Pro, outperforming both prior research and commercial systems in handling real-world software engineering tasks.</div>
<div class="mono" style="margin-top:8px">开发Confucius Code Agent（CCA）的动机是解决现有编码代理在实际代码库中的可扩展性不足以及灵活性和可解释性有限的问题。CCA基于Confucius SDK构建，该平台通过三个视角——代理体验（AX）、用户体验（UX）和开发者体验（DX）结构化地支持代理开发。SDK包含统一的编排器用于长上下文推理、持久的笔记系统用于跨会话的持续学习，以及模块化的扩展系统用于可靠的工具集成。引入的元代理通过构建-测试-改进的循环自动化代理的创建与优化，从而实现高效代理开发。实验结果显示，CCA在SWE-Bench-Pro上实现了59%的Resolve@1得分，超过了先前的研究基准和商业系统的表现。</div>
</details>
</div>
<div class="card">
<div class="title">IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</div>
<div class="meta-line">Authors: Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T03:39:31+00:00 · Latest: 2026-02-03T03:39:31+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03060v1">PDF</a> · <a href="https://github.com/FireRedTeam/IVC-Prune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVC-Prune: 揭示LVLMs中隐含的视觉坐标以实现视觉标记剪枝</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在多个任务中表现出色。然而，处理高分辨率视觉输入时，其推理成本却很高。尽管视觉标记剪枝已成为一种有前景的解决方案，但现有方法主要关注语义相关性，往往丢弃对空间推理至关重要的标记。我们通过一种新颖的洞察来解决这一问题，即\emph{LVLMs如何处理空间推理}。具体而言，我们发现LVLMs通过旋转位置嵌入（RoPE）隐式地建立视觉坐标系统，其中特定的标记位置作为\textbf{隐含视觉坐标}（IVC tokens），对空间推理至关重要。基于这一洞察，我们提出\textbf{IVC-Prune}，一种无需训练、基于提示的剪枝策略，保留IVC标记和语义相关的前景标记。IVC标记通过理论分析RoPE的数学性质来识别，目标是那些其旋转矩阵近似单位矩阵或$90^\circ$旋转矩阵的位置。前景标记则通过一个稳健的两阶段过程来识别：首先发现语义种子，然后通过值向量相似性进行上下文优化。在四个代表性LVLMs和二十个多样化基准上的广泛评估表明，IVC-Prune可将视觉标记减少约50\%，同时保持$\geq$ 99\%的原始性能，并在多个基准上实现性能提升。源代码可在https://github.com/FireRedTeam/IVC-Prune获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high inference cost in Large Vision-Language Models (LVLMs) when handling high-resolution visual inputs. The authors propose IVC-Prune, a training-free and prompt-aware token pruning method that identifies and retains tokens critical for spatial reasoning. By analyzing the mathematical properties of Rotary Position Embeddings (RoPE), they determine that specific token positions act as implicit visual coordinates (IVC tokens) essential for spatial understanding. The method also incorporates a two-stage process to identify semantically relevant foreground tokens. Experimental results across four LVLMs and twenty benchmarks demonstrate that IVC-Prune achieves a 50% reduction in visual tokens while preserving over 99% of the original performance and improving results on several tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉语言模型（LVLMs）在处理高分辨率视觉输入时的高昂推理成本问题。作者提出了一种无需训练且基于提示的视觉标记剪枝方法IVC-Prune，通过分析Rotary Position Embeddings（RoPE）的数学特性，识别出对空间推理至关重要的标记，称为隐式视觉坐标（IVC tokens）。此外，通过两阶段流程识别语义相关的前景标记。实验结果表明，IVC-Prune可在保留原模型99%以上性能的同时，将视觉标记数量减少约50%，并在多个基准测试中实现性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</div>
<div class="meta-line">Authors: Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</div>
<div class="meta-line">First: 2026-02-03T03:10:27+00:00 · Latest: 2026-02-03T03:10:27+00:00</div>
<div class="meta-line">Comments: In Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03045v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03045v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>绘制前请澄清：面向稳健文本到CAD生成的主动代理</div>
<div class="mono" style="margin-top:8px">大型语言模型最近使文本到CAD系统成为可能，这些系统可以从自然语言提示中合成参数化CAD程序（例如CadQuery）。然而在实际应用中，几何描述可能不够具体或内部不一致：关键尺寸可能缺失，约束可能冲突。现有的微调模型倾向于被动地遵循用户指令，并在文本模糊时凭空捏造尺寸。为了解决这一问题，我们提出了一种名为ProCAD的主动代理框架，用于文本到CadQuery的生成，该框架在代码合成之前解决规格问题。我们的框架结合了一个主动澄清代理，它仅在必要时审核提示并提出有针对性的澄清问题，以生成自洽的规格，以及一个CAD编码代理，它将规格转换为可执行的CadQuery程序。我们在一个精心挑选的高质量文本到CadQuery数据集上对编码代理进行了微调，并通过代理式监督微调（SFT）在澄清轨迹上训练澄清代理。实验表明，主动澄清显著提高了系统对模糊提示的鲁棒性，同时保持了较低的交互开销。ProCAD在包括Claude Sonnet 4.5在内的前沿闭源模型上表现更优，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。我们的代码和数据集将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of generating accurate CAD models from ambiguous natural language descriptions by proposing a proactive agentic framework called ProCAD. The framework integrates a clarifying agent that identifies and resolves inconsistencies or missing specifications in the input prompt before code synthesis, and a CAD coding agent that translates the clarified specification into an executable CadQuery program. The clarifying agent is trained using agentic SFT on clarification trajectories, while the coding agent is fine-tuned on a high-quality dataset. Experimental results demonstrate that ProCAD significantly improves robustness to ambiguous prompts, reducing the mean Chamfer distance by 79.9 percent and the invalidity ratio from 4.8 percent to 0.9 percent, outperforming closed-source models like Claude Sonnet 4.5.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升文本到CAD系统在处理模糊或不一致的几何描述时的鲁棒性。提出的方法ProCAD采用了一种主动代理框架，包含一个澄清代理和一个CAD编码代理。澄清代理审计输入提示，并在必要时提出针对性问题以确保规范的一致性，而编码代理则将该规范转换为可执行的CadQuery程序。实验结果表明，ProCAD在性能上优于现有模型，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。</div>
</details>
</div>
<div class="card">
<div class="title">CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability</div>
<div class="meta-line">Authors: Xianzhen Luo, Jingyuan Zhang, Shiqi Zhou, Rain Huang, Chuan Xiao, Qingfu Zhu, Zhiyuan Ma, Xing Yue, Yang Yue, Wencong Zeng, Wanxiang Che</div>
<div class="meta-line">First: 2026-02-03T02:27:16+00:00 · Latest: 2026-02-03T02:27:16+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03012v1">PDF</a> · <a href="https://github.com/livecvebench/CVE-Factory">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CVE-Factory：为代码安全漏洞实现专家级代理任务的扩展</div>
<div class="mono" style="margin-top:8px">评估和提升代码代理的安全能力需要高质量、可执行的漏洞任务。然而，现有工作依赖于成本高昂且难以扩展的手动复现，并且数据分布过时。为了解决这些问题，我们提出了CVE-Factory，这是首个能够将稀疏的CVE元数据自动转换为可执行的代理任务的多代理框架，并达到专家级质量。与人类专家复现的交叉验证表明，CVE-Factory实现了95\%的解决方案正确性和96\%的环境保真度，证实了其专家级质量。它还被评估在最新的现实漏洞上，取得了66.2\%的验证成功率。这种自动化实现了两个下游贡献。首先，我们构建了LiveCVEBench，这是一个持续更新的基准，包含190个任务，涵盖14种语言和153个仓库，捕捉包括AI工具漏洞在内的新兴威胁。其次，我们合成超过1,000个可执行的训练环境，这是代码安全领域首次实现代理任务的大规模扩展。经过微调的Qwen3-32B在LiveCVEBench上的表现从5.3\%提升至35.8\%，超越了Claude 4.5 Sonnet，且性能提升在Terminal Bench上也得到验证（12.5\%到31.3\%）。我们开源了CVE-Factory、LiveCVEBench、Abacus-cve（微调模型）、训练数据集和排行榜。所有资源均可在https://github.com/livecvebench/CVE-Factory 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the evaluation and improvement of code agents&#x27; security capabilities by providing high-quality, executable vulnerability tasks. The authors introduce CVE-Factory, a novel multi-agent framework that automatically transforms sparse CVE metadata into fully executable agentic tasks, achieving expert-level quality. Cross-validation against human expert reproductions demonstrates 95% solution correctness and 96% environment fidelity, while evaluations on recent vulnerabilities show a 66.2% verified success rate. This work also contributes by creating LiveCVEBench, a continuously updated benchmark, and synthesizing over 1,000 training environments. Fine-tuned Qwen3-32B significantly improves performance on LiveCVEBench, surpassing Claude 4.5 Sonnet, with results generalizing to other benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提供高质量、可执行的漏洞任务来提升代码代理的安全评估与改进能力。为此，作者提出了CVE-Factory，这是一个首个实现专家级质量的多代理框架，能够自动将稀疏的CVE元数据转换为可执行的代理任务。该框架通过与人工专家任务的交叉验证，达到了95%的解决方案正确性和96%的环境保真度，并在最新的现实漏洞上取得了66.2%的验证成功率。其两个下游贡献包括构建LiveCVEBench，一个包含190个任务、覆盖14种语言和153个仓库的持续更新基准，以及合成超过1000个可执行的训练环境。经过微调的Qwen3-32B在LiveCVEBench上的表现从5.3%提升至35.8%，超越了Claude 4.5 Sonnet，并且在Terminal Bench上也表现出显著的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing</div>
<div class="meta-line">Authors: Saeid Sheikhi</div>
<div class="meta-line">First: 2026-02-02T21:44:01+00:00 · Latest: 2026-02-02T21:44:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02842v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟链：一种面向大型语言模型的双模式推理框架，具有动态问题路由</div>
<div class="mono" style="margin-top:8px">我们提出了模拟链（CoS），一种新颖的双模式推理框架，该框架通过动态路由将问题发送到大型语言模型（LLMs）中的专用推理策略。与现有的统一提示方法不同，CoS采用三种不同的推理模式：(1) 用于数学问题的计算流程与自洽性，(2) 用于空间推理的符号状态跟踪与JSON表示，(3) 用于多跳推理的混合事实提取。通过在GSM8K、StrategyQA和bAbI基准上使用四个最先进的模型（Gemma-3 27B、LLaMA-3.1 8B、Mistral 7B和Qwen-2.5 14B）进行综合评估，我们证明CoS在GSM8K上达到71.5%的准确率（绝对提升1.0%），在StrategyQA上达到90.0%（提升2.5%），在bAbI上达到19.0%（相对提升65.2%），优于最强的基线方法。分析表明，针对特定问题选择合适的推理模式至关重要，当正确应用于数学问题时，计算模式可达到81.2%的准确率，而错误路由则导致0%的准确率。我们提供了模式选择、状态跟踪和答案提取的详细算法，确立了CoS作为一种无需额外训练即可提升LLM推理能力的有效方法。与Self-Consistency相比，该框架在准确率和效率之间提供了更优的权衡，计算成本降低了54%，同时实现了相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces Chain of Simulation (CoS), a dual-mode reasoning framework designed to enhance the performance of Large Language Models (LLMs) by dynamically routing problems to specialized reasoning strategies. The framework incorporates three distinct modes: computational flow for mathematical problems, symbolic state tracking using JSON for spatial reasoning, and hybrid fact-extraction for multi-hop inference. Evaluation on GSM8K, StrategyQA, and bAbI benchmarks shows that CoS improves accuracy by 1.0%, 2.5%, and 65.2% respectively compared to existing methods, with the computational mode achieving 81.2% accuracy when correctly applied. The framework also offers better efficiency than Self-Consistency, achieving similar performance at 54% lower computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过结构化方法提升大型语言模型（LLMs）在处理不同类型问题时的推理能力。Chain of Simulation（CoS）提出了一种双模式推理框架，能够动态地将问题路由到专门的推理策略，包括用于数学问题的计算流程、用于空间推理的符号状态跟踪以及用于多跳推理的混合事实提取。在GSM8K、StrategyQA和bAbI基准测试中，CoS分别达到了71.5%、90.0%和19.0%的准确率，相比现有最佳基线分别提升了1.0%、2.5%和65.2%。实验结果还表明，该框架在保持性能的同时，计算成本降低了54%，展现出更优的准确率与效率之间的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-02-02T19:55:32+00:00</div>
<div class="meta-line">Comments: 21 main pages, 6 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该比闭源系统具有根本性的优势：它们可以专门针对私有代码库，直接在权重中编码特定仓库的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，使得能够快速且低成本地创建专门针对私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法、代码）模型中实现了最先进的结果，同时其性能与前沿的开放权重模型（如Devstral-Small-2）相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到同等性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数万个轨迹。结合成本效益，这使得专门针对私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过20万个合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和混淆因素进行了详细分析。总体而言，我们相信我们的工作将大大加速开放编码代理的研究，并展示能够专门针对私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this paper is to demonstrate the practicality of training open-weight coding agents specialized to private codebases, which was previously considered theoretical due to high costs and complexity. The authors propose SERA, an efficient method that leverages supervised fine-tuning (SFT) to achieve state-of-the-art results with fully open-source models, matching the performance of advanced open-weight models like Devstral-Small-2. SERA is significantly more cost-effective, being 26 times cheaper than reinforcement learning and 57 times cheaper than prior synthetic data methods. Additionally, the Soft Verified Generation (SVG) technique enables the creation of thousands of synthetic trajectories from a single code repository, supporting further analysis of scaling laws and training factors for coding agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是实现开放权重编码代理对私有代码库的实用化定制，这一优势因训练成本和复杂性而长期停留在理论层面。作者提出了SERA，一种通过仅使用监督微调（SFT）训练编码代理的高效方法，其在开放数据和方法下取得了最先进的结果，同时与前沿的开放权重模型如Devstral-Small-2表现相当。他们提出的方法Soft Verified Generation（SVG）能够从单个代码库生成数千条合成轨迹，相较于强化学习和以往的合成数据方法，训练成本降低了26倍和57倍。此外，该方法还扩展到更大的代码库集合，生成超过20万条合成轨迹，用于深入分析编码代理的扩展规律、消融实验及训练中的干扰因素。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将现实世界可验证环境扩展到百万级</div>
<div class="mono" style="margin-top:8px">我们提出了SWE-Universe，这是一个可扩展且高效的框架，用于从GitHub拉取请求（PRs）中自动构建现实世界软件工程（SWE）可验证环境。为了解决自动构建中普遍存在的挑战，如低生产率、弱验证器和高昂成本，我们的框架采用了一个由高效自定义训练模型驱动的构建代理。该代理通过迭代自验证和循环内黑客检测，确保生成高保真、可验证任务的可靠性。使用这种方法，我们将现实世界多语言SWE环境的数量扩展到百万级（807,693）。我们通过大规模代理中间训练和强化学习展示了这些环境的深远价值。最后，我们将该技术应用于Qwen3-Max-Thinking，并在SWE-Bench Verified上取得了75.3%的得分。我们的工作为推进下一代代码代理提供了关键资源和稳健的方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the challenges of automatically building high-quality, verifiable software engineering environments at scale. The framework introduces a building agent based on a custom-trained model that performs iterative self-verification and in-loop hacking detection to ensure reliable and high-fidelity task generation. Through this approach, the framework constructs over 807,693 real-world multilingual SWE environments. The method is validated by applying it to Qwen3-Max-Thinking, achieving a 75.3% score on the SWE-Bench Verified benchmark, demonstrating its effectiveness in enhancing the capabilities of coding agents.</div>
<div class="mono" style="margin-top:8px">SWE-Universe的提出旨在解决现有方法在自动构建现实软件工程环境时存在的诸多问题，如生产率低、验证能力弱和成本高昂。该框架引入了一个由定制模型驱动的构建代理，通过迭代自验证和循环内漏洞检测，确保生成高质量、可验证的任务。该方法成功将多语言软件工程环境的数量扩展至807,693个以上，并通过大规模代理中期训练和强化学习验证其有效性。当应用于Qwen3-Max-Thinking时，在SWE-Bench Verified基准测试中取得了75.3%的得分，展示了其作为资源和方法论的价值，有助于推动下一代代码代理的发展。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
