<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 03:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0350</div>
    <div class="row"><div class="card">
<div class="title">Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</div>
<div class="meta-line">Authors: Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long</div>
<div class="meta-line">First: 2026-01-27T17:40:07+00:00 · Latest: 2026-01-27T17:40:07+00:00</div>
<div class="meta-line">Comments: Project page: https://thuml.github.io/Reasoning-Visual-World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://thuml.github.io/Reasoning-Visual-World">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉生成通过多模态世界模型实现类人推理</div>
<div class="mono" style="margin-top:8px">人类通过构建内部世界模型并操作其中的概念来进行推理。最近的AI进展，特别是链式思维（CoT）推理，已经近似模拟了这种人类认知能力，其中世界模型被认为嵌入在大型语言模型中。当前系统主要依赖于语言推理，在数学和编程等正式和抽象领域已达到专家级表现。然而，在需要更丰富表示和先验知识的物理和空间智能领域，它们仍远不如人类。因此，统一的多模态模型（UMMs）的出现，引发了人们对基于互补多模态路径的更类人推理方式的兴趣，尽管其优势尚不明确。从世界模型的角度来看，本文首次系统地研究了视觉生成何时以及如何促进推理。我们的核心观点是视觉优势假说：对于某些任务，特别是那些基于物理世界的任务，视觉生成更自然地作为世界模型，而纯语言世界模型则因表示限制或先验知识不足而遇到瓶颈。理论上，我们将内部世界建模形式化为CoT推理的核心组成部分，并分析不同形式世界模型之间的区别。实证上，我们识别出需要交替使用视觉和语言CoT推理的任务，构建了一个新的评估套件VisWorld-Eval。在最先进的UMM上进行的受控实验表明，在有利于视觉世界建模的任务中，交替CoT显著优于纯语言CoT，但在其他任务中则没有明显优势。综上，本工作阐明了多模态世界建模在构建更强大、更类人多模态AI方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how visual generation can enhance human-like reasoning in AI systems by leveraging multimodal world models. It proposes the visual superiority hypothesis, suggesting that visual generation is more effective for tasks involving physical and spatial understanding. Through controlled experiments on a state-of-the-art unified multimodal model, the study demonstrates that interleaving visual and verbal reasoning significantly improves performance on tasks requiring visual world modeling, while offering no clear benefit for purely verbal tasks. These findings highlight the potential of multimodal approaches in achieving more robust and human-like AI reasoning.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉生成如何增强AI系统在需要丰富空间和物理理解的领域中的人类类比推理能力。论文提出了视觉优越性假说，认为视觉生成比纯语言模型更自然地支持世界建模。通过在先进统一多模态模型上的受控实验，研究发现交错使用视觉和语言推理在需要视觉世界建模的任务中显著提升性能，而在其他领域则没有明显优势。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-27T12:16:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的代理原生中间训练</div>
<div class="mono" style="margin-top:8px">最近，大型语言模型（LLM）的能力前沿已从单轮代码生成转向代理软件工程——一种模型能够自主导航、编辑和测试复杂仓库的范式。尽管后训练方法已成为代码代理的主流方法，但由于需要大量资源，**代理中间训练**（agentic mid-training）——即在模拟真实代理工作流的大规模数据上进行中间训练——仍被严重忽视。与仅依赖昂贵的强化学习相比，代理中间训练为实现基础代理行为提供了更可扩展的路径。实现有效的代理中间训练的核心挑战在于静态训练数据与真实开发中动态且反馈丰富的环境之间的分布不匹配。为了解决这一问题，我们系统地研究了代理中间训练，建立了适用于大规模代理开发的数据合成原则和训练方法。我们方法的核心是**代理原生数据**——包含两种互补类型的轨迹：**上下文原生轨迹**，保留代理经历的完整信息流，提供广泛覆盖和多样性；以及**环境原生轨迹**，从可执行仓库中收集，其观察来源于实际工具调用和测试执行，提供深度和交互的真实性。我们在 `SWE-Bench Verified` 上验证了模型的代理能力。在使用对齐基础模型和代理框架的两种后训练设置下，我们的方法在使用不到一半中间训练令牌（73.1B）的情况下，优于之前的开源软件工程中间训练方案 `Kimi-Dev`。除了相对优势外，我们的最佳表现模型（32B 和 72B）分别实现了 **56.1%** 和 **58.5%** 的解决率，分别...</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to explore agentic mid-training as a more scalable and efficient alternative to expensive reinforcement learning for developing code agents. The authors propose a method that leverages agent-native data, which includes contextually-native and environmentally-native trajectories to better align training with real-world software engineering workflows. Their experiments on SWE-Bench Verified show that their approach outperforms Kimi-Dev with significantly fewer training tokens, achieving resolution rates of 56.1% and 58.5% for their 32B and 72B models, respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索一种比强化学习更可扩展且成本更低的代码代理开发方法——代理原生中期训练。作者提出了代理原生数据，包括上下文原生轨迹和环境原生轨迹，以弥合静态训练数据与动态开发环境之间的差距。在SWE-Bench Verified上的实验表明，他们的方法在使用不到一半的中期训练令牌的情况下优于之前的Kimi-Dev方案，其最佳表现的32B和72B模型分别实现了56.1%和58.5%的解决率。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</div>
<div class="meta-line">Authors: Daniel Ogenrwot, John Businge</div>
<div class="meta-line">First: 2026-01-24T20:27:04+00:00 · Latest: 2026-01-27T06:14:36+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17581v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff&#x27;s $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何修改代码：GitHub拉取请求的大型研究</div>
<div class="mono" style="margin-top:8px">AI编码代理正越来越多地作为自主贡献者，通过生成和提交拉取请求（PRs）参与开发。然而，我们缺乏关于这些由代理生成的PR与人类贡献之间差异的实证证据，尤其是在代码修改方式和变更描述方面。理解这些差异对于评估其可靠性及其对开发流程的影响至关重要。我们使用MSR 2026 Mining Challenge版本的AIDev数据集，分析了24,014个已合并的Agentic PR（440,295次提交）和5,081个已合并的人类PR（23,242次提交）。我们考察了新增内容、删除内容、提交次数以及涉及的文件，并通过词法和语义相似性评估PR描述与其差异的一致性。在提交次数方面，Agentic PR与Human PR存在显著差异（Cliff&#x27;s δ=0.5429），在涉及的文件和删除行方面表现出中等差异。它们在所有衡量标准下也显示出略微更高的描述与差异相似性。这些发现提供了关于AI编码代理如何参与开源开发的大规模实证特征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents contribute to open source development by analyzing their pull requests compared to those generated by humans. The research is motivated by the need to understand the differences in code modification patterns and PR descriptions between AI and human contributors. Using the AIDev dataset, the authors examine 24,014 merged Agentic PRs and 5,081 merged Human PRs, focusing on commit counts, files touched, and the semantic consistency between PR descriptions and code changes. The results show that Agentic PRs have significantly higher commit counts and exhibit moderate differences in files touched and deleted lines, while also showing slightly higher similarity between descriptions and diffs across various measures.</div>
<div class="mono" style="margin-top:8px">本研究通过分析AI编码代理生成的拉取请求与人类生成的拉取请求，探讨其在开源开发中的贡献方式。研究使用MSR 2026 Mining Challenge版本的AIDev数据集，考察了24,014个合并的Agentic PR和5,081个合并的人类PR，重点关注代码修改、提交模式以及描述与代码变更的一致性。结果表明，Agentic PR的提交数量显著高于人类PR，文件修改和删除行数存在中等差异，同时其描述与代码变更之间的词法和语义相似性也略高。</div>
</details>
</div>
<div class="card">
<div class="title">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</div>
<div class="meta-line">Authors: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou</div>
<div class="meta-line">First: 2025-10-13T05:51:22+00:00 · Latest: 2026-01-27T05:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vlaser：具有协同具身推理能力的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管已有大量研究致力于利用视觉-语言模型（VLMs）开发具身推理能力，或将其集成到视觉-语言-动作（VLA）模型中以实现端到端的机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。在本工作中，我们通过引入Vlaser——一种具有协同具身推理能力的视觉-语言-动作模型，迈出初步一步，以弥合具身推理与VLA策略学习之间的鸿沟。Vlaser是一种基础的视觉-语言模型，旨在将高层次推理与低层次控制相结合，以支持具身智能体。基于高质量的Vlaser-6M数据集，Vlaser在一系列具身推理基准测试中取得了最先进的性能，包括空间推理、具身锚定、具身问答和任务规划。此外，我们系统地研究了不同VLM初始化方式对监督VLA微调的影响，提供了关于缓解互联网规模预训练数据与具身特定策略学习数据之间领域偏移的新见解。基于这些见解，我们的方法在WidowX基准测试中取得了最先进的结果，并在Google Robot基准测试中表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating high-level vision-language reasoning with low-level action control in embodied agents, aiming to bridge the gap between upstream vision-language models and downstream vision-language-action policy learning. The proposed Vlaser model introduces synergistic embodied reasoning, combining a foundational vision-language model with task-specific control mechanisms. Evaluated on multiple benchmarks, Vlaser demonstrates state-of-the-art performance in spatial reasoning, embodied grounding, embodied QA, and task planning, and provides insights into improving supervised fine-tuning by examining the impact of different VLM initializations on domain adaptation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将高阶视觉语言推理与低阶动作控制相结合的问题，以弥合上游基于视觉语言模型（VLM）的推理与下游视觉语言动作（VLA）策略学习之间的差距。提出的Vlaser模型引入了协同具身推理能力，结合了一个基础的视觉语言模型与任务特定的控制机制。在多个基准测试中，Vlaser在空间推理、具身定位、具身问答和任务规划方面均取得最先进的性能，并通过系统分析不同VLM初始化对监督VLA微调的影响，提供了关于缓解互联网规模预训练数据与具身策略学习数据之间领域偏移的新见解。</div>
</details>
</div>
<div class="card">
<div class="title">m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</div>
<div class="meta-line">Authors: Yosub Shin, Michael Buriek, Igor Molybog</div>
<div class="meta-line">First: 2026-01-27T02:01:56+00:00 · Latest: 2026-01-27T02:01:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>m2sv：一种用于地图到街景空间推理的可扩展基准</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在许多多模态基准上表现出色，但在需要将抽象的俯视图表示与以自我为中心视角对齐的空间推理任务中仍显得脆弱。我们引入了m2sv，一个用于地图到街景空间推理的可扩展基准，要求模型通过将北向俯视地图与同一现实交叉路口拍摄的街景图像对齐，推断相机的观看方向。我们发布了m2sv-20k，一个地理多样性且具有可控模糊性的基准，以及m2sv-sft-11k，一个用于监督微调的精心挑选的结构化推理轨迹集。尽管在现有多模态基准上表现强劲，但评估最好的VLM在m2sv上的准确率仅为65.2%，远低于人类基线的95%。虽然监督微调和强化学习带来了持续的性能提升，但跨基准评估显示其迁移能力有限。除了整体准确率外，我们还通过结构信号和人类努力系统地分析了地图到街景推理的难度，并对适应的开放模型进行了广泛的失败分析。我们的研究结果突显了几何对齐、证据聚合和推理一致性方面的持续差距，为未来多视角的基于场景的空间推理研究提供了动力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces m2sv, a scalable benchmark designed to evaluate vision--language models&#x27; ability to perform spatial reasoning by aligning overhead maps with egocentric street-view images. The benchmark includes m2sv-20k, a diverse dataset with controlled ambiguity, and m2sv-sft-11k, a set of structured reasoning traces for fine-tuning. Despite strong performance on other multimodal benchmarks, the best VLM achieves only 65.2% accuracy on m2sv, significantly lower than the human baseline of 95%. The research also shows that while supervised fine-tuning and reinforcement learning improve performance, there is limited transfer across benchmarks. Further analysis reveals persistent challenges in geometric alignment, evidence aggregation, and reasoning consistency in map-to-street-view tasks.</div>
<div class="mono" style="margin-top:8px">本文提出m2sv，这是一个用于评估视觉-语言模型在地图到街景视图空间推理能力的可扩展基准。该基准要求模型通过将北向地图与同一现实路口的街景图像对齐，推断相机的视角方向。研究提供了m2sv-20k，一个地理多样且具有可控模糊度的数据集，以及m2sv-sft-11k，用于监督微调的结构化推理轨迹集。尽管在现有多模态基准上表现优异，但最佳评估的视觉-语言模型在m2sv上的准确率仅为65.2%，远低于人类基线的95%。研究发现，监督微调和强化学习都能带来性能提升，但跨基准迁移效果有限。此外，作者还通过结构信号和人工分析系统地探讨了地图到街景推理的难度，并对调整后的开放模型进行了广泛失败分析，揭示了几何对齐、证据聚合和推理一致性等方面存在的持续性差距。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward</div>
<div class="meta-line">Authors: Dipendra Misra, Aldo Pacchiano, Ta-Chung Chi, Ge Gao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-27T00:31:04+00:00 · Latest: 2026-01-27T00:31:04+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19055v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent&#x27;s response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains adapted from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于用户编辑的LLM原理化微调：偏好、监督与奖励的融合</div>
<div class="mono" style="margin-top:8px">我们研究如何利用由用户编辑生成的部署数据对LLM进行微调，该数据集包含上下文、代理的响应以及用户的编辑。这种部署数据自然地出现在基于LLM的写作助手和编码代理等应用中。由于用户编辑的自然来源，它成为适应和个性化LLM的理想数据源。在此设置下，各种通常在文献中独立研究的反馈类型（偏好、监督标签和成本）得以统一。在本文中，我们启动了从用户编辑中学习的理论研究。我们首先推导出从这些反馈类型中学习的算法的学习界。我们证明这些算法在用户、数据分布和模型类别上具有不同的权衡。随后，我们提出了一种简单的集成方法，以联合学习这些反馈类型。在两个改编自Gao等人2024年的领域上，我们展示了我们的集成方法优于仅从单一反馈类型学习的方法。此外，我们还证明了所提出的程序在测试时能够稳健地适应不同的用户编辑分布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the fine-tuning of large language models (LLMs) using user-edit deployment data, which includes context, agent responses, and user modifications. The motivation stems from the natural and informative nature of user edits, making them a valuable resource for adapting and personalizing LLMs. The authors analyze the trade-offs of learning algorithms based on preferences, supervision, and reward signals, and propose an ensembling method that combines these feedback types. Experimental results on two domains show that this approach outperforms individual feedback-based methods and demonstrates robustness across different user-edit distributions.</div>
<div class="mono" style="margin-top:8px">本文研究了如何利用用户编辑数据对大型语言模型（LLMs）进行微调，该数据包含上下文、代理响应以及用户的修改。研究动机源于此类数据在实际应用如基于LLM的写作助手和代码代理中自然产生，使其成为个性化和适应的重要资源。作者分析了从不同反馈类型（偏好、监督和成本）学习的理论特性，并证明每种类型在不同用户、数据分布和模型类别下具有不同的性能权衡。随后，他们提出了一种简单的集成方法，联合利用这些反馈类型，在两个改编自Gao等人2024年的领域上展示了优于单独方法的性能。此外，该方法在测试时对不同的用户编辑分布表现出良好的适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Foundation Models for Radar Scene Understanding</div>
<div class="meta-line">Authors: Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia</div>
<div class="meta-line">First: 2025-11-26T06:41:00+00:00 · Latest: 2026-01-26T20:56:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21105v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21105v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>雷达场景理解的基础模型扩展</div>
<div class="mono" style="margin-top:8px">雷达传感器在恶劣天气、光照和远距离条件下提供了可靠的感知能力。近年来，基础模型在视觉和语言理解方面取得了显著进展，但其与雷达感知的结合仍处于初步探索阶段。现有的雷达方法往往是碎片化且任务特定的；每个下游任务都采用不同的架构和训练目标，阻碍了任务间的迁移能力。在本文中，我们提出了RadarFM：一种通过结构化空间语言监督学习统一场景表示的雷达基础模型。我们做出两项关键贡献：(1) 一种结构化描述框架，能够以原生雷达坐标编码车辆分布；(2) 一种感知哈希的对比学习目标，通过量化连续场景相似性而非二元匹配，实现细粒度的空间推理。借助CARLA模拟器，我们生成了大规模且标注良好的雷达数据集，涵盖多种驾驶场景。我们还提出了定位感知的评估指标，以超越传统检测度量的评估方式衡量空间准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of integrating foundation models into radar scene understanding, where existing methods are task-specific and lack transferability. The authors propose RadarFM, a foundation model that learns unified scene-level representations using structured spatial language supervision. Two key innovations include a structured caption framework for encoding vehicle distributions in native radar coordinates and a hash-aware contrastive learning objective that captures continuous scene similarity for fine-grained spatial reasoning. Experimental results on large-scale radar datasets generated via CARLA demonstrate improved performance in spatial understanding compared to traditional detection-based approaches.</div>
<div class="mono" style="margin-top:8px">本文旨在解决雷达场景理解中基础模型整合的挑战，现有方法多为任务特定且缺乏迁移能力。作者提出了RadarFM，一种通过结构化空间语言监督学习统一场景表示的雷达基础模型。主要创新包括：一种结构化描述框架，用于在雷达坐标中编码车辆分布；以及一种基于哈希的对比学习目标，通过量化连续场景相似性实现更精细的空间推理。在CARLA模拟器生成的大规模雷达数据集上进行实验，结果表明RadarFM在空间理解方面优于传统基于检测的评估方法。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests</div>
<div class="meta-line">Authors: Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang</div>
<div class="meta-line">First: 2026-01-08T12:31:02+00:00 · Latest: 2026-01-26T17:05:34+00:00</div>
<div class="meta-line">Comments: Accepted by MSR&#x27;26 Mining Challenge Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04886v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04886v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that &quot;descriptions claim unimplemented changes&quot; was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5 times longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分析AI编码代理撰写的拉取请求中的消息-代码不一致</div>
<div class="mono" style="margin-top:8px">由AI编码代理生成的拉取请求（PR）描述是向人类审阅者传达代码更改的主要渠道。然而，这些消息与实际更改之间的对齐情况尚未被探索，引发了对AI代理可信度的担忧。为填补这一空白，我们使用PR消息-代码不一致（PR-MCI）分析了五个代理的23,247个代理PR。我们贡献了974个手动标注的PR，发现其中406个（1.7%）表现出高PR-MCI，并识别出八种PR-MCI类型，其中最常见的问题是“描述声称未实现的更改”（占45.4%）。统计检验表明，高MCI的PR接受率低51.7%（28.3% vs. 80.0%），且合并时间是普通PR的3.5倍（55.8小时 vs. 16.0小时）。我们的研究结果表明，不可靠的PR描述会损害对AI代理的信任，突显了需要PR-MCI验证机制和改进PR生成以实现可信的人机协作的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the issue of message-code inconsistency in pull requests authored by AI coding agents, as PR descriptions serve as the main means of communication for code changes. By analyzing 23,247 PRs across five agents, the researchers identified 406 PRs with high inconsistency, contributing 974 manually annotated examples. They categorized the inconsistencies into eight types, with &quot;descriptions claim unimplemented changes&quot; being the most prevalent. The results show that high-MCI PRs have significantly lower acceptance rates and longer merge times, indicating that unreliable descriptions hinder trust in AI agents and emphasize the importance of improving PR generation and verification processes for effective human-AI collaboration.</div>
<div class="mono" style="margin-top:8px">本研究探讨了由AI编码代理生成的拉取请求（PR）中消息与代码不一致的问题，因为PR描述是人类审阅者理解代码更改的关键渠道。通过对五个代理生成的23,247个PR进行分析，研究人员发现了406个存在高不一致性的PR，并贡献了974个手动标注的案例。其中45.4%的不一致源于描述中声称实现了未实际完成的更改，统计分析表明高不一致性的PR被接受率低51.7%，合并耗时长3.5倍，凸显了提升PR描述可靠性以实现可信人机协作的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过预测编码实现共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是一个关键挑战，其中部分可观测性和有限带宽常常导致协调中的灾难性失败。我们引入了一种多智能体预测编码框架，将协调建模为智能体之间相互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习与谁以及交流什么，还学习何时进行交流。该框架的基础是一种类似网格细胞的度量，作为内部空间编码用于自我定位，这种编码自发地从自监督运动预测中产生。在此基础上，智能体逐步发展出一种带宽高效的通信机制，并形成专门的神经群体来编码合作伙伴的位置——这是海马体社会位置细胞（SPCs）的人工类比。这些社会表征进一步被分层强化学习策略所利用，该策略主动探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法表现出对带宽限制的卓越鲁棒性：当带宽从128位/步减少到4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则从67.6%骤降至28.6%。我们的研究结果为复杂社会表征如何从统一的预测驱动中涌现提供了理论上有根据且生物上合理的基础，从而实现集体智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of maintaining a consistent shared spatial memory in multi-agent systems, particularly under conditions of partial observability and limited communication bandwidth. The proposed method introduces a predictive coding framework that minimizes mutual uncertainty among agents by learning who, what, and when to communicate. This framework uses a grid-cell-like metric for self-localization, which arises from self-supervised motion prediction. Agents then develop efficient communication strategies and specialized neural populations to encode partners&#x27; locations. The main experimental results show that the approach maintains high success rates even under severe bandwidth limitations, outperforming a full-broadcast baseline on the Memory-Maze benchmark.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多智能体系统中保持一致共享空间记忆的挑战，特别是在部分可观测性和通信带宽受限的情况下。提出的方法引入了一种基于预测编码的框架，使智能体能够通过学习谁、什么和何时进行通信来最小化相互不确定性。该框架利用类似网格细胞的度量作为内部空间编码，源自自监督运动预测。智能体随后发展出专门的神经群体来编码同伴的位置，类似于海马体中的社会位置细胞。主要实验结果表明，该方法在带宽受限条件下仍能保持较高性能，当带宽降至4位/步时，成功率仍为64.4%，而全广播基线则骤降至28.6%。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-26T10:49:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, SWE-EVO comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on SWE-EVO, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长期软件演进场景中对代码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI代码代理基准主要关注孤立的、单一问题的任务，如修复错误或实现小功能。然而，现实中的软件工程本质上是一项长期任务：开发者需要理解高层需求，协调多个文件的更改，并在多次迭代中演进代码库，同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长期软件演进挑战上的基准。SWE-EVO基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而相比之下在单一问题的SWE-Bench Verified上解决率为65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces SWE-EVO, a benchmark designed to evaluate AI coding agents in long-horizon software evolution scenarios, which are more complex and realistic than single-issue tasks. SWE-EVO is built using release notes and version histories from seven open-source Python projects, featuring 48 tasks that require multi-step modifications across an average of 21 files, validated by extensive test suites. Experimental results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on the single-issue SWE-Bench Verified, highlighting the current limitations of coding agents in sustained, multi-file reasoning.</div>
<div class="mono" style="margin-top:8px">本文提出SWE-EVO基准，用于评估AI编码代理在长期软件演进场景中的表现，这些场景比单一问题任务更为复杂和贴近现实。该基准基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个需要跨平均21个文件进行多步骤修改的任务。实验结果显示，即使是GPT-5与OpenHands结合的先进模型，在SWE-EVO上的解决率也只有21%，远低于单一问题SWE-Bench Verified上的65%，表明当前代理在持续多文件推理方面存在明显不足。研究还提出了一种细粒度指标Fix Rate，用于衡量在解决这些复杂任务中的部分进展。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Conditioned Reasoning in Long-Egocentric Videos</div>
<div class="meta-line">Authors: James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi</div>
<div class="meta-line">First: 2026-01-26T03:21:35+00:00 · Latest: 2026-01-26T03:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时域第一视角视频中的空间条件推理</div>
<div class="mono" style="margin-top:8px">由于视角漂移和缺乏持久的几何上下文，长时域第一视角视频对视觉导航提出了重大挑战。尽管近期的视觉-语言模型在图像和短视频推理上表现良好，但它们在长第一视角序列中的空间推理能力仍有限。在本工作中，我们研究了显式空间信号如何影响基于VLM的视频理解，而无需修改模型架构或推理过程。我们引入了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在面向导航的空间查询上对多个VLM进行了基准测试。为了检验输入级别的归纳偏置，我们进一步将深度图与RGB帧融合，并评估其对空间推理的影响。我们的结果揭示了通用准确性和空间特异性之间的权衡，表明具有深度感知和空间基础的表示可以提高对行人和障碍物检测等关键安全任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenges of visual navigation in long-horizon egocentric videos, where viewpoint drift and lack of persistent geometric context hinder spatial understanding. The authors investigate how explicit spatial signals can enhance video understanding by VLMs without altering model structures or inference processes. They introduce Sanpo-D, a refined version of the Google Sanpo dataset, and evaluate various VLMs on spatial queries related to navigation. By integrating depth maps with RGB frames, they assess the impact of input-level inductive bias on spatial reasoning, finding that depth-aware representations improve performance in critical tasks like pedestrian and obstruction detection, albeit at the cost of reduced general-purpose accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对长时地缘视角视频中因视角漂移和缺乏持久几何上下文而带来的视觉导航挑战。作者探讨了如何通过显式的空间信号提升基于视觉语言模型（VLM）的视频理解能力，而无需修改模型结构或推理流程。他们提出了Sanpo-D，这是对Google Sanpo数据集的细粒度重新标注，并在导航导向的空间查询任务上评估了多种VLM的表现。通过融合深度图与RGB图像，他们分析了输入层面的归纳偏置对空间推理的影响，结果表明深度感知的表示方法在行人和障碍物检测等关键安全任务中提升了性能，但可能牺牲一般性任务的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-26T00:36:33+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com First two authors contribute equally. The 3th - 6th authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能来寻找共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的600多个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道被模糊、时机不当和不准确的信息堵塞；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理往往对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the limitations of current coding agents in collaborative settings, particularly their inability to effectively coordinate with each other. The authors introduce CooperBench, a benchmark consisting of over 600 collaborative coding tasks across four programming languages and twelve libraries, designed to test agents&#x27; coordination abilities. Each task involves two agents with independent but potentially conflicting features, grounded in real open-source repositories with expert tests. The main experimental results show that state-of-the-art coding agents perform significantly worse when working together, achieving on average 30% lower success rates than when completing tasks individually. This highlights the &#x27;curse of coordination&#x27; and underscores the need for developing social intelligence in AI agents to function as effective teammates.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨当前编码代理在协作环境中的局限性，尤其是它们在协调方面的能力不足。作者提出了CooperBench，这是一个包含超过600个协作编码任务的基准测试平台，涵盖四种编程语言和十二个库，旨在评估代理的协调能力。主要实验结果显示，最先进的编码代理在协作时平均成功率比单独完成任务低30%，突显了AI代理中的&#x27;协调诅咒&#x27;，这与人类团队通常因协作而提高生产力形成鲜明对比。分析指出三个关键问题：沟通不畅、承诺偏离以及对他人计划的错误预期。研究还观察到一些罕见但有趣的协调行为，如角色分工和协商，为未来改进提供了潜在方向。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</div>
<div class="meta-line">First: 2025-09-21T03:02:59+00:00 · Latest: 2026-01-25T23:11:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16891v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.16891v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM作为布局设计师：增强的空间推理能力用于内容感知布局生成</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）在文本领域展示了出色的推理和规划能力，并能有效遵循复杂任务的指令，但它们对空间关系的理解和操控能力仍有限。这种能力对于内容感知的图形布局设计至关重要，其目标是将异构元素排列在画布上，以确保最终设计在视觉上平衡且结构上可行。该问题需要在受限的视觉空间中精确协调多个元素的放置、对齐和结构组织。为了解决这一局限性，我们引入了LaySPA，这是一个基于强化学习的框架，通过为基于LLM的智能体添加显式空间推理能力来增强布局设计。LaySPA采用混合奖励信号，共同捕捉几何约束、结构保真度和视觉质量，使智能体能够导航画布、建模元素间关系并优化空间布局。通过群体相对策略优化，智能体生成的内容感知布局能够反映显著区域，尊重空间约束，并产生可解释的推理轨迹以解释放置决策和结构化布局规范。实验结果表明，LaySPA显著提升了生成结构有效且视觉吸引人的布局的能力，优于更大的通用LLM，并且其性能与最先进的专用布局模型相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of Large Language Models (LLMs) in understanding and manipulating spatial relationships, which is essential for content-aware graphic layout design. The authors propose LaySPA, a reinforcement learning framework that enhances LLM-based agents with explicit spatial reasoning capabilities by using hybrid reward signals to incorporate geometric constraints, structural fidelity, and visual quality. Experimental results demonstrate that LaySPA generates structurally valid and visually appealing layouts, surpassing larger general-purpose LLMs and matching the performance of specialized layout models.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在理解和操作空间关系方面的不足，这对于内容感知的图形布局设计至关重要。作者提出了LaySPA，一个基于强化学习的框架，通过混合奖励信号增强LLMs的空间推理能力，以捕捉几何约束、结构保真度和视觉质量。该框架使智能体能够在画布上进行布局生成，尊重空间限制，并提供可解释的推理轨迹。实验结果表明，LaySPA在生成结构合理且视觉吸引人的布局方面显著优于通用LLMs，并与最先进的专用布局模型表现相当。</div>
</details>
</div>
<div class="card">
<div class="title">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</div>
<div class="meta-line">Authors: Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao</div>
<div class="meta-line">First: 2026-01-25T15:29:32+00:00 · Latest: 2026-01-25T15:29:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17885v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://peafowlvla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PEAfowl：增强感知的多视角视觉-语言-动作策略用于双臂操作</div>
<div class="mono" style="margin-top:8px">在杂乱场景中进行双臂操作需要在遮挡、视角和场景变化下保持稳定的策略。现有视觉-语言-动作模型往往难以泛化，原因包括：(i) 多视角特征通过无视角感知的token拼接进行融合，导致较弱的3D一致空间理解；(ii) 语言作为全局条件注入，导致指令定位粗糙。本文提出PEAfowl，一种增强感知的多视角VLA策略，用于双臂操作。在空间推理方面，PEAfowl预测每个token的深度分布，执行可微分的3D提升，并聚合局部跨视角邻居以形成几何基础且跨视角一致的表示。在指令定位方面，我们提出用Perceiver风格的文本感知读出机制替代全局条件注入，利用冻结的CLIP视觉特征，实现迭代证据积累。为克服无冗余和不完整的商品深度数据且不增加推理开销，我们应用仅训练阶段的深度蒸馏，从预训练的深度教师模型中监督深度分布头，为感知前端提供几何感知先验。在领域随机化设置下的RoboTwin 2.0上，PEAfowl将最强基线的成功率提升了23.0个百分点，真实机器人实验进一步验证了其可靠的模拟到现实迁移能力以及深度蒸馏带来的持续改进。项目网站：https://peafowlvla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Bimanual manipulation in cluttered environments demands policies that maintain stability despite occlusions and varying viewpoints. PEAfowl addresses these challenges by enhancing spatial reasoning through per-token depth prediction, differentiable 3D lifting, and cross-view neighbor aggregation, creating geometrically consistent representations. For instruction grounding, it uses a Perceiver-style readout over frozen CLIP features to iteratively accumulate evidence, improving alignment with language. The model also employs depth distillation during training to leverage a pretrained depth teacher, providing geometry-aware priors without increasing inference cost. Experiments on RoboTwin 2.0 under domain randomization show a 23.0% improvement in success rate over the strongest baseline, with real-robot tests confirming reliable sim-to-real transfer and consistent performance gains.</div>
<div class="mono" style="margin-top:8px">在杂乱场景中进行双臂操作需要能够在遮挡、视角和场景变化下保持稳定的策略。PEAfowl通过预测每个标记的深度分布、进行可微分的3D提升以及聚合跨视角的局部邻居信息，增强了空间推理能力，从而形成几何一致的跨视角表示。在指令对齐方面，它采用基于冻结CLIP特征的Perceiver风格读取器，通过迭代证据积累提升语言与视觉的对齐效果。此外，PEAfowl在训练阶段使用深度蒸馏技术，从预训练的深度教师模型中获取几何感知先验，避免增加推理开销。在RoboTwin 2.0的领域随机化设置下，PEAfowl的成功率比最强基线提升了23.0个百分点，真实机器人实验进一步验证了其可靠的仿真到现实迁移能力和深度蒸馏带来的持续性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</div>
<div class="meta-line">Authors: Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</div>
<div class="meta-line">First: 2025-08-19T15:58:25+00:00 · Latest: 2026-01-25T02:33:47+00:00</div>
<div class="meta-line">Comments: EACL 2026 Camera-Ready. Code and data: https://github.com/tianyiniu/RotBench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13968v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13968v3">PDF</a> · <a href="https://github.com/tianyiniu/RotBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench, a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270° rotated images. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models&#x27; ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs&#x27; spatial reasoning capabilities and human perception in identifying rotation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RotBench：评估多模态大语言模型在识别图像旋转方向上的能力</div>
<div class="mono" style="margin-top:8px">我们研究多模态大语言模型（MLLMs）在识别输入图像旋转方向（0°, 90°, 180°, 270°）方面的准确性。该任务需要强大的视觉推理能力，以检测旋转线索并理解图像中的空间关系，无论图像的方向如何。为评估MLLMs在这些能力上的表现，我们引入了RotBench，这是一个包含生活方式、人像和风景图像的350张手动筛选的基准数据集。尽管该任务相对简单，但我们发现一些最先进的开源和专有MLLMs（如GPT-5、o3和Gemini-2.5-Pro）无法可靠地识别输入图像的旋转方向。即使为模型提供辅助信息（如标题、深度图等）或使用链式推理提示，也只能带来微小且不一致的提升。我们的结果表明，大多数模型能够可靠地识别正向（0°）图像，而某些模型能够识别倒置（180°）图像。但没有任何模型能可靠地区分90°和270°旋转的图像。同时展示不同方向旋转的图像，对推理模型的性能有适度提升，而使用投票机制的修改设置则能提升较弱模型的性能。此外，我们还发现，微调虽然显著提升了模型识别180°旋转图像的能力，但并未改善其区分90°和270°旋转图像的能力。这些结果揭示了MLLMs在空间推理能力与人类识别旋转方向感知之间存在显著差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to assess the ability of Multimodal Large Language Models (MLLMs) to identify the rotation of input images. The authors introduce RotBench, a benchmark consisting of 350 manually filtered images across different categories, to evaluate how well MLLMs can detect image rotation. Their experiments show that while most models can reliably identify 0° and 180° rotations, they struggle to distinguish between 90° and 270° rotations. Providing additional information or using reasoning prompts yields limited improvements, and fine-tuning does not enhance the ability to differentiate 90° and 270° rotations, highlighting a gap between MLLMs&#x27; spatial reasoning and human perception.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估多模态大语言模型（MLLMs）识别图像旋转角度的能力，特别是0°、90°、180°和270°，这需要强大的视觉推理能力。作者提出了RotBench，一个包含350张人工筛选图像的基准数据集，涵盖生活方式、人像和风景等类别。实验结果表明，尽管大多数模型能够可靠地识别0°和180°的旋转，但在区分90°和270°旋转方面表现不佳。即使提供辅助信息或使用链式思维提示，性能提升有限，揭示了MLLMs在空间推理能力上与人类感知之间存在显著差距。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing the Impact of Code Changes on the Fault Localizability of Large Language Models</div>
<div class="meta-line">Authors: Sabaat Haroon, Ahmad Faraz Khan, Ahmad Humayun, Waris Gill, Abdul Haddi Amjad, Ali R. Butt, Mohammad Taha Khan, Muhammad Ali Gulzar</div>
<div class="meta-line">First: 2025-04-06T05:59:29+00:00 · Latest: 2026-01-24T10:52:01+00:00</div>
<div class="meta-line">Comments: This paper is currently Under Review. It consists of 12 pages, 11 Figures, and 5 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.04372v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.04372v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative Large Language Models (LLMs) are increasingly used in non-generative software maintenance tasks, such as fault localization (FL). Success in FL depends on a models ability to reason about program semantics beyond surface-level syntactic and lexical features. However, widely used LLM benchmarks primarily evaluate code generation, which differs fundamentally from semantic program reasoning. Meanwhile, traditional FL benchmarks such as Defect4J and BugsInPy are either not scalable or obsolete, as their datasets have become part of LLM training data, leading to biased results. This paper presents the first large-scale empirical investigation into the robustness of LLMs fault localizability. Inspired by mutation testing, we develop an end-to-end evaluation framework that addresses key limitations in existing LLM evaluation, including data contamination, scalability, automation, and extensibility. Using real-world programs with specifications, we inject unseen faults and ask LLMs to localize them, filtering out underspecified programs where localization is ambiguous. For each successfully localized program, we apply semantic-preserving mutations (SPMs) and rerun localization to assess robustness and determine whether LLM reasoning relies on syntactic cues rather than semantics. We evaluate 10 state-of-the-art LLMs on 750,013 fault localization tasks from over 1,300 Java and Python programs. We find that SPMs cause LLMs to fail on previously localized faults in 78% of cases, and that reasoning is stronger when relevant code appears earlier in context. These results indicate that LLM code reasoning is often tied to features irrelevant to semantics. We also identify code patterns that are challenging for LLMs to reason about. Overall, our findings motivate fundamental advances in how LLMs represent, interpret, and prioritize code semantics to reason more deeply about program logic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估代码更改对大型语言模型故障定位能力的影响</div>
<div class="mono" style="margin-top:8px">生成式大型语言模型（LLMs）越来越多地用于非生成式软件维护任务，如故障定位（FL）。在FL中取得成功取决于模型能否超越表面语法和词汇特征，对程序语义进行推理。然而，广泛使用的LLM基准测试主要评估代码生成能力，这与语义程序推理存在本质区别。同时，传统FL基准测试如Defect4J和BugsInPy要么无法扩展，要么已经过时，因为它们的数据集已成为LLM训练数据的一部分，导致结果偏倚。本文提出了首个大规模实证研究，探讨LLMs在故障定位方面的鲁棒性。受变异测试启发，我们开发了一个端到端的评估框架，解决了现有LLM评估中的关键限制，包括数据污染、可扩展性、自动化和可扩展性。我们使用带有规范的真实世界程序，注入未见过的故障，并要求LLMs进行定位，过滤掉定位模糊的欠规范程序。对于每个成功定位的程序，我们应用语义保持变异（SPMs），并重新运行定位以评估鲁棒性，并确定LLM推理是否依赖于语法提示而非语义。我们对超过1300个Java和Python程序中的750,013个故障定位任务进行了评估，发现SPMs在78%的情况下导致LLMs无法定位之前成功定位的故障，且当相关代码在上下文中出现得更早时，推理能力更强。这些结果表明，LLM的代码推理往往依赖于与语义无关的特征。我们还识别出LLMs难以推理的代码模式。总体而言，我们的发现促使LLMs在表示、解释和优先处理代码语义方面实现根本性进展，以更深入地理解程序逻辑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how code changes affect the fault localizability of large language models (LLMs) in software maintenance tasks. Motivated by the limitations of existing benchmarks that focus on code generation rather than semantic reasoning, the study introduces an end-to-end evaluation framework inspired by mutation testing. The framework injects unseen faults into real-world programs and evaluates LLMs&#x27; ability to localize them, while filtering out ambiguous cases. The main experimental results show that semantic-preserving mutations significantly reduce LLMs&#x27; success in fault localization, with 78% of previously localized faults failing after mutation. Additionally, the study finds that reasoning performance improves when relevant code appears earlier in the context, suggesting that LLMs may rely more on syntactic cues than semantic understanding.</div>
<div class="mono" style="margin-top:8px">本文探讨了代码变更对大型语言模型（LLMs）在软件维护任务中故障定位能力的影响。由于现有基准主要评估代码生成而非语义推理，研究提出了一种受突变测试启发的端到端评估框架，以解决数据污染、可扩展性、自动化和可扩展性等关键问题。该框架通过在具有规范的真实程序中注入未见过的故障，评估LLMs的故障定位能力，并过滤掉定位模糊的程序。实验结果显示，78%的情况下，LLMs在语义保持的突变后无法定位先前识别的故障，表明其推理依赖于语法特征而非语义。此外，研究还识别了LLMs难以处理的代码模式，强调了提升代码语义理解能力的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</div>
<div class="meta-line">Authors: Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-10T20:04:08+00:00 · Latest: 2026-01-23T21:03:18+00:00</div>
<div class="meta-line">Comments: Conference: NeurIPS 2025 (main)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10046v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10046v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimWorld-Robotics：为多模态机器人导航与协作合成逼真且动态的城市环境</div>
<div class="mono" style="margin-top:8px">近年来，基础模型在开发能够根据多模态输入在开放场景中执行多样化任务的通用型机器人方面取得了令人鼓舞的成果。然而，当前的研究主要集中在室内和家庭场景。在本工作中，我们提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境的具身AI模拟平台。SWR基于Unreal Engine 5构建，能够程序化生成包含行人和交通系统等动态元素的无限逼真城市场景，其在真实感、复杂性和可扩展性方面超越了以往的城市模拟。它还支持多机器人控制与通信。借助这些关键特性，我们构建了两个具有挑战性的机器人基准测试：(1) 多模态指令跟随任务，其中机器人必须根据视觉-语言导航指令在行人和交通存在的情况下到达目标；(2) 多智能体搜索任务，其中两个机器人必须通过通信协作定位并相遇。与现有基准不同，这两个新基准在现实场景中全面评估了多种关键机器人能力，包括(1) 多模态指令的语义对齐，(2) 大规模环境中的三维空间推理，(3) 在行人和交通中的安全长距离导航，(4) 多机器人协作，以及(5) 基于环境的通信。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SimWorld-Robotics (SWR), a simulation platform designed to support the development of generalist robotics in large-scale, photorealistic urban environments. The motivation stems from the need to evaluate and improve robot capabilities in complex, real-world settings beyond indoor scenarios. SWR is built on Unreal Engine 5 and generates dynamic urban scenes with pedestrians and traffic systems, enabling realistic and scalable testing. The platform supports multi-robot control and communication, and is used to create two challenging benchmarks: a multimodal instruction-following task and a multi-agent search task. Experimental results show that current state-of-the-art models, including vision-language models, struggle with these tasks, highlighting deficiencies in perception, reasoning, and planning for urban navigation.</div>
<div class="mono" style="margin-top:8px">本文提出了SimWorld-Robotics（SWR），一个用于在大规模、逼真的城市环境中开发通用机器人技术的仿真平台。研究动机源于对传统室内场景之外、复杂动态环境中的机器人能力评估与提升的需求。SWR基于Unreal Engine 5构建，能够生成包含行人和交通系统的无限逼真城市场景，具备高真实感、复杂性和可扩展性。该平台支持多机器人控制与通信，并构建了两个挑战性基准：多模态指令跟随任务和多智能体搜索任务。实验结果表明，当前最先进的模型，包括视觉语言模型，在这些任务中表现不佳，缺乏应对城市环境所需的有效感知、推理和规划能力。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</div>
<div class="meta-line">Authors: Riyang Bao, Cheng Yang, Dazhou Yu, Zhexiang Tang, Gengchen Mai, Liang Zhao</div>
<div class="meta-line">First: 2026-01-23T18:33:45+00:00 · Latest: 2026-01-23T18:33:45+00:00</div>
<div class="meta-line">Comments: 15pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16965v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spatial-Agent: 基于科学核心概念的代理式地理空间推理</div>
<div class="mono" style="margin-top:8px">地理空间推理对于城市分析、交通规划和灾害响应等现实世界应用至关重要。然而，现有的基于大语言模型（LLM）的代理通常无法进行真正的地理空间计算，而是依赖网络搜索或模式匹配，并在空间关系上产生幻觉。我们提出了Spatial-Agent，这是一种基于地理空间信息科学基础理论的AI代理。我们的方法将地理分析问题回答形式化为概念转换问题，其中自然语言问题被解析为可执行的工作流，表示为GeoFlow图——由空间概念节点和转换边组成的有向无环图。基于空间信息理论，Spatial-Agent通过基于模板的生成提取空间概念，分配具有原则性排序约束的功能角色，并组合转换序列。在MapEval-API和MapQA基准上的大量实验表明，Spatial-Agent显著优于现有的基线方法，包括ReAct和Reflexion，同时生成可解释且可执行的地理空间工作流。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial-Agent was developed to address the limitations of current LLM-based agents in performing genuine geospatial computations. These agents often rely on web search or pattern matching, leading to hallucinations in spatial relationships. The proposed approach integrates foundational theories of spatial information science, treating geo-analytical question answering as a concept transformation problem. Natural-language questions are converted into executable workflows represented as GeoFlow Graphs, which are directed acyclic graphs with nodes for spatial concepts and edges for transformations. The method employs template-based generation to extract spatial concepts, assign functional roles, and enforce ordering constraints. Experimental results on the MapEval-API and MapQA benchmarks show that Spatial-Agent significantly outperforms existing baselines like ReAct and Reflexion, producing both interpretable and executable geospatial workflows.</div>
<div class="mono" style="margin-top:8px">Spatial-Agent 是为了解决当前基于 LLM 的代理在进行准确地理空间推理时的局限性而开发的。现有代理常常依赖网络搜索或模式匹配，导致空间关系的错误假设。该方法将地理空间问答形式化为概念转换任务，使用 GeoFlow 图来表示可执行的工作流程。通过应用空间信息理论，代理提取并结构化空间概念，赋予其有序的角色，并通过模板生成转换序列。在 MapEval-API 和 MapQA 基准上的实验结果表明，Spatial-Agent 显著优于 ReAct 和 Reflexion 等现有方法，能够生成可解释且可执行的地理空间工作流程。</div>
</details>
</div>
<div class="card">
<div class="title">EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents</div>
<div class="meta-line">Authors: Xinze Li, Ziyue Zhu, Siyuan Liu, Yubo Ma, Yuhang Zang, Yixin Cao, Aixin Sun</div>
<div class="meta-line">First: 2026-01-23T12:09:59+00:00 · Latest: 2026-01-23T12:09:59+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent&#x27;s own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMemBench：面向VLM代理的事件记忆交互基准测试</div>
<div class="mono" style="margin-top:8px">我们引入了EMemBench，这是一个用于通过交互游戏评估代理长期记忆的程序化基准。与使用固定问题集不同，EMemBench从每个代理自身的轨迹中生成问题，涵盖文本和视觉游戏环境。每个模板通过底层游戏信号计算可验证的基准答案，具有可控的可回答性，并在记忆技能上保持平衡覆盖：单跳/多跳回忆、归纳、时间、空间、逻辑和对抗性记忆。我们使用上下文提示作为基线，评估具有强大语言模型/VLM作为骨干的记忆代理。在15个文本游戏和多个视觉种子上，结果远未饱和：归纳和空间推理仍然是持续的瓶颈，尤其是在视觉环境中。持久记忆在文本游戏中为开放骨干模型带来了明显提升，但对VLM代理的改进则不够一致，这表明基于视觉的事件记忆仍然是一个开放性挑战。一项人类研究进一步验证了EMemBench的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EMemBench is introduced as a programmatic benchmark to evaluate the long-term memory capabilities of agents in interactive games. It generates questions based on each agent&#x27;s trajectory, encompassing both text and visual environments, and computes verifiable ground truth from game signals. The benchmark covers various memory skills, including single/multi-hop recall, induction, temporal, spatial, logical, and adversarial reasoning. Evaluation shows that induction and spatial reasoning are significant challenges, particularly in visual settings, and that while persistent memory benefits open backbones in text games, improvements for VLM agents are less consistent, highlighting the difficulty of visually grounded episodic memory.</div>
<div class="mono" style="margin-top:8px">EMemBench 是一种用于评估智能体长期记忆能力的程序化基准，通过交互式游戏进行测试。与传统的固定问题基准不同，EMemBench 根据每个智能体的轨迹动态生成问题，涵盖文本和视觉游戏环境。该基准包含模板，能够从游戏信号中推导出可验证的参考答案，确保答案的可控性和对记忆技能的均衡覆盖，如单/多跳回忆、归纳、时间、空间、逻辑和对抗性推理。在15个文本游戏和多个视觉种子的测试中，结果表明归纳和空间推理仍是主要瓶颈，尤其是在视觉环境中。尽管持久记忆在文本游戏中显著提升了开放架构的表现，但对VLM智能体的效果则不够一致，表明基于视觉的事件记忆仍是一个开放性挑战。</div>
</details>
</div>
<div class="card">
<div class="title">TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning</div>
<div class="meta-line">Authors: Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin, Haoyu Cao, Xing Sun, Ying Shen, Hai-Tao Zheng, Liang Lin, Philip S. Yu</div>
<div class="meta-line">First: 2026-01-23T07:35:05+00:00 · Latest: 2026-01-23T07:35:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16520v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramPuzzle：通过组合空间推理评估多模态大语言模型</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在视觉识别和语义理解方面取得了显著进展。然而，它们在精确的组合空间推理方面的能力仍 largely 未被探索。现有基准测试通常涉及相对简单的任务，并依赖语义近似或粗略的相对位置，而其评估指标通常有限且缺乏严谨的数学定义。为弥合这一差距，我们引入 TangramPuzzle，这是一个基于几何的基准测试，旨在通过经典的 Tangram 游戏视角来评估组合空间推理。我们提出了 Tangram Construction Expression（TCE），一种符号几何框架，通过精确的、可由机器验证的坐标规范来定义 Tangram 组装，以减少视觉近似带来的歧义。我们设计了两个互补的任务：轮廓预测，要求从局部组件推断整体形状；以及端到端代码生成，要求解决逆向几何组装问题。我们在多个先进的开源和专有模型上进行了广泛评估实验，发现了一个有趣的见解：MLLMs 倾向于优先匹配目标轮廓，而忽视几何约束，导致组件发生形变或扭曲。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of rigorous evaluation of compositional spatial reasoning in Multimodal Large Language Models (MLLMs). The authors introduce TangramPuzzle, a geometry-based benchmark inspired by the classic Tangram game, which assesses MLLMs&#x27; ability to reason about spatial compositions through precise coordinate-based tasks. Two tasks, Outline Prediction and End-to-End Code Generation, are designed to evaluate the models&#x27; performance in inferring global shapes from local components and solving inverse geometric assembly problems. The experiments show that MLLMs often focus on matching the target silhouette rather than adhering to geometric constraints, resulting in distorted or deformed tangram pieces.</div>
<div class="mono" style="margin-top:8px">本研究旨在弥补多模态大语言模型（MLLMs）在组合空间推理能力方面的评估不足。作者提出了TangramPuzzle，这是一个基于几何的基准测试，通过经典的七巧板游戏来评估MLLMs的组合空间推理能力。他们设计了Tangram Construction Expression（TCE），一种基于坐标符号的框架，用于精确表示七巧板的组合。研究设计了两个互补任务：轮廓预测和端到端代码生成。实验结果表明，MLLMs往往更关注匹配目标轮廓，而忽视几何约束，导致拼图组件出现变形或错误排列。</div>
</details>
</div>
<div class="card">
<div class="title">Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models</div>
<div class="meta-line">Authors: Bridget Leonard, Scott O. Murray</div>
<div class="meta-line">First: 2026-01-23T00:21:27+00:00 · Latest: 2026-01-23T00:21:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent&#x27;s visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于认知的标记克服多模态模型中的自我中心偏差</div>
<div class="mono" style="margin-top:8px">多模态语言模型（MLMs）在语义视觉-语言任务上表现良好，但在需要采用其他代理视觉视角的空间推理任务上表现不佳。这些错误反映了持续存在的自我中心偏差，并引发了关于当前模型是否支持参照中心推理的疑问。受人类空间认知启发，我们引入了视角标记，这些专门的嵌入通过（1）具身身体关键点线索或（2）支持心理旋转的抽象表示来编码方向。将这些标记整合到LLaVA-1.5-13B中，可实现对二级视觉视角任务的性能提升。在合成和自然基准（Isle Bricks V2、COCO、3DSRBench）中，视角标记提高了准确性，基于旋转的标记可推广至非人类参考代理。表示分析表明，微调增强了基础模型中已有的潜在方向敏感性，这表明MLMs包含参照中心推理的前身，但缺乏适当的内部结构。总体而言，将基于认知的空间结构直接嵌入到标记空间中，为视角采取提供了一种轻量且模型无关的机制，并实现了更接近人类的空间推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitation of multimodal language models (MLMs) in spatial reasoning tasks that require adopting another agent&#x27;s visual perspective, a challenge stemming from their egocentric bias. To overcome this, the authors propose perspective tokens, which are specialized embeddings that encode spatial orientation using either body-keypoint cues or abstract mental rotation representations. By integrating these tokens into LLaVA-1.5-13B, the model achieves improved performance on level-2 visual perspective-taking tasks. Experimental results across synthetic and naturalistic benchmarks show that perspective tokens significantly enhance accuracy, with rotation-based tokens demonstrating better generalization to non-human reference agents. The findings suggest that while MLMs may already possess latent orientation sensitivity, they lack the internal structure necessary for allocentric reasoning, and embedding cognitively inspired spatial information into the token space offers a promising, lightweight solution.</div>
<div class="mono" style="margin-top:8px">本文旨在解决多模态语言模型（MLMs）在空间推理中表现出的以自我为中心的偏差问题，该偏差限制了模型从其他代理视角理解视觉信息的能力。作者提出了视角标记（perspective tokens），这些是专门设计的嵌入，通过身体关键点线索或抽象的思维旋转表示来编码空间方向。将这些标记整合到LLaVA-1.5-13B模型中，使其在二级视觉视角任务中表现提升。在合成和自然基准测试（如Isle Bricks V2、COCO、3DSRBench）中，视角标记显著提高了准确性，其中基于旋转的标记在非人类参考代理上表现出更好的泛化能力。研究还表明，尽管MLMs可能已经具备潜在的方向敏感性，但缺乏支持非自我中心推理的适当内部结构。</div>
</details>
</div>
<div class="card">
<div class="title">VOCAL: Visual Odometry via ContrAstive Learning</div>
<div class="meta-line">Authors: Chi-Yao Huang, Zeel Bhatt, Yezhou Yang</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-06-30T20:26:13+00:00 · Latest: 2026-01-22T22:04:57+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.00243v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.00243v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Breakthroughs in visual odometry (VO) have fundamentally reshaped the landscape of robotics, enabling ultra-precise camera state estimation that is crucial for modern autonomous systems. Despite these advances, many learning-based VO techniques rely on rigid geometric assumptions, which often fall short in interpretability and lack a solid theoretical basis within fully data-driven frameworks. To overcome these limitations, we introduce VOCAL (Visual Odometry via ContrAstive Learning), a novel framework that reimagines VO as a label ranking challenge. By integrating Bayesian inference with a representation learning framework, VOCAL organizes visual features to mirror camera states. The ranking mechanism compels similar camera states to converge into consistent and spatially coherent representations within the latent space. This strategic alignment not only bolsters the interpretability of the learned features but also ensures compatibility with multimodal data sources. Extensive evaluations on the KITTI dataset highlight VOCAL&#x27;s enhanced interpretability and flexibility, pushing VO toward more general and explainable spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOCAL: 通过对比学习实现的视觉里程计</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）领域的突破彻底改变了机器人学的格局，使超精确的相机状态估计成为现代自主系统的关键。尽管取得了这些进展，许多基于学习的VO技术仍依赖于刚性的几何假设，这些假设在可解释性和理论基础方面往往存在不足，尤其是在完全数据驱动的框架中。为克服这些局限性，我们提出了VOCAL（通过对比学习实现的视觉里程计），一个新颖的框架，将VO重新构想为一个标签排序问题。通过将贝叶斯推断与表示学习框架相结合，VOCAL将视觉特征组织为与相机状态相对应的形式。排序机制迫使相似的相机状态在潜在空间中收敛为一致且空间连贯的表示。这种策略性对齐不仅增强了所学特征的可解释性，还确保了其与多模态数据源的兼容性。在KITTI数据集上的广泛评估突显了VOCAL在可解释性和灵活性方面的提升，推动视觉里程计向更通用和可解释的空间智能发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual odometry (VO) is a critical component in autonomous systems, yet many learning-based approaches suffer from limited interpretability and theoretical grounding due to their reliance on rigid geometric assumptions. VOCAL addresses these issues by redefining VO as a label ranking problem, leveraging Bayesian inference and representation learning to align visual features with camera states. The framework enforces spatial coherence in the latent space through a ranking mechanism, improving both interpretability and compatibility with multimodal data. Evaluations on the KITTI dataset demonstrate VOCAL&#x27;s superior performance in terms of interpretability and adaptability, advancing the field toward more general and explainable spatial perception solutions.</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）是自主系统的关键技术，但现有的学习方法常依赖于刚性的几何假设，限制了其可解释性和理论基础。VOCAL提出了一种新框架，将VO重新定义为标签排序问题，结合贝叶斯推断和表征学习，使视觉特征与相机状态对齐。排序机制确保相似的相机状态在潜在空间中形成空间一致的表征，从而提升模型的可解释性和对多模态数据的适应性。在KITTI数据集上的广泛评估表明，VOCAL在可解释性和灵活性方面优于传统方法，推动了VO向更通用和可解释的空间智能发展。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-22T19:05:41+00:00</div>
<div class="meta-line">Comments: Work done as part of the EleutherAI SOAR Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，其训练方法往往将图像扁平化为1D的patch序列，丢弃了空间推理所需的2D结构。我们认为，这种缺乏空间感知是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of vision-language models (VLMs) has revealed a limitation in their ability to capture spatial relationships, which is critical for applications like robotics and embodied AI. This study identifies that current VLMs, often trained with CLIP-style image encoders, flatten images into 1D patch sequences, neglecting the essential 2D structure for spatial reasoning. To enhance spatial awareness, the authors explore alternative training objectives for image encoders and the incorporation of 2D positional encodings. Their experiments demonstrate that these modifications significantly improve spatial reasoning performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在空间推理方面的不足，指出当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，将图像扁平化为1D序列，忽略了其2D结构。作者提出了两种改进方法：使用不同训练目标的图像编码器以及引入2D位置编码。实验结果表明，这些架构改进在多个基准测试中提升了空间推理能力，显示出其在机器人和具身AI等应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知</div>
<div class="mono" style="margin-top:8px">近期，大型音频语言模型（LALMs）在语音识别、音频描述和听觉问答任务中取得了显著进展。然而，这些模型是否能够感知空间动态，特别是声源的运动，仍不清楚。在本研究中，我们发现当前音频语言模型（ALLMs）存在系统性的运动感知缺陷。为探讨这一问题，我们引入了AudioMotionBench，这是首个专门设计用于评估听觉运动理解的基准测试。AudioMotionBench引入了一个受控的问答基准测试，用于评估音频语言模型能否从双耳音频中推断出移动声源的方向和轨迹。全面的定量和定性分析表明，当前模型在可靠识别运动线索或区分方向模式方面存在困难。平均准确率仍低于50\%，突显了听觉空间推理中的基本局限性。我们的研究指出了人类与模型在听觉空间推理之间的根本差距，为未来增强音频语言模型的空间认知能力提供了诊断工具和新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether large audio-language models (LALMs) can perceive auditory motion, specifically the direction and trajectory of moving sound sources. The researchers introduce AudioMotionBench, a novel benchmark designed to evaluate this capability using binaural audio. Their analysis shows that current models perform poorly, with average accuracy below 50%, indicating a significant limitation in understanding spatial dynamics through sound.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型音频语言模型（LALMs）是否能够感知听觉运动，特别是移动声源的方向和轨迹。研究人员提出了AudioMotionBench，这是首个专门用于评估听觉运动理解能力的基准测试。通过全面的定量和定性分析发现，当前模型在识别运动线索或区分方向模式方面表现不佳，平均准确率低于50%，表明其在听觉空间推理方面存在根本性缺陷。研究结果揭示了人类与模型在听觉空间理解上的基本差距，并为未来提升音频语言模型的空间认知能力提供了诊断工具和新思路。</div>
</details>
</div>
<div class="card">
<div class="title">Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</div>
<div class="meta-line">Authors: Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</div>
<div class="meta-line">First: 2025-05-19T16:56:31+00:00 · Latest: 2026-01-22T14:25:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13353v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.13353v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code&#x27;s operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs&#x27; accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval&#x27;s 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知与敏感性：探究语义回忆对长上下文代码推理的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用于理解大型代码库，但它们是真正理解长代码上下文的操作语义，还是依赖于模式匹配的捷径，仍不清楚。我们区分了词汇回忆（逐字检索代码）和语义回忆（理解操作语义）。通过评估10个最先进的LLMs，我们发现虽然前沿模型能够实现近乎完美的、位置无关的词汇回忆，但当代码位于长上下文的中心位置时，语义回忆会严重退化。我们引入了语义回忆敏感性这一指标，用于衡量任务是否需要理解代码的操作语义，还是允许使用模式匹配的捷径。通过一种新颖的反事实测量方法，我们表明模型在解决现有代码理解基准时严重依赖模式匹配的捷径。我们提出了一项新任务SemTrace，通过不可预测的操作实现高语义回忆敏感性；LLMs的准确性表现出严重的位置效应，当相关代码片段接近输入代码上下文的中间时，中位数准确率下降了92.73%，而CRUXEval则下降了53.36%。我们的研究结果表明，当前的评估方法在长上下文代码理解中严重低估了语义回忆失败的情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how large language models (LLMs) handle long code contexts, focusing on whether they rely on pattern matching or truly understand the operational semantics of code. The research differentiates between lexical recall, which involves retrieving code verbatim, and semantic recall, which requires comprehension of code meaning. By evaluating ten state-of-the-art LLMs, the authors found that while models perform well in lexical recall, their semantic recall significantly declines when code is located in the central part of long contexts. They introduce a metric called semantic recall sensitivity to assess the necessity of understanding code semantics in tasks. Using a counterfactual evaluation approach, they demonstrate that models heavily depend on pattern matching shortcuts. Their proposed task, SemTrace, highlights the severe positional impact on model accuracy, with a median drop of 92.73% compared to CRUXEval&#x27;s 53.36%.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在处理长上下文代码推理时的表现，关注其是依赖模式匹配还是真正理解代码的运行语义。研究区分了字面回忆（逐字检索代码）和语义回忆（理解代码含义），通过评估10个最先进的LLMs发现，尽管模型在字面回忆上表现优异，但当代码位于长上下文的中间位置时，其语义回忆能力显著下降。论文引入了新的指标——语义回忆敏感性，用于评估任务是否需要语义理解。通过一种新颖的反事实测量方法，研究显示模型在现有基准测试中严重依赖模式匹配。提出的SemTrace任务通过不可预测的操作提高了语义回忆敏感性，结果显示当相关代码片段接近输入中间时，LLMs的准确率下降高达92.73%，而CRUXEval的下降仅为53.36%。这些发现表明，当前的评估方法可能低估了长上下文代码理解中的语义回忆失败问题。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</div>
<div class="meta-line">Authors: Pascal Benschop, Justin Dauwels, Jan van Gemert</div>
<div class="meta-line">First: 2026-01-22T09:14:11+00:00 · Latest: 2026-01-22T09:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用合成生成视频评估视觉语言模型的情境与空间感知</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在空间推理方面仍存在脆弱性，当语义依赖于细微的时间或几何线索时。我们引入一个合成基准测试，用于评估两种互补能力：情境感知（判断互动是否具有伤害性或无害性）和空间感知（追踪谁对谁做了什么，以及推理相对位置和运动）。通过最小化的视频对，我们测试了三个挑战：区分暴力行为与无害活动、跨视角绑定攻击者角色、以及判断细粒度轨迹对齐。尽管我们在无训练设置下评估了近期的VLMs，但该基准适用于任何视频分类模型。结果显示，模型在各项任务中的表现仅略高于随机猜测。一个简单的辅助手段——稳定的颜色线索——部分缓解了攻击者角色的混淆，但并未解决其根本性缺陷。通过发布数据和代码，我们旨在提供可复现的诊断工具，并为轻量级空间先验的探索提供基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to evaluate the situational and spatial awareness capabilities of vision language models (VLMs) by introducing a synthetic video benchmark. The benchmark tests three key challenges: distinguishing between violent and non-violent interactions, binding roles of aggressors across different viewpoints, and assessing fine-grained trajectory alignment. The results indicate that current VLMs perform only slightly better than chance in these tasks, suggesting limitations in their ability to process subtle temporal and geometric information. The introduction of stable color cues provides partial improvement in role confusion but does not address the fundamental shortcomings in spatial reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过一个合成视频基准评估视觉语言模型（VLMs）的情境和空间感知能力。该基准测试了三个关键挑战：区分暴力与良性互动、跨视角绑定攻击者角色以及评估细粒度轨迹对齐。结果显示，当前VLMs在这些任务上的表现仅略优于随机猜测，表明其在空间推理方面存在明显不足。使用稳定的颜色提示在一定程度上缓解了角色绑定的混淆，但未能根本解决这一问题，提示需要更有效的空间先验知识来提升模型性能。</div>
</details>
</div>
<div class="card">
<div class="title">AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</div>
<div class="meta-line">Authors: Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Yingheng Wang, Bram Hoex, Zhicheng Zhong, Tong Xie</div>
<div class="meta-line">First: 2025-10-06T11:17:56+00:00 · Latest: 2026-01-22T05:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04704v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.04704v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomWorld：评估大型语言模型在晶体材料中空间推理能力的基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在文本推理方面表现出色，并开始发展空间理解能力，这引发了关于这些能力是否可以结合用于复杂、领域特定任务的疑问。这一问题在材料科学等需要深入理解三维原子结构的领域尤为重要。尽管初步研究已成功将LLMs应用于纯晶体生成或坐标理解任务，但缺乏一个标准化的基准来系统评估其在多样化原子结构中的核心推理能力。为解决这一问题，我们引入了AtomWorld基准，用于基于晶体学信息文件（CIFs）的LLMs任务评估。这些任务包括结构编辑、CIF感知和性质引导建模，揭示了一个关键限制：尽管当前模型已建立有希望的基线，但在结构理解和空间推理方面仍持续失败。我们的实验表明，这些模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析和材料洞察中的累积性错误。通过定义这些标准化任务，AtomWorld为推动LLMs向稳健的原子尺度建模发展奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces the AtomWorld benchmark to evaluate spatial reasoning capabilities of large language models (LLMs) in the context of crystalline materials. Motivated by the need for a standardized assessment of LLMs&#x27; ability to understand and manipulate 3D atomic structures, the benchmark is based on Crystallographic Information Files (CIFs). The main experimental results show that current models, although showing promise in some tasks, consistently struggle with structural understanding and spatial reasoning, often making frequent errors in structure modification and CIF format interpretation, which can lead to cumulative inaccuracies in materials analysis.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型（LLMs）在晶体材料领域的空间推理能力，这对于材料科学至关重要。作者提出了AtomWorld基准，基于晶体学信息文件（CIFs），用于评估LLMs在结构编辑、CIF感知和性质引导建模等任务上的表现。实验结果表明，当前模型尽管有良好表现，但在结构理解和空间推理方面仍存在显著不足，经常在修改晶体结构任务中出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析中的累积误差。</div>
</details>
</div>
<div class="card">
<div class="title">VibeTensor: System Software for Deep Learning, Fully Generated by AI Agents</div>
<div class="meta-line">Authors: Bing Xu, Terry Chen, Fengzhe Zhou, Tianqi Chen, Yangqing Jia, Vinod Grover, Haicheng Wu, Wei Liu, Craig Wittenbrink, Wen-mei Hwu, Roger Bringmann, Ming-Yu Liu, Luis Ceze, Michael Lightstone, Humphrey Shi</div>
<div class="meta-line">First: 2026-01-21T19:29:00+00:00 · Latest: 2026-01-21T19:29:00+00:00</div>
<div class="meta-line">Comments: Open-source: https://github.com/NVLabs/vibetensor</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16238v1">PDF</a> · <a href="https://github.com/NVLabs/vibetensor">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">VIBETENSOR is an open-source research system software stack for deep learning, generated by LLM-powered coding agents under high-level human guidance. In this paper, &quot;fully generated&quot; refers to code provenance: implementation changes were produced and applied as agent-proposed diffs; validation relied on agent-run builds, tests, and differential checks, without per-change manual diff review. It implements a PyTorch-style eager tensor library with a C++20 core (CPU+CUDA), a torch-like Python overlay via nanobind, and an experimental Node.js/TypeScript interface. Unlike thin bindings, VIBETENSOR includes its own tensor/storage system, schema-lite dispatcher, reverse-mode autograd, CUDA runtime (streams/events/graphs), a stream-ordered caching allocator with diagnostics, and a stable C ABI for dynamically loaded operator plugins. We view this release as a milestone for AI-assisted software engineering: it shows coding agents can generate a coherent deep learning runtime spanning language bindings down to CUDA memory management, validated primarily by builds and tests. We describe the architecture, summarize the workflow used to produce and validate the system, and evaluate the artifact. We report repository scale and test-suite composition, and summarize reproducible microbenchmarks from an accompanying AI-generated kernel suite, including fused attention versus PyTorch SDPA/FlashAttention. We also report end-to-end training sanity checks on 3 small workloads (sequence reversal, ViT, miniGPT) on NVIDIA H100 (Hopper, SM90) and Blackwell-class GPUs; multi-GPU results are Blackwell-only and use an optional CUTLASS-based ring-allreduce plugin gated on CUDA 13+ and sm103a toolchain support. Finally, we discuss failure modes in generated system software, including a &quot;Frankenstein&quot; composition effect where locally correct subsystems interact to yield globally suboptimal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VibeTensor：由AI代理完全生成的深度学习系统软件</div>
<div class="mono" style="margin-top:8px">VIBETENSOR是一个开源的深度学习研究系统软件栈，由LLM驱动的编码代理在高水平人类指导下生成。在本文中，&quot;完全生成&quot;指的是代码溯源：实现的更改由代理提议并应用；验证依赖于代理运行的构建、测试和差异检查，而无需逐项人工审查差异。它实现了一个PyTorch风格的即时张量库，包含C++20核心（CPU + CUDA），通过nanobind实现了一个类似Torch的Python封装，并提供了一个实验性的Node.js/TypeScript接口。与薄绑定不同，VIBETENSOR包含自己的张量/存储系统、schema-lite调度器、反向模式自动微分、CUDA运行时（流/事件/图），一个流顺序的缓存分配器及其诊断功能，以及一个稳定的C ABI，用于动态加载的操作符插件。我们认为此次发布是AI辅助软件工程的一个里程碑：它表明编码代理能够生成一个连贯的深度学习运行时，从语言绑定一直到CUDA内存管理。我们描述了该系统的架构，总结了用于生成和验证系统的流程，并评估了该成果。我们报告了仓库规模和测试套件组成，并总结了来自配套AI生成内核套件的可复现微基准测试结果，包括融合注意力与PyTorch SDPA/FlashAttention的对比。我们还报告了在NVIDIA H100（Hopper，SM90）和Blackwell系列GPU上对三个小型工作负载（序列反转、ViT、miniGPT）进行的端到端训练合理性检查；多GPU结果仅限于Blackwell，并使用基于CUTLASS的可选环形all-reduce插件，该插件依赖于CUDA 13+和sm103a工具链支持。最后，我们讨论了生成系统软件中的失败模式，包括一种&quot;弗兰肯斯坦&quot;组合效应，其中局部正确的子系统相互作用导致全局性能不佳。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of VIBETENSOR is to explore the potential of AI agents in generating complex deep learning system software with minimal human intervention. The system is developed using LLM-powered coding agents guided by high-level human input, implementing a PyTorch-style eager tensor library with a C++20 core and Python overlay. The main experimental results show that the system passes comprehensive builds and tests, demonstrating its coherence and correctness. Performance evaluations include comparisons with PyTorch&#x27;s SDPA and FlashAttention, as well as end-to-end training checks on small workloads. The study also identifies challenges in AI-generated system software, such as the &quot;Frankenstein&quot; effect, where individual subsystems may perform well but interact poorly in the overall system.</div>
<div class="mono" style="margin-top:8px">VIBETENSOR的研究动机是展示AI代理在生成完整且协调的深度学习系统软件栈方面的可行性。该系统通过LLM驱动的代码生成代理，在人类指导下实现了一个PyTorch风格的即时张量库，包含C++20核心和Python覆盖层。主要实验结果表明，该系统通过构建和测试验证，其微基准测试（如融合注意力）性能与PyTorch相当。此外，对小型工作负载的端到端训练检查确认了其在现代GPU上的功能，同时也指出了由于&quot;Frankenstein&quot;组合效应可能导致的子系统间全局性能不佳的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理为何失败？对GitHub中失败代理拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编码代理现在正在向软件项目提交拉取请求（PRs），不仅作为助手，还作为自主贡献者。随着这些代理贡献在真实仓库中迅速增加，人们对它们在实际中的行为以及为何许多未能被合并仍知之甚少。在本文中，我们对GitHub上五个编码代理提交的33,000个代理撰写的PR进行了大规模研究。（RQ1）我们首先从四个广泛维度定量描述了合并与未合并的PR：1）任务类型下的合并结果，2）代码更改，3）CI构建结果，以及4）评审动态。我们观察到，与文档、CI和构建更新相关的任务合并成功率最高，而性能和错误修复任务表现最差。未被合并的PR往往涉及更大的代码更改，修改更多文件，并且通常无法通过项目的CI/CD流水线验证。（RQ2）为进一步探究某些代理PR未被合并的原因，我们对600个PR进行了定性分析，以构建拒绝模式的分层分类法。这种分析通过揭示定量指标未涵盖的拒绝原因，如缺乏有意义的评审互动、重复PR、不受欢迎的功能实现以及代理与项目目标的不一致，补充了RQ1的定量发现。我们的研究结果突显了关键的社会技术因素和人机协作因素，这些因素对于提高未来代理工作流的成功率至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of AI coding agents in contributing to software projects through GitHub pull requests. By analyzing 33,000 agent-authored PRs from five coding agents, the research identifies that documentation, CI, and build update tasks have the highest merge success, while performance and bug-fix tasks show the lowest. Not-merged PRs are typically larger, touch more files, and often fail CI/CD validation. A qualitative analysis of 600 PRs reveals additional rejection reasons such as lack of reviewer engagement, duplicate submissions, unwanted features, and agent misalignment, providing insights into socio-technical and human-AI collaboration challenges.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在通过GitHub提交拉取请求（PR）时的失败情况。通过对五个编码代理提交的33,000个PR进行分析，发现与文档、CI和构建相关的任务合并成功率最高，而性能和错误修复任务的合并成功率最低。未被合并的PR通常涉及更大的代码改动，影响更多文件，并且常因CI/CD流程验证失败而被拒绝。对600个PR的定性分析进一步揭示了未合并的原因，包括缺乏评审者互动、重复提交、不受欢迎的功能实现以及代理行为与项目目标不一致，为改善未来的代理工作流程提供了关键的社交技术与人机协作因素的洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</div>
<div class="meta-line">Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</div>
<div class="meta-line">First: 2025-07-07T18:17:06+00:00 · Latest: 2026-01-21T13:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05386v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.05386v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model&#x27;s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT&#x27;s gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化微调自然缓解持续微调中的遗忘</div>
<div class="mono" style="margin-top:8px">持续微调（CPT）是一种流行且有效的技术，用于将基础模型（如多模态大语言模型）适应于特定且不断演化的下游任务。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，但CPT中学习范式的根本作用仍被广泛忽视。本文对两种核心的微调范式——监督微调（SFT）和强化微调（RFT）进行了比较分析，探讨它们在持续微调过程中对知识保留的影响。我们的实验基于包含七个多样化多模态任务的基准数据集，使用Qwen2.5-VL-7B-Instruct作为持续微调的基础模型。研究得出两个重要发现：（1）在持续学习下游任务时，SFT会导致先前学习任务的灾难性遗忘，而RFT则能自然保留先前知识，其性能与多任务训练相当。（2）RFT成功保护并甚至增强了模型在标准基准（如MMMU和MMLU-Pro）上的通用知识，而SFT则严重损害了模型的通用能力。进一步分析表明，这种稳定性并非主要归因于显式的机制，如KL惩罚或思维链推理，而是源于RFT中隐含的正则化机制。我们的理论分析表明，RFT的梯度更新自然受到奖励方差的缩放，作为一种数据依赖的正则化器，其内在机制能够保护先前获得的知识。最后，我们提出了一种基于rollout的实例过滤算法，以提升RFT的稳定性和效率。我们的全面研究展示了RFT作为持续微调的稳健范式的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the issue of catastrophic forgetting in continual post-training (CPT) of foundation models, focusing on the role of learning paradigms. It compares supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), finding that RFT inherently mitigates forgetting and maintains performance comparable to multi-task training. Experiments on seven multimodal tasks using Qwen2.5-VL-7B-Instruct show that RFT preserves prior knowledge and enhances general capabilities on standard benchmarks, unlike SFT which leads to significant degradation. The study identifies an implicit regularization mechanism in RFT, driven by reward variance during gradient updates, rather than explicit methods like KL penalty. A rollout-based instance filtering algorithm is proposed to improve RFT&#x27;s stability and efficiency.</div>
<div class="mono" style="margin-top:8px">本文探讨了基础模型在持续微调过程中出现灾难性遗忘的问题，重点比较了监督微调（SFT）和强化微调（RFT）两种学习范式的有效性。研究发现，RFT能够自然地缓解遗忘，保留先前学习的知识，其性能可与多任务训练相媲美。在七个不同的多模态任务上使用Qwen2.5-VL-7B-Instruct进行实验表明，RFT不仅维持了模型的通用知识，还能在标准基准（如MMMU和MMLU-Pro）上提升表现，而SFT则会导致严重的能力退化。研究指出，RFT的稳定性主要源于其内在的隐式正则化机制，该机制由奖励方差驱动，而非显式的KL惩罚或思维链推理等方法。基于此，作者提出了一种基于rollout的实例过滤算法，以提高RFT在持续学习中的效率和鲁棒性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
