<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-20 03:59</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260220_0359</div>
    <div class="row"><div class="card">
<div class="title">Learning Situated Awareness in the Real World</div>
<div class="meta-line">Authors: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan, Jiajun Wu, Ming-Hsuan Yang, Xin Eric Wang</div>
<div class="meta-line">First: 2026-02-18T18:22:52+00:00 · Latest: 2026-02-18T18:22:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16682v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent&#x27;s viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model&#x27;s observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在真实世界中学习情境意识</div>
<div class="mono" style="margin-top:8px">人类感知的一个核心方面是情境意识，即能够将自己与周围的物理环境联系起来，并在特定情境中推理可能的动作。然而，大多数现有的多模态基础模型（MFMs）基准测试强调以环境为中心的空间关系（场景中物体之间的关系），而忽略了以观察者为中心的关系，这种关系需要基于代理的视角、姿态和运动进行推理。为弥合这一差距，我们引入了SAW-Bench（真实世界中的情境意识），这是一个用于评估以自我为中心的情境意识的新基准测试，基于真实世界视频。SAW-Bench包含786段使用Ray-Ban Meta（Gen 2）智能眼镜自拍录制的视频，涵盖多样化的室内外环境，并包含超过2,071对人工标注的问题-答案对。它通过六个不同的意识任务来探测模型对以观察者为中心的理解能力。我们的全面评估显示，即使使用表现最好的多模态基础模型Gemini 3 Flash，人类与模型之间仍存在37.66%的性能差距。此外，我们的深入分析揭示了几个显著发现；例如，虽然模型可以利用以自我为中心视频中的部分几何线索，但它们常常无法推断出连贯的摄像机几何结构，导致系统性的空间推理错误。我们将SAW-Bench定位为情境空间智能的基准测试，超越被动观察，转向对物理基础、以观察者为中心动态的理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the limitation of existing multimodal foundation models (MFMs) in capturing observer-centric spatial relationships, which are essential for situated awareness. SAW-Bench, a new benchmark using real-world videos recorded with Ray-Ban Meta (Gen 2) smart glasses, evaluates models&#x27; ability to reason from an agent&#x27;s perspective, including viewpoint, pose, and motion. The benchmark includes 786 videos and over 2,071 human-annotated question-answer pairs, testing six awareness tasks. Evaluation shows a significant performance gap between humans and models, with the best MFM, Gemini 3 Flash, achieving only 37.66% accuracy. The analysis highlights that models struggle with coherent camera geometry inference, resulting in systematic spatial reasoning errors.</div>
<div class="mono" style="margin-top:8px">本研究针对现有多模态基础模型（MFMs）在捕捉以观察者为中心的空间关系方面的不足，提出了新的基准测试SAW-Bench。该基准基于真实世界视频，使用Ray-Ban Meta智能眼镜录制了786段涵盖室内外环境的视频，并包含超过2071对人工标注的问题-答案对，用于评估模型在第一视角下的空间理解能力。实验结果显示，即使是最先进的MFM——Gemini 3 Flash，其表现也仅达到人类水平的37.66%，表明模型在从第一人称视角进行连贯空间推理方面仍存在显著挑战。</div>
</details>
</div>
<div class="card">
<div class="title">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</div>
<div class="meta-line">Authors: Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield</div>
<div class="meta-line">Venue: CVPR 2025 Oral</div>
<div class="meta-line">First: 2024-11-25T16:21:34+00:00 · Latest: 2026-02-18T04:26:35+00:00</div>
<div class="meta-line">Comments: CVPR 2025 (Oral); Project Website: https://chanh.ee/RoboSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.16537v5">Abs</a> · <a href="https://arxiv.org/pdf/2411.16537v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image datasets that often lack sophisticated spatial understanding. For example, datasets frequently do not capture reference frame comprehension, yet effective spatial reasoning requires understanding whether to reason from ego-, world-, or object-centric perspectives. To address this issue, we introduce RoboSpatial, a large-scale dataset for spatial understanding in robotics. It consists of real indoor and tabletop scenes, captured as 3D scans and egocentric images, and annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial relationships, and the pairing of 2D egocentric images with 3D scans makes it both 2D- and 3D- ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robot manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSpatial：为机器人赋予空间理解能力的2D和3D视觉-语言模型</div>
<div class="mono" style="margin-top:8px">空间理解是使机器人能够感知环境、推理环境并有意义地与环境互动的关键能力。在现代机器人技术中，这些能力越来越多地由视觉-语言模型提供。然而，这些模型在空间推理任务中面临重大挑战，因为它们的训练数据基于通用图像数据集，这些数据集通常缺乏复杂的空间理解。例如，数据集经常无法捕捉参考系的理解，而有效的空间推理需要明确是从自体中心、世界中心还是物体中心视角进行推理。为了解决这一问题，我们引入了RoboSpatial，这是一个用于机器人空间理解的大规模数据集。该数据集包含真实室内外和桌面场景，以3D扫描和第一视角图像形式采集，并标注了与机器人相关的丰富空间信息。数据集包含100万张图像、5000个3D扫描和300万个标注的空间关系，且2D第一视角图像与3D扫描的配对使其同时适用于2D和3D任务。我们的实验表明，使用RoboSpatial训练的模型在空间能力预测、空间关系预测和机器人操作等下游任务中均优于基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboSpatial addresses the challenge of spatial understanding in vision-language models used for robotics, where existing datasets lack the necessary spatial context for effective reasoning. The dataset includes real-world indoor and tabletop scenes, annotated with rich spatial relationships, and combines 3D scans with egocentric images to support both 2D and 3D reasoning. Experimental results demonstrate that models trained on RoboSpatial achieve superior performance in tasks like spatial affordance prediction, spatial relationship prediction, and robot manipulation compared to existing baselines.</div>
<div class="mono" style="margin-top:8px">RoboSpatial 为解决现有视觉语言模型在机器人领域空间推理能力不足的问题，引入了一个大规模数据集，通过3D扫描和第一视角图像捕捉真实室内外及桌面场景，并标注了与机器人相关的丰富空间信息。该数据集支持2D和3D模型训练，包含100万张图像、5000个3D扫描和300万个标注的空间关系。实验结果表明，使用RoboSpatial训练的模型在空间能力预测、空间关系预测和机器人操作等下游任务中均优于基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis</div>
<div class="meta-line">Authors: Tianwei Lin, Zhongwei Qiu, Wenqiao Zhang, Jiang Liu, Yihan Xie, Mingjian Gao, Zhenxuan Fan, Zhaocheng Li, Sijing Li, Zhongle Xie, Peng LU, Yueting Zhuang, Yingda Xia, Ling Zhang, Beng Chin Ooi</div>
<div class="meta-line">First: 2026-02-18T00:42:41+00:00 · Latest: 2026-02-18T00:42:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16110v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16110v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCT：面向全面CT分析的统一切片-体积大型视觉-语言模型</div>
<div class="mono" style="margin-top:8px">计算机断层扫描（CT）是最广泛使用且诊断信息密集的成像方式之一，涵盖心脏、肺、肝脏和结肠等关键器官。临床解释依赖于切片驱动的局部特征（如亚厘米结节、病灶边界）和体积驱动的空间表示（如肿瘤浸润、器官间解剖关系）。然而，现有的大型视觉-语言模型（LVLMs）在CT切片与体积理解方面仍存在碎片化问题：切片驱动的LVLMs表现出强大的泛化能力，但缺乏跨切片的空间一致性；而体积驱动的LVLMs明确捕捉体积语义，但存在粒度粗糙且与切片输入兼容性差的问题。缺乏统一的建模范式是医学LVLMs临床转化的主要瓶颈。我们提出了OmniCT，一种强大的统一切片-体积LVLM，用于CT场景，其贡献包括：(i) 空间一致性增强（SCE）：结合体积切片组合与三轴位置嵌入，引入体积一致性，并通过MoE混合投影实现高效的切片-体积适配；(ii) 器官级语义增强（OSE）：分割和感兴趣区域（ROI）定位明确对齐解剖区域，强调病灶和器官级语义；(iii) MedEval-CT：最大的切片-体积CT数据集和混合基准测试，整合了全面的评估指标。OmniCT在多种临床任务中显著优于现有方法，同时满足微观细节敏感性和宏观空间推理能力。更重要的是，它为跨模态医学影像理解建立了一个新的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of OmniCT is to address the limitations of existing Large Vision-Language Models (LVLMs) in handling both slice and volumetric CT data effectively. The method introduces a unified slice-volume LVLM with three key components: Spatial Consistency Enhancement (SCE) for improved volumetric understanding through slice composition and positional embeddings, Organ-level Semantic Enhancement (OSE) for precise anatomical alignment and semantic focus, and MedEval-CT, a new large-scale dataset and benchmark for comprehensive evaluation. The model achieves significant performance improvements across various clinical tasks, demonstrating strong capability in both detailed slice analysis and broader volumetric reasoning.</div>
<div class="mono" style="margin-top:8px">OmniCT旨在解决现有大型视觉-语言模型（LVLMs）在处理CT切片和体积数据时的局限性。当前模型要么侧重于切片级特征但缺乏跨切片的空间一致性，要么关注体积级语义但粒度较粗。为此，OmniCT提出了一种统一的切片-体积LVLM，包含三个主要贡献：通过体积组合与三轴位置嵌入提升空间一致性（SCE），以及通过MoE混合投影实现高效的切片-体积适配；在器官级别引入语义增强（OSE），通过分割和感兴趣区域定位来强调病变和器官的语义信息；构建了MedEval-CT，这是目前最大的切片-体积混合CT数据集和评估基准。实验结果表明，OmniCT在多种临床任务中显著优于现有方法，展现出对切片细节和体积级空间推理的优异能力。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-17T22:24:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到政策：为内容感知布局设计增强语言模型的空间推理</div>
<div class="mono" style="margin-top:8px">我们引入了LaySPA，这是一个强化学习框架，为大型语言模型（LLMs）赋予明确且可解释的空间推理能力，以实现内容感知的图形布局设计。LaySPA解决了两个关键挑战：LLMs在空间推理方面的局限性以及设计决策过程中的不透明性。我们不以像素级操作为基础，而是将布局设计重新表述为一个在结构化文本空间环境上的策略学习问题，该环境明确编码了画布几何、元素属性和元素间关系。LaySPA生成包含可解释推理轨迹和结构化布局规范的双层级输出，从而实现透明且可控的设计决策。布局设计策略通过多目标空间批评进行优化，将布局质量分解为几何有效性、关系一致性与审美一致性，并采用相对群体优化进行训练，以稳定开放设计空间中的学习过程。实验表明，LaySPA在结构有效性与视觉质量方面均有提升，优于更大的专有LLMs，并在使用更少标注样本和更低延迟的情况下，其性能与专门的SOTA布局生成器相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces LaySPA, a reinforcement learning framework designed to enhance the spatial reasoning capabilities of large language models (LLMs) for content-aware layout design. The motivation stems from the limitations of LLMs in understanding spatial relationships and the opacity of design decisions. LaySPA reformulates layout design as a policy learning task in a structured textual spatial environment, encoding canvas geometry, element attributes, and inter-element relationships. The framework generates dual-level outputs, including reasoning traces and layout specifications, and optimizes the layout policy using a multi-objective spatial critique that evaluates geometric validity, relational coherence, and aesthetic consistency. Experimental results show that LaySPA improves structural validity and visual quality, surpassing larger proprietary LLMs and matching the performance of specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">本文提出LaySPA，一种强化学习框架，旨在提升大型语言模型（LLMs）在内容感知图形布局设计中的空间推理能力。研究动机源于LLMs在理解空间关系方面的不足以及设计决策过程的不透明性。LaySPA将布局设计问题转化为在结构化文本空间环境中的策略学习任务，该环境显式编码了画布几何、元素属性及元素间关系。框架生成双层级输出，包括可解释的推理轨迹和结构化的布局规范，以实现透明和可控的设计决策。通过多目标空间批评机制，将布局质量分解为几何有效性、关系一致性和审美一致性，并采用相对群体优化方法稳定训练过程。实验结果表明，LaySPA在结构有效性和视觉质量方面表现优异，超越了更大规模的专有LLMs，并在标注样本更少、延迟更低的情况下达到了与专用顶级布局生成器相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families</div>
<div class="meta-line">Authors: Yuval Levental</div>
<div class="meta-line">First: 2026-02-17T19:06:19+00:00 · Latest: 2026-02-17T19:06:19+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, 2 tables. Workshop-length paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15950v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15950v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型能识别正方形吗？文本识别在三种模型家族中中介了空间推理</div>
<div class="mono" style="margin-top:8px">我们提出一个简单的实验，揭示了视觉-语言模型（VLMs）的一个基本限制：当单元格没有文本标识时，无法准确定位二进制网格中的填充单元格。我们生成了十五个15x15的网格，填充单元格密度从10.7%到41.8%不等，并将每个网格渲染为两种图像类型——文本符号（. 和 #）和无网格线的填充正方形。然后我们要求三个前沿的VLMs（Claude Opus、ChatGPT 5.2和Gemini 3 Thinking）进行转录。在文本符号条件下，Claude和ChatGPT的单元格准确率约为91%，F1值为84%，而Gemini的准确率为84%，F1值为63%。在填充正方形条件下，所有三个模型的准确率都下降到60-73%，F1值为29-39%。关键的是，所有条件都通过相同的视觉编码器——文本符号是图像，而非标记化文本。文本与正方形之间的F1差距在模型间从34到54分不等，这表明VLMs似乎具备一种高保真度的文本识别路径，用于空间推理，其表现远超其原生的视觉路径。每个模型在正方形条件下都表现出不同的失败模式——系统性低估（Claude）、严重高估（ChatGPT）以及模板幻觉（Gemini）——但它们都共享相同的根本缺陷：对非文本视觉元素的空间定位严重退化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the spatial reasoning capabilities of vision-language models (VLMs) by testing their ability to transcribe binary grids. The experiment uses two image representations of the same grid: one with text symbols and one with filled squares. While VLMs perform well with text symbols, their accuracy drops significantly when dealing with filled squares, indicating a fundamental limitation in their ability to localize non-textual visual elements. The results show that the F1 scores for filled squares range from 29% to 39%, compared to 63% to 84% for text symbols, highlighting a disparity in their visual processing abilities.</div>
<div class="mono" style="margin-top:8px">本研究通过测试视觉语言模型（VLMs）对二进制网格中填充单元格的转录能力，探讨其空间推理能力。实验采用两种相同的网格视觉表示：一种使用文本符号，另一种使用填充方块。尽管VLMs在文本符号条件下表现良好，但在填充方块条件下准确率显著下降，揭示了其在非文本视觉元素的空间定位上的根本性局限。结果表明，不同模型在文本符号和填充方块条件下的F1分数差异较大，文本符号条件下的表现远高于填充方块条件，突显了VLMs在空间推理中对文本特征的潜在偏好。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Gradients for Cumulative Prospect Theory in Reinforcement Learning</div>
<div class="meta-line">Authors: Olivier Lepel, Anas Barakat</div>
<div class="meta-line">First: 2024-10-03T15:45:39+00:00 · Latest: 2026-02-17T17:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.02605v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.02605v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We derive a policy gradient theorem for Cumulative Prospect Theory (CPT) objectives in finite-horizon Reinforcement Learning (RL), generalizing the standard policy gradient theorem and encompassing distortion-based risk objectives as special cases. Motivated by behavioral economics, CPT combines an asymmetric utility transformation around a reference point with probability distortion. Building on our theorem, we design a first-order policy gradient algorithm for CPT-RL using a Monte Carlo gradient estimator based on order statistics. We establish statistical guarantees for the estimator and prove asymptotic convergence of the resulting algorithm to first-order stationary points of the (generally non-convex) CPT objective. Simulations illustrate qualitative behaviors induced by CPT and compare our first-order approach to existing zeroth-order methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中基于累积前景理论的策略梯度</div>
<div class="mono" style="margin-top:8px">我们在有限时间范围的强化学习（RL）中推导出一个针对累积前景理论（CPT）目标的策略梯度定理，该定理推广了标准的策略梯度定理，并涵盖了基于概率扭曲的风险目标作为特例。受行为经济学启发，CPT结合了围绕参考点的非对称效用变换和概率扭曲。基于我们的定理，我们设计了一个基于顺序统计量的蒙特卡洛梯度估计器的一阶策略梯度算法用于CPT-RL。我们建立了该估计器的统计保证，并证明了所得到的算法在（通常非凸的）CPT目标上渐近收敛于一阶平稳点。仿真展示了CPT引发的定性行为，并将我们的第一阶方法与现有的零阶方法进行了比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of incorporating Cumulative Prospect Theory (CPT) into reinforcement learning by deriving a policy gradient theorem for finite-horizon CPT objectives. The motivation stems from behavioral economics, where CPT models decision-making with asymmetric utility and probability distortion. The authors propose a first-order policy gradient algorithm using a Monte Carlo estimator based on order statistics, and they establish statistical guarantees and prove its asymptotic convergence to stationary points of the non-convex CPT objective. Experimental simulations demonstrate the qualitative behavior of CPT in RL and highlight the advantages of the first-order approach over existing zeroth-order methods.</div>
<div class="mono" style="margin-top:8px">本文旨在将累积前景理论（CPT）应用于强化学习，通过推导有限时间范围内CPT目标的策略梯度定理来解决这一问题。研究动机来源于行为经济学，认为决策者在决策过程中会基于参考点进行不对称效用转换和概率扭曲。作者基于该定理设计了一种一阶策略梯度算法，利用基于顺序统计量的蒙特卡洛梯度估计器，并对估计器提供了统计保证，证明了算法在一般非凸CPT目标下的渐近收敛性。实验结果展示了CPT在强化学习中的定性行为，并表明所提出的一阶方法优于现有的零阶方法。</div>
</details>
</div>
<div class="card">
<div class="title">EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery</div>
<div class="meta-line">Authors: Zelin Xu, Yupu Zhang, Saugat Adhikari, Saiful Islam, Tingsong Xiao, Zibo Liu, Shigang Chen, Da Yan, Zhe Jiang</div>
<div class="meta-line">First: 2026-02-17T06:08:43+00:00 · Latest: 2026-02-17T06:08:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15918v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EarthSpatialBench：评估多模态大语言模型在地球影像上的空间推理能力</div>
<div class="mono" style="margin-top:8px">评估多模态大语言模型（MLLMs）在地球影像上的空间推理能力在计算机视觉领域引起了越来越多的关注，因其对具身AI及其他需要与物理世界进行精确交互的代理系统至关重要。然而，地球影像上的空间推理研究相对滞后，因为其独特性在于需要将对象定位在地理参考图像中，并利用视觉线索和向量几何坐标（如2D边界框、折线和多边形）进行距离、方向和拓扑关系的定量推理。现有的地球影像基准测试主要关注二维空间定位、图像描述和粗略空间关系（如简单的方向或邻近线索），缺乏对定量方向和距离推理、系统性拓扑关系以及超出边界框的复杂对象几何的支持。为填补这一空白，我们提出了\textbf{EarthSpatialBench}，一个全面的基准测试，用于评估多模态大语言模型在地球影像上的空间推理能力。该基准包含超过325,000个问答对，涵盖以下方面：(1) 空间距离和方向的定性和定量推理；(2) 系统性拓扑关系；(3) 单对象查询、对象对查询和组合聚合组查询；(4) 通过文本描述、视觉叠加和显式几何坐标（包括2D边界框、折线和多边形）表达的对象引用。我们在开源和专有模型上进行了广泛的实验，以识别多模态大语言模型在空间推理方面的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to address the lack of comprehensive benchmarks for evaluating spatial reasoning capabilities in multimodal large language models (MLLMs) when dealing with Earth imagery. The proposed EarthSpatialBench introduces a benchmark that covers qualitative and quantitative spatial reasoning, topological relations, and complex object geometries beyond simple bounding boxes. The benchmark includes over 325K question-answer pairs and was used to conduct extensive experiments on both open-source and proprietary models, revealing limitations in their ability to reason about spatial relationships using visual and geometric cues.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决对多模态大语言模型（MLLMs）在地球影像上空间推理能力评估不足的问题，这对于具身AI和需要与物理世界精确交互的代理系统至关重要。为此，提出了EarthSpatialBench，这是一个全面的基准，涵盖空间距离和方向的定性与定量推理、系统性的拓扑关系以及超出简单边界框的复杂物体几何形状。该基准包含超过325,000个问答对，支持文本描述、视觉叠加和显式几何坐标等多种空间查询方式。在开源和专有模型上的广泛实验揭示了当前MLLMs在空间推理方面存在显著局限。</div>
</details>
</div>
<div class="card">
<div class="title">Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery</div>
<div class="meta-line">Authors: Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Abhay Yadav, Ananth Vishnu Bhaskar, Hardik Prajapati, Cheng Peng, Rama Chellappa, Shivkumar Chandrasekaran, B. S. Manjunath</div>
<div class="meta-line">First: 2026-02-16T17:06:54+00:00 · Latest: 2026-02-16T17:06:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14929v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14929v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wrivinder：面向地面图像在卫星影像上地理定位的空间智能方法</div>
<div class="mono" style="margin-top:8px">将地面图像与地理注册的卫星地图对齐对于地图绘制、导航和态势感知至关重要，但在视角差异较大或GPS不可靠的情况下仍具挑战性。我们提出了Wrivinder，这是一个零样本、基于几何的框架，通过聚合多张地面照片来重建一致的3D场景，并将其与上方的卫星影像对齐。Wrivinder结合了SfM重建、3D高斯点云、语义定位以及基于单目深度的度量线索，生成稳定的顶视渲染，可直接与卫星上下文匹配，实现度量准确的相机地理定位。为支持该任务的系统性评估（目前缺乏合适的基准数据集），我们还发布了MC-Sat，这是一个经过整理的数据集，连接了多视角地面图像与地理注册的卫星瓦片，涵盖多种户外环境。Wrivinder与MC-Sat共同提供了首个全面的基线和测试平台，用于研究无需配对监督的几何中心跨视角对齐。在零样本实验中，Wrivinder在密集和大范围场景中均实现了低于30米的地理定位精度，突显了基于几何的聚合方法在稳健地面到卫星定位中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of aligning ground-level images with satellite maps in scenarios with large viewpoint differences or unreliable GPS. Wrivinder, a zero-shot, geometry-driven framework, is introduced to reconstruct a consistent 3D scene from multiple ground photographs and align it with overhead satellite imagery. It integrates SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth-based metric cues to generate a stable zenith-view rendering for accurate camera geo-localization. The main experimental results show that Wrivinder achieves sub-30 meter geolocation accuracy in both dense and large-area scenes, demonstrating the effectiveness of geometry-based aggregation for robust ground-to-satellite alignment.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在大视角差异或GPS不可靠情况下，地面图像与卫星地图对齐的挑战。提出了一种零样本、基于几何的框架Wrivinder，通过多张地面照片重建一致的3D场景并将其与俯视卫星图像对齐。该方法结合了SfM重建、3D高斯点云、语义定位和单目深度度量线索，生成稳定的顶视渲染以实现精确的相机地理定位。在新发布的MC-Sat数据集上的实验结果表明，Wrivinder在密集和大面积场景中均实现了低于30米的地理定位精度，展示了基于几何的聚合方法在地面到卫星定位中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Haichao Zhu, Zhaorui Yang, Qian Zhang</div>
<div class="meta-line">First: 2026-02-16T02:26:59+00:00 · Latest: 2026-02-16T02:26:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14409v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14409v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习提出，几何处理：一种高效的模块化空间推理框架</div>
<div class="mono" style="margin-top:8px">空间感知旨在从视觉观测中估计相机运动和场景结构，这一问题传统上通过几何建模和物理一致性约束来解决。近年来，基于学习的方法在几何感知中展现出强大的表示能力，并在实践中越来越多地用于增强以几何为中心的经典系统。然而，学习组件是否应直接替代几何估计，还是应作为此类流程中的中间模块，仍是一个开放性问题。
在本工作中，我们针对这一问题，研究了一种端到端的模块化框架，以实现有效的空间推理，其中学习部分提出几何假设，而几何算法则处理估计决策。具体而言，我们在RGB-D序列的相对相机姿态估计背景下研究了这一原则。使用VGGT作为代表性学习模型，我们在不同运动幅度和场景动态下评估了基于学习的姿态和深度提案，并随后采用经典点对平面RGB-D ICP作为几何后端。我们在TUM RGB-D基准上的实验揭示了三个一致的发现：(1) 仅依靠基于学习的姿态提案是不可靠的；(2) 当学习提出的几何未正确对齐相机内参时，性能会下降；(3) 当学习提出的深度在几何上对齐，并随后进行几何处理阶段时，在中等挑战性的刚性场景中会出现一致的性能提升。
这些结果表明，几何不仅仅是优化组件，而是验证和吸收基于学习的几何观测的关键仲裁者。我们的研究强调了模块化、几何感知系统设计在实现稳健空间感知中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of learning-based components in spatial perception systems, specifically focusing on whether they should replace or complement geometric estimation. The authors propose a modular framework where learning models generate geometric hypotheses, and classical geometric algorithms are used to refine and validate these proposals. Through experiments on the TUM RGB-D benchmark, they find that learning-based pose proposals alone are unreliable, improper alignment of learned geometry with camera parameters can reduce performance, but when combined with a geometric disposal stage, significant improvements are achieved in rigid environments. These findings emphasize the necessity of integrating geometric validation into learning-based spatial reasoning pipelines.</div>
<div class="mono" style="margin-top:8px">本文探讨了学习组件在空间推理系统中的作用，特别是在从RGB-D序列中估计相对相机姿态的问题上。研究提出了一种模块化框架，其中学习模型生成几何假设，而经典几何算法用于细化和验证这些假设。在TUM RGB-D基准上的实验结果表明，仅依靠学习生成的姿态估计是不可靠的，若学习得到的几何信息与相机内参对齐不当，可能降低性能；但在适当对齐并结合几何处理阶段的情况下，学习生成的深度信息在中等挑战性的刚性场景中表现出一致的提升。这些结果强调了在学习基础上引入几何验证的重要性，以实现稳健的空间感知。</div>
</details>
</div>
<div class="card">
<div class="title">AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines</div>
<div class="meta-line">Authors: Yifan Wu, Yiran Peng, Yiyu Chen, Jianhao Ruan, Zijie Zhuang, Cheng Yang, Jiayi Zhang, Man Chen, Yenchi Tseng, Zhaoyang Yu, Liang Chen, Yuyao Zhai, Bang Liu, Chenglin Wu, Yuyu Luo</div>
<div class="meta-line">First: 2026-02-15T20:03:19+00:00 · Latest: 2026-02-15T20:03:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14296v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoWebWorld: 通过有限状态机合成无限可验证的网络环境</div>
<div class="mono" style="margin-top:8px">自主Web GUI代理的性能严重依赖于其训练数据的质量和数量。然而，一个根本性的瓶颈依然存在：从真实网站中收集交互轨迹既昂贵又难以验证。底层状态转换是隐藏的，导致需要依赖不一致且成本高昂的外部验证器来评估每一步的正确性。为了解决这一问题，我们提出了AutoWebWorld，这是一个新颖的框架，通过将网络环境建模为有限状态机（FSMs）来合成可控且可验证的网络环境，并利用编码代理将FSMs转换为可交互的网站。与真实网站中隐式的状态转换不同，AutoWebWorld明确定义了所有状态、动作和转换规则。这使得可以进行程序化验证：动作的正确性通过预定义的规则进行检查，任务成功则通过在FSM图中达到目标状态来确认。AutoWebWorld实现了完全自动化的搜索与验证流程，仅以每轨迹0.04美元的成本，从29个多样化的网络环境中生成超过11,663条验证轨迹。在这些合成数据上进行训练显著提升了实际应用中的性能。我们的7B参数Web GUI代理在WebVoyager上仅需15步就超越了所有基线。此外，我们观察到一个清晰的扩展定律：随着合成数据量的增加，AutoWebWorld在WebVoyager和Online-Mind2Web上的性能持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of AutoWebWorld is to overcome the challenges of collecting and verifying interaction trajectories for training autonomous Web GUI agents. The framework models web environments as Finite State Machines (FSMs), explicitly defining states, actions, and transitions, which allows for programmatic verification. By using coding agents to translate FSMs into interactive websites, AutoWebWorld generates a large number of verified trajectories at a low cost, achieving over 11,663 verified trajectories from 29 diverse environments for $0.04 per trajectory. Training on this synthetic data significantly improves the performance of real-world agents, with the 7B Web GUI agent outperforming all baselines within 15 steps on WebVoyager. The results also demonstrate a clear scaling law, where increased synthetic data volume leads to consistent performance improvements on WebVoyager and Online-Mind2Web.</div>
<div class="mono" style="margin-top:8px">AutoWebWorld的动机是解决自主Web GUI代理训练中交互轨迹收集与验证的困难。该框架通过将Web环境建模为有限状态机（FSMs），显式定义状态、动作和转移规则，从而实现代理行为的程序化验证。利用编码代理将FSMs转换为可交互的网站，AutoWebWorld以低成本生成大量可验证的轨迹，显著提升了真实环境中的代理性能。实验结果表明，基于AutoWebWorld合成数据训练的7B参数Web GUI代理在WebVoyager任务中15步内优于所有基线模型，并且随着合成数据量的增加，其在WebVoyager和Online-Mind2Web任务中的表现也持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-15T19:48:43+00:00 · Latest: 2026-02-15T19:48:43+00:00</div>
<div class="meta-line">Comments: 15 pages, 33 pages with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KernelBlaster: 通过记忆增强的上下文强化学习实现持续跨任务CUDA优化</div>
<div class="mono" style="margin-top:8px">在多代GPU架构上优化CUDA代码具有挑战性，因为实现最佳性能需要在日益复杂且特定于硬件的优化空间中进行广泛探索。传统编译器受限于固定的启发式方法，而对大型语言模型（LLMs）进行微调成本较高。然而，CUDA代码优化的代理工作流在整合先前探索的知识方面能力有限，导致采样偏差和次优解。我们提出了KernelBlaster，这是一个基于记忆增强的上下文强化学习（MAIC-RL）框架，旨在提升基于LLM的GPU编码代理的CUDA优化搜索能力。KernelBlaster通过将知识积累到可检索的持久CUDA知识库中，使代理能够从经验中学习，并在未来的任务中做出系统性的决策。我们提出了一种新颖的基于配置文件引导和文本梯度的代理流程，用于CUDA生成和优化，以在多代GPU架构上实现高性能。KernelBlaster引导LLM代理系统地探索超越简单重写的高潜力优化策略。与PyTorch基线相比，我们的方法在KernelBench Level 1、2、3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。我们发布了KernelBlaster作为一个开源代理框架，配套测试框架、验证组件和可复现的评估流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of KernelBlaster is to address the challenges of optimizing CUDA code across different generations of GPU architectures, where traditional methods and fixed heuristics fall short. The framework introduces a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) approach that allows LLM-based agents to accumulate and retrieve optimization knowledge from prior tasks, improving their decision-making process. Experimental results show that KernelBlaster achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively, compared to the PyTorch baseline.</div>
<div class="mono" style="margin-top:8px">KernelBlaster的研究动机是解决在不同GPU架构上优化CUDA代码的挑战，克服传统编译器和LLM微调方法的局限性。该方法采用记忆增强的上下文强化学习（MAIC-RL）框架，通过构建可检索的持久CUDA知识库，使智能体能够积累经验并做出系统性的决策。KernelBlaster提出了一种基于配置文件和文本梯度的智能体流程，用于指导CUDA代码生成与优化，从而实现跨多代GPU架构的高性能。实验结果表明，KernelBlaster在KernelBench Levels 1、2和3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速，优于PyTorch基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement</div>
<div class="meta-line">Authors: Xiaojun Jia, Jie Liao, Simeng Qin, Jindong Gu, Wenqi Ren, Xiaochun Cao, Yang Liu, Philip Torr</div>
<div class="meta-line">First: 2026-02-15T16:09:48+00:00 · Latest: 2026-02-15T16:09:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14211v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkillJect：面向编码代理的基于技能的隐蔽提示注入自动化框架</div>
<div class="mono" style="margin-top:8px">代理技能正逐渐成为编码代理的核心抽象，通过封装长文本指令和辅助脚本扩展工具增强行为。这种抽象引入了一个未被充分衡量的攻击面：基于技能的提示注入，其中被污染的技能可能引导代理偏离用户的意图和安全策略。在实践中，简单的注入往往失败，因为恶意意图过于明显或偏离原始技能，导致代理忽略或拒绝这些注入；现有的攻击方法也大多是手工构建的。我们提出了首个针对代理技能的隐蔽提示注入自动化框架。该框架与三个代理形成闭环：一个攻击代理在显式的隐蔽约束下合成注入技能，一个代码代理在一个现实的工具环境中使用注入技能执行任务，一个评估代理记录操作轨迹（如工具调用和文件操作）并验证是否发生了目标恶意行为。我们还提出了一种恶意负载隐藏策略，将对抗性操作隐藏在辅助脚本中，同时注入优化的诱导提示以触发工具执行。在多种编码代理设置和现实软件工程任务上的广泛实验表明，我们的方法在现实环境中能够持续实现高攻击成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of skill-based abstractions in coding agents creates a new vulnerability, as these skills can be manipulated through prompt injection to deviate from user intent and safety policies. To address this, the authors introduce SkillJect, an automated framework for stealthy skill-based prompt injection. The framework employs a closed-loop system involving three agents: an Attack Agent that generates injection skills under stealth constraints, a Code Agent that executes tasks in a realistic environment, and an Evaluate Agent that monitors and verifies malicious behaviors. Experimental results across various coding-agent scenarios and real-world software engineering tasks demonstrate that SkillJect achieves high attack success rates while maintaining stealth.</div>
<div class="mono" style="margin-top:8px">随着编码代理中技能抽象的广泛应用，新的安全漏洞也随之产生，这些技能可能被利用进行提示注入攻击，从而偏离用户意图和安全策略。为此，作者提出了SkillJect，一个专门用于技能型提示注入的自动化框架。该框架通过三个代理形成闭环系统：攻击代理在隐秘性约束下生成注入技能，代码代理在真实环境中执行任务，评估代理则记录行为轨迹并验证是否发生恶意操作。在多种编码代理场景和实际软件工程任务中的实验表明，SkillJect在保持隐秘性的同时实现了高攻击成功率。</div>
</details>
</div>
<div class="card">
<div class="title">EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming</div>
<div class="meta-line">Authors: Sen Fang, Weiyuan Ding, Mengshi Zhang, Zihao Chen, Bowen Xu</div>
<div class="meta-line">First: 2025-05-18T01:02:33+00:00 · Latest: 2026-02-15T05:28:25+00:00</div>
<div class="meta-line">Comments: 27 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12185v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.12185v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI&#x27;s o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVALOOOP：一种以自一致性为中心的框架，用于评估大型语言模型在编程中的鲁棒性</div>
<div class="mono" style="margin-top:8px">评估大型语言模型（LLMs）在编程中的鲁棒性对于确保其在基于人工智能的软件开发中的可靠性至关重要。然而，对抗性攻击存在根本性局限，影响了公平的鲁棒性评估：它们在不同攻击策略下会产生矛盾的评估结果，并且更关键的是，它们仅通过外部扰动进行操作，无法捕捉到自主编码代理在内部生成后续输入时所必需的内在稳定性。我们引入了EVALOOOP，这是一种新颖的评估框架，从自一致性角度评估鲁棒性，利用软件工程任务中固有的自然二元性（例如代码生成和代码摘要）。EVALOOOP建立了一个自包含的反馈循环，其中LLM在代码和自然语言之间反复转换，直至功能失效，鲁棒性通过一种新的平均可持续循环（ASL）指标进行量化——即在基准任务中保持功能正确性的平均迭代次数。这种循环策略无需依赖外部攻击配置，内在地评估鲁棒性，提供了一个统一的指标，揭示了LLM在持续自指转换中如何有效保持语义完整性。我们在EVALOOOP上评估了96个流行的LLM，参数范围从0.5B到685B，并发现EVALOOOP通常在十次循环内导致pass@1准确率下降2.65%-47.62%。有趣的是，鲁棒性并不总是与初始性能（即单次查询）一致；例如，Qwen3-235B-A22B-Instruct-2507，尽管其初始代码生成能力不如OpenAI的o系列模型和DeepSeek-V3，但其鲁棒性（ASL得分）却更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to address the limitations of adversarial attacks in assessing the robustness of large language models (LLMs) for programming tasks, as these attacks fail to capture intrinsic model stability. EVALOOOP introduces a self-consistency-based framework that evaluates robustness through a feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs. The framework uses the Average Sustainable Loops (ASL) metric to quantify robustness, measuring the mean number of iterations maintaining correctness. Experimental results on 96 LLMs with the MBPP Plus benchmark show that EVALOOOP typically causes a 2.65%-47.62% drop in pass@1 accuracy within ten loops, highlighting that robustness does not always correlate with initial performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种更可靠的方法来评估大型语言模型（LLMs）在编程任务中的鲁棒性，因为传统的对抗性攻击无法捕捉模型的内在稳定性。EVALOOOP提出了一种基于自一致性框架的新评估方法，通过代码与自然语言之间的迭代转换来模拟自主编码代理的自指过程。该框架引入了一个新的度量指标——平均可持续循环数（ASL），用于衡量模型在保持功能正确性方面的持续能力。在对96个不同参数规模的LLMs进行实验后发现，EVALOOOP通常会在十次循环内使pass@1准确率下降2.65%-47.62%，表明鲁棒性并不总是与初始性能一致，例如Qwen3-235B-A22B-Instruct-2507虽然在初始代码生成方面不如OpenAI的o系列模型和DeepSeek-V3，但在ASL评分上表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Xiaoyu Li, Yitao Wu, Xian Wu, Haolin Zhuo, Lijun Zhao, Lining Sun</div>
<div class="meta-line">First: 2026-02-14T13:34:21+00:00 · Latest: 2026-02-14T13:34:21+00:00</div>
<div class="meta-line">Comments: Based on this work, we achieved 1st place on the KITTI tracking leaderboard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13772v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13772v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Offline-Poly：面向离线三维多目标跟踪的多面体框架</div>
<div class="mono" style="margin-top:8px">离线三维多目标跟踪（MOT）是4D自动标注（4DAL）流程中的关键组成部分。它通过引入时序上下文来增强高性能检测器生成的伪标签。然而，现有的离线三维MOT方法是在线框架的直接扩展，未能充分利用离线设置的优势。此外，这些方法通常依赖于固定的上游模块和定制化架构，限制了其适应性。为了解决这些局限性，我们提出了Offline-Poly，一种基于以跟踪为中心设计的通用离线三维MOT方法。我们引入了一种标准化范式，称为Tracking-by-Tracking（TBT），该范式仅基于任意现成的跟踪输出，并生成离线优化的轨迹片段。这种设计将离线跟踪器与特定的上游检测器或跟踪器解耦。在TBT范式下，Offline-Poly接受一个或多个粗略的跟踪结果，并通过包含预处理、分层匹配与融合以及轨迹片段优化的结构化流程进行处理。每个模块都旨在利用离线跟踪的两个基本特性：资源不受限性，允许在实时限制之外进行全局优化；以及未来可观测性，使得可以在完整的时间范围内进行轨迹推理。Offline-Poly首先利用全局场景上下文消除短期鬼轨迹，并通过重新识别碎片化片段进行轨迹恢复。然后，它通过构建场景级相似性，将多个输入源的轨迹片段进行关联。最后，Offline-Poly通过联合利用局部和全局运动模式对轨迹片段进行优化。在nuScenes数据集上，我们实现了77.6%的AMOTA最优性能；在KITTI数据集上，取得了83.00%的HOTA领先结果。全面的实验进一步验证了Offline-Poly的灵活性、通用性和模块有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Offline-Poly is proposed to address the limitations of existing offline 3D multi-object tracking methods, which are often direct extensions of online frameworks and lack adaptability. The method introduces a tracking-centric approach called Tracking-by-Tracking (TBT), which operates on arbitrary off-the-shelf tracking outputs to generate refined tracklets. Offline-Poly utilizes a structured pipeline involving pre-processing, hierarchical matching and fusion, and tracklet refinement, leveraging the global optimization and full temporal observability inherent in offline settings. It achieves state-of-the-art performance on nuScenes with 77.6% AMOTA and leading results on KITTI with 83.00% HOTA, demonstrating its flexibility and effectiveness.</div>
<div class="mono" style="margin-top:8px">Offline-Poly 是为了解决现有离线 3D 多目标跟踪方法的局限性而提出的，这些方法通常结构固定且缺乏适应性。该方法引入了一个以跟踪为中心的框架，称为 Tracking-by-Tracking (TBT)，它基于任意现成的跟踪输出生成优化后的轨迹片段，而不依赖特定的上游检测器。通过利用离线设置中的全局优化和未来可观测性，Offline-Poly 实现了预处理、分层匹配与融合以及轨迹片段优化等步骤，从而提升跟踪精度。在 nuScenes 数据集上，它取得了 77.6% AMOTA 的最先进性能，在 KITTI 数据集上达到了 83.00% HOTA 的领先结果，验证了其灵活性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird&#x27;s-Eye View with Deformable Attention and Sparse Goal Proposals</div>
<div class="meta-line">Authors: Minsang Kong, Myeongjun Kim, Sang Gu Kang, Hejiu Lu, Yupeng Zhong, Sang Hun Lee</div>
<div class="meta-line">First: 2025-09-12T09:17:54+00:00 · Latest: 2026-02-14T08:37:57+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Intelligent Transportation Systems (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10080v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10080v2">PDF</a> · <a href="https://github.com/Kongminsang/bevtraj">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In autonomous driving, trajectory prediction is essential for safe and efficient navigation. While recent methods often rely on high-definition (HD) maps to provide structured environmental priors, such maps are costly to maintain, geographically limited, and unreliable in dynamic or unmapped scenarios. Directly leveraging raw sensor data in Bird&#x27;s-Eye View (BEV) space offers greater flexibility, but BEV features are dense and unstructured, making agent-centric spatial reasoning challenging and computationally inefficient. To address this, we propose Bird&#x27;s-Eye View Trajectory Prediction (BEVTraj), a map-free framework that employs deformable attention to adaptively aggregate task-relevant context from sparse locations in dense BEV features. We further introduce a Sparse Goal Candidate Proposal (SGCP) module that predicts a small set of realistic goals, enabling fully end-to-end multimodal forecasting without heuristic post-processing. Extensive experiments show that BEVTraj achieves performance comparable to state-of-the-art HD map-based methods while providing greater robustness and flexibility without relying on pre-built maps. The source code is available at https://github.com/Kongminsang/bevtraj.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BEVTraj: 无需地图的鸟瞰视角端到端轨迹预测方法，采用可变形注意力与稀疏目标提案</div>
<div class="mono" style="margin-top:8px">在自动驾驶中，轨迹预测对于安全和高效导航至关重要。尽管近期方法常依赖高精度（HD）地图来提供结构化的环境先验知识，但此类地图维护成本高、地理范围有限，并且在动态或未映射场景中不可靠。直接利用鸟瞰视角（BEV）空间中的原始传感器数据提供了更大的灵活性，但BEV特征密集且无结构，使得以智能体为中心的空间推理具有挑战性且计算效率低下。为了解决这一问题，我们提出了鸟瞰视角轨迹预测（BEVTraj），这是一种无需地图的框架，通过可变形注意力机制从密集的BEV特征中自适应地聚合任务相关的上下文信息。我们进一步引入了稀疏目标候选提案（SGCP）模块，用于预测一组现实的目标，从而实现完全端到端的多模态预测，无需启发式后处理。大量实验表明，BEVTraj在性能上与基于HD地图的最先进方法相当，同时在不依赖预构建地图的情况下提供了更高的鲁棒性和灵活性。源代码可在 https://github.com/Kongminsang/bevtraj 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trajectory prediction is crucial for autonomous driving systems to navigate safely and efficiently. Traditional approaches often depend on high-definition maps, which are expensive, limited in scope, and unreliable in dynamic environments. BEVTraj introduces a map-free framework that uses deformable attention to selectively gather relevant context from dense Bird&#x27;s-Eye View features, enabling more flexible and efficient spatial reasoning. It also incorporates a Sparse Goal Candidate Proposal module to generate a small set of realistic goals, supporting end-to-end multimodal forecasting. Experimental results demonstrate that BEVTraj matches the performance of HD map-based methods while being more robust and adaptable in scenarios where maps are unavailable or unreliable.</div>
<div class="mono" style="margin-top:8px">轨迹预测对于自动驾驶系统安全高效地行驶至关重要。传统方法通常依赖高精地图提供结构化的环境先验信息，但这类地图维护成本高、地理范围有限且在动态或未标注场景中不可靠。BEVTraj 提出了一种无需地图的框架，利用可变形注意力机制从密集的鸟瞰图特征中选择性提取任务相关的上下文信息，从而实现更灵活高效的空问推理。同时，该框架引入了稀疏目标候选提案模块，生成少量现实可行的目标，支持端到端的多模态预测。实验结果表明，BEVTraj 在无需地图的情况下，其性能可与基于高精地图的方法相媲美，并展现出更强的鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories</div>
<div class="meta-line">Authors: Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen</div>
<div class="meta-line">First: 2025-04-29T22:22:44+00:00 · Latest: 2026-02-14T01:32:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21205v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.21205v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 29 standalone LLMs and 15 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SecRepoBench：在现实仓库中评估代码代理的安全代码补全基准</div>
<div class="mono" style="margin-top:8px">本文介绍了SecRepoBench，这是一个用于评估代码代理在现实仓库中进行安全代码补全的基准。SecRepoBench包含27个C/C++仓库中的318个代码补全任务，涵盖15种CWE漏洞。我们使用该基准评估了29个独立的LLMs和15个代码代理。我们发现，最先进的LLMs在生成正确且安全的代码补全方面存在困难。然而，代码代理显著优于独立LLMs。我们展示了SecRepoBench比之前的先进基准更具挑战性。最后，我们的全面分析为提升代码代理在现实仓库中编写正确且安全代码的能力提供了潜在方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents SecRepoBench, a benchmark designed to assess code agents&#x27; performance in secure code completion within real-world repositories. The benchmark includes 318 tasks across 27 C/C++ repositories, addressing 15 common software vulnerabilities. Evaluations of 29 standalone large language models (LLMs) and 15 code agents using three state-of-the-art agent frameworks reveal that while LLMs struggle with generating secure and correct code, code agents significantly outperform them. The results also indicate that SecRepoBench is more challenging than existing benchmarks, highlighting the need for improved code agent capabilities in real-world settings.</div>
<div class="mono" style="margin-top:8px">本文提出了SecRepoBench，这是一个用于评估代码代理在真实仓库中生成安全代码补全能力的基准测试。该基准包含27个C/C++仓库中的318个代码补全任务，涵盖15种常见的软件漏洞（CWE）。通过评估29个独立的大型语言模型（LLMs）和15个代码代理，使用三种最先进的代理框架，研究发现尽管先进LLMs在生成正确且安全的代码方面表现不佳，但代码代理显著优于独立模型。结果表明，SecRepoBench比现有基准更具挑战性，突显了提升代码代理在真实仓库中生成安全代码能力的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Spear-Phishing Website Generation in Large Language Model Coding Agents</div>
<div class="meta-line">Authors: Tailia Malloy, Tegawende F. Bissyande</div>
<div class="meta-line">First: 2026-02-13T12:12:53+00:00 · Latest: 2026-02-13T12:12:53+00:00</div>
<div class="meta-line">Comments: 18 Pages, 7 Figures, 1 Table. Accepted to the conference Human Computer Interaction International</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are expanding beyond being a tool humans use and into independent agents that can observe an environment, reason about solutions to problems, make changes that impact those environments, and understand how their actions impacted their environment. One of the most common applications of these LLM Agents is in computer programming, where agents can successfully work alongside humans to generate code while controlling programming environments or networking systems. However, with the increasing ability and complexity of these agents comes dangers about the potential for their misuse. A concerning application of LLM agents is in the domain cybersecurity, where they have the potential to greatly expand the threat imposed by attacks such as social engineering. This is due to the fact that LLM Agents can work autonomously and perform many tasks that would normally require time and effort from skilled human programmers. While this threat is concerning, little attention has been given to assessments of the capabilities of LLM coding agents in generating code for social engineering attacks. In this work we compare different LLMs in their ability and willingness to produce potentially dangerous code bases that could be misused by cyberattackers. The result is a dataset of 200 website code bases and logs from 40 different LLM coding agents. Analysis of models shows which metrics of LLMs are more and less correlated with performance in generating spear-phishing sites. Our analysis and the dataset we present will be of interest to researchers and practitioners concerned in defending against the potential misuse of LLMs in spear-phishing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型编码代理在钓鱼网站生成中的能力</div>
<div class="mono" style="margin-top:8px">大型语言模型正在从人类使用的工具扩展为能够观察环境、推理解决问题、对环境产生影响并理解自身行为影响的独立代理。这些LLM代理最常见的应用之一是计算机编程，它们可以与人类协同工作，生成代码并控制编程环境或网络系统。然而，随着这些代理的能力和复杂性不断增加，其被滥用的潜在风险也随之而来。在网络安全领域，LLM代理的一个令人担忧的应用是它们可能大大扩展社会工程攻击的威胁。这是因为LLM代理可以自主工作，并执行通常需要熟练程序员投入大量时间和精力的任务。尽管这种威胁令人担忧，但对LLM编码代理在生成社会工程攻击代码方面的能力评估却很少受到关注。在本研究中，我们比较了不同LLM在生成可能被网络攻击者滥用的危险代码库方面的能力和意愿。研究结果是一个包含200个网站代码库和来自40个不同LLM编码代理的日志数据集。模型分析展示了哪些LLM指标与生成定向钓鱼网站的性能更为或更不相关。我们的分析和所呈现的数据集将对关注如何防范LLM在定向钓鱼中被滥用的研究人员和实践者具有参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential risks associated with Large Language Model (LLM) coding agents in the context of cybersecurity, specifically focusing on their ability to generate spear-phishing websites. The motivation stems from the growing concern about the misuse of LLMs in creating sophisticated social engineering attacks. The research compares different LLMs to assess their capability and willingness to produce harmful code bases. The main experimental results include a dataset of 200 website code bases and logs from 40 LLM coding agents, which reveal correlations between specific LLM metrics and their effectiveness in generating spear-phishing sites.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在生成钓鱼网站方面的潜在滥用风险，这是网络安全领域的重要关注点。通过比较不同LLMs生成恶意代码库的能力和意愿，研究创建了一个包含200个网站代码库和40个LLM编码代理日志的数据集，分析了哪些模型指标与生成定向钓鱼网站的效果最为相关。研究结果突显了LLMs在网络安全中的潜在威胁，并为防范此类滥用提供了有价值的参考。</div>
</details>
</div>
<div class="card">
<div class="title">3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset</div>
<div class="meta-line">Authors: Mehran Advand, Zahra Dehghanian, Navid Faraji, Reza Barati, Seyed Amir Ahmad Safavi-Naini, Hamid R. Rabiee</div>
<div class="meta-line">First: 2026-02-13T11:08:15+00:00 · Latest: 2026-02-13T11:08:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mehrn79.github.io/3DLAND">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DLAND：腹部异常病变三维定位数据集</div>
<div class="mono" style="margin-top:8px">现有的腹部CT医学影像数据集通常缺乏三维标注、多器官覆盖或精确的病灶-器官关联，阻碍了稳健的表示学习和临床应用。为解决这一问题，我们引入了3DLAND，这是一个大规模基准数据集，包含超过6,000个增强CT体数据，具有超过20,000个高保真度的三维病灶标注，涵盖七个腹部器官：肝脏、肾脏、胰腺、脾脏、胃和胆囊。我们的三阶段简化流程集成了自动化空间推理、优化提示的2D分割以及记忆引导的3D传播，经专家放射科医生验证，表面Dice评分超过0.75。通过提供多样化的病灶类型和患者人口统计信息，3DLAND支持对医学AI异常检测、定位和跨器官迁移学习的可扩展评估。我们的数据集为评估器官感知的三维分割模型设立了新的基准，为面向医疗的AI发展铺平了道路。为促进可重复性和进一步研究，3DLAND数据集及实现代码已公开在https://mehrn79.github.io/3DLAND。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing abdominal CT datasets, which often lack comprehensive 3D annotations, multi-organ coverage, and precise lesion-to-organ associations. The authors introduce 3DLAND, a large-scale dataset containing over 6,000 contrast-enhanced CT volumes with more than 20,000 high-fidelity 3D lesion annotations across seven abdominal organs. The dataset is constructed using a three-phase pipeline that combines automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores above 0.75. This work provides a valuable resource for evaluating anomaly detection, localization, and cross-organ transfer learning in medical AI.</div>
<div class="mono" style="margin-top:8px">本研究提出3DLAND数据集，旨在解决现有腹部CT数据集中缺乏三维标注、多器官覆盖以及精确病灶-器官关联的问题。该数据集包含超过6000例增强CT影像，覆盖肝脏、肾脏、胰腺、脾脏、胃和胆囊等七个腹部器官，共提供超过20000个高保真度的三维病灶标注。其核心方法采用三阶段流程，结合自动化空间推理、提示优化的2D分割和记忆引导的3D传播，经放射科专家验证，表面Dice分数超过0.75。实验结果表明，3DLAND支持病灶检测、定位及跨器官迁移学习的可扩展评估，为器官感知的3D分割模型提供了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Easy-Poly: An Easy Polyhedral Framework For 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Peng Zhang, Xin Li, Xin Lin, Liang He</div>
<div class="meta-line">First: 2025-02-25T04:01:25+00:00 · Latest: 2026-02-13T08:05:15+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17822v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.17822v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent 3D multi-object tracking (3D MOT) methods mainly follow tracking-by-detection pipelines, but often suffer from high false positives, missed detections, and identity switches, especially in crowded and small-object scenarios. To address these challenges, we propose Easy-Poly, a filter-based 3D MOT framework with four key innovations: (1) CNMSMM, a novel Camera-LiDAR fusion detection method combining multi-modal augmentation and an efficient NMS with a new loss function to improve small target detection; (2) Dynamic Track-Oriented (DTO) data association that robustly handles uncertainties and occlusions via class-aware optimal assignment and parallel processing strategies; (3) Dynamic Motion Modeling (DMM) using a confidence-weighted Kalman filter with adaptive noise covariance to enhance tracking accuracy; and (4) an extended life-cycle management system reducing identity switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 65.65% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 75.6%), while also running in real-time. Our framework advances robustness and adaptability in complex driving environments, paving the way for safer autonomous driving perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Easy-Poly：一种用于3D多目标跟踪的简单多面体框架</div>
<div class="mono" style="margin-top:8px">近期的3D多目标跟踪(3D MOT)方法主要采用基于检测的跟踪流程，但在密集和小目标场景中常面临高误检率、漏检和身份切换的问题。为解决这些挑战，我们提出了Easy-Poly，一个基于滤波的3D MOT框架，包含四项关键创新：(1) CNMSMM，一种结合多模态增强和高效NMS的新损失函数的相机-激光雷达融合检测方法，以提升小目标检测性能；(2) 动态目标导向(DTO)数据关联，通过类感知最优分配和平行处理策略，鲁棒地处理不确定性与遮挡；(3) 动态运动建模(DMM)，使用带有自适应噪声协方差的置信度加权卡尔曼滤波，以提高跟踪精度；(4) 扩展生命周期管理系统，减少身份切换和误终止。实验结果表明，Easy-Poly在mAP（如LargeKernel3D中从63.30%提升至65.65%）和AMOTA（如从73.1%提升至75.6%）指标上优于Poly-MOT和Fast-Poly等先进方法，同时实现实时运行。我们的框架提升了复杂驾驶环境下的鲁棒性和适应性，为更安全的自动驾驶感知铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the performance of 3D multi-object tracking in challenging scenarios such as crowded environments and small-object detection. Easy-Poly introduces a filter-based framework with four key innovations: a novel Camera-LiDAR fusion detection method (CNMSMM) that enhances small target detection through multi-modal augmentation and a new loss function; Dynamic Track-Oriented data association that manages occlusions and uncertainties with class-aware assignment and parallel processing; Dynamic Motion Modeling using a confidence-weighted Kalman filter with adaptive noise covariance; and an extended life-cycle management system to reduce identity switches and false terminations. Experimental results demonstrate that Easy-Poly outperforms existing methods like Poly-MOT and Fast-Poly, achieving higher mAP and AMOTA scores while maintaining real-time performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升在拥挤场景和小目标检测等挑战性环境下的3D多目标跟踪性能。Easy-Poly提出了一种基于滤波器的框架，包含四项关键创新：一种结合多模态增强和高效NMS的新Camera-LiDAR融合检测方法（CNMSMM），用于提升小目标检测效果；动态目标导向（DTO）数据关联方法，通过类感知最优分配和平行处理策略，有效应对不确定性与遮挡问题；动态运动建模（DMM）采用带有自适应噪声协方差的置信度加权卡尔曼滤波，以提高跟踪精度；以及扩展的生命周期管理系统，减少身份切换和错误终止。实验结果表明，Easy-Poly在mAP和AMOTA指标上优于Poly-MOT和Fast-Poly等现有方法，同时保持实时运行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models</div>
<div class="meta-line">Authors: Joshua Ong Jun Leang, Yu Zhao, Mihaela Cătălina Stoian, Wenda Li, Shay B. Cohen, Eleonora Giunchiglia</div>
<div class="meta-line">First: 2026-02-13T03:56:22+00:00 · Latest: 2026-02-13T03:56:22+00:00</div>
<div class="meta-line">Comments: 8 pages, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12586v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我可以点单吗？扩散语言模型中用于槽填充顺序的蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">尽管在掩码扩散模型（MDMs）中，计划-填充解码在数学和代码推理方面表现出潜力，但性能仍高度依赖于槽填充顺序，常常导致输出方差显著。我们引入了McDiffuSE框架，将槽选择建模为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。McDiffuSE利用前瞻模拟在承诺之前评估部分完成情况，系统地探索生成顺序的组合空间。实验表明，与自回归基线相比，平均提升3.2%，与基线计划-填充方法相比提升8.0%，在MBPP和MATH500上分别取得19.5%和4.9%的显著提升。我们的分析表明，虽然McDiffuSE主要遵循顺序填充，但引入非顺序生成对于最大化性能是必要的。我们观察到，为了克服模型置信度偏差并发现有效的填充顺序，需要更大的探索常数，而不是增加模拟次数。这些发现确立了基于MCTS的规划作为提升MDMs生成质量的有效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of slot filling order sensitivity in Masked Diffusion Models (MDMs), which affects the consistency and quality of generated outputs, particularly in mathematical and code reasoning tasks. The proposed framework, McDiffuSE, employs Monte Carlo Tree Search (MCTS) to optimise the infilling order by treating slot selection as a decision-making process and using look-ahead simulations to evaluate partial completions. Experimental results demonstrate that McDiffuSE achieves an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill methods, with significant gains of 19.5% on MBPP and 4.9% on MATH500. The analysis indicates that while sequential ordering is common, non-sequential generation is crucial for performance, and larger exploration constants are more effective than increasing the number of simulations in overcoming model confidence biases.</div>
<div class="mono" style="margin-top:8px">该研究针对Masked Diffusion Models (MDMs)在计划和填充解码过程中因槽填充顺序敏感而导致的输出方差问题。McDiffuSE是一个新框架，将槽选择视为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。该方法利用前瞻模拟，在确定最终序列前评估部分填充结果，从而系统地探索生成顺序的组合空间。实验结果显示，McDiffuSE在自回归基线方法上平均提升3.2%，在基线计划和填充方法上提升8.0%，在MBPP和MATH500数据集上分别取得19.5%和4.9%的显著提升。分析表明，尽管顺序填充较为常见，但非顺序生成对性能提升至关重要，且更大的探索常数比增加模拟次数更能克服模型置信度偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-12T21:47:09+00:00</div>
<div class="meta-line">Comments: added more results on scaling law analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使LLMs在推荐系统中首次实现扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这可能是由于先前持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性所致。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，表明其在学习可泛化的用户偏好模式方面具有优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具有稳健的幂律扩展特性。我们的实验表明，在多种合成数据模态中，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of developing Large Language Models (LLMs) for recommender systems by introducing a principled synthetic data generation framework. The motivation stems from the lack of predictable scaling laws in previous continual pre-training approaches, which were hindered by noisy, biased, and incomplete real user interaction data. The proposed method creates a curated, pedagogical curriculum for LLMs to learn user preference patterns more effectively. Experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, with a $+130\%$ improvement in recall@100 for SasRec. Furthermore, the study demonstrates robust power-law scaling for LLMs trained on the synthetic data, indicating consistent perplexity reduction across different modalities, which provides a foundational approach for reliable scaling in recommendation systems.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在推荐系统中开发大型语言模型（LLMs）的挑战，提出了一种基于原则的合成数据生成框架。研究动机源于以往方法中由于真实用户交互数据存在噪声、偏差和不完整性，导致缺乏可预测的扩展定律。该方法通过构建一个精心设计的、教学导向的课程体系，使LLMs能够在合成数据上进行训练，从而更可靠地学习用户偏好模式。实验结果表明，使用该合成数据训练的模型在排序任务中显著优于基于真实数据训练的模型，其中SasRec在recall@100指标上提升了130%。此外，研究首次实证了在基于高质量推荐特定数据持续预训练的LLM中存在稳健的幂律扩展特性，展示了在不同数据模态下一致且可预测的困惑度下降。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study</div>
<div class="meta-line">Authors: Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim</div>
<div class="meta-line">First: 2025-10-26T16:49:06+00:00 · Latest: 2026-02-12T21:11:14+00:00</div>
<div class="meta-line">Comments: Accepted at LREC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22747v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Québec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Québec French LLMs on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模语言模型的低资源方言适应：以法语方言为例</div>
<div class="mono" style="margin-top:8px">尽管大规模语言模型（LLMs）已被广泛采用，但其最强能力仍主要局限于少数高资源语言，这些语言拥有大量训练数据。最近，持续预训练（CPT）作为一种方法，被用于微调这些模型以适应低资源的区域方言。本文研究了在数据和计算资源受限的情况下使用CPT进行方言学习的效果。我们利用低秩适应（LoRA）和计算高效的持续预训练技术，使用一个非常小的数据集将三个LLMs适配为魁北克法语方言，并在COLE数据集上进行基准测试。实验结果表明，在不到1%的模型参数更新的情况下，对少数方言基准测试有显著提升，而对标准语言基准测试的退化则非常有限。结果分析表明，提升效果高度依赖于语料库的构成。这些发现表明，结合参数高效微调（PEFT）的持续预训练可以有效缩小方言差距，通过提供成本效益高且可持续的语言资源创建方式，使高质量LLM能够扩展至少数语言群体。我们发布了首个魁北克法语LLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting large language models to low-resource dialects, focusing on Québec French. The study explores continual pre-training (CPT) combined with parameter-efficient techniques like low-rank adaptation (LoRA) to fine-tune three LLMs using minimal data and computational resources. Experimental results on the COLE suite show significant improvements in performance for the Québec French dialect with little impact on the original high-resource language, highlighting the effectiveness of CPT with PEFT in reducing the dialect gap. The findings suggest that such methods can provide a cost-effective way to enhance LLM capabilities for minority languages, thereby increasing access to high-quality language models for underrepresented linguistic communities.</div>
<div class="mono" style="margin-top:8px">本文探讨了在有限数据和计算资源下，通过持续预训练（CPT）方法将大型语言模型（LLMs）适配到低资源区域方言，如魁北克法语。研究采用低秩适应（LoRA）和计算高效的CPT技术，仅更新不到1%的模型参数，对三种LLM进行适配，并在COLE数据集上进行基准测试。实验结果表明，CPT在提升少数方言表现的同时，对主流语言表现影响较小，且其效果高度依赖于训练语料的构成。这些发现表明，结合参数高效微调（PEFT）的CPT方法能够有效缩小方言差距，为少数语言社区提供可持续的高质量语言资源。</div>
</details>
</div>
<div class="card">
<div class="title">LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens</div>
<div class="meta-line">Authors: Zekun Li, Sizhe An, Chengcheng Tang, Chuan Guo, Ivan Shugurov, Linguang Zhang, Amy Zhao, Srinath Sridhar, Lingling Tao, Abhay Mittal</div>
<div class="meta-line">First: 2026-02-12T20:02:21+00:00 · Latest: 2026-02-12T20:02:21+00:00</div>
<div class="meta-line">Comments: Project page: https://kunkun0w0.github.io/project/LLaMo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kunkun0w0.github.io/project/LLaMo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (&gt;30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaMo：通过连续自回归标记统一运动理解与生成的预训练语言模型扩展</div>
<div class="mono" style="margin-top:8px">近年来，大模型在统一多模态生成与理解方面取得了显著进展。然而，将运动与语言生成和理解统一的模型开发仍被广泛忽视。现有方法通常在配对的运动-文本数据上微调大型语言模型（LLMs），由于可用的文本-运动对规模有限，这可能导致语言能力的灾难性遗忘。此外，先前的方法通常通过量化将运动转换为离散表示，以与语言模型集成，从而引入了显著的抖动伪影。为了解决这些挑战，我们提出了LLaMo，一个统一框架，通过一种特定模态的混合变换器（MoT）架构扩展预训练的LLMs。该设计在保留基础模型语言理解能力的同时，实现了可扩展的多模态适应。我们将人类运动编码为因果连续的潜在空间，并通过一个轻量级的流匹配头在解码器骨干中保持下一个标记预测范式，从而实现实时流式运动生成（&gt;30 FPS）。通过利用预训练LLMs的全面语言理解和大规模运动-文本预训练，我们的实验表明，LLaMo在一般场景下实现了高保真度的文本到运动生成和运动到文本描述，尤其是在零样本运动生成方面，标志着向通用统一运动-语言大模型迈出的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop a unified model for both motion understanding and generation, addressing the limitations of existing approaches that suffer from catastrophic forgetting and jitter artifacts. LLaMo introduces a modality-specific Mixture-of-Transformers (MoT) architecture to extend pretrained language models, preserving their linguistic capabilities while enabling scalable multimodal adaptation. The model encodes human motion into a continuous latent space and uses a lightweight flow-matching head to maintain the next-token prediction paradigm, supporting real-time motion generation at over 30 FPS. Experimental results show that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, particularly in zero-shot scenarios, demonstrating its effectiveness in unifying motion and language understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个能够同时理解和生成运动的统一模型，以解决现有方法依赖离散表示并导致语言能力遗忘的问题。LLaMo提出了一种模态特定的Mixture-of-Transformers（MoT）架构，扩展了预训练语言模型，保留其语言理解能力的同时实现可扩展的多模态适应。该模型将人体运动编码为连续的潜在空间，并通过轻量级的流匹配头保持下一个token预测范式，支持实时运动生成（&gt;30 FPS）。实验结果表明，LLaMo在零样本运动生成等一般场景中实现了高质量的文本到运动生成和运动到文本描述，展示了其在统一运动-语言模型方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态往往在语言语义上缺乏足够的关联，影响了跨模态对齐的效果，尤其是在需要跨远距离区域或多张图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，一种新的框架，将视觉操作重新定义为语言引导的特征调制。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重新编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们在ViLaVT中实现了这一范式，ViLaVT是一个新型的LVLM，配备了专门用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的双阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the visual reasoning capabilities of large vision-language models (LVLMs) by addressing the limitations of single-pass visual encoding that result in the loss of fine-grained visual information. The proposed method, &#x27;chatting with images,&#x27; introduces a framework where visual manipulation is guided by language prompts, enabling dynamic joint re-encoding across multiple image regions. This approach strengthens the alignment between linguistic and visual modalities, particularly in tasks requiring reasoning over distant regions or multiple images. Experimental results across eight benchmarks show that the novel model, ViLaVT, achieves significant improvements, especially in complex spatial reasoning tasks involving multiple images and videos.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型视觉-语言模型（LVLMs）在单次视觉编码过程中丢失细粒度视觉信息的问题，提出了一种新的框架&#x27;chatting with images&#x27;，该框架通过语言提示引导视觉操作，实现对多个图像区域的动态联合重编码。这种方法加强了语言推理与视觉状态更新之间的耦合，特别是在需要跨远距离区域或多图像进行推理的任务中。ViLaVT 是一个新型 LVLM，配备了专门用于交互式视觉推理的动态视觉编码器，并采用监督微调与强化学习相结合的双阶段课程进行训练，在八个基准测试中表现出显著且一致的性能提升，尤其在涉及多图像和视频的复杂空间推理任务中效果突出。</div>
</details>
</div>
<div class="card">
<div class="title">3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu</div>
<div class="meta-line">First: 2026-02-12T16:41:26+00:00 · Latest: 2026-02-12T16:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aczheng-cai.github.io/3dgsnav.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DGSNav：通过主动3D高斯点云增强视觉-语言模型的物体导航推理</div>
<div class="mono" style="margin-top:8px">物体导航是具身智能的核心能力，使代理能够在未知环境中定位目标物体。近年来，视觉-语言模型（VLMs）的进步推动了零样本物体导航（ZSON）的发展。然而，现有方法通常依赖于场景抽象，将环境转换为语义地图或文本表示，导致高层决策受限于低层感知的准确性。在本工作中，我们提出了3DGSNav，一个新颖的ZSON框架，将3D高斯点云（3DGS）作为持久记忆嵌入到VLM中以增强空间推理能力。通过主动感知，3DGSNav逐步构建环境的3DGS表示，从而实现前沿感知的第一人称视角轨迹引导自由视角渲染。此外，我们设计了结构化的视觉提示，并将其与思维链（Chain-of-Thought, CoT）提示相结合，进一步提升VLM的推理能力。在导航过程中，实时物体检测器过滤潜在目标，而由VLM驱动的主动视角切换则执行目标重新验证，确保高效且可靠的识别。在多个基准测试和四足机器人上的实验证明，我们的方法在鲁棒性和竞争力方面均优于最先进的方法。项目页面：https://aczheng-cai.github.io/3dgsnav.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to improve the spatial reasoning capabilities of vision-language models (VLMs) for zero-shot object navigation by overcoming limitations of traditional scene abstraction methods. 3DGSNav introduces a novel framework that integrates 3D Gaussian Splatting as persistent memory, allowing VLMs to maintain a detailed 3D representation of the environment through active perception. This enables trajectory-guided rendering of first-person views and enhances the agent&#x27;s ability to navigate towards target objects. The method also incorporates structured visual prompts and Chain-of-Thought prompting to refine VLM reasoning, supported by a real-time object detector and active viewpoint switching for target re-verification. Experimental results across multiple benchmarks and real-world robot navigation tasks show that 3DGSNav achieves robust and competitive performance compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉语言模型（VLMs）在零样本物体导航任务中的空间推理能力，以克服现有场景抽象方法的局限性。3DGSNav提出了一种新颖的框架，利用3D高斯点云作为持久记忆机制，通过主动感知逐步构建环境的三维表示，从而实现轨迹引导的自由视角渲染和前沿感知的第一人称视角。该方法还结合了结构化的视觉提示与思维链提示，以增强VLM的推理能力。在导航过程中，实时物体检测器用于筛选潜在目标，而由VLM驱动的主动视角切换则用于目标重新验证。实验结果表明，该方法在多个基准测试和真实世界四足机器人导航任务中均展现出鲁棒性和竞争力，优于现有先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Adoption of AI Coding Agents in Open-source Android and iOS Development</div>
<div class="meta-line">Authors: Muhammad Ahmad Khan, Hasnain Ali, Muneeb Rana, Muhammad Saqib Ilyas, Abdul Ali Bangash</div>
<div class="meta-line">First: 2026-02-12T16:30:29+00:00 · Latest: 2026-02-12T16:30:29+00:00</div>
<div class="meta-line">Comments: Accepted at MSR 2026 Mining Challenge track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在开源Android和iOS开发中采用AI编码代理的影响</div>
<div class="mono" style="margin-top:8px">AI编码代理在软件开发中的作用日益增加，但其对移动开发的影响却鲜有实证研究。本文提出了首个针对开源移动应用项目中代理生成代码的类别级实证研究。我们利用AIDev数据集中的193个已验证的Android和iOS开源GitHub仓库中的2,901个AI撰写的拉取请求（PRs），分析了跨移动平台、代理和任务类别的PR接受行为。我们发现，Android项目接收的AI撰写的PR数量是iOS的两倍，并且Android的PR接受率（71%）高于iOS（63%），在Android上还存在显著的代理间差异。在任务类别方面，常规任务（功能、修复和UI）的PR接受率最高，而重构和构建等结构性变更的PR成功率较低且解决时间较长。此外，我们的演化分析表明，Android的PR解决时间在2025年中期有所改善，之后又开始下降。我们的研究结果为AI代理对开源移动项目的影响提供了首个基于实证的描述，并为评估代理生成贡献对设计平台感知代理系统的影响建立了实证基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the adoption of AI coding agents in open-source mobile development, focusing on Android and iOS projects. The research is motivated by the growing role of AI in software development and the lack of empirical analysis on its impact in mobile ecosystems. By analyzing 2,901 AI-authored pull requests across 193 verified repositories, the authors find that Android projects accept twice as many AI-generated PRs and have a higher acceptance rate (71%) compared to iOS (63%). Task categories such as feature, fix, and UI changes show higher acceptance rates, whereas structural tasks like refactor and build are less successful and take longer to resolve. The study also reveals a trend of decreasing PR resolution time on Android after mid-2025.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在开源移动开发中的采用情况，重点关注Android和iOS项目。研究动机源于AI在软件开发中日益增长的作用以及其在移动开发领域缺乏实证分析。通过对193个仓库中2,901个AI生成的拉取请求（PR）进行分析，研究发现Android项目接收了两倍于iOS的AI生成PR，并且其接受率（71%）也高于iOS（63%）。功能类任务（如功能、修复和UI）的PR接受率最高，而重构和构建等结构性任务则成功率较低且解决时间较长。此外，研究还显示Android项目在2025年中期之前PR解决时间有所改善，之后又出现下降趋势。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?</div>
<div class="meta-line">Authors: Thibaud Gloaguen, Niels Mündler, Mark Müller, Veselin Raychev, Martin Vechev</div>
<div class="meta-line">First: 2026-02-12T14:15:22+00:00 · Latest: 2026-02-12T14:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11988v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents&#x27; task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估AGENTS.md：仓库级上下文文件对编码代理是否有帮助？</div>
<div class="mono" style="margin-top:8px">在软件开发中，一种普遍的做法是通过手动或自动生成上下文文件（如AGENTS.md）来定制编码代理以适应特定仓库。尽管这种做法被代理开发者强烈推荐，但目前尚无严谨的研究探讨此类上下文文件是否对实际任务真正有效。在本研究中，我们探讨了这一问题，并在两种互补的设置下评估了编码代理的任务完成性能：一是使用LLM生成的上下文文件，基于代理开发者建议，对流行仓库中的SWE-bench任务进行评估；二是从包含开发者提交的上下文文件的仓库中收集的新问题集。我们发现，在多个编码代理和LLM中，上下文文件往往会降低任务成功率，同时增加推理成本超过20%。行为上，无论是LLM生成的还是开发者提供的上下文文件，都会鼓励更广泛的探索（例如更彻底的测试和文件遍历），而编码代理往往遵循其指令。最终，我们得出结论：上下文文件中不必要的要求会使任务更加困难，而人工撰写的上下文文件应仅描述最低限度的要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the effectiveness of repository-level context files, such as AGENTS.md, in improving the performance of coding agents. It evaluates task completion rates across multiple coding agents and large language models in two settings: using LLM-generated context files based on developer recommendations and using context files committed by developers. The results show that context files generally lower task success rates and increase inference costs by more than 20%, suggesting that they may introduce unnecessary complexity. The analysis also reveals that both types of context files lead to broader agent exploration, but coding agents tend to follow their instructions closely, which may not always be beneficial.</div>
<div class="mono" style="margin-top:8px">本研究探讨了仓库级上下文文件（如AGENTS.md）对编码代理性能的影响。研究人员在两种互补的设置中评估了多个编码代理和大语言模型（LLMs）的任务完成率：一种是使用基于开发者建议生成的LLM上下文文件，另一种是使用开发者提交的上下文文件。研究结果表明，上下文文件通常会降低任务成功率并增加推理成本超过20%，暗示它们可能引入不必要的复杂性。此外，研究还指出，无论是LLM生成还是开发者提供的上下文文件，都会促使编码代理进行更广泛的探索行为，但这些文件的存在可能使任务更加困难。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</div>
<div class="meta-line">Authors: Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang, Hongyang Wei, Yi-Fan Zhang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Long Chen</div>
<div class="meta-line">First: 2026-02-12T14:12:14+00:00 · Latest: 2026-02-12T14:12:14+00:00</div>
<div class="meta-line">Comments: 19 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11980v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11980v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model&#x27;s layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间链式思维：连接理解与生成模型的空间推理生成</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在美学图像合成方面表现出色，但它们通常在复杂空间理解和推理方面存在困难。现有方法依赖于多模态大语言模型（MLLMs）来增强这一能力。然而，这些方法要么通过联合训练带来高昂的计算成本，要么仅依赖文本提示导致空间信息丢失。为了解决这些限制，我们提出了一种空间链式思维（SCoT）框架，这是一种即插即用的方法，能够有效连接MLLMs的推理能力和扩散模型的生成能力。具体而言，我们首先通过交错文本-坐标指令格式训练扩散模型，以增强其布局感知能力。然后，我们利用最先进的MLLMs作为规划器，生成全面的布局计划，并将它们的空间规划能力直接转移到生成过程中。大量实验表明，我们的方法在图像生成基准上取得了最先进的性能，并在复杂推理任务中显著优于基线方法，同时在图像编辑场景中也表现出强大的效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of diffusion models in spatial understanding and reasoning by proposing the Spatial Chain-of-Thought (SCoT) framework. The motivation stems from the fact that while diffusion models excel in image generation, they lack the ability to comprehend and reason about spatial structures effectively. The method combines the layout awareness of diffusion models with the spatial planning capabilities of MLLMs by training the diffusion model on interleaved text-coordinate instructions and using MLLMs as planners to generate detailed layout plans. Experimental results show that SCoT achieves state-of-the-art performance on image generation benchmarks and outperforms existing baselines in complex spatial reasoning tasks, demonstrating its effectiveness in both generation and image editing scenarios.</div>
<div class="mono" style="margin-top:8px">本文旨在解决扩散模型在空间理解和推理方面的不足，提出了Spatial Chain-of-Thought（SCoT）框架。其动机源于扩散模型虽在图像生成方面表现出色，但在处理复杂空间结构时存在局限。该方法通过训练扩散模型在交错的文本-坐标指令格式上，增强其布局感知能力，并利用先进的多模态大语言模型（MLLMs）作为规划器生成详细的布局计划，将MLLMs的空间规划能力直接引入生成过程。实验结果表明，SCoT在图像生成基准测试中达到最先进水平，在复杂空间推理任务中显著优于基线方法，同时在图像编辑场景中也表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Suraj Ranganath, Anish Patnaik, Vaishak Menon</div>
<div class="meta-line">First: 2026-02-12T12:32:51+00:00 · Latest: 2026-02-12T12:32:51+00:00</div>
<div class="meta-line">Comments: Workshop submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11882v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11882v1">PDF</a> · <a href="https://github.com/suraj-ranganath/DINO-MBQuant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在世界模型规划中比特的重要性：一种用于高效空间推理的配对混合比特研究</div>
<div class="mono" style="margin-top:8px">高效的空问推理需要在有限精度预算下仍保持可靠的世界模型。我们研究低比特规划行为主要由总比特宽度决定，还是由模块间比特分配决定。在Wall规划任务中，我们使用DINO-WM对均匀、混合、非对称和逐层变体进行了配对目标混合比特评估，在两个规划预算下观察到一致的三种状态模式：8比特和6比特设置接近FP16，3比特设置崩溃，而4比特设置则对分配敏感。在该过渡区域，保持编码器精度的规划效果优于均匀量化，且接近大小的非对称变体表现出相同的编码器侧方向。在后续的严格22单元复制实验中，使用更少的每单元情节数，混合与均匀INT4的符号变得依赖预算，这进一步突显了该过渡区域的敏感性。这些发现为高效空间推理的模块感知和预算感知量化策略提供了研究方向。代码和运行产物可在https://github.com/suraj-ranganath/DINO-MBQuant获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of bit allocation on the performance of world models in spatial reasoning tasks, motivated by the need for efficient and reliable planning under limited precision budgets. The authors evaluate different mixed-bit quantization strategies, including uniform, mixed, asymmetric, and layerwise, using DINO-WM on the Wall planning task with two planner budgets. Their results reveal a three-regime pattern, where 8-bit and 6-bit settings maintain performance close to FP16, 3-bit settings experience significant degradation, and 4-bit settings show sensitivity to bit allocation. Notably, preserving encoder precision over uniform quantization improves planning, and asymmetric variants demonstrate similar trends. These findings suggest that quantization policies should be both module-aware and budget-aware to optimize spatial reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究探讨了位宽分配对世界模型在空间推理中可靠性的影响，旨在确定低位宽规划性能主要由总位宽还是模块间位宽分配决定。通过在Wall任务中使用DINO-WM模型，作者在两种规划预算下评估了多种量化策略，发现存在三种表现模式：8位和6位设置的性能接近FP16，3位设置则显著下降，而4位设置对位宽分配敏感。结果表明，在过渡区域中，保持编码器精度和采用非对称量化策略可以提升规划性能。</div>
</details>
</div>
<div class="card">
<div class="title">Code2Worlds: Empowering Coding LLMs for 4D World Generation</div>
<div class="meta-line">Authors: Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang</div>
<div class="meta-line">First: 2026-02-12T09:34:28+00:00 · Latest: 2026-02-12T09:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11757v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11757v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/Code2Worlds">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://aigeeksgroup.github.io/Code2Worlds">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Code2Worlds: 为4D世界生成赋能的代码大模型</div>
<div class="mono" style="margin-top:8px">实现空间智能需要超越视觉合理性，构建基于物理定律的世界模拟器。尽管代码大模型在静态3D场景生成方面取得了进展，但将其范式扩展到4D动态仍然是一个关键的前沿领域。该任务面临两个根本性挑战：多尺度上下文纠缠，其中整体生成无法平衡局部物体结构与全局环境布局；以及语义-物理执行差距，其中开环代码生成会导致缺乏动态保真的物理幻觉。我们提出了Code2Worlds框架，将4D生成建模为语言到模拟代码生成。首先，我们提出了一种双流架构，将检索增强的物体生成与分层环境编排解耦。其次，为确保动态保真度，我们建立了一种物理感知的闭环机制，其中PostProcess Agent编写动态，结合VLM-Motion Critic进行自我反思，以迭代优化模拟代码。在Code4D基准上的评估表明，Code2Worlds在SGS指标上比基线提升41%，在丰富度上提高49%，并且能够生成此前静态方法中缺失的物理感知动态。代码：https://github.com/AIGeeksGroup/Code2Worlds。网站：https://aigeeksgroup.github.io/Code2Worlds。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance coding LLMs&#x27; ability to generate dynamic 4D worlds by incorporating physical realism beyond static 3D scene generation. The proposed method, Code2Worlds, introduces a dual-stream architecture that separates object generation from environmental orchestration, and employs a physics-aware closed-loop mechanism involving a PostProcess Agent and a VLM-Motion Critic for iterative refinement. Experimental results on the Code4D benchmark demonstrate significant improvements, with a 41% increase in SGS and 49% higher Richness compared to existing baselines, highlighting the framework&#x27;s effectiveness in generating physically consistent dynamics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升编码语言模型在生成具有物理真实性的4D动态世界方面的能力，突破静态3D场景生成的局限。Code2Worlds方法引入了双流架构，将物体生成与环境编排分离，并采用物理感知的闭环机制，结合PostProcess Agent和VLM-Motion Critic进行动态代码的迭代优化。在Code4D基准测试中，实验结果表明该框架在空间接地得分（SGS）上提升了41%，在丰富度上提高了49%，有效实现了物理一致的动态生成。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260219_0408.html">20260219_0408</a>
<a href="archive/20260218_0406.html">20260218_0406</a>
<a href="archive/20260217_0354.html">20260217_0354</a>
<a href="archive/20260216_0344.html">20260216_0344</a>
<a href="archive/20260215_0344.html">20260215_0344</a>
<a href="archive/20260213_0409.html">20260213_0409</a>
<a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
