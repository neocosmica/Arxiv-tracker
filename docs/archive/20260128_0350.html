<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 03:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0350</div>
    <div class="row"><div class="card">
<div class="title">Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests</div>
<div class="meta-line">Authors: Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang</div>
<div class="meta-line">First: 2026-01-08T12:31:02+00:00 · Latest: 2026-01-26T17:05:34+00:00</div>
<div class="meta-line">Comments: Accepted by MSR&#x27;26 Mining Challenge Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04886v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04886v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that &quot;descriptions claim unimplemented changes&quot; was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5 times longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分析AI编码代理撰写的拉取请求中的消息-代码不一致</div>
<div class="mono" style="margin-top:8px">由AI编码代理生成的拉取请求（PR）描述是向人类审阅者传达代码变更的主要渠道。然而，这些消息与实际变更之间的对齐情况尚未被研究，引发了对AI代理可信度的担忧。为填补这一空白，我们使用PR消息-代码不一致（PR-MCI）分析了五个代理生成的23,247个PR。我们贡献了974个手动标注的PR，发现其中406个（1.7%）表现出高PR-MCI，并识别出八种PR-MCI类型，其中最常见的问题是“描述声称未实现的变更”（占45.4%）。统计检验表明，高PR-MCI的PR接受率低51.7%（28.3% vs. 80.0%），且合并时间是普通PR的3.5倍（55.8小时 vs. 16.0小时）。我们的研究结果表明，不可靠的PR描述会损害对AI代理的信任，强调了需要PR-MCI验证机制和改进PR生成以实现可信的人机协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the issue of message-code inconsistency in pull requests authored by AI coding agents, as this gap in alignment between PR descriptions and actual code changes may affect the trustworthiness of AI systems. By analyzing 23,247 PRs across five agents, the researchers identified 406 PRs with high inconsistency, contributing 974 manually annotated examples. They categorized the inconsistencies into eight types, with &quot;descriptions claim unimplemented changes&quot; being the most prevalent. The results show that PRs with high inconsistency have significantly lower acceptance rates and longer merge times, indicating the importance of improving PR description reliability for effective human-AI collaboration.</div>
<div class="mono" style="margin-top:8px">本研究探讨了由AI编码代理撰写的拉取请求（PR）中消息与代码不一致的问题，因为PR描述是人类审阅者理解代码变更的关键渠道。通过对五个代理生成的23,247个PR进行分析，研究者发现了406个存在高不一致性的PR，并贡献了974个手动标注的案例。他们将不一致性分为八类，其中&#x27;描述声称未实现的变更&#x27;最为常见。实验结果表明，高不一致性的PR接受率低51.7%，合并时间长3.5倍，说明不可靠的描述削弱了对AI代理的信任，突显了改进PR生成和验证机制的重要性，以实现可信的人机协作。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-26T12:20:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的代理原生中间训练</div>
<div class="mono" style="margin-top:8px">最近，大型语言模型（LLM）的能力前沿已从单轮代码生成转向代理软件工程——一种模型能够自主导航、编辑和测试复杂仓库的范式。尽管后训练方法已成为代码代理的主流方法，但**代理中间训练**（agentic mid-training）——在大规模数据上进行中间训练，这些数据模拟真实的代理工作流——由于资源需求巨大，仍被严重忽视。然而，与仅依赖昂贵的强化学习相比，它为培养基础代理行为提供了一条更具可扩展性的路径。实现有效的代理中间训练的核心挑战在于静态训练数据与真实开发中动态且反馈丰富的环境之间的分布不匹配。为了解决这一问题，我们系统地研究了代理中间训练，建立了适用于大规模代理开发的有效数据合成原则和训练方法。我们的方法核心在于**代理原生数据**——包含两种互补类型的轨迹：**上下文原生轨迹**，保留代理经历的完整信息流，提供广泛覆盖和多样性；以及**环境原生轨迹**，从可执行仓库中收集，其观察来源于实际工具调用和测试执行，提供深度和交互的真实性。我们在 `SWE-Bench Verified` 上验证了模型的代理能力。在两种后训练设置下，我们展示了基于对齐基础模型和代理框架的模型优于之前的开源软件工程中间训练方案 `Kimi-Dev`，同时使用的中间训练标记数不到其一半（73.1B）。除了相对优势外，我们的表现最佳的32B和72B模型分别实现了**56.1%**和**58.5%**的解决率，分别...</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to explore agentic mid-training as a more scalable and efficient alternative to expensive reinforcement learning for developing code agents. The authors propose a method involving the creation of agent-native data, which includes contextually-native and environmentally-native trajectories to better simulate real development workflows. Their experiments on SWE-Bench Verified show that their approach outperforms Kimi-Dev with fewer training tokens, achieving resolution rates of 56.1% and 58.5% for their 32B and 72B models respectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索一种更可扩展且高效的代码代理训练方法，即引入代理原生的中期训练，以克服后训练方法的局限性。作者提出了一种基于代理原生数据合成的方法，包括上下文原生轨迹和环境原生轨迹，以更好地匹配真实开发工作流。在SWE-Bench Verified上的实验表明，他们的方法在使用不到一半训练令牌的情况下优于之前的Kimi-Dev方案，其最佳表现的32B和72B模型分别实现了56.1%和58.5%的解决率。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过预测编码实现共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是一个关键挑战，其中部分可观测性和有限带宽常常导致协调中的灾难性失败。我们引入了一种多智能体预测编码框架，将协调建模为智能体之间相互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习与谁以及交流什么，还学习何时进行交流。该框架的基础是一种类似网格细胞的度量，作为内部空间编码用于自我定位，这种编码自发地从自监督运动预测中产生。在此基础上，智能体逐步发展出一种带宽高效的通信机制，并形成专门的神经群体来编码伙伴的位置——这是海马体社会位置细胞（SPCs）的人工类比。这些社会表征进一步被分层强化学习策略所利用，该策略主动探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法表现出对带宽限制的卓越鲁棒性：当带宽从128位/步减少到4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则从67.6%骤降至28.6%。我们的研究结果为复杂社会表征如何从统一的预测驱动中涌现提供了理论上有原则且生物上合理的基础，从而实现集体智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of creating a consistent shared spatial memory in multi-agent systems, particularly under conditions of partial observability and limited communication bandwidth. The authors propose a predictive coding framework that enables agents to minimize mutual uncertainty by learning who, what, and when to communicate. This framework uses a grid-cell-like metric for internal spatial coding, which arises from self-supervised motion prediction. The method also develops specialized neural populations that encode partners&#x27; locations, akin to hippocampal social place cells. Experimental results on the Memory-Maze benchmark demonstrate that their approach maintains high success rates even with reduced bandwidth, showing significantly better performance compared to a full-broadcast baseline.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多智能体系统中构建一致共享空间记忆的挑战，这通常受到部分可观测性和通信带宽限制的影响。作者提出了一种基于预测编码的框架，使智能体能够通过学习谁、什么以及何时进行通信来最小化相互不确定性。该框架采用类似网格细胞的度量方法用于自定位，该方法源于自监督运动预测。此外，该方法还发展出专门的神经群体，用于编码伙伴的位置，模拟海马体中的社会位置细胞。在Memory-Maze基准测试中，实验结果表明该方法在严重带宽限制下仍能保持较高的成功率，优于全广播基线。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-26T10:49:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, SWE-EVO comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on SWE-EVO, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长期软件演进场景中对编码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI编码代理基准主要关注孤立的、单一问题的任务，例如修复错误或实现小功能。然而，现实中的软件工程本质上是一项长期任务：开发者需要理解高层需求，协调多个文件的更改，并在多次迭代中演进代码库，同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长期软件演进挑战上的基准。SWE-EVO基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而相比之下在单一问题的SWE-Bench Verified上解决率为65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing benchmarks for AI coding agents, which focus on isolated, single-issue tasks rather than the complex, long-term nature of real-world software evolution. SWE-EVO is introduced as a benchmark that evaluates agents on multi-step, multi-file software evolution tasks derived from the release notes and version histories of seven mature Python projects. The benchmark includes 48 tasks requiring changes across an average of 21 files, validated by extensive test suites with up to 874 tests per instance. Experimental results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on the single-issue SWE-Bench Verified, highlighting the challenge of sustained, multi-file reasoning in current coding agents.</div>
<div class="mono" style="margin-top:8px">SWE-EVO的提出旨在解决现有AI编码代理基准测试过于关注孤立任务的局限性，转而评估代理在长期软件演进场景中的表现。该基准基于七个成熟Python开源项目的发布说明和版本历史构建，包含48个需要跨平均21个文件进行多步骤修改的任务。实验结果显示，即使是GPT-5与OpenHands结合的先进模型，在SWE-EVO上的解决率也只有21%，远低于单任务SWE-Bench Verified的65%，表明当前代理在持续多文件推理方面存在显著困难。研究还提出了Fix Rate这一细粒度指标，用于衡量在解决复杂长期任务中的部分进展。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Conditioned Reasoning in Long-Egocentric Videos</div>
<div class="meta-line">Authors: James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi</div>
<div class="meta-line">First: 2026-01-26T03:21:35+00:00 · Latest: 2026-01-26T03:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时域第一视角视频中的空间条件推理</div>
<div class="mono" style="margin-top:8px">由于视角漂移和缺乏持久的几何上下文，长时域第一视角视频对视觉导航提出了重大挑战。尽管近期的视觉-语言模型在图像和短视频推理上表现良好，但它们在长第一视角序列中的空间推理能力仍有限。在本工作中，我们研究了显式空间信号如何影响基于VLM的视频理解，而无需修改模型架构或推理过程。我们引入了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在面向导航的空间查询上对多个VLM进行了基准测试。为了考察输入级别的归纳偏置，我们进一步将深度图与RGB帧融合，并评估其对空间推理的影响。我们的结果揭示了通用准确性与空间特异性之间的权衡，表明具有深度感知和空间基础的表示可以提高对行人和障碍物检测等关键安全任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenges of visual navigation in long-horizon egocentric videos, where viewpoint drift and lack of persistent geometric context hinder spatial understanding. The authors investigate how explicit spatial signals can enhance video understanding using vision-language models (VLMs) without altering model architecture or inference processes. They introduce Sanpo-D, a refined version of the Google Sanpo dataset, and evaluate several VLMs on spatial navigation tasks. By integrating depth maps with RGB frames, they demonstrate that depth-aware representations improve performance in safety-critical tasks like pedestrian and obstruction detection, albeit at the cost of reduced general-purpose accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对长时地缘视角视频中因视角漂移和缺乏持久几何信息而带来的视觉导航挑战。作者探讨了在不修改模型结构或推理流程的前提下，显式的空间信号如何提升基于视觉语言模型的视频理解能力。他们提出了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在导航导向的空间查询任务上评估了多种VLM。通过将深度图与RGB帧融合，他们发现深度感知的表示方法能显著提升空间推理能力，尤其在行人和障碍物检测等关键任务中，但可能牺牲通用准确性。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-26T00:36:33+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com First two authors contribute equally. The 3th - 6th authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能以找到共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力，才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的600多个协作编码任务的基准测试集。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队中增加队友通常能提高生产力的情况形成鲜明对比。我们的分析揭示了三个关键问题：(1) 通信渠道充斥着模糊、时机不当和不准确的信息；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个针对协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the limitations of current coding agents in collaborative settings, highlighting the need for social intelligence in team-based AI systems. The authors introduce CooperBench, a benchmark consisting of over 600 collaborative coding tasks across four programming languages and twelve libraries, designed to test agents&#x27; ability to coordinate while implementing independent features. The main experimental results show that state-of-the-art coding agents perform significantly worse when working together, achieving on average 30% lower success rates than when completing tasks individually, indicating the &#x27;curse of coordination.&#x27; The analysis identifies three key challenges: ineffective communication, commitment deviation, and misaligned expectations. Additionally, the study observes some emergent coordination behaviors in large-scale simulations, such as role division and negotiation, suggesting potential pathways for future development.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨当前编码代理在协作环境中的局限性，强调团队协调中社交智能的重要性。为此，作者引入了CooperBench，一个包含12个库和4种编程语言的600多个协作编码任务的基准测试集。主要实验结果表明，最先进的编码代理在协作时平均成功率比单独执行任务低30%，揭示了‘协调的诅咒’现象。分析指出三个关键问题：沟通不畅、承诺偏离以及对他人计划的错误预期。通过大规模模拟，研究还观察到一些有趣的协调行为，如角色分工和协商。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen, Naren Ramakrishnan</div>
<div class="meta-line">First: 2025-09-21T03:02:59+00:00 · Latest: 2026-01-25T23:11:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16891v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.16891v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM作为布局设计师：增强的空间推理能力用于内容感知布局生成</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）在文本领域展示了出色的推理和规划能力，并能有效执行复杂任务的指令，但它们对空间关系的理解和操控能力仍然有限。这种能力对于内容感知的图形布局设计至关重要，其目标是将异构元素排列在画布上，以确保最终设计在视觉上平衡且结构上可行。该问题需要在受限的视觉空间中，对多个元素的放置、对齐和结构组织进行精确协调。为了解决这一局限性，我们引入了LaySPA，这是一个基于强化学习的框架，通过为基于LLM的智能体增加显式的空间推理能力，来增强布局设计。LaySPA采用混合奖励信号，共同捕捉几何约束、结构保真度和视觉质量，使智能体能够导航画布、建模元素间关系，并优化空间布局。通过群体相对策略优化，智能体生成的内容感知布局能够反映显著区域，尊重空间约束，并产生可解释的推理轨迹，解释放置决策和结构化布局规范。实验结果表明，LaySPA显著提升了生成结构有效且视觉吸引人的布局的能力，优于更大的通用LLM，并且其性能与最先进的专用布局模型相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of Large Language Models (LLMs) in understanding and manipulating spatial relationships, which is essential for content-aware graphic layout design. The proposed method, LaySPA, is a reinforcement learning framework that enhances LLM-based agents with explicit spatial reasoning capabilities by incorporating hybrid reward signals that consider geometric constraints, structural fidelity, and visual quality. Experimental results demonstrate that LaySPA generates structurally valid and visually appealing layouts, surpassing general-purpose LLMs and matching the performance of specialized layout models.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在理解和操作空间关系方面的不足，这对于内容感知的图形布局设计至关重要。作者提出LaySPA，这是一个基于强化学习的框架，通过混合奖励信号增强LLM代理的空间推理能力，以兼顾几何约束、结构保真度和视觉质量。实验结果表明，LaySPA能够生成结构合理且视觉吸引人的布局，优于通用型LLMs，并与专门的布局模型表现相当。</div>
</details>
</div>
<div class="card">
<div class="title">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</div>
<div class="meta-line">Authors: Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao</div>
<div class="meta-line">First: 2026-01-25T15:29:32+00:00 · Latest: 2026-01-25T15:29:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17885v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://peafowlvla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PEAfowl: 增强感知的多视角视觉-语言-动作策略用于双臂操作</div>
<div class="mono" style="margin-top:8px">在杂乱场景中进行双臂操作需要在遮挡、视角和场景变化下保持稳定的策略。现有视觉-语言-动作模型往往无法泛化，原因包括：(i) 多视角特征通过无视角感知的token拼接进行融合，导致弱的3D一致空间理解；(ii) 语言作为全局条件注入，导致粗略的指令定位。本文提出PEAfowl，一种增强感知的多视角VLA策略用于双臂操作。在空间推理方面，PEAfowl预测每个token的深度分布，执行可微分的3D提升，并聚合局部跨视角邻居以形成几何基础且跨视角一致的表示。在指令定位方面，我们提出用Perceiver风格的文本感知读出机制替代全局条件注入，利用冻结的CLIP视觉特征进行迭代证据积累。为克服无噪声和不完整的商品深度数据且不增加推理开销，我们应用仅训练的深度蒸馏，从预训练的深度教师中监督深度分布头，为感知前端提供几何感知先验。在领域随机化设置下的RoboTwin 2.0上，PEAfowl将最强基线的成功率提升了23.0个百分点，真实机器人实验进一步验证了其可靠的模拟到现实迁移能力以及深度蒸馏带来的持续改进。项目网站：https://peafowlvla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Bimanual manipulation in cluttered environments demands policies that maintain stability despite occlusions and varying viewpoints. PEAfowl addresses these challenges by enhancing spatial reasoning through per-token depth prediction, differentiable 3D lifting, and cross-view neighbor aggregation, leading to geometrically consistent representations. For instruction grounding, it uses a Perceiver-style readout over frozen CLIP features to iteratively accumulate evidence, improving alignment with language. The model also employs depth distillation during training to leverage a pretrained depth teacher, providing geometry-aware priors without increasing inference cost. Experimental results on RoboTwin 2.0 show a 23.0 percentage point improvement in success rate over the strongest baseline, with real-robot tests confirming reliable sim-to-real transfer and consistent performance gains.</div>
<div class="mono" style="margin-top:8px">在杂乱场景中进行双臂操作需要能够在遮挡、视角变化和场景差异下保持稳定的策略。现有视觉-语言-动作模型在泛化能力上存在不足，主要由于视图无关的token拼接导致3D空间理解薄弱，以及语言作为全局条件注入导致指令定位粗糙。PEAfowl通过预测每个token的深度分布、执行可微分的3D提升以及聚合跨视图的局部特征，构建出几何一致且跨视图稳定的表示。它还采用类似Perceiver的文本感知读出机制，基于冻结的CLIP视觉特征进行迭代证据积累。为在不增加推理开销的前提下处理噪声和不完整的商品深度信息，PEAfowl引入了训练阶段的深度蒸馏，利用预训练的深度教师提供几何感知的先验知识。在RoboTwin 2.0的领域随机化设置下，PEAfowl将最强基线的成功率提升了23.0%，真实机器人实验进一步验证了其可靠的仿真到现实迁移能力和深度蒸馏带来的持续性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</div>
<div class="meta-line">Authors: Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</div>
<div class="meta-line">First: 2025-08-19T15:58:25+00:00 · Latest: 2026-01-25T02:33:47+00:00</div>
<div class="meta-line">Comments: EACL 2026 Camera-Ready. Code and data: https://github.com/tianyiniu/RotBench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13968v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13968v3">PDF</a> · <a href="https://github.com/tianyiniu/RotBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench, a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270° rotated images. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models&#x27; ability to distinguish 90° and 270° rotations, despite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs&#x27; spatial reasoning capabilities and human perception in identifying rotation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RotBench：评估多模态大语言模型在识别图像旋转方向上的能力</div>
<div class="mono" style="margin-top:8px">我们研究了多模态大语言模型（MLLMs）在识别输入图像旋转方向（0°, 90°, 180°, 270°）方面的准确性。该任务需要强大的视觉推理能力，以检测旋转线索并理解图像中的空间关系，无论图像的方向如何。为评估MLLMs在这些能力上的表现，我们引入了RotBench，这是一个包含生活方式、人像和风景图像的350张图像人工筛选基准数据集。尽管该任务相对简单，但我们发现一些最先进的开源和专有MLLMs（包括GPT-5、o3和Gemini-2.5-Pro）无法可靠地识别输入图像的旋转方向。即使为模型提供辅助信息（如标题、深度图等）或使用链式推理提示，也只能带来微小且不一致的提升。我们的结果表明，大多数模型能够可靠地识别正向（0°）图像，而某些模型能够识别倒置（180°）图像。但没有任何模型能够可靠地区分90°和270°旋转的图像。同时展示不同方向旋转的图像，对推理模型的性能带来中等提升，而使用投票机制的修改设置则能提升较弱模型的性能。我们进一步表明，微调虽然显著提升了模型识别180°旋转图像的能力，但并未改善其区分90°和270°旋转的能力。这些结果揭示了MLLMs在空间推理能力与人类识别旋转方向感知之间存在显著差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to assess the ability of Multimodal Large Language Models (MLLMs) to identify image rotation angles of 0°, 90°, 180°, and 270°, highlighting the need for improved spatial reasoning in these models. The authors introduce RotBench, a benchmark consisting of 350 manually curated images across different categories. Their experiments show that while most models can reliably detect 0° and 180° rotations, they struggle with distinguishing between 90° and 270° rotations. Even with additional information or prompting strategies, performance improvements are limited, suggesting a fundamental gap in MLLMs&#x27; understanding of spatial orientation compared to human perception.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估多模态大语言模型（MLLMs）识别输入图像旋转角度（0°、90°、180°、270°）的能力。研究者构建了RotBench基准测试集，包含350张经过人工筛选的生活、肖像和风景图像。实验结果表明，尽管大多数模型能够可靠地识别0°和180°的旋转，但在区分90°和270°旋转时表现不佳。提供辅助信息或使用推理提示仅带来小幅且不稳定的提升，而微调虽能改善180°旋转识别，却无法提升对90°和270°旋转的区分能力。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</div>
<div class="meta-line">Authors: Daniel Ogenrwot, John Businge</div>
<div class="meta-line">First: 2026-01-24T20:27:04+00:00 · Latest: 2026-01-24T20:27:04+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17581v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17581v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff&#x27;s $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何修改代码：GitHub拉取请求的大型研究</div>
<div class="mono" style="margin-top:8px">AI编码代理正越来越多地作为自主贡献者，通过生成和提交拉取请求（PRs）参与开发。然而，我们缺乏关于这些由代理生成的PR与人类贡献之间差异的实证证据，尤其是在代码修改方式和PR描述方面。理解这些差异对于评估其可靠性和对开发流程的影响至关重要。我们使用MSR 2026 Mining Challenge版本的AIDev数据集，分析了24,014个已合并的Agentic PR（440,295次提交）和5,081个已合并的人类PR（23,242次提交）。我们考察了新增内容、删除内容、提交次数以及涉及的文件，并通过词法和语义相似性评估PR描述与代码差异的一致性。在提交次数方面，Agentic PR与Human PR存在显著差异（Cliff&#x27;s δ=0.5429），在涉及的文件和删除行方面表现出中等差异。它们在所有衡量标准下也显示出略高的描述与代码差异的相似性。这些发现提供了关于AI编码代理如何参与开源开发的大规模实证特征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents contribute to open source development by analyzing their pull requests compared to those made by humans. The research is motivated by the need to understand the differences in code modification patterns and PR descriptions between AI-generated and human contributions. Using the AIDev dataset, the authors examine 24,014 merged Agentic PRs and 5,081 merged Human PRs, focusing on commit counts, files touched, and the semantic consistency between PR descriptions and code changes. The results show that Agentic PRs have significantly higher commit counts and moderate differences in files touched and deleted lines, while also demonstrating slightly higher similarity between descriptions and diffs across various measures.</div>
<div class="mono" style="margin-top:8px">本研究通过分析GitHub拉取请求（PR）数据集，探讨AI编码代理在开源开发中的贡献方式。其动机在于理解代理生成的PR与人类PR在代码修改和描述上的差异。研究人员使用MSR 2026 Mining Challenge版本的AIDev数据集，对比了24,014个合并的代理PR与5,081个合并的人类PR，考察了提交次数、修改文件及描述与代码变更的一致性。结果显示，代理PR的提交次数显著高于人类PR，修改的文件数量和删除行数存在中等差异，且在描述与代码变更的语义一致性方面表现略优。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing the Impact of Code Changes on the Fault Localizability of Large Language Models</div>
<div class="meta-line">Authors: Sabaat Haroon, Ahmad Faraz Khan, Ahmad Humayun, Waris Gill, Abdul Haddi Amjad, Ali R. Butt, Mohammad Taha Khan, Muhammad Ali Gulzar</div>
<div class="meta-line">First: 2025-04-06T05:59:29+00:00 · Latest: 2026-01-24T10:52:01+00:00</div>
<div class="meta-line">Comments: This paper is currently Under Review. It consists of 12 pages, 11 Figures, and 5 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.04372v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.04372v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative Large Language Models (LLMs) are increasingly used in non-generative software maintenance tasks, such as fault localization (FL). Success in FL depends on a models ability to reason about program semantics beyond surface-level syntactic and lexical features. However, widely used LLM benchmarks primarily evaluate code generation, which differs fundamentally from semantic program reasoning. Meanwhile, traditional FL benchmarks such as Defect4J and BugsInPy are either not scalable or obsolete, as their datasets have become part of LLM training data, leading to biased results. This paper presents the first large-scale empirical investigation into the robustness of LLMs fault localizability. Inspired by mutation testing, we develop an end-to-end evaluation framework that addresses key limitations in existing LLM evaluation, including data contamination, scalability, automation, and extensibility. Using real-world programs with specifications, we inject unseen faults and ask LLMs to localize them, filtering out underspecified programs where localization is ambiguous. For each successfully localized program, we apply semantic-preserving mutations (SPMs) and rerun localization to assess robustness and determine whether LLM reasoning relies on syntactic cues rather than semantics. We evaluate 10 state-of-the-art LLMs on 750,013 fault localization tasks from over 1,300 Java and Python programs. We find that SPMs cause LLMs to fail on previously localized faults in 78% of cases, and that reasoning is stronger when relevant code appears earlier in context. These results indicate that LLM code reasoning is often tied to features irrelevant to semantics. We also identify code patterns that are challenging for LLMs to reason about. Overall, our findings motivate fundamental advances in how LLMs represent, interpret, and prioritize code semantics to reason more deeply about program logic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估代码更改对大型语言模型故障定位能力的影响</div>
<div class="mono" style="margin-top:8px">生成式大型语言模型（LLMs）越来越多地用于非生成式软件维护任务，如故障定位（FL）。在FL中取得成功取决于模型能够超越表面语法和词汇特征，对程序语义进行推理。然而，广泛使用的LLM基准测试主要评估代码生成，这与语义程序推理有本质区别。同时，传统FL基准如Defect4J和BugsInPy要么无法扩展，要么已过时，因为它们的数据集已成为LLM训练数据的一部分，导致结果偏倚。本文提出了首个大规模实证研究，探讨LLMs在故障定位方面的鲁棒性。受变异测试启发，我们开发了一个端到端的评估框架，解决了现有LLM评估中的关键限制，包括数据污染、可扩展性、自动化和可扩展性。我们使用带有规范的真实世界程序，注入未见过的故障，并要求LLMs进行定位，过滤掉定位模糊的欠规范程序。对于每个成功定位的程序，我们应用语义保持的变异（SPMs），并重新运行定位以评估鲁棒性，并确定LLM推理是否依赖于语法提示而非语义。我们对超过1300个Java和Python程序中的750,013个故障定位任务进行了10个最先进的LLMs评估。我们发现，在78%的情况下，SPMs会导致LLMs在之前定位的故障上失败，并且当相关代码在上下文中出现得更早时，推理能力更强。这些结果表明，LLM的代码推理通常与语义无关的特征相关联。我们还识别了LLMs难以推理的代码模式。总体而言，我们的发现推动了LLMs在表示、解释和优先处理代码语义方面进行根本性改进，以更深入地理解程序逻辑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how code changes affect the fault localizability of large language models (LLMs) in software maintenance tasks. Motivated by the limitations of existing benchmarks that focus on code generation rather than semantic reasoning, the study introduces an end-to-end evaluation framework inspired by mutation testing. The framework injects unseen faults into real-world programs with specifications and assesses LLMs&#x27; ability to localize these faults, while filtering out ambiguous cases. The main experimental results show that semantic-preserving mutations significantly reduce LLMs&#x27; success in fault localization, with 78% of previously localized faults becoming undetectable. Additionally, the study finds that reasoning performance improves when relevant code appears earlier in the context, suggesting that LLMs may rely more on syntactic features than semantic understanding.</div>
<div class="mono" style="margin-top:8px">本文研究了代码更改对大型语言模型（LLMs）在软件维护任务中故障定位能力的影响。由于现有基准主要评估代码生成而非语义推理，研究动机在于探索更有效的评估方法。论文提出了一种受突变测试启发的端到端评估框架，通过在具有规范的现实程序中注入未见过的故障来评估LLMs的定位能力，并过滤掉定位模糊的程序。实验结果显示，78%的情况下，语义保持的突变会导致LLMs无法定位之前成功识别的故障，表明其推理可能依赖于语法特征而非语义。此外，研究还识别出一些对LLMs推理构成挑战的代码模式，强调了提升代码语义理解能力的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</div>
<div class="meta-line">Authors: Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-10T20:04:08+00:00 · Latest: 2026-01-23T21:03:18+00:00</div>
<div class="meta-line">Comments: Conference: NeurIPS 2025 (main)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10046v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10046v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimWorld-Robotics：为多模态机器人导航与协作合成逼真且动态的城市环境</div>
<div class="mono" style="margin-top:8px">近年来，基础模型在开发能够根据多模态输入在开放场景中执行多样化任务的通用型机器人方面取得了令人鼓舞的成果。然而，当前的研究主要集中在室内和家庭场景。在本工作中，我们提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境的具身AI模拟平台。SWR基于Unreal Engine 5构建，能够程序化生成包含行人和交通系统等动态元素的无限逼真城市场景，其在真实感、复杂性和可扩展性方面超越了以往的城市模拟。它还支持多机器人控制与通信。借助这些关键特性，我们构建了两个具有挑战性的机器人基准测试：(1) 多模态指令跟随任务，其中机器人必须根据视觉-语言导航指令在行人和交通存在的情况下到达目标；(2) 多智能体搜索任务，其中两个机器人必须通过通信协作定位并相遇。与现有基准不同，这两个新基准在真实场景中全面评估了多种关键机器人能力，包括(1) 多模态指令的语义对齐，(2) 大规模环境中的三维空间推理，(3) 在行人和交通中的安全长距离导航，(4) 多机器人协作，以及(5) 基于环境的通信。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SimWorld-Robotics (SWR), a simulation platform designed to support the development of generalist robotics in complex, large-scale urban environments. Motivated by the need for more realistic and dynamic training scenarios for robots, SWR leverages Unreal Engine 5 to generate photorealistic and procedurally infinite urban scenes with dynamic elements such as pedestrians and traffic systems. The platform enables multi-robot control and communication, and is used to create two challenging benchmarks: a multimodal instruction-following task and a multi-agent search task. Experimental results show that state-of-the-art models, including vision-language models, struggle with these tasks, highlighting deficiencies in perception, reasoning, and planning for real-world urban navigation.</div>
<div class="mono" style="margin-top:8px">本文提出SimWorld-Robotics（SWR），一个用于在复杂、逼真的城市环境中开发通用机器人技术的仿真平台。研究动机源于对现有室内场景基准的局限性，以及对真实世界开放场景中机器人能力评估的需求。SWR基于Unreal Engine 5构建，能够生成包含行人和交通系统的动态城市场景，支持多模态指令跟随和多智能体搜索任务。实验结果表明，当前最先进的模型，如视觉语言模型，在这些任务中表现不佳，缺乏应对城市环境所需的稳健感知、推理和规划能力。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</div>
<div class="meta-line">Authors: Riyang Bao, Cheng Yang, Dazhou Yu, Zhexiang Tang, Gengchen Mai, Liang Zhao</div>
<div class="meta-line">First: 2026-01-23T18:33:45+00:00 · Latest: 2026-01-23T18:33:45+00:00</div>
<div class="meta-line">Comments: 15pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16965v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spatial-Agent: 基于科学核心概念的代理式地理空间推理</div>
<div class="mono" style="margin-top:8px">地理空间推理对于城市分析、交通规划和灾害响应等现实世界应用至关重要。然而，现有的基于大语言模型（LLM）的代理通常无法进行真正的地理空间计算，而是依赖网络搜索或模式匹配，并在空间关系上产生幻觉。我们提出了Spatial-Agent，这是一种基于空间信息科学基础理论的AI代理。我们的方法将地理分析问题回答形式化为概念转换问题，其中自然语言问题被解析为可执行的工作流，表示为GeoFlow图——一种有向无环图，节点对应空间概念，边表示转换。基于空间信息理论，Spatial-Agent通过基于模板的生成提取空间概念，分配具有原则性排序约束的功能角色，并组合转换序列。在MapEval-API和MapQA基准上的大量实验表明，Spatial-Agent在性能上显著优于现有基线方法，包括ReAct和Reflexion，同时生成可解释且可执行的地理空间工作流。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial-Agent was developed to address the limitations of current LLM-based agents in performing genuine geospatial computations, which are crucial for applications like urban analytics and disaster response. The agent uses foundational theories from spatial information science to formalize geo-analytical question answering as a concept transformation problem, translating natural language questions into executable workflows called GeoFlow Graphs. These graphs are directed acyclic structures where nodes represent spatial concepts and edges denote transformations. The approach employs template-based generation to extract spatial concepts, assign functional roles, and enforce ordering constraints. Experimental results on MapEval-API and MapQA benchmarks show that Spatial-Agent significantly outperforms existing methods such as ReAct and Reflexion, producing both interpretable and executable geospatial workflows.</div>
<div class="mono" style="margin-top:8px">Spatial-Agent 是为了解决现有基于 LLM 的代理在进行真实地理计算时的不足而开发的，这些计算对于城市分析和灾害响应等应用至关重要。该方法基于空间信息科学，将自然语言问题转化为可执行的 GeoFlow 图，这是一种表示空间概念及其转换的有向无环图。在 MapEval-API 和 MapQA 基准上的实验结果表明，Spatial-Agent 显著优于 ReAct 和 Reflexion 等基线模型，能够生成可解释且可执行的地理空间工作流。</div>
</details>
</div>
<div class="card">
<div class="title">EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents</div>
<div class="meta-line">Authors: Xinze Li, Ziyue Zhu, Siyuan Liu, Yubo Ma, Yuhang Zang, Yixin Cao, Aixin Sun</div>
<div class="meta-line">First: 2026-01-23T12:09:59+00:00 · Latest: 2026-01-23T12:09:59+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent&#x27;s own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EMemBench：面向VLM代理的事件记忆交互基准测试</div>
<div class="mono" style="margin-top:8px">我们引入了EMemBench，这是一个用于通过交互游戏评估代理长期记忆的程序化基准。与使用固定问题集不同，EMemBench从每个代理自身的轨迹中生成问题，涵盖文本和视觉游戏环境。每个模板通过底层游戏信号计算可验证的基准答案，具有可控的可回答性，并在记忆技能上保持平衡覆盖：单跳/多跳回忆、归纳、时间、空间、逻辑和对抗性。我们使用上下文提示作为基线，评估具有强大语言模型/VLM作为骨干的记忆代理。在15个文本游戏和多个视觉种子上，结果远未饱和：归纳和空间推理仍然是持续的瓶颈，尤其是在视觉环境中。持久记忆在文本游戏中为开放骨干模型带来了明显提升，但对VLM代理的改进则不够一致，这表明基于视觉的事件记忆仍然是一个开放性挑战。一项人类研究进一步验证了EMemBench的难度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EMemBench is introduced as a programmatic benchmark to assess the long-term memory capabilities of agents in interactive environments. It generates questions based on each agent&#x27;s trajectory, encompassing both text and visual game settings, and computes verifiable ground truth from game signals. The benchmark covers various memory skills, including single/multi-hop recall, induction, temporal, spatial, logical, and adversarial reasoning. Evaluation results across 15 text games and multiple visual seeds show that induction and spatial reasoning are persistent challenges, particularly in visual settings. While persistent memory benefits open backbones in text games, improvements for VLM agents are less consistent, indicating that visually grounded episodic memory remains an open problem.</div>
<div class="mono" style="margin-top:8px">EMemBench 是一个用于评估智能体长期记忆能力的程序化基准，通过生成基于每个智能体轨迹的问题，覆盖文本和视觉游戏环境。该基准利用底层游戏信号计算可验证的参考答案，并在多种记忆技能上保持答案可性与覆盖平衡。评估结果显示，归纳推理和空间推理在视觉环境中仍是持续存在的瓶颈，表明基于视觉的片段记忆仍是一个开放性问题。一项人类研究进一步证实了该基准的难度。</div>
</details>
</div>
<div class="card">
<div class="title">TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning</div>
<div class="meta-line">Authors: Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin, Haoyu Cao, Xing Sun, Ying Shen, Hai-Tao Zheng, Liang Lin, Philip S. Yu</div>
<div class="meta-line">First: 2026-01-23T07:35:05+00:00 · Latest: 2026-01-23T07:35:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16520v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16520v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramPuzzle：通过组合空间推理评估多模态大语言模型</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在视觉识别和语义理解方面取得了显著进展。然而，它们在精确组合空间推理方面的能力仍 largely 未被探索。现有基准测试通常涉及相对简单的任务，并依赖语义近似或粗略的相对位置，而其评估指标通常有限且缺乏严谨的数学定义。为弥合这一差距，我们引入 TangramPuzzle，这是一个基于几何的基准测试，旨在通过经典的 Tangram 游戏视角评估组合空间推理能力。我们提出了 Tangram Construction Expression（TCE），一种符号几何框架，通过精确、可由机器验证的坐标规范来定义 Tangram 组装，以减少视觉近似带来的歧义。我们设计了两个互补任务：轮廓预测，要求从局部组件推断整体形状；以及端到端代码生成，要求解决逆向几何组装问题。我们在先进的开源和专有模型上进行了广泛的评估实验，发现了一个有趣的见解：MLLMs 倾向于优先匹配目标轮廓，而忽视几何约束，导致组件发生形变或扭曲。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of rigorous evaluation of compositional spatial reasoning in Multimodal Large Language Models (MLLMs). The authors introduce TangramPuzzle, a geometry-based benchmark inspired by the classic Tangram game, to assess MLLMs&#x27; ability to reason about spatial compositions. They propose Tangram Construction Expression (TCE), a symbolic framework that uses precise coordinate specifications to eliminate ambiguity in visual approximations. Two tasks, Outline Prediction and End-to-End Code Generation, are designed to test different aspects of spatial reasoning. Experimental results show that MLLMs tend to focus on matching the target silhouette rather than adhering to geometric constraints, resulting in distorted or deformed tangram pieces.</div>
<div class="mono" style="margin-top:8px">本文旨在填补多模态大语言模型（MLLMs）在组合空间推理方面的评估空白。作者提出了TangramPuzzle，一个基于几何的经典七巧板游戏启发的基准测试，用于通过精确的坐标规范评估MLLMs的空间组合推理能力。他们设计了Tangram Construction Expression（TCE）这一符号化框架，并构建了两个互补任务：轮廓预测和端到端代码生成。实验结果表明，MLLMs往往更关注匹配目标轮廓而忽视几何约束，导致拼图组件出现变形或错误排列。</div>
</details>
</div>
<div class="card">
<div class="title">Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models</div>
<div class="meta-line">Authors: Bridget Leonard, Scott O. Murray</div>
<div class="meta-line">First: 2026-01-23T00:21:27+00:00 · Latest: 2026-01-23T00:21:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent&#x27;s visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于认知的标记克服多模态模型中的自我中心偏差</div>
<div class="mono" style="margin-top:8px">多模态语言模型（MLMs）在语义视觉-语言任务上表现良好，但在需要采用其他代理视觉视角的空间推理任务上表现不佳。这些错误反映了持续存在的自我中心偏差，并引发了关于当前模型是否支持参照中心推理的疑问。受人类空间认知启发，我们引入了视角标记，这些专门的嵌入通过（1）具身身体关键点线索或（2）支持心理旋转的抽象表示来编码方向。将这些标记整合到LLaVA-1.5-13B中，使其在二级视觉视角任务上表现出色。在合成和自然基准（Isle Bricks V2、COCO、3DSRBench）中，视角标记提高了准确性，基于旋转的标记能够泛化到非人类参考代理。表示分析表明，微调增强了基础模型中已有的潜在方向敏感性，这表明MLMs包含参照中心推理的前身，但缺乏适当的内部结构。总体而言，将基于认知的空间结构直接嵌入到标记空间中，为视角采取和更类人空间推理提供了一种轻量且模型无关的机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the egocentric bias in multimodal language models (MLMs), which limits their ability to perform spatial reasoning from another agent&#x27;s visual perspective. To overcome this, the authors introduce perspective tokens—specialized embeddings that encode spatial orientation using either body-keypoint cues or abstract mental rotation representations. By integrating these tokens into LLaVA-1.5-13B, the model achieves improved performance on level-2 visual perspective-taking tasks. Experimental results on synthetic and naturalistic benchmarks show that perspective tokens significantly enhance accuracy, with rotation-based tokens demonstrating better generalization to non-human reference agents. The findings suggest that while MLMs may already possess latent orientation sensitivity, they lack the internal structure necessary for allocentric reasoning, and embedding cognitively inspired spatial information into the token space offers a promising, lightweight solution for more human-like spatial understanding.</div>
<div class="mono" style="margin-top:8px">本文旨在解决多模态语言模型（MLMs）在空间推理中表现出的以自我为中心的偏差问题，该偏差限制了模型从其他代理视角进行视觉推理的能力。为此，作者引入了视角标记（perspective tokens），通过身体关键点提示或抽象的旋转表示来编码空间方向。将这些标记集成到LLaVA-1.5-13B模型中，使其在二级视觉视角任务中表现提升。在合成和自然基准测试（如Isle Bricks V2、COCO和3DSRBench）中，视角标记显著提高了准确性，其中基于旋转的标记在非人类参考代理上表现出更强的泛化能力。研究还表明，尽管MLMs可能已经具备一定的方向敏感性，但缺乏支持非自我中心推理的适当内部结构。</div>
</details>
</div>
<div class="card">
<div class="title">VOCAL: Visual Odometry via ContrAstive Learning</div>
<div class="meta-line">Authors: Chi-Yao Huang, Zeel Bhatt, Yezhou Yang</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-06-30T20:26:13+00:00 · Latest: 2026-01-22T22:04:57+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.00243v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.00243v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Breakthroughs in visual odometry (VO) have fundamentally reshaped the landscape of robotics, enabling ultra-precise camera state estimation that is crucial for modern autonomous systems. Despite these advances, many learning-based VO techniques rely on rigid geometric assumptions, which often fall short in interpretability and lack a solid theoretical basis within fully data-driven frameworks. To overcome these limitations, we introduce VOCAL (Visual Odometry via ContrAstive Learning), a novel framework that reimagines VO as a label ranking challenge. By integrating Bayesian inference with a representation learning framework, VOCAL organizes visual features to mirror camera states. The ranking mechanism compels similar camera states to converge into consistent and spatially coherent representations within the latent space. This strategic alignment not only bolsters the interpretability of the learned features but also ensures compatibility with multimodal data sources. Extensive evaluations on the KITTI dataset highlight VOCAL&#x27;s enhanced interpretability and flexibility, pushing VO toward more general and explainable spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOCAL: 通过对比学习实现的视觉里程计</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）领域的突破彻底改变了机器人学的格局，使超精确的相机状态估计成为现代自主系统的关键。尽管取得了这些进展，许多基于学习的VO技术仍依赖于刚性的几何假设，这在可解释性和完全数据驱动框架中的理论基础方面往往存在不足。为克服这些局限性，我们引入了VOCAL（通过对比学习实现的视觉里程计），一个新颖的框架，将VO重新构想为一个标签排序问题。通过将贝叶斯推断与表示学习框架相结合，VOCAL将视觉特征组织成与相机状态相对应的形式。排序机制迫使相似的相机状态在潜在空间中收敛为一致且空间连贯的表示。这种策略性对齐不仅增强了所学特征的可解释性，还确保了其与多模态数据源的兼容性。在KITTI数据集上的广泛评估突显了VOCAL在可解释性和灵活性方面的提升，推动视觉里程计向更通用和可解释的空间智能发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual odometry (VO) is a critical component in autonomous systems, yet many learning-based approaches suffer from limited interpretability and theoretical grounding due to their reliance on rigid geometric assumptions. VOCAL addresses these issues by redefining VO as a label ranking problem, integrating Bayesian inference with representation learning to align visual features with camera states. The framework enforces spatial coherence in the latent space by compelling similar camera states to form consistent representations, thereby improving interpretability and enabling multimodal compatibility. Evaluations on the KITTI dataset demonstrate VOCAL&#x27;s superior performance in terms of both interpretability and adaptability, advancing the field toward more general and explainable spatial perception.</div>
<div class="mono" style="margin-top:8px">视觉里程计（VO）是自主系统中的关键组成部分，但许多基于学习的方法由于依赖刚性几何假设而存在可解释性不足和理论基础薄弱的问题。VOCAL通过将VO重新定义为标签排序问题，结合贝叶斯推理与表征学习，使视觉特征与相机状态对齐。该框架通过排序机制在潜在空间中强制相似相机状态形成一致且空间连贯的表征，从而提升模型的可解释性和灵活性。在KITTI数据集上的广泛评估表明，VOCAL在可解释性和泛化能力方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-22T19:05:41+00:00</div>
<div class="meta-line">Comments: Work done as part of the EleutherAI SOAR Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲区</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲区。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器进行构建。训练方法往往将图像扁平化为1D的patch序列，丢弃了空间推理所需的2D结构。我们认为这种缺乏空间感知是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of vision-language models (VLMs) has revealed a limitation in their ability to capture spatial relationships, which is critical for applications like robotics and embodied AI. This study investigates two approaches to enhance spatial awareness: using image encoders trained with alternative objectives and incorporating 2D positional encodings. Experimental results demonstrate that these modifications lead to improved performance in spatial reasoning tasks across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在空间推理方面的不足，指出当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，将图像扁平化为1D序列，从而丢失了对空间结构至关重要的2D信息。作者提出了两种改进方法：使用替代目标训练图像编码器以及引入2D位置编码。实验结果表明，这些架构改进显著提升了多个基准测试中的空间推理能力，强调了空间结构在VLM设计中的重要性，特别是在机器人和具身AI等需要空间定位的应用中。</div>
</details>
</div>
<div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知</div>
<div class="mono" style="margin-top:8px">大型音频语言模型（LALMs）在语音识别、音频描述和听觉问答任务中最近取得了显著进展。然而，这些模型是否能够感知空间动态，特别是声源的运动，仍不清楚。在本研究中，我们发现当前音频语言模型（ALLMs）存在系统性的运动感知缺陷。为了解决这一问题，我们引入了AudioMotionBench，这是首个专门设计用于评估听觉运动理解的基准测试。AudioMotionBench引入了一个受控的问答基准测试，用于评估音频语言模型（LALMs）能否从双耳音频中推断出移动声源的方向和轨迹。全面的定量和定性分析表明，当前模型在可靠识别运动线索或区分方向模式方面存在困难。平均准确率仍低于50\%，突显了听觉空间推理的基本局限性。我们的研究指出了人类与模型在听觉空间推理之间的根本差距，为未来增强音频语言模型的空间认知能力提供了诊断工具和新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of Large Audio-Language Models (LALMs) to perceive auditory motion, specifically the direction and trajectory of moving sound sources from binaural audio. The researchers introduce AudioMotionBench, a novel benchmark designed to evaluate this capability systematically. Their analysis shows that current models perform poorly, with average accuracy below 50\%, indicating a significant limitation in understanding spatial dynamics through audio.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型音频语言模型（LALMs）在感知听觉运动方面的能力，特别是对移动声源方向和轨迹的识别。研究人员提出了AudioMotionBench，这是首个专门用于评估听觉运动理解的基准测试。实验结果显示，当前模型在识别运动线索和区分方向模式方面表现不佳，平均准确率低于50%，表明其在听觉空间推理方面存在根本性缺陷。这些发现揭示了人类与模型在听觉空间理解上的显著差距，并为未来提升音频语言模型的空间认知能力提供了诊断工具和新思路。</div>
</details>
</div>
<div class="card">
<div class="title">Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</div>
<div class="meta-line">Authors: Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</div>
<div class="meta-line">First: 2025-05-19T16:56:31+00:00 · Latest: 2026-01-22T14:25:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13353v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.13353v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code&#x27;s operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs&#x27; accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval&#x27;s 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知与敏感性：探究语义回忆对长上下文代码推理的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用于理解大型代码库，但它们是通过理解长代码上下文的操作语义还是依赖模式匹配捷径仍不清楚。我们区分了词汇回忆（逐字检索代码）和语义回忆（理解操作语义）。通过评估10种最先进的LLMs，我们发现虽然前沿模型能够实现近乎完美的、位置无关的词汇回忆，但当代码位于长上下文的中心位置时，语义回忆会严重退化。我们引入了语义回忆敏感性这一指标，用于衡量任务是否需要理解代码的操作语义，还是允许使用模式匹配捷径。通过一种新颖的反事实测量方法，我们表明模型在解决现有代码理解基准时严重依赖模式匹配捷径。我们提出了一项新任务SemTrace，其通过不可预测的操作实现高语义回忆敏感性；LLMs的准确性表现出严重的位移效应，当相关代码片段接近输入代码上下文的中间位置时，中位数准确率下降了92.73%，而CRUXEval则下降了53.36%。我们的研究结果表明，当前的评估方法在长上下文代码理解中严重低估了语义回忆失败的情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how large language models (LLMs) handle long code contexts, focusing on whether they rely on pattern matching or truly understand the operational semantics. The research differentiates between lexical recall, which involves retrieving code verbatim, and semantic recall, which requires comprehension of code meaning. By evaluating ten state-of-the-art LLMs, the authors found that while models perform well in lexical recall, their semantic recall significantly declines when code is located in the central part of long contexts. They introduce a metric called semantic recall sensitivity to assess the necessity of semantic understanding in tasks. Using a counterfactual measurement approach, they demonstrate that models heavily depend on pattern matching shortcuts for existing benchmarks. The proposed task SemTrace shows that LLMs&#x27; accuracy drops sharply as code snippets move toward the middle of the input, indicating a substantial underestimation of semantic recall failures in current evaluations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在处理长代码上下文时是否依赖模式匹配还是真正理解代码的运行语义。研究区分了字面记忆（直接检索代码）和语义记忆（理解代码含义）。通过对十种最先进的LLMs进行评估，发现虽然模型在字面记忆方面表现接近完美，但当代码位于长上下文的中间位置时，其语义记忆能力严重下降。作者引入了语义记忆敏感性这一指标，用于评估任务是否需要理解代码的运行语义。通过一种新颖的反事实测量方法，他们表明模型在现有代码理解基准中高度依赖模式匹配。他们提出的SemTrace任务通过不可预测的操作实现了高语义记忆敏感性，结果显示模型准确率存在显著的位置效应，中间位置的代码导致准确率中位数下降92.73%，远高于CRUXEval的53.36%，表明当前评估可能低估了长上下文代码理解中的语义记忆失败问题。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</div>
<div class="meta-line">Authors: Pascal Benschop, Justin Dauwels, Jan van Gemert</div>
<div class="meta-line">First: 2026-01-22T09:14:11+00:00 · Latest: 2026-01-22T09:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用合成生成视频评估视觉语言模型的情境与空间感知</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在空间推理方面仍存在脆弱性，当语义依赖于细微的时间或几何线索时。我们引入了一个合成基准测试，用于评估两种互补能力：情境感知（判断互动是否有害或无害）和空间感知（追踪谁对谁做了什么，以及推理相对位置和运动）。通过最小视频对，我们测试了三个挑战：区分暴力行为与良性活动、跨视角绑定攻击者角色、以及判断细粒度轨迹对齐。尽管我们在无训练设置下评估了近期的VLMs，但该基准适用于任何视频分类模型。结果显示，模型在各项任务中的表现仅略高于随机猜测。一个简单的辅助手段——稳定的颜色线索——部分缓解了攻击者角色的混淆，但并未解决其根本弱点。通过发布数据和代码，我们旨在提供可复现的诊断工具，并为补充大规模预训练的轻量级空间先验探索提供起点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to evaluate the situational and spatial awareness capabilities of vision language models (VLMs) by introducing a synthetic video benchmark. The benchmark tests three key challenges: distinguishing between violent and benign interactions, binding assailant roles across different viewpoints, and assessing fine-grained trajectory alignment. The results indicate that current VLMs perform only slightly better than chance in these tasks, suggesting limitations in their ability to process subtle temporal and geometric information. A simple modification, such as stable color cues, helps reduce some role confusions but does not fully address the core issues.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLMs）在空间推理方面的不足，提出一个合成基准测试，通过生成的视频评估情境意识和空间意识。该基准测试包含三个关键挑战：区分暴力与良性行为、跨视角绑定攻击者角色以及评估细粒度轨迹对齐。实验结果表明，当前VLMs在这些任务上的表现仅略优于随机猜测，凸显了其在理解细微时间与几何线索方面的脆弱性。引入稳定的颜色提示在一定程度上缓解了攻击者角色混淆的问题，但未能根本解决模型的内在缺陷，提示需要更强大的空间先验知识来提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</div>
<div class="meta-line">Authors: Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Yingheng Wang, Bram Hoex, Zhicheng Zhong, Tong Xie</div>
<div class="meta-line">First: 2025-10-06T11:17:56+00:00 · Latest: 2026-01-22T05:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04704v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.04704v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomWorld：评估大型语言模型在晶体材料中空间推理能力的基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在文本推理方面表现出色，并开始发展空间理解能力，这引发了关于这些能力是否可以结合用于复杂、领域特定任务的疑问。这一问题在材料科学等需要深入理解三维原子结构的领域尤为重要。尽管初步研究已成功将LLMs应用于纯晶体生成或坐标理解任务，但缺乏一个标准化的基准来系统评估其在多样化原子结构中的核心推理能力。为了解决这一问题，我们引入了AtomWorld基准，用于基于晶体学信息文件（CIFs）的LLMs任务评估。这些任务包括结构编辑、CIF感知和性质引导建模，揭示了一个关键限制：尽管当前模型建立了有希望的基线，但在结构理解和空间推理方面仍存在持续性失败。我们的实验表明，这些模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析和材料洞察中的累积错误。通过定义这些标准化任务，AtomWorld为推动LLMs向稳健的原子尺度建模发展奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to assess the spatial reasoning capabilities of large language models (LLMs) in the context of crystalline materials, which is vital for materials science. The authors introduce AtomWorld, a benchmark based on Crystallographic Information Files (CIFs), to evaluate LLMs on tasks such as structural editing, CIF perception, and property-guided modeling. The main experimental results indicate that current models, although showing promise, struggle with structural understanding and spatial reasoning, often making frequent errors in modifying crystal structures and interpreting CIF data, which can lead to significant issues in downstream analysis and materials discovery.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大型语言模型（LLMs）在晶体材料领域的空间推理能力，这对于材料科学至关重要。作者提出了AtomWorld基准，基于晶体学信息文件（CIFs），包含结构编辑、CIF感知和性质引导建模等任务。实验结果表明，当前的LLMs虽然在基础任务上表现有潜力，但在结构理解和空间推理方面存在显著不足，经常在修改原子结构和解析CIF数据时出错，可能导致后续分析和材料洞察的累积误差。</div>
</details>
</div>
<div class="card">
<div class="title">VibeTensor: System Software for Deep Learning, Fully Generated by AI Agents</div>
<div class="meta-line">Authors: Bing Xu, Terry Chen, Fengzhe Zhou, Tianqi Chen, Yangqing Jia, Vinod Grover, Haicheng Wu, Wei Liu, Craig Wittenbrink, Wen-mei Hwu, Roger Bringmann, Ming-Yu Liu, Luis Ceze, Michael Lightstone, Humphrey Shi</div>
<div class="meta-line">First: 2026-01-21T19:29:00+00:00 · Latest: 2026-01-21T19:29:00+00:00</div>
<div class="meta-line">Comments: Open-source: https://github.com/NVLabs/vibetensor</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16238v1">PDF</a> · <a href="https://github.com/NVLabs/vibetensor">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">VIBETENSOR is an open-source research system software stack for deep learning, generated by LLM-powered coding agents under high-level human guidance. In this paper, &quot;fully generated&quot; refers to code provenance: implementation changes were produced and applied as agent-proposed diffs; validation relied on agent-run builds, tests, and differential checks, without per-change manual diff review. It implements a PyTorch-style eager tensor library with a C++20 core (CPU+CUDA), a torch-like Python overlay via nanobind, and an experimental Node.js/TypeScript interface. Unlike thin bindings, VIBETENSOR includes its own tensor/storage system, schema-lite dispatcher, reverse-mode autograd, CUDA runtime (streams/events/graphs), a stream-ordered caching allocator with diagnostics, and a stable C ABI for dynamically loaded operator plugins. We view this release as a milestone for AI-assisted software engineering: it shows coding agents can generate a coherent deep learning runtime spanning language bindings down to CUDA memory management, validated primarily by builds and tests. We describe the architecture, summarize the workflow used to produce and validate the system, and evaluate the artifact. We report repository scale and test-suite composition, and summarize reproducible microbenchmarks from an accompanying AI-generated kernel suite, including fused attention versus PyTorch SDPA/FlashAttention. We also report end-to-end training sanity checks on 3 small workloads (sequence reversal, ViT, miniGPT) on NVIDIA H100 (Hopper, SM90) and Blackwell-class GPUs; multi-GPU results are Blackwell-only and use an optional CUTLASS-based ring-allreduce plugin gated on CUDA 13+ and sm103a toolchain support. Finally, we discuss failure modes in generated system software, including a &quot;Frankenstein&quot; composition effect where locally correct subsystems interact to yield globally suboptimal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VibeTensor：由AI代理完全生成的深度学习系统软件</div>
<div class="mono" style="margin-top:8px">VIBETENSOR是一个开源的深度学习研究系统软件栈，由LLM驱动的编码代理在高水平人类指导下生成。在本文中，&quot;完全生成&quot;指的是代码溯源：实现的更改由代理提议并应用；验证依赖于代理运行的构建、测试和差异检查，而无需逐项人工审查差异。它实现了一个PyTorch风格的即时张量库，包含C++20核心（CPU+CUDA），通过nanobind实现了一个类似Torch的Python封装，并提供了一个实验性的Node.js/TypeScript接口。与薄绑定不同，VIBETENSOR包含自己的张量/存储系统、schema-lite调度器、反向模式自动微分、CUDA运行时（流/事件/图），一个流顺序的缓存分配器及其诊断功能，以及一个稳定的C ABI，用于动态加载的操作符插件。我们认为此次发布是AI辅助软件工程的一个里程碑：它表明编码代理可以生成一个连贯的深度学习运行时，从语言绑定一直到CUDA内存管理，主要通过构建和测试进行验证。我们描述了该系统的架构，总结了用于生成和验证系统的流程，并评估了该成果。我们报告了仓库规模和测试套件组成，并总结了从配套的AI生成内核套件中获得的可复现的微基准测试结果，包括融合注意力与PyTorch SDPA/FlashAttention的对比。我们还报告了在NVIDIA H100（Hopper，SM90）和Blackwell系列GPU上对三个小型工作负载（序列反转、ViT、miniGPT）进行的端到端训练合理性检查；多GPU结果仅限于Blackwell，并使用基于CUTLASS的可选环形AllReduce插件，该插件依赖于CUDA 13+和sm103a工具链支持。最后，我们讨论了生成系统软件中的故障模式，包括一个&quot;弗兰肯斯坦&quot;组合效应，其中局部正确的子系统相互作用导致全局性能不佳。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind VIBETENSOR is to explore the potential of AI agents in generating complex deep learning system software with high-level human guidance. The system is fully generated by AI agents, utilizing LLM-powered coding agents to produce and apply code changes as diffs, and validate them through automated builds, tests, and differential checks. VIBETENSOR implements a PyTorch-style eager tensor library with a C++20 core, Python overlay via nanobind, and experimental Node.js/TypeScript interface, including its own tensor/storage system, reverse-mode autograd, CUDA runtime, and a stable C ABI for plugins. Experimental results show that the system achieves comparable performance to PyTorch in microbenchmarks and passes end-to-end training sanity checks on small workloads, although it also exhibits failure modes such as the &quot;Frankenstein&quot; composition effect, where subsystems perform well individually but interact poorly in the overall system.</div>
<div class="mono" style="margin-top:8px">VIBETENSOR的动机是探索AI代理在生成完整且一致的深度学习系统软件栈方面的潜力。该系统通过LLM驱动的代码生成代理，在人类高层指导下开发，实现了PyTorch风格的即时张量库，包含C++20核心（CPU+CUDA）和通过nanobind实现的Python封装层。主要实验结果表明，该系统通过构建和测试验证了其正确性和一致性，具备反向模式自动微分、CUDA运行时、缓存分配器等功能。AI生成的内核套件性能与PyTorch的SDPA和FlashAttention进行了对比，同时在三个小型任务上进行了端到端训练验证。然而，研究也指出生成系统软件可能存在的问题，如&quot;Frankenstein&quot;组合效应，即局部正确的子系统可能在整体系统中导致次优性能。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理为何失败？对GitHub中失败代理拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编码代理现在正在向软件项目提交拉取请求（PRs），不仅作为助手，还作为自主贡献者。随着这些代理贡献在真实仓库中迅速增长，人们对它们在实际中的行为以及为何许多未能被合并仍知之甚少。在本文中，我们对GitHub上五个编码代理提交的33,000个代理撰写的PR进行了大规模研究。 (RQ1) 我们首先从四个广泛维度定量描述了合并和未合并的PR：1）任务类型下的合并结果，2）代码更改，3）CI构建结果，以及4）评审动态。我们观察到，与文档、CI和构建更新相关的任务合并成功率最高，而性能和错误修复任务表现最差。未合并的PR通常涉及更大的代码更改，修改更多文件，并且常常无法通过项目的CI/CD流水线验证。 (RQ2) 为了进一步探讨为何某些代理PR未被合并，我们对600个PR进行了定性分析，以建立拒绝模式的分层分类法。这种分析通过揭示定量指标未涵盖的拒绝原因，如缺乏有意义的评审互动、重复PR、不受欢迎的功能实现以及代理与项目目标的不一致，补充了RQ1的定量发现。我们的研究结果突显了关键的社会技术因素和人机协作因素，这些因素对于提高未来代理工作流的成功率至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of AI coding agents in contributing to software projects through GitHub pull requests. By analyzing 33,000 PRs from five coding agents, the research identifies patterns in merge success across different task types, revealing that documentation, CI, and build-related tasks have higher success rates, while performance and bug-fix tasks are less likely to be merged. The analysis also shows that unmerged PRs typically involve more extensive code changes, affect more files, and fail CI/CD validation. Through a qualitative examination of 600 PRs, the study uncovers additional rejection reasons such as lack of reviewer interaction, duplicate submissions, and misaligned agent behavior, providing insights into the socio-technical challenges in AI-assisted software development.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在通过GitHub提交拉取请求（PR）时的失败情况。通过对五个编码代理提交的33,000个PR进行分析，发现文档、CI和构建相关的任务合并成功率最高，而性能和错误修复任务则最常被拒绝。未被合并的PR通常涉及较大的代码改动、修改更多文件，并且常无法通过项目的CI/CD流水线验证。对600个PR的定性分析进一步揭示了常见的拒绝模式，如缺乏审阅者互动、重复提交、不受欢迎的功能实现以及代理与项目目标的不一致，为AI辅助开发的社交技术挑战提供了洞见。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</div>
<div class="meta-line">Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</div>
<div class="meta-line">First: 2025-07-07T18:17:06+00:00 · Latest: 2026-01-21T13:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05386v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.05386v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model&#x27;s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT&#x27;s gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化微调自然缓解持续微调中的遗忘</div>
<div class="mono" style="margin-top:8px">持续微调（CPT）是一种广泛且有效的技术，用于将基础模型（如多模态大语言模型）适应于特定且不断变化的下游任务。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，但CPT中学习范式的根本作用仍被广泛忽视。本文对两种核心的微调范式——监督微调（SFT）和强化微调（RFT）进行了比较分析，探讨它们在持续微调过程中对知识保留的影响。我们的实验基于包含七个多样化多模态任务的基准数据集，使用Qwen2.5-VL-7B-Instruct作为持续微调的基础模型。研究得出两个重要发现：（1）在持续学习下游任务时，SFT会导致先前学习任务的灾难性遗忘，而RFT则能自然保留先前知识，并达到多任务训练的性能水平。（2）RFT能够保护并增强模型在标准基准（如MMMU和MMLU-Pro）上的通用知识，而SFT则严重损害模型的通用能力。进一步分析表明，这种稳定性并非主要源于显式的机制，如KL惩罚或思维链推理，而是源于RFT中隐含的正则化机制。我们的理论分析表明，RFT的梯度更新自然受到奖励方差的缩放，从而作为一种数据依赖的正则化器，保护先前获得的知识。最后，我们提出了一种基于rollout的实例过滤算法，以提升RFT的稳定性和效率。我们的全面研究展示了RFT作为持续微调的稳健范式的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the issue of catastrophic forgetting in continual post-training of foundation models, focusing on the effectiveness of different learning paradigms. It compares supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), finding that RFT naturally preserves prior knowledge and maintains performance comparable to multi-task training, while SFT leads to significant degradation. Experiments on seven multimodal tasks using Qwen2.5-VL-7B-Instruct show that RFT&#x27;s stability arises from an implicit regularization mechanism tied to reward variance, rather than explicit methods like KL penalty. The study also proposes a rollout-based instance filtering algorithm to improve RFT&#x27;s efficiency and robustness in continual learning.</div>
<div class="mono" style="margin-top:8px">本文探讨了基础模型在持续微调过程中出现灾难性遗忘的问题，比较了监督微调（SFT）与强化微调（RFT）两种方法的效果。研究发现，SFT在持续学习下游任务时会导致先前任务性能严重下降，而RFT能够自然保留已有知识并保持与多任务训练相当的性能。在七个多模态任务上使用Qwen2.5-VL-7B-Instruct进行实验的结果表明，RFT的稳定性来源于与奖励方差相关的隐式正则化机制，而非显式的KL惩罚等方法。作者进一步提出一种基于rollout的实例过滤算法，以提升RFT在持续学习中的效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</div>
<div class="meta-line">Authors: Xinyi Zheng, Yunze Liu, Chi-Hao Wu, Fan Zhang, Hao Zheng, Wenqi Zhou, Walterio W. Mayol-Cuevas, Junxiao Shen</div>
<div class="meta-line">First: 2026-01-21T11:32:24+00:00 · Latest: 2026-01-21T11:32:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialMem：基于度量锚定和快速检索的统一三维记忆</div>
<div class="mono" style="margin-top:8px">我们提出了SpatialMem，这是一个以记忆为中心的系统，将三维几何、语义和语言统一为一个可查询的表示。从随意捕捉的视角RGB视频出发，SpatialMem重建度量缩放的室内环境，检测结构化的三维锚点（墙壁、门、窗）作为第一层框架，并填充一个分层记忆，包含开放词汇的对象节点——将证据片段、视觉嵌入和两层文本描述与三维坐标相链接——以实现紧凑存储和快速检索。该设计支持对空间关系（如距离、方向、可见性）的可解释推理，并支持无需专用传感器的下游任务，如语言引导的导航和对象检索。在三个真实室内场景中的实验表明，SpatialMem在增加杂乱和遮挡的情况下仍能保持强大的锚点-描述级导航完成度和分层检索准确性，为具身空间智能提供了一个高效且可扩展的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpatialMem is motivated by the need for a unified representation of 3D geometry, semantics, and language in indoor environments. The system constructs a metrically scaled 3D model from egocentric RGB videos, identifies structural 3D anchors as a foundational framework, and builds a hierarchical memory with open-vocabulary object nodes that connect visual and textual information to spatial coordinates. Experimental results across three real-world indoor scenes show that SpatialMem achieves high navigation completion and hierarchical retrieval accuracy even in cluttered and occluded conditions, demonstrating its effectiveness for embodied spatial intelligence tasks.</div>
<div class="mono" style="margin-top:8px">SpatialMem的动机是构建一个统一的三维几何、语义和语言表示，以实现高效的空问推理与导航。该系统从第一人称视角的RGB视频中重建出度量尺度的室内环境，并以结构化的三维锚点（如墙壁、门、窗）作为基础框架。随后，它通过证据贴片、视觉嵌入和文本描述构建分层记忆，将开放词汇的对象节点与三维坐标关联。在三个真实室内场景的实验中，结果表明即使在增加的杂乱和遮挡条件下，SpatialMem仍能保持较高的导航完成度和检索准确性，展示了其在具身空间智能任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">3D Space as a Scratchpad for Editable Text-to-Image Generation</div>
<div class="meta-line">Authors: Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</div>
<div class="meta-line">First: 2026-01-21T02:40:19+00:00 · Latest: 2026-01-21T02:40:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://oindrilasaha.github.io/3DScratchpad/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D空间作为可编辑文本到图像生成的草稿纸</div>
<div class="mono" style="margin-top:8px">近期在大语言模型（LLMs）方面的进展表明，当将中间思维外部化为显式的空间，如思维链轨迹或工具增强推理时，推理能力会得到提升。然而，视觉语言模型（VLMs）缺乏类似的空间推理机制，限制了其生成准确反映几何关系、物体身份和构图意图的图像的能力。我们引入了空间草稿纸的概念——一种连接语言意图与图像合成的3D推理基底。给定一个文本提示，我们的框架解析主体和背景元素，将它们实例化为可编辑的3D网格，并采用代理场景规划来确定位置、方向和视角选择。最终的3D布局通过保留身份的提示被渲染回图像域，使VLM能够生成空间一致且视觉连贯的输出。与以往基于2D布局的方法不同，我们的方法支持直观的3D编辑，这些编辑能够可靠地传播到最终图像中。在实验上，它在GenAI-Bench上实现了文本对齐度的32%提升，证明了显式的3D推理对精确可控图像生成的优势。我们的结果突显了一种新的视觉-语言模型范式，即不仅在语言层面进行推理，也在空间层面进行推理。代码和可视化结果见 https://oindrilasaha.github.io/3DScratchpad/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of visual language models (VLMs) in generating images that accurately reflect spatial relationships and compositional intent. The authors propose a 3D spatial scratchpad as a novel mechanism to externalize reasoning, enabling VLMs to translate text prompts into editable 3D meshes and perform scene planning for placement, orientation, and viewpoint selection. The 3D arrangement is then rendered back into the image domain with identity-preserving cues, resulting in spatially consistent and visually coherent outputs. Experimental results on GenAI-Bench show a 32% improvement in text alignment compared to prior 2D layout-based methods, demonstrating the effectiveness of explicit 3D reasoning in enhancing controllability and precision in text-to-image generation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉语言模型（VLMs）在生成图像时难以准确反映几何关系和构图意图的问题，提出了一种3D空间scratchpad机制。该框架通过解析文本提示提取主体和背景元素，将其表示为可编辑的3D网格，并利用自主场景规划来确定其位置、方向和视角。随后，将3D布局渲染回图像域，并保留身份信息，从而生成空间一致且视觉连贯的图像。在GenAI-Bench上的实验结果显示，与传统的2D布局方法相比，文本对齐度提高了32%，证明了显式的3D推理在提升图像生成的可控性和精确性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics</div>
<div class="meta-line">Authors: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li</div>
<div class="meta-line">First: 2026-01-20T14:51:45+00:00 · Latest: 2026-01-20T14:51:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14027v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14027v1">PDF</a> · <a href="https://github.com/project-numina/numina-lean-agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Numina-Lean-Agent：一个开放且通用的代理推理系统用于形式化数学</div>
<div class="mono" style="margin-top:8px">代理系统最近已成为形式化定理证明的主导范式，通过协调多个模型和工具实现了强大的性能。然而，现有方法通常依赖于任务特定的流水线和训练过的形式化证明器，限制了其灵活性和可复现性。在本文中，我们提出了一种范式，即直接使用通用编码代理作为形式化数学推理器。该范式受到以下三点启发：(1) 通用编码代理为超越证明的多样化推理任务提供了自然的接口；(2) 仅通过替换底层基础模型即可提升性能，而无需训练；(3) MCP支持灵活扩展和自主调用专用工具，避免了复杂的设计。基于这一范式，我们引入了Numina-Lean-Agent，它结合了Claude Code与Numina-Lean-MCP，以实现与Lean的自主交互、相关定理检索、非形式化证明以及辅助推理工具。使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent解决了Putnam 2025中的所有问题（12/12），与最佳的闭源系统表现相当。除了基准评估，我们还通过与数学家互动，成功地将Brascamp-Lieb定理形式化，进一步展示了其通用性。我们将在https://github.com/project-numina/numina-lean-agent上发布Numina-Lean-Agent及其所有解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to develop a more flexible and reproducible agentic reasoning system for formal mathematics by leveraging general coding agents rather than task-specific pipelines or trained formal provers. The proposed method, Numina-Lean-Agent, integrates a general coding agent with a modular code proving component (Numina-Lean-MCP) to enable autonomous interaction with the Lean theorem prover, theorem retrieval, and use of auxiliary reasoning tools. Experimental results show that Numina-Lean-Agent, using Claude Opus 4.5 as the base model, solves all 12 problems in the Putnam 2025 competition, matching the performance of the best closed-source systems. Additionally, it successfully formalizes the Brascamp-Lieb theorem through interaction with mathematicians, demonstrating its broad applicability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过摒弃任务特定的流水线和训练过的证明器，开发一个更灵活且可复现的数学形式化推理系统。所提出的方法采用通用编码代理作为系统核心，并结合模块化组件协议（MCP）实现与Lean的自主交互、相关定理检索及辅助推理工具的调用。实验结果表明，基于Claude Opus 4.5模型的Numina-Lean-Agent能够解决Putnam 2025竞赛中的全部12道题目，性能与最佳闭源系统相当。此外，通过与数学家的交互，该系统成功实现了Brascamp-Lieb定理的形式化，展示了其广泛适用性。</div>
</details>
</div>
<div class="card">
<div class="title">CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments</div>
<div class="meta-line">Authors: Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li</div>
<div class="meta-line">First: 2026-01-20T13:44:02+00:00 · Latest: 2026-01-20T13:44:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14339v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CityCube：在城市环境中评估视觉-语言模型的跨视角空间推理能力</div>
<div class="mono" style="margin-top:8px">跨视角空间推理对于具身AI至关重要，是空间理解、心理模拟和复杂环境中规划的基础。现有基准主要关注室内或街道场景，忽略了开放城市空间中独特的挑战，这些场景具有丰富的语义、复杂的几何结构和视角变化。为了解决这一问题，我们引入CityCube，这是一个系统性的基准，旨在评估当前视觉-语言模型（VLMs）在城市环境中的跨视角推理能力。CityCube整合了四种视角动态，以模拟相机运动，并涵盖了从车辆、无人机和卫星等多个平台获取的广泛视角。为了全面评估，它包含5,022个精心标注的多视角问答对，分为五个认知维度和三种空间关系表达。对33个VLMs的全面评估显示，其性能与人类存在显著差距：即使是大规模模型也难以超过54.1%的准确率，仍比人类表现低34.2%。相比之下，小型模型经过微调后可达到超过60.0%的准确率，突显了我们基准的重要性。进一步分析表明，VLMs与类人推理之间存在任务相关性和根本性的认知差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CityCube, a benchmark designed to evaluate cross-view spatial reasoning capabilities of vision-language models (VLMs) in urban environments. Motivated by the limitations of existing benchmarks that focus on indoor or street settings, CityCube incorporates four viewpoint dynamics and multiple platforms to simulate realistic urban scenarios. It includes 5,022 annotated multi-view QA pairs organized into five cognitive dimensions and three spatial relation expressions. Evaluation of 33 VLMs shows that even large models achieve only 54.1% accuracy, significantly lower than human performance, while small-scale fine-tuned models surpass this threshold, indicating the importance of the benchmark for advancing spatial reasoning in complex urban settings.</div>
<div class="mono" style="margin-top:8px">该研究提出了CityCube，这是一个用于评估视觉语言模型（VLMs）在城市环境中跨视图空间推理能力的基准。由于现有基准主要关注室内或街道场景，未能体现开放城市空间中丰富的语义、复杂的几何结构和多样的视角变化，因此本研究设计了CityCube。该基准集包含5,022个精心标注的多视角问答对，涵盖五个认知维度和三种空间关系表达。实验结果显示，即使是大规模的VLMs也只能达到54.1%的准确率，远低于人类表现；而经过微调的小规模模型则超过了这一水平，突显了针对城市空间推理任务进行定制训练的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍存在分裂：视觉-语言模型（如CLIP）在全局语义对齐方面表现优异，但缺乏空间精度；而自监督方法（如MAE、DINO）能够捕捉复杂的局部结构，但在高层次语义上下文方面存在困难。我们认为这些范式本质上是互补的，可以整合到一个有原则的多任务框架中，并通过密集空间监督进一步增强。我们提出了MTV，一个联合优化视觉-语言对比、自监督和密集空间目标的多任务视觉预训练框架。为了减少对人工标注的依赖，我们利用高容量的&quot;专家&quot;模型，如Depth Anything V2和OWLv2，大规模合成密集且结构化的伪标签。除了框架本身，我们还系统地研究了多任务视觉学习的机制，分析了：(i) 每个目标的边际增益，(ii) 任务间的协同作用与干扰，以及(iii) 在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了&quot;兼顾两者优势&quot;的性能，在不损害全局语义理解的前提下，显著提升了细粒度空间推理能力。我们的发现表明，借助高质量的伪监督，多任务学习是一条通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current visual representation learning approaches by proposing a multi-task framework that integrates vision-language contrastive learning, self-supervised learning, and dense spatial supervision. The motivation stems from the complementary strengths of vision-language models and self-supervised methods, which respectively excel in global semantics and local structures. The proposed MTV framework jointly optimizes a shared backbone across these tasks, using expert models to generate pseudo-labels for dense spatial supervision. Experimental results show that MTV achieves superior performance in fine-grained spatial reasoning while maintaining strong global semantic understanding, demonstrating the effectiveness of multi-task learning with high-quality pseudo-supervision.</div>
<div class="mono" style="margin-top:8px">本文针对当前视觉表征学习方法的局限性，提出了一种结合视觉-语言对比学习、自监督学习和密集空间监督的多任务框架。研究动机源于视觉-语言模型和自监督方法在全局语义和局部结构上的互补优势。框架MTV通过专家模型生成的伪标签，联合优化共享主干网络。实验结果表明，MTV在细粒度空间推理方面表现出色，同时保持了对全局语义的深刻理解，验证了高质量伪监督驱动的多任务学习的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
