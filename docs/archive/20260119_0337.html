<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-19 03:37</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260119_0337</div>
    <div class="row"><div class="card">
<div class="title">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</div>
<div class="meta-line">Authors: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-10T12:54:04+00:00 · Latest: 2026-01-15T13:22:05+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, accepted to AAAI 2026. Project page:https://github.com/CASIA-IVA-Lab/UrbanNav</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09607v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09607v2">PDF</a> · <a href="https://github.com/CASIA-IVA-Lab/UrbanNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanNav: 从大规模人类轨迹中学习语言引导的城市导航</div>
<div class="mono" style="margin-top:8px">使用自然语言指令在复杂的城市环境中导航对具身智能体提出了重大挑战，包括嘈杂的语言指令、模糊的空间参照、多样化的地标以及动态的街道场景。当前的视觉导航方法通常局限于模拟或非街道环境，并且往往依赖于精确的目标格式，如特定坐标或图像。这限制了它们在自主智能体（如最后一公里配送机器人）在陌生城市中导航时的有效性。为了解决这些限制，我们引入了UrbanNav，一个可扩展的框架，用于训练具身智能体在多样化城市环境中遵循自由形式的语言指令。通过利用大规模的城市步行视频，我们开发了一个可扩展的标注流程，将人类导航轨迹与基于真实地标的语言指令对齐。UrbanNav包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，涵盖了广泛的城市场景。我们的模型学习了鲁棒的导航策略，以应对复杂的城市场景，展示了优越的空间推理能力、对嘈杂指令的鲁棒性以及对未见过的城市环境的泛化能力。实验结果表明，UrbanNav显著优于现有方法，突显了大规模网络视频数据在实现具身智能体语言引导的真实城市导航方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UrbanNav addresses the challenges of navigating complex urban environments using natural language instructions for embodied agents. It introduces a scalable framework that trains agents to follow free-form language guidance in real-world settings by utilizing web-scale city walking videos. The framework includes an annotation pipeline that aligns human trajectories with language instructions based on real-world landmarks, resulting in a large dataset of over 1,500 hours and 3 million instruction-trajectory-landmark triplets. Experimental results demonstrate that UrbanNav outperforms existing methods in spatial reasoning, robustness to noisy instructions, and generalization to new urban environments.</div>
<div class="mono" style="margin-top:8px">UrbanNav旨在解决基于自然语言指令在复杂城市环境中导航的挑战，为具身智能体提供更有效的解决方案。该框架利用大规模城市步行视频，构建了一个可扩展的标注流程，将人类导航轨迹与现实地标相关的语言指令对齐。框架包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，覆盖广泛的城市场景。实验结果表明，UrbanNav在处理噪声指令和未知城市环境方面显著优于现有方法，展示了大规模网络视频数据在实现语言引导的真实城市导航中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding</div>
<div class="meta-line">Authors: Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui</div>
<div class="meta-line">First: 2026-01-15T12:36:08+00:00 · Latest: 2026-01-15T12:36:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10343v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10343v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OctoBench：基于仓库的智能编码中对支架感知指令遵循的基准测试</div>
<div class="mono" style="margin-top:8px">现代编码支架使LLMs成为有能力的软件代理，但它们遵循支架指定指令的能力仍被低估，尤其是在约束条件异构且持续跨交互的情况下。为填补这一空白，我们引入了OctoBench，用于评估基于仓库的智能编码中对支架感知指令遵循的能力。OctoBench包含34个环境和217个任务实例，涵盖三种支架类型，并配有7,098个目标检查清单项。为了将任务解决与规则遵循分离，我们提供了一个自动化的观察与评分工具包，可捕捉完整交互轨迹并进行细粒度检查。在八个代表性模型上的实验揭示了任务解决与支架感知合规之间存在系统性差距，强调了需要专门针对异构指令遵循进行训练和评估的重要性。我们发布该基准测试以支持可重复的基准测试，并加速开发更具支架感知能力的编码代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the ability of large language models (LLMs) to follow scaffold-specified instructions in repository-grounded agentic coding, particularly when dealing with heterogeneous constraints across interactions. The authors propose OctoBench, a benchmark that includes 34 environments, 217 tasks, and 7,098 objective checklist items, organized under three scaffold types. To assess scaffold-aware compliance independently from task-solving, they developed an automated observation-and-scoring toolkit that tracks full interaction trajectories and performs fine-grained evaluations. Experimental results on eight representative models show a systematic discrepancy between task completion and adherence to scaffold instructions, highlighting the importance of targeted training and evaluation for improved instruction following in coding agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型（LLMs）在基于仓库的智能编码中遵循支架指定指令的能力，尤其是在约束条件多样且持续跨交互的情况下。作者提出了OctoBench基准测试，包含34个环境和217个任务，涵盖三种支架类型，并配有7098个目标检查清单项。他们开发了一个自动化观察与评分工具包，用于区分任务解决与规则遵循，实现细粒度评估。在八个代表性模型上的实验结果显示，任务完成与支架感知指令遵循之间存在系统性差距，强调了需要更针对性的训练和评估方法。</div>
</details>
</div>
<div class="card">
<div class="title">Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants</div>
<div class="meta-line">Authors: Tsvi Cherny-Shahar, Amiram Yehudai</div>
<div class="meta-line">First: 2026-01-15T06:42:45+00:00 · Latest: 2026-01-15T06:42:45+00:00</div>
<div class="meta-line">Comments: 35 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10112v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.
  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\% and reduces completion time by 53.9\%, yielding a mean 57.8\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\% in accuracy and 69.5\% in efficiency on average, compared to 6.6\% and 46.1\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>仓库智能图：面向大语言模型代码助手的确定性架构图</div>
<div class="mono" style="margin-top:8px">仓库感知的编码代理通常难以恢复构建和测试结构，尤其是在多语言项目中，跨语言依赖关系被编码在异构构建系统和工具中。我们引入了仓库智能图（RIG），这是一种确定性的、有据可依的架构图，它表示可构建的组件、聚合器、运行器、测试、外部包和包管理器，这些元素通过显式的依赖和覆盖边连接，可追溯到具体的构建和测试定义。我们还提出了SPADE，一种确定性的提取器，它从构建和测试工件（目前包括基于CMake文件API和CTest元数据的自动CMake插件）中构建RIG，并将其作为LLM友好的JSON视图暴露出来，代理可以将其视为仓库结构的权威描述。
我们评估了三个商业代理（Claude Code、Cursor、Codex）在八个涵盖从低到高构建导向复杂度的仓库上的表现，包括现实中的MetaFFI项目。每个代理在有和没有RIG上下文的情况下，针对每个仓库回答三十个结构化问题，我们测量了准确率、实际完成时间和效率（每正确答案所需秒数）。在所有仓库和代理中，提供RIG使平均准确率提高了12.2%，完成时间减少了53.9%，从而平均每正确答案所需时间减少了57.8%。在多语言仓库中，收益更大，平均准确率提高17.7%，效率提高69.5%，相比之下，单语言仓库的准确率和效率分别提高6.6%和46.1%。定性分析表明，RIG将失败从结构误解转移到正确结构上的推理错误，而罕见的回归则突显了基于图的推理质量仍然是关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge faced by repository-aware coding agents in understanding build and test structures, particularly in multilingual projects with complex and heterogeneous build systems. The authors propose the Repository Intelligence Graph (RIG), a deterministic architectural map that explicitly captures dependencies and coverage between code components, tests, and external packages. They also introduce SPADE, an extractor that builds RIG from build and test artifacts, such as CMake and CTest data, and presents it in a JSON format suitable for LLMs. Evaluation on eight repositories with three commercial agents shows that using RIG improves mean accuracy by 12.2% and reduces completion time by 53.9%, with more significant gains in multilingual projects.</div>
<div class="mono" style="margin-top:8px">本文针对代码助手在理解复杂构建和测试结构时面临的挑战，尤其是多语言项目中异构构建系统和工具带来的跨语言依赖问题。作者提出了Repository Intelligence Graph（RIG），一种基于明确依赖和覆盖边的确定性架构图，用于表示可构建组件、聚合器、运行器、测试和外部包管理器。他们还开发了SPADE工具，能够从构建和测试工件（如CMake和CTest数据）中自动构建RIG，并以LLM友好的JSON格式呈现。在八个不同复杂度的仓库上对三个商业代理进行评估，结果显示使用RIG可使平均准确率提高12.2%，完成时间减少53.9%，在多语言仓库中提升更为显著，平均准确率提高17.7%，效率提升69.5%。定性分析表明，RIG将错误从结构误解转移到了对正确结构的推理错误，但基于图的推理质量仍然是关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jean Oh, Jihie Kim</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-15T05:09:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未找到安全（404）：基于大语言模型的机器人决策中的隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键环境中，人工智能系统的一个错误可能导致生命损失。随着大语言模型（LLMs）在机器人决策中的应用日益广泛，风险的物理维度也在扩大；一个错误的指令可能直接危及人类安全。本文旨在系统评估LLM在即使微小错误也可能导致灾难的场景中的表现。通过一个火灾疏散场景的定性评估，我们识别了基于LLM的决策中的关键失败案例。基于这些案例，我们设计了七个任务用于定量评估，分为：完整信息任务、不完整信息任务和安全导向空间推理（SOSR）任务。完整信息任务使用ASCII地图以最小化解释歧义，并将空间推理与视觉处理分离。不完整信息任务要求模型推断缺失的上下文，测试其空间连续性与幻觉之间的区别。SOSR任务使用自然语言评估在生命危险情境下的安全决策能力。我们对各种LLMs和视觉-语言模型（VLMs）在这些任务上的表现进行了基准测试。除了整体表现外，我们还分析了1%失败率的潜在影响，强调了“罕见”错误如何演变为灾难性后果。结果揭示了严重的漏洞：一些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，模型指示机器人向危险区域移动而非紧急出口。我们的研究得出令人警醒的结论：当前的LLMs尚不适用于直接部署在安全关键系统中。在机器人领域，99%的准确率是危险的误导，因为它意味着每100次执行中可能有一次导致灾难性伤害。我们证明了即使是最先进的模型也无法保证安全，完全依赖它们会带来不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the safety risks associated with using Large Language Models (LLMs) in robotics decision-making, particularly in critical scenarios where minor errors can lead to severe consequences. The authors designed seven tasks to evaluate LLMs and Vision-Language Models (VLMs) under different information conditions, including complete, incomplete, and safety-oriented spatial reasoning. Experimental results show that several models failed completely in ASCII navigation tasks, and in a simulated fire drill, some models directed robots toward hazardous areas instead of emergency exits, highlighting the potential for catastrophic outcomes even with a 1% failure rate. These findings suggest that current LLMs are not suitable for direct deployment in safety-critical systems, as high accuracy rates do not equate to safe behavior in real-world robotics applications.</div>
<div class="mono" style="margin-top:8px">本文探讨了在安全关键场景中使用大型语言模型（LLMs）进行机器人决策所存在的潜在风险，特别是在轻微错误可能导致严重后果的情况下。作者设计了七个任务来评估LLMs和视觉语言模型（VLMs）在不同情境下的表现，包括完整信息、不完整信息以及安全导向的空间推理。实验结果显示了模型的重大缺陷，例如在ASCII导航任务中达到0%的成功率，以及在模拟火灾演练中模型错误地指示机器人向危险区域移动。这些发现强调了在安全关键应用中过度依赖LLMs所带来的风险，即使1%的失败率也可能导致灾难性后果。</div>
</details>
</div>
<div class="card">
<div class="title">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</div>
<div class="meta-line">Authors: Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</div>
<div class="meta-line">First: 2026-01-12T16:26:42+00:00 · Latest: 2026-01-15T03:58:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07695v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes &quot;near-miss&quot; samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑操作符：平滑可验证奖励激活视觉-语言模型的空间推理能力</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在实现精确数值预测以理解3D场景时面临关键瓶颈。传统的强化学习（RL）方法主要基于相对排名，常受严重奖励稀疏性和梯度不稳定性困扰，无法有效利用3D物理约束提供的可验证信号。值得注意的是，在标准GRPO框架中，相对归一化会导致&quot;近失&quot;样本（具有小但非零误差的样本）出现优势坍塌。这导致了严重的数据利用瓶颈，即在优化过程中有价值的边界样本被丢弃。为了解决这一问题，我们引入了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA采用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励空间。同时，AP-GRPO整合了绝对标量梯度，以缓解传统相对排名机制中固有的数值信息损失。通过这种方法，我们构建了Numerical3D-50k数据集，包含50,000个可验证的3D子任务。实证结果表明，AP-GRPO在保持较高数据效率的同时，实现了与大规模监督方法相当的性能，有效激活了VLMs中的潜在3D推理能力，而无需对模型架构进行修改。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge that Vision-Language Models (VLMs) face in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning methods suffer from reward sparsity and gradient instability, which hinder effective use of verifiable signals from 3D physical constraints. To resolve these issues, the authors propose the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA uses a dynamically parameterized Sigmoid function to generate dense, continuous rewards, while AP-GRPO incorporates absolute scalar gradients to preserve numerical information. The proposed approach is validated using Numerical3D-50k, a dataset of 50,000 verifiable 3D subtasks, showing that AP-GRPO matches the performance of large-scale supervised methods with higher data efficiency and activates latent 3D reasoning capabilities in VLMs without altering their architecture.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉-语言模型（VLMs）在3D场景理解中进行精确数值预测的挑战。传统强化学习方法由于依赖相对排名，常面临奖励稀疏性和梯度不稳定性问题，导致有价值边界样本被丢弃。为此，作者提出了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA利用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励，而AP-GRPO通过引入绝对标量梯度来保留数值信息。实验使用包含50,000个可验证3D子任务的Numerical3D-50k数据集进行验证，结果表明该方法在数据效率上优于传统方法，且性能与大规模监督方法相当。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，其训练方法往往将图像扁平化为1D的图像块序列，丢弃了空间推理所需的2D结构。我们认为，这种缺乏空间感知能力是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the spatial reasoning limitations in vision-language models (VLMs), which are typically trained using contrastive language-image pretraining (CLIP) style image encoders that flatten images into 1D sequences, neglecting their 2D structure. The authors propose two approaches to enhance spatial awareness: training image encoders with alternative objectives and incorporating 2D positional encodings. Experimental results demonstrate that these modifications improve spatial reasoning performance across multiple benchmarks, highlighting their potential to advance applications in robotics and embodied AI.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在空间推理方面的不足，指出当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，将图像扁平化为1D序列，忽略了其2D结构。作者提出了两种改进方法：使用不同训练目标的图像编码器和引入2D位置编码。实验结果表明，这些架构改进在多个基准测试中提升了空间推理能力，显示出其在机器人和具身AI等应用中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</div>
<div class="meta-line">Authors: Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong</div>
<div class="meta-line">First: 2026-01-14T21:21:00+00:00 · Latest: 2026-01-14T21:21:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09879v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedVL-SAM2：一种统一的三维医学视觉-语言模型，用于多模态推理和提示驱动的分割</div>
<div class="mono" style="margin-top:8px">近年来，医学视觉-语言模型（VLMs）在图像级文本中心任务如报告生成和视觉问答（VQA）上取得了显著进展。然而，在三维医学VLM中实现细粒度视觉定位和体素空间推理仍具挑战性，尤其是在试图在一个通用框架内统一这些能力时。为了解决这一挑战，我们提出了MedVL-SAM2，这是一种统一的三维医学多模态模型，能够同时支持报告生成、VQA以及包括语义分割、指称分割和交互分割在内的多种分割范式。MedVL-SAM2通过专为三维医学影像设计的统一架构，整合了图像级推理和像素级感知，并引入了基于SAM2的体素分割模块，以实现精确的多粒度空间推理。该模型采用多阶段训练流程：首先在大规模的三维CT图像-文本对语料库上进行预训练，以对齐体素视觉特征与放射学语言嵌入；随后利用一个全面的三维CT分割数据集，联合优化语言理解和分割目标。这种联合训练使得模型能够通过语言、点或框提示进行灵活交互，从而统一高级视觉推理与空间精确定位。我们的统一架构在报告生成、VQA以及多种三维分割任务中均实现了最先进的性能。进一步的广泛分析表明，该模型能够提供可靠的三维视觉定位、可控的交互分割以及强大的跨模态推理能力，证明了在统一的三维医学VLM中可以同时实现高级语义推理和精确的三维定位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing 3D medical vision-language models in achieving fine-grained visual grounding and volumetric spatial reasoning. MedVL-SAM2 is introduced as a unified framework that integrates report generation, visual question answering, and multi-paradigm segmentation tasks. The model employs a cohesive architecture tailored for 3D medical imaging and incorporates a SAM2-based volumetric segmentation module to enable precise spatial reasoning. It is trained through a multi-stage pipeline, first on a large-scale 3D CT image-text corpus for feature alignment, then jointly optimized with segmentation and language understanding objectives. The experimental results show that MedVL-SAM2 achieves state-of-the-art performance across multiple tasks and demonstrates reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前三维医学视觉语言模型在实现细粒度视觉定位和体积空间推理方面的不足。提出MedVL-SAM2模型，作为支持报告生成、视觉问答和多种分割任务的统一框架。该模型通过专门设计的架构，将图像级推理与像素级感知相结合，并引入基于SAM2的体积分割模块。模型采用多阶段训练流程，首先在大量三维CT图像-文本对上进行预训练以对齐体积视觉特征与放射学语言嵌入，随后在包含语言理解和分割目标的综合三维CT分割数据集上进行联合优化。实验结果表明，MedVL-SAM2在多个任务上达到最先进的性能，能够提供可靠的三维视觉定位、可控的交互式分割以及强大的跨模态推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</div>
<div class="meta-line">Authors: Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</div>
<div class="meta-line">First: 2026-01-14T18:59:59+00:00 · Latest: 2026-01-14T18:59:59+00:00</div>
<div class="meta-line">Comments: Project page: https://jasper0314-huang.github.io/fast-thinkact/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09708v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jasper0314-huang.github.io/fast-thinkact/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fast-ThinkAct: 通过可表述的潜在规划实现高效的视觉-语言-动作推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）任务需要对复杂的视觉场景进行推理，并在动态环境中执行适应性动作。尽管近期关于推理VLA的研究表明，显式的思维链（CoT）可以提升泛化能力，但它们由于推理轨迹过长而面临较高的推理延迟。我们提出Fast-ThinkAct，这是一种高效的推理框架，通过可表述的潜在推理实现紧凑且高效的规划。Fast-ThinkAct通过从教师模型中蒸馏学习，利用偏好引导的目标对操作轨迹进行对齐，从而迁移语言和视觉规划能力，用于具身控制。这使得推理增强的策略学习能够有效连接紧凑推理与动作执行。在多种具身操作和推理基准上的广泛实验表明，Fast-ThinkAct在推理延迟上比最先进的推理VLA减少了高达89.3\%，同时保持了有效的长时规划、少样本适应和失败恢复能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the high inference latency in existing Vision-Language-Action (VLA) systems that rely on explicit chain-of-thought reasoning. Fast-ThinkAct introduces an efficient framework that enables compact yet effective planning through verbalizable latent reasoning, distilling knowledge from a teacher model guided by a preference-based objective. The method transfers both linguistic and visual planning capabilities for embodied control, allowing for reasoning-enhanced policy learning. Experimental results show that Fast-ThinkAct achieves strong performance across various benchmarks, reducing inference latency by up to 89.3% compared to state-of-the-art reasoning VLAs, while preserving long-horizon planning, few-shot adaptation, and failure recovery abilities.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有视觉-语言-动作（VLA）系统中依赖显式链式推理所带来的高推理延迟问题。Fast-ThinkAct 提出了一种高效的框架，通过可语言化的潜在推理实现紧凑而有效的规划，利用教师模型进行知识蒸馏，并以偏好引导的目标进行对齐。该方法能够迁移语言和视觉规划能力，增强具身智能体的策略学习。实验结果表明，Fast-ThinkAct 在多个具身操作与推理基准测试中表现出色，相比最先进的推理 VLA 系统，推理延迟降低了高达 89.3%，同时保持了长时规划、少样本适应和故障恢复能力。</div>
</details>
</div>
<div class="card">
<div class="title">Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs</div>
<div class="meta-line">Authors: Rui Zhu, Xin Shen, Shuchen Wu, Chenxi Miao, Xin Yu, Yang Li, Weikang Li, Deguo Xia, Jizhou Huang</div>
<div class="meta-line">First: 2026-01-14T12:24:47+00:00 · Latest: 2026-01-14T12:24:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09430v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09430v1">PDF</a> · <a href="https://github.com/ruiz-nju/Video-MSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Video-MSR：评估多跳空间推理能力的多模态大语言模型基准</div>
<div class="mono" style="margin-top:8px">空间推理已成为多模态大语言模型（MLLMs）的一项关键能力，受到越来越多的关注并迅速发展。然而，现有基准主要关注单步的感知到判断任务，对需要复杂视觉空间逻辑链的场景探索不足。为弥合这一差距，我们引入了Video-MSR，这是首个专门设计用于评估动态视频场景中多跳空间推理（MSR）能力的基准。Video-MSR通过四个不同的任务系统地探测MSR能力：受限定位、基于链的参考检索、路径规划和反事实物理推理。我们的基准包含3,052个高质量视频实例和4,993个问答对，通过结合先进模型生成与严格人工验证的可扩展、视觉基础的流程构建。通过对20个最先进的MLLMs进行全面评估，我们发现其在MSR任务中存在显著局限，尽管模型在表层感知上表现出色，但在多步推理过程中常出现空间迷失和幻觉。为缓解这些不足并增强模型的MSR能力，我们进一步整理了MSR-9K，一个专门的指令微调数据集，并对Qwen-VL进行了微调，在Video-MSR上实现了+7.82%的绝对提升。我们的结果突显了多跳空间指令数据的有效性，并确立了Video-MSR作为未来研究的重要基础。代码和数据将在https://github.com/ruiz-nju/Video-MSR上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Video-MSR, a benchmark designed to evaluate multi-hop spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) in dynamic video scenarios. The motivation stems from the observation that existing benchmarks mainly focus on single-step tasks, neglecting complex visual-spatial logical chains. Video-MSR includes four tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction, with 3,052 video instances and 4,993 question-answer pairs. Evaluation on 20 state-of-the-art MLLMs reveals significant performance drops in multi-step spatial reasoning, with issues like spatial disorientation and hallucination. The authors further propose MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL to achieve a 7.82% absolute improvement on Video-MSR.</div>
<div class="mono" style="margin-top:8px">本研究旨在填补多模态大语言模型（MLLMs）在动态视频场景中评估多步空间推理能力的空白。作者提出了Video-MSR，这是首个专门用于评估多步空间推理（MSR）的基准测试，包含四个任务以系统性地探测复杂视觉空间逻辑推理能力。该数据集由3,052个高质量视频实例和4,993个问答对组成，通过可扩展且视觉接地的流程构建。通过对20个最先进的MLLMs进行全面评估，研究发现这些模型在多步空间推理任务中表现不佳，常出现空间迷失和幻觉现象。为改善这一问题，研究者进一步整理了MSR-9K专用指令微调数据集，并对Qwen-VL进行微调，实现了在Video-MSR上7.82%的绝对性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction</div>
<div class="meta-line">Authors: Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang</div>
<div class="meta-line">First: 2026-01-14T08:45:07+00:00 · Latest: 2026-01-14T08:45:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09285v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs&#x27; high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强大型语言模型在金属有机框架结构预测中的空间推理能力</div>
<div class="mono" style="margin-top:8px">金属有机框架（MOFs）是一种具有广泛应用（如碳捕集和药物输送）的多孔晶体材料，但其三维结构的准确预测仍然是一个重大挑战。尽管大型语言模型（LLMs）在生成晶体方面展现出潜力，但其在MOFs中的应用受到MOFs高原子复杂性的限制。受深度生成模型中块状范式成功启发，我们通过引入MOF-LLM，首个专门针对块级MOF结构预测的大型语言模型框架，开创了该领域LLMs的应用。为了有效利用LLMs进行这种模块化组装任务，我们的训练范式结合了空间感知的持续预训练（CPT）、结构监督微调（SFT）以及基于匹配的强化学习（RL）。通过引入显式的空间先验知识，并利用软自适应策略优化（SAPO）优化结构稳定性，我们的方法显著增强了Qwen-3 8B模型的空间推理能力，从而实现准确的MOF结构预测。全面的实验表明，MOF-LLM在性能上优于当前最先进的去噪方法和基于LLM的方法，同时表现出更高的采样效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The prediction of 3D structures for metal-organic frameworks (MOFs) is challenging due to their complex atomic arrangements. To address this, the paper introduces MOF-LLM, the first large language model (LLM) framework tailored for block-level MOF structure prediction. The model is trained using a combination of spatial-aware continual pre-training, structural supervised fine-tuning, and reinforcement learning with a matching-driven strategy. By integrating explicit spatial priors and optimizing structural stability through Soft Adaptive Policy Optimization (SAPO), MOF-LLM significantly improves spatial reasoning in the Qwen-3 8B model. Experimental results show that MOF-LLM achieves better performance than existing denoising-based and LLM-based methods, with enhanced sampling efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高金属有机框架（MOFs）三维结构预测的准确性，MOFs作为一种复杂的多孔材料，在碳捕获和药物输送等领域有广泛应用。作者提出MOF-LLM框架，这是首个专门用于MOFs块级结构预测的大型语言模型（LLM），受深度生成模型中块式范式的启发。该方法结合了空间感知的持续预训练（CPT）、结构监督微调（SFT）和基于匹配的强化学习（RL），特别强调通过引入显式的空间先验和使用软自适应策略优化（SAPO）来优化结构稳定性。实验结果表明，MOF-LLM在性能上优于现有的去噪和LLM方法，并表现出更高的采样效率。</div>
</details>
</div>
<div class="card">
<div class="title">DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos</div>
<div class="meta-line">Authors: Jiajun Chen, Jing Xiao, Shaohan Cao, Yuming Zhu, Liang Liao, Jun Pan, Mi Wang</div>
<div class="meta-line">First: 2026-01-14T07:22:44+00:00 · Latest: 2026-01-14T07:22:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeTracker：非稳定卫星视频中的运动解耦车辆检测与跟踪</div>
<div class="mono" style="margin-top:8px">卫星视频能够持续观测地表动态，但在非稳定条件下，多目标跟踪（MOT）面临重大挑战，尤其是平台抖动和微小目标弱外观共同降低了跟踪性能。为解决这一问题，我们提出DeTracker，一个专门针对非稳定卫星视频的联合检测与跟踪框架。DeTracker引入了一个全局-局部运动解耦（GLMD）模块，通过全局对齐和局部优化，显式地将卫星平台运动与真实目标运动分离，从而提升轨迹稳定性与运动估计精度。此外，我们还开发了一个时序依赖特征金字塔（TDFP）模块，用于跨帧时序特征融合，增强微小目标表示的连续性和可区分性。我们进一步构建了一个新的基准数据集SDM-Car-SU，模拟多方向和多速度平台运动，以实现对不同运动扰动下跟踪鲁棒性的系统评估。在模拟和真实非稳定卫星视频上的大量实验表明，DeTracker显著优于现有方法，在SDM-Car-SU数据集上达到61.1%的MOTA，在真实卫星视频数据上达到47.3%的MOTA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeTracker is proposed to address the challenges of multi-object tracking in unstabilized satellite videos, where platform jitter and weak object appearance hinder performance. The framework employs a Global--Local Motion Decoupling (GLMD) module to separate platform motion from object motion through global alignment and local refinement, thereby improving trajectory stability and motion estimation. Additionally, a Temporal Dependency Feature Pyramid (TDFP) module is introduced to enhance temporal feature fusion across frames, improving the continuity and discriminability of tiny-object representations. The method is evaluated on a newly constructed benchmark dataset, SDM-Car-SU, which simulates various platform motions, and achieves significant performance improvements, with 61.1% MOTA on the simulated data and 47.3% MOTA on real satellite videos.</div>
<div class="mono" style="margin-top:8px">DeTracker 是为了解决未稳定卫星视频中多目标跟踪的挑战而提出的，其中平台抖动和小目标外观弱化影响了跟踪性能。该方法引入了全局-局部运动解耦模块，通过全局对齐和局部优化分离平台运动与目标运动，从而提升轨迹稳定性和运动估计精度。同时，它采用时序依赖特征金字塔模块，增强跨帧特征融合，提高小目标表示的连续性和可区分性。在新构建的基准数据集和真实卫星视频上的实验表明，DeTracker 在 MOTA 指标上分别达到 61.1% 和 47.3%，显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents</div>
<div class="meta-line">Authors: Yuchong Xie, Zesen Liu, Mingyu Luo, Zhixiang Zhang, Kaikai Zhang, Yuanyuan Yuan, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She</div>
<div class="meta-line">First: 2025-10-27T07:04:08+00:00 · Latest: 2026-01-14T07:07:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23675v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23675v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding agents integrated into IDEs orchestrate powerful tools and high-privilege system access, creating a high-stakes attack surface. Prior work on Indirect Prompt Injection (IPI) is mainly query-specific, requiring particular user queries as triggers and leading to poor generalizability. We propose query-agnostic IPI, a new attack paradigm that reliably executes malicious payloads under arbitrary user queries. Our key insight is that malicious payloads should leverage the invariant prompt context (i.e., system prompt and tool descriptions) rather than variant user queries. We present QueryIPI, an automated framework that uses tool descriptions as optimizable payloads and refines them via iterative, prompt-based blackbox optimization. QueryIPI leverages system invariants for initial seed generation aligned with agent conventions, and iterative reflection to resolve instruction-following failures and safety refusals. Experiments on five simulated agents show that QueryIPI achieves up to 87% success rate, outperforming the best baseline (50%). Crucially, generated malicious descriptions transfer to real-world coding agents, highlighting a practical security risk.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueryIPI：面向编码代理的无查询依赖间接提示注入</div>
<div class="mono" style="margin-top:8px">现代集成到IDE中的编码代理协调强大的工具和高权限系统访问，形成了高风险的攻击面。先前关于间接提示注入（IPI）的工作主要针对特定查询，需要特定用户查询作为触发条件，导致泛化能力较差。我们提出了一种无查询依赖的IPI新攻击范式，能够在任意用户查询下可靠地执行恶意负载。我们的核心洞察是，恶意负载应利用不变的提示上下文（即系统提示和工具描述），而非变化的用户查询。我们提出了QueryIPI，一个自动框架，使用工具描述作为可优化的负载，并通过迭代的基于提示的黑盒优化对其进行优化。QueryIPI利用系统不变量生成符合代理惯例的初始种子，并通过迭代反思来解决指令遵循失败和安全拒绝问题。在五个模拟代理上的实验表明，QueryIPI的成功率高达87%，优于最佳基线（50%）。关键的是，生成的恶意描述可迁移至现实中的编码代理，突显了实际的安全风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing Indirect Prompt Injection (IPI) attacks, which are highly dependent on specific user queries and lack generalizability. QueryIPI introduces a query-agnostic approach by focusing on the invariant prompt context, such as system prompts and tool descriptions, to reliably execute malicious payloads under arbitrary user inputs. The framework employs an automated method that optimizes tool descriptions as payloads and iteratively refines them through blackbox optimization and reflection to overcome instruction-following failures. Experimental results on five simulated coding agents demonstrate that QueryIPI achieves an 87% success rate, significantly outperforming the best baseline of 50%, and shows that the generated malicious payloads can transfer to real-world coding agents, indicating a serious security vulnerability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有间接提示注入（IPI）攻击方法的局限性，这些方法通常依赖特定用户查询，泛化能力较差。所提出的方法QueryIPI采用查询无关的攻击范式，通过利用系统提示和工具描述等不变提示上下文，而非用户输入，来执行恶意负载。该方法构建了一个自动化框架，将工具描述作为可优化的负载，并通过黑盒优化迭代地进行优化。在五个模拟编码代理上的实验表明，QueryIPI的成功率达到87%，显著优于最佳基线方法的50%，并且能够有效迁移到实际编码代理中，凸显了严重的安全风险。</div>
</details>
</div>
<div class="card">
<div class="title">CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation</div>
<div class="meta-line">Authors: Haodi Yao, Fenghua He, Ning Hao, Yao Su</div>
<div class="meta-line">First: 2026-01-14T07:03:01+00:00 · Latest: 2026-01-14T07:03:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIDD：用于高效且具有判别力的局部特征表示的跨层独立可变形描述</div>
<div class="mono" style="margin-top:8px">稳健的局部特征表示对于空间智能任务（如机器人导航和增强现实）至关重要。建立可靠的对应关系需要在高判别力和计算效率之间取得平衡。为了解决这一问题，我们引入了跨层独立可变形描述（CLIDD），该方法通过直接从独立特征层次结构中采样，实现了卓越的区分性。这种方法利用可学习的偏移量来捕捉多尺度下的细粒度结构细节，同时避免了统一密集表示的计算负担。为了确保实时性能，我们实施了一种硬件感知的内核融合策略，以最大化推理吞吐量。此外，我们开发了一个可扩展的框架，将轻量级架构与结合度量学习和知识蒸馏的训练协议相结合。该方案生成了一系列针对不同部署约束优化的模型变体。广泛评估表明，我们的方法在匹配精度和计算效率方面均表现出色。具体而言，超紧凑变体在参数量仅为0.004M的情况下，达到了与SuperPoint相当的精度，实现了模型大小减少99.7%。此外，我们的高性能配置在边缘设备上超过了200 FPS，同时优于所有当前最先进的方法，包括高容量的DINOv2框架。这些结果表明，CLIDD在计算开销最小的情况下实现了高精度的局部特征匹配，为实时空间智能任务提供了一种稳健且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop efficient and discriminative local feature representations for spatial intelligence tasks like robot navigation and augmented reality. The proposed method, CLIDD, introduces a cross-layer independent deformable description approach that samples directly from independent feature hierarchies, using learnable offsets to capture structural details across scales without the computational cost of dense representations. A hardware-aware kernel fusion strategy enables real-time performance, and a scalable framework with lightweight architectures and a training protocol combining metric learning and knowledge distillation produces optimized model variants. Experimental results show that the ultra-compact variant achieves 99.7% smaller model size than SuperPoint with comparable precision, and the high-performance version outperforms state-of-the-art methods while exceeding 200 FPS on edge devices.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为机器人导航和增强现实等空间智能任务开发高效且具有判别力的局部特征表示。提出的方法CLIDD通过直接从独立特征层次采样，结合可学习偏移量捕捉多尺度的细粒度结构信息，从而避免统一密集表示的计算负担。该方法还采用了一种硬件感知的内核融合策略，并构建了一个可扩展框架，将轻量级架构与度量学习和知识蒸馏相结合。实验结果表明，CLIDD的超紧凑变体在参数量上仅为SuperPoint的0.4%，精度相当；而高性能配置在准确率上超越了当前所有最先进的方法，且在边缘设备上实现了超过200 FPS的处理速度。</div>
</details>
</div>
<div class="card">
<div class="title">MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences</div>
<div class="meta-line">Authors: Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, Huacan Wang</div>
<div class="meta-line">First: 2026-01-11T06:41:26+00:00 · Latest: 2026-01-13T14:48:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a &quot;closed-world&quot; limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemGovern: 通过学习受管理的人类经验提升代码代理</div>
<div class="mono" style="margin-top:8px">尽管自主软件工程（SWE）代理正在重塑编程范式，但它们目前受到“封闭世界”限制：它们尝试从头解决 bug 或仅使用局部上下文，而忽略了 GitHub 等平台上大量可用的历史人类经验。访问这种开放世界经验受到现实世界问题跟踪数据无结构和碎片化性质的阻碍。本文中，我们引入 MemGovern，这是一个框架，旨在管理和转换原始 GitHub 数据为代理可用的经验记忆。MemGovern 采用经验治理，将人类经验转换为代理友好的经验卡片，并引入一种代理经验搜索策略，使代理能够基于逻辑检索人类专业知识。通过生成 135,000 个受管理的经验卡片，MemGovern 实现了显著的性能提升，在 SWE-bench 验证中提高了 4.65% 的解决率。作为一种插件方法，MemGovern 为代理友好的记忆基础设施提供了解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the closed-world limitation of autonomous software engineering agents, which fail to leverage the vast amount of historical human experience available on platforms like GitHub. MemGovern is introduced as a framework that governs and transforms raw GitHub data into structured, agent-friendly experience cards. The framework utilizes experience governance and an agentic experience search strategy to enable logic-driven retrieval of human expertise. Experimental results show that MemGovern significantly improves the performance of code agents, achieving a 4.65% increase in resolution rates on the SWE-bench Verified dataset.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自主软件工程代理的封闭世界限制，即它们无法有效利用GitHub等平台上丰富的历史人类经验。MemGovern被引入作为一项框架，用于治理和转换原始GitHub数据为结构化的、适合代理使用的经验卡片。该框架采用经验治理和代理经验搜索策略，实现基于逻辑的人类专业知识检索。实验结果表明，MemGovern在SWE-bench Verified上的解决率提高了4.65%，展示了其通过结构化记忆基础设施提升代理性能的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation</div>
<div class="meta-line">Authors: Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang</div>
<div class="meta-line">First: 2026-01-13T10:31:09+00:00 · Latest: 2026-01-13T10:31:09+00:00</div>
<div class="meta-line">Comments: 2nd International Conference on Drones and Unmanned Systems (DAUS&#x27; 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08412v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08412v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于思维链引导的混合蒸馏方法用于边缘无人机控制代码生成</div>
<div class="mono" style="margin-top:8px">随着大语言模型在代码生成任务中展现出巨大潜力，其在资源受限的无人机平台上用于 onboard 控制的应用已成为重要的研究方向。然而，大模型的高资源消耗与无人机平台对实时性和轻量化的要求之间存在显著矛盾。本文提出了一种集成方法，结合知识蒸馏、思维链引导和监督微调，用于无人机多 SDK 控制任务，旨在高效地将复杂推理和代码生成能力转移到更小的模型中。首先，构建了一个涵盖多种主流无人机 SDK 的高质量数据集，包含指令-代码-推理链，并引入了反事实负样本进行数据增强，引导模型从指令解析到代码生成学习端到端的逻辑。其次，利用通过 QLoRA 量化后的 DeepSeek-Coder-V2-Lite 作为教师模型，基于混合黑盒与白盒蒸馏策略生成高质量的思维链软标签。这些软标签与硬标签结合，使用加权交叉熵损失函数，将复杂推理能力转移到较小的学生模型中。最后，通过针对无人机控制场景优化的提示调优工程，提升了模型在 SDK 类型识别和函数调用匹配等核心任务上的性能。实验结果表明，蒸馏后的轻量模型在保持高代码生成准确率的同时，显著提升了部署和推理效率，有效验证了我们方法在实现精确且轻量化的无人机智能控制方面的可行性与优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying large language models to UAV control code generation, where resource constraints and real-time requirements limit model deployment. The proposed method integrates knowledge distillation, chain-of-thought guidance, and supervised fine-tuning to transfer complex reasoning and code generation capabilities to smaller models. A high-quality dataset with instruction-code-reasoning chains and counterfactual samples is constructed, and a hybrid distillation strategy is used to generate soft labels. Prompt tuning is further optimized for UAV control tasks, leading to improved performance in SDK recognition and function call matching. Experimental results show that the distilled model achieves high code generation accuracy with significant gains in deployment and inference efficiency.</div>
<div class="mono" style="margin-top:8px">本文针对将大语言模型应用于无人机控制代码生成的挑战，提出了结合知识蒸馏、思维链引导和监督微调的方法，以在资源受限的无人机平台上实现高效推理与代码生成。首先构建了一个包含多种主流无人机SDK指令-代码-推理链的高质量数据集，并引入反事实负样本进行数据增强。接着利用DeepSeek-Coder-V2-Lite通过QLoRA量化作为教师模型，采用混合黑盒与白盒蒸馏策略生成高质量的思维链软标签，并结合加权交叉熵损失进行训练。实验结果表明，蒸馏后的轻量模型在保持高代码生成准确率的同时，显著提升了部署与推理效率，验证了该方法在实现精准且轻量级无人机智能控制方面的可行性与优势。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models</div>
<div class="meta-line">Authors: Shaoxiu Wei, Mingchao Liang, Florian Meyer</div>
<div class="meta-line">First: 2025-06-22T18:15:08+00:00 · Latest: 2026-01-13T08:40:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.18124v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.18124v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiobject tracking (MOT) is an important task in applications including autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT methods are model-based and combine sequential Bayesian estimation with data association and an object birth model. More recent methods are fully data-driven and rely on the training of neural networks. Both approaches offer distinct advantages in specific settings. In particular, model-based methods are generally applicable across a wide range of scenarios, whereas data-driven MOT achieves superior performance in scenarios where abundant labeled data for training is available. A natural thought is whether a general framework can integrate the two approaches. This paper introduces a hybrid method that utilizes neural networks to enhance specific aspects of the statistical model in Bayesian MOT that have been identified as overly simplistic. By doing so, the performance of the prediction and update steps of Bayesian MOT is improved. To ensure tractable computation, our framework uses belief propagation to avoid high-dimensional operations combined with sequential Monte Carlo methods to perform low-dimensional operations efficiently. The resulting method combines the flexibility and robustness of model-based approaches with the capability to learn complex information from data of neural networks. We evaluate the performance of the proposed method based on the nuScenes autonomous driving dataset and demonstrate that it has state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经增强运动和观测模型的贝叶斯多目标跟踪</div>
<div class="mono" style="margin-top:8px">多目标跟踪（MOT）在自动驾驶、海洋科学和航空航天监控等应用中是一项重要任务。传统MOT方法是基于模型的，结合了序贯贝叶斯估计、数据关联和目标生成模型。近年来的方法则是完全数据驱动的，依赖于神经网络的训练。两种方法在特定场景下各有优势。特别是基于模型的方法通常适用于各种广泛场景，而数据驱动的MOT方法在有大量标注训练数据的场景中表现出色。一个自然的想法是，是否存在一个通用框架可以将这两种方法结合起来。本文介绍了一种混合方法，利用神经网络增强贝叶斯MOT中被识别为过于简化的统计模型的特定方面。通过这样做，提升了贝叶斯MOT预测和更新步骤的性能。为了确保可计算性，我们的框架使用信念传播来避免高维运算，并结合序贯蒙特卡洛方法高效地执行低维运算。所得到的方法结合了基于模型方法的灵活性和鲁棒性，以及神经网络从数据中学习复杂信息的能力。我们基于nuScenes自动驾驶数据集评估了所提出方法的性能，并证明其具有最先进的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multiobject tracking (MOT) by proposing a hybrid approach that combines the strengths of model-based and data-driven methods. The method enhances the motion and measurement models in Bayesian MOT using neural networks to address their limitations in capturing complex dynamics. To maintain computational efficiency, the framework integrates belief propagation with sequential Monte Carlo methods, enabling tractable and effective inference. Experimental results on the nuScenes dataset show that the proposed method achieves state-of-the-art performance, demonstrating its effectiveness in real-world autonomous driving scenarios.</div>
<div class="mono" style="margin-top:8px">本文针对多目标跟踪（MOT）任务提出了一种混合方法，结合了基于模型和数据驱动方法的优势。研究动机源于传统贝叶斯MOT方法中运动和观测模型过于简单，以及数据驱动方法对大量标注数据的依赖。该方法通过神经网络增强贝叶斯MOT中特定的统计模型组件，从而提升预测和更新步骤的性能。为保证计算可行性，框架采用信念传播处理高维操作，并结合序贯蒙特卡洛方法高效完成低维操作。在nuScenes数据集上的实验结果表明，该方法在MOT任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</div>
<div class="meta-line">Authors: Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang</div>
<div class="meta-line">First: 2025-03-29T04:51:50+00:00 · Latest: 2026-01-12T04:43:21+00:00</div>
<div class="meta-line">Comments: Project page: https://logosroboticsgroup.github.io/SPAR/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.22976v7">Abs</a> · <a href="https://arxiv.org/pdf/2503.22976v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://logosroboticsgroup.github.io/SPAR/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从二维世界到三维空间：训练视觉-语言模型感知与推理三维场景</div>
<div class="mono" style="margin-top:8px">尽管最近在视觉-语言模型（LVLMs）方面取得了进展，提高了视觉-语言理解能力，但它们在空间感知方面仍存在困难，限制了其对复杂三维场景的推理能力。与之前将3D表示纳入模型以提升空间理解的方法不同，我们旨在通过利用空间相关的图像数据来释放视觉语言模型的潜力。为此，我们引入了一种基于具有三维真实标签场景数据的新型2D空间数据生成与标注流程。该流程能够创建从基础感知任务到更复杂推理任务的多样化空间任务。借助这一流程，我们构建了SPAR-7M，这是一个从多个公开数据集中生成的大型数据集，涵盖数千个场景。此外，我们还引入了SPAR-Bench，这是一个旨在比现有空间基准更全面评估空间能力的基准，支持单视角和多视角输入。在SPAR-7M和大规模2D数据集上进行训练使我们的模型在2D空间基准上达到了最先进的性能。进一步在3D任务专用数据集上微调，取得了具有竞争力的结果，突显了我们数据集在提升空间推理能力方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of vision-language models (VLMs) in spatial perception, which hinders their ability to reason about complex 3D scenes. The authors propose a novel 2D spatial data generation and annotation pipeline based on scene data with 3D ground-truth, enabling the creation of diverse spatial tasks. They construct SPAR-7M, a large-scale dataset derived from multiple public sources, and introduce SPAR-Bench for comprehensive evaluation of spatial capabilities. Training on these datasets achieves state-of-the-art results on 2D spatial benchmarks, and further fine-tuning on 3D-specific tasks demonstrates competitive performance, highlighting the effectiveness of their approach in enhancing spatial reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对视觉语言模型（VLMs）在空间感知方面的不足，指出其限制了对复杂三维场景的推理能力。作者提出了一种基于三维真实场景的新型二维空间数据生成与标注流程，能够创建从基础感知到复杂推理的各种空间任务。他们构建了SPAR-7M这一大规模数据集，并引入了SPAR-Bench基准，用于更全面地评估空间能力。在这些数据集上训练的模型在二维空间基准测试中达到了最先进的性能，而在三维任务特定数据集上的微调也取得了有竞争力的结果，表明该方法在提升空间推理能力方面具有显著效果。</div>
</details>
</div>
<div class="card">
<div class="title">X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests</div>
<div class="meta-line">Authors: Jie Wu, Haoling Li, Xin Zhang, Jiani Guo, Jane Luo, Steven Liu, Yangyu Huang, Ruihang Chu, Scarlett Li, Yujiu Yang</div>
<div class="meta-line">First: 2026-01-11T15:22:33+00:00 · Latest: 2026-01-11T15:22:33+00:00</div>
<div class="meta-line">Comments: Project: https://github.com/JieWu02/X-Coder</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06953v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06953v1">PDF</a> · <a href="https://github.com/JieWu02/X-Coder">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Coder：通过完全合成任务、解决方案和测试推动编程竞赛的发展</div>
<div class="mono" style="margin-top:8px">由于编程竞赛对推理能力和逻辑复杂度要求极高，这对代码语言模型（Code LLMs）提出了巨大挑战。然而，当前的Code LLMs仍然严重依赖真实世界数据，这限制了其可扩展性。本文探索了一种完全合成的方法：使用完全生成的任务、解决方案和测试用例来训练Code LLMs，从而在不依赖真实世界数据的情况下增强代码推理能力。为此，我们利用基于特征的合成方法，提出了一种新的数据合成流水线SynthSmith。SynthSmith在生成多样化且具有挑战性的任务，以及经过验证的解决方案和测试用例方面展现出巨大潜力，支持监督微调和强化学习。基于所提出的合成SFT和RL数据集，我们引入了X-Coder模型系列，在LiveCodeBench v5和v6上分别实现了62.9 avg@8和55.8的显著通过率，尽管其参数量仅为7B，仍优于DeepCoder-14B-Preview和AReal-boba2-14B。深入分析表明，扩展定律在我们的合成数据集上成立，我们探讨了哪些维度更有效进行扩展。此外，我们还提供了以代码为中心的强化学习的见解，并通过详细的消融实验和分析，突出了影响性能的关键因素。我们的研究结果表明，扩展高质量合成数据并采用分阶段训练可以显著提升代码推理能力，同时减少对真实世界编码数据的依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this paper is to enhance the performance of code reasoning models in competitive programming by reducing dependence on real-world data. The authors propose a fully synthetic approach using a novel data synthesis pipeline named SynthSmith, which generates diverse and challenging tasks, along with verified solutions and test cases. Experimental results show that the X-Coder model series, trained on synthetic data, achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming larger models like DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. The study also investigates scaling laws and provides insights into code-centric reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本文针对代码大模型在编程竞赛任务中的训练难题，这些任务需要较强的推理能力和高逻辑复杂度。为克服真实数据的限制，作者提出了一种完全合成的方法，利用名为SynthSmith的新数据合成管道生成多样且具有挑战性的任务、解决方案和测试用例。基于这些合成数据集训练的X-Coder模型系列在LiveCodeBench v5和v6上分别取得了62.9 avg@8和55.8的显著表现，超越了参数更大的DeepCoder-14B-Preview和AReal-boba2-14B模型。研究还探讨了合成数据的扩展规律，并通过详细消融实验和分析提供了代码强化学习的关键见解。</div>
</details>
</div>
<div class="card">
<div class="title">Code Reasoning for Software Engineering Tasks: A Survey and A Call to Action</div>
<div class="meta-line">Authors: Saurabh Pujar, Ira Ceka, Irene Manotas, Gail Kaiser, Baishakhi Ray, Shyam Ramji</div>
<div class="meta-line">First: 2025-06-16T19:18:09+00:00 · Latest: 2026-01-11T14:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.13932v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.13932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. Their performance on certain tasks can be further enhanced by incorporating test-time reasoning techniques. These inference-time advances have been adopted into the code domain, enabling complex software engineering (SWE) tasks such as code generation, test generation and issue resolution. However, the impact of different reasoning techniques on code-centric SWE tasks has not been systematically explored. In this work, we survey code reasoning techniques that underpin these capabilities, with a focus on test-time compute and inference-time reasoning paradigms. We examine a variety of code-specific reasoning methods and progressively build up to SWE agents, which combine planning, tool use, and multi-step interaction. We also compare the impact of different techniques on coding tasks, highlighting their relative importance and outlining open challenges and future research directions. Our contributions are: (1) to the best of our knowledge, the first dedicated survey of code reasoning for SWE tasks, highlighting overarching reasoning strategies, hybrid methods, and agentic approaches; (2) a taxonomy of inference-time techniques used to drive code reasoning, accompanied by a curated set of under-explored benchmarks with high potential for SWE evaluation; (3) a comparative analysis of reasoning design patterns across commonly used models and benchmarks; and (4) a synthesis of gaps in current methods and evaluation practices, identifying under-explored areas and concrete opportunities for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向软件工程任务的代码推理：综述与行动呼吁</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的兴起在各种自然语言任务中带来了显著的性能提升。通过引入测试时推理技术，某些任务的性能可以进一步增强。这些推理技术的进步已被应用于代码领域，从而支持诸如代码生成、测试生成和问题解决等复杂的软件工程（SWE）任务。然而，不同推理技术对以代码为中心的SWE任务的影响尚未被系统性地研究。在本工作中，我们综述了支撑这些能力的代码推理技术，重点探讨了测试时计算和推理时推理范式。我们分析了多种针对代码的推理方法，并逐步构建到结合规划、工具使用和多步交互的SWE代理。此外，我们还比较了不同技术对编码任务的影响，突出了其相对重要性，并指出了开放性挑战和未来研究方向。我们的贡献包括：(1) 据我们所知，这是首个专门针对SWE任务的代码推理综述，强调了总体推理策略、混合方法和代理方法；(2) 一个推理时技术的分类体系，附带一组尚未充分研究的基准测试，具有很高的SWE评估潜力；(3) 对常用模型和基准测试中推理设计模式的比较分析；(4) 对当前方法和评估实践中的空白进行综合，识别出尚未充分探索的领域和未来研究的具体机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for a systematic understanding of how reasoning techniques affect code-centric software engineering tasks. It provides a comprehensive survey of code reasoning methods, focusing on test-time computation and inference-time reasoning paradigms, and explores their application in complex tasks like code generation and issue resolution. The study compares various reasoning strategies across models and benchmarks, identifies gaps in current approaches, and outlines future research directions, including the development of SWE agents that integrate planning, tool use, and multi-step interactions.</div>
<div class="mono" style="margin-top:8px">本文旨在探讨如何通过推理技术提升大型语言模型在软件工程任务中的表现。它系统综述了支撑这些能力的代码推理方法，重点关注测试时计算和推理时推理范式，并研究其在代码生成、测试生成和问题解决等复杂任务中的应用。文章比较了不同推理策略在常用模型和基准上的效果，指出现有方法和评估实践中的不足，并提出了未来研究的方向，强调了尚未充分探索的领域和潜在的研究机会。</div>
</details>
</div>
<div class="card">
<div class="title">Training Versatile Coding Agents in Synthetic Environments</div>
<div class="meta-line">Authors: Yiqi Zhu, Apurva Gandhi, Graham Neubig</div>
<div class="meta-line">First: 2025-12-13T07:02:28+00:00 · Latest: 2026-01-11T10:24:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12216v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12216v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在合成环境中训练多功能编码代理</div>
<div class="mono" style="margin-top:8px">先前关于训练软件工程代理的研究工作探索了利用现有资源（如GitHub仓库中的问题）来构建软件工程任务和相应的测试套件。这些方法面临两个关键限制：(1) 它们依赖于已有的GitHub仓库，灵活性有限；(2) 它们主要关注于问题解决任务，限制了其在软件工程师必须处理的更广泛任务中的适用性。为克服这些挑战，我们引入了SWE-Playground，这是一种新颖的生成环境和轨迹的管道，支持多功能编码代理的训练。与先前的工作不同，SWE-Playground利用强大的语言模型和代理从头开始合成生成项目和任务，消除了对外部数据源的依赖。这使我们能够处理更广泛的编码任务，例如通过生成单元测试来复现问题，以及从头实现库。我们在三个不同的基准上展示了这种方法的有效性，结果表明SWE-Playground生成的轨迹具有密集的训练信号，使代理能够以显著更少的轨迹达到与之前工作相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing methods for training software engineering agents, which rely on pre-existing GitHub repositories and focus narrowly on issue resolution tasks. The authors propose SWE-Playground, a synthetic environment generation pipeline that creates projects and tasks from scratch using strong language models and agents, thereby reducing dependence on external data sources. Experimental results on three benchmarks show that SWE-Playground generates trajectories with dense training signals, allowing agents to achieve comparable performance with far fewer trajectories than previous approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有软件工程代理训练方法的局限性，这些方法依赖于已有的GitHub仓库且主要针对问题解决任务。所提出的方法引入了SWE-Playground，这是一个从头生成项目和任务的合成环境生成管道，利用强大的语言模型和代理，无需依赖外部数据源。在三个基准测试上的实验结果表明，SWE-Playground生成的轨迹具有密集的训练信号，使代理能够在远少于以往方法的示例下达到相当的性能水平。</div>
</details>
</div>
<div class="card">
<div class="title">LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Yuhua Zhu, Wenhui Zhao</div>
<div class="meta-line">First: 2026-01-10T12:18:12+00:00 · Latest: 2026-01-10T12:18:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06550v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06550v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMTrack：基于多模态大语言模型的语义多目标跟踪</div>
<div class="mono" style="margin-top:8px">传统多目标跟踪（MOT）系统在定位和关联方面取得了显著精度，有效回答了\textit{where}和\textit{who}的问题。然而，它们通常作为自闭式观察者，能够追踪几何路径，却无法理解目标行为背后的语义\textit{what}和\textit{why}。为弥合几何感知与认知推理之间的鸿沟，我们提出了\textbf{LLMTrack}，一种用于语义多目标跟踪（SMOT）的新型端到端框架。我们采用仿生设计哲学，将强大的定位能力与深层次的理解能力分离，使用Grounding DINO作为视觉模块，LLaVA-OneVision多模态大模型作为认知模块。我们引入了时空融合模块，聚合实例级交互特征和视频级上下文，使大语言模型（LLM）能够理解复杂轨迹。此外，我们设计了一种渐进式的三阶段训练策略，通过视觉对齐、时间微调和LoRA语义注入，高效地将大规模模型适配到跟踪领域。在BenSMOT基准上的大量实验表明，LLMTrack实现了最先进的性能，在实例描述、交互识别和视频摘要方面显著优于现有方法，同时保持了稳定的跟踪效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLMTrack is proposed to address the limitations of traditional Multi-Object Tracking (MOT) systems, which focus on geometric localization and association but lack semantic understanding of object behaviors. The framework integrates a bionic design, using Grounding DINO for localization and LLaVA-OneVision for semantic reasoning, and introduces a Spatio-Temporal Fusion Module to enhance trajectory comprehension. Through a progressive three-stage training strategy involving Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA, LLMTrack achieves state-of-the-art performance on the BenSMOT benchmark, outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining stable tracking.</div>
<div class="mono" style="margin-top:8px">传统多目标跟踪（MOT）系统在定位和关联方面表现出色，但缺乏对目标行为语义的理解，无法回答&#x27;是什么&#x27;和&#x27;为什么&#x27;的问题。为提升跟踪中的语义推理能力，LLMTrack提出了一种端到端框架，将定位与理解分离，采用Grounding DINO进行视觉感知，LLaVA-OneVision作为认知模块。框架引入了时空融合模块，整合实例级特征与视频级上下文，使大语言模型能够理解复杂轨迹。通过视觉对齐、时间微调和语义注入（LoRA）的三阶段渐进式训练策略，有效适应大规模模型到跟踪领域。在BenSMOT基准上的实验表明，LLMTrack在实例描述、交互识别和视频摘要任务中显著优于现有方法，同时保持了稳定的跟踪性能。</div>
</details>
</div>
<div class="card">
<div class="title">3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</div>
<div class="meta-line">Authors: Hao Tang, Ting Huang, Zeyu Zhang</div>
<div class="meta-line">First: 2026-01-10T09:13:10+00:00 · Latest: 2026-01-10T09:13:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06496v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06496v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/3DCoCav2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D CoCa v2：基于测试时搜索的对比学习框架用于可泛化的空间智能</div>
<div class="mono" style="margin-top:8px">空间智能指的是在三维环境中感知、推理和描述物体及其关系的能力，是具身感知和场景理解的基础。3D字幕生成旨在用自然语言描述三维场景；然而，由于点云的稀疏性和不规则性，以及现有字幕生成模型在不同环境（包括室内外三维场景）中存在弱锚定和有限的分布外（OOD）泛化能力，该任务仍具有挑战性。为了解决这一问题，我们提出了3D CoCa v2，这是一个可泛化的三维字幕生成框架，将对比视觉-语言学习与三维字幕生成统一起来，并通过测试时搜索（TTS）进一步提升鲁棒性，而无需更新字幕生成模型的参数。3D CoCa v2基于冻结的CLIP语义先验，结合了空间感知的三维场景编码器和一个与对比学习和字幕生成目标联合优化的多模态解码器，避免使用外部检测器或手工设计的提案。在推理阶段，TTS生成多样化的字幕候选，并利用紧凑的场景摘要进行奖励引导选择。实验结果表明，在ScanRefer和Nr3D数据集上，3D CoCa v2相比3D CoCa分别提升了+1.50和+1.61的CIDEr@0.5IoU，在TOD3Cap的零样本OOD评估中提升了+3.8的CIDEr@0.25。代码将在https://github.com/AIGeeksGroup/3DCoCav2上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the generalizability of 3D captioning models across diverse environments, particularly addressing the challenges posed by sparse and irregular point clouds as well as weak grounding and limited out-of-distribution (OOD) performance. 3D CoCa v2 integrates contrastive vision-language learning with 3D caption generation, utilizing a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder, and a multimodal decoder jointly optimized for both contrastive and captioning tasks. The framework introduces test-time search (TTS) to improve robustness by generating diverse caption candidates and selecting the best one through reward-guided mechanisms based on a compact scene summary. Experimental results demonstrate significant improvements over the previous version, 3D CoCa, with increases of +1.50 CIDEr@0.5IoU on ScanRefer, +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D场景描述模型在不同环境下的泛化能力，特别是应对点云数据稀疏不规则以及现有方法在环境差异中弱语义关联的问题。3D CoCa v2提出了一种结合对比学习与3D描述生成的框架，采用冻结的CLIP语义先验、空间感知的3D场景编码器以及联合优化的多模态解码器。该方法通过测试时搜索（TTS）增强鲁棒性，生成多样化的描述候选并基于场景摘要进行奖励引导选择。实验结果表明，与3D CoCa相比，该方法在ScanRefer上CIDEr@0.5IoU提升1.50，在Nr3D上提升1.61，并在TOD3Cap的零样本OOD评估中CIDEr@0.25提升3.8。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-09T20:17:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长周期软件演进场景中对代码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI代码代理基准主要关注孤立的、单一问题的任务，例如修复错误或实现小功能。然而，现实中的软件工程本质上是一项长周期的工作：开发者必须理解高层需求，协调修改多个文件，并在多次迭代中演进代码库同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长周期软件演进挑战上的基准。该基准基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而单任务SWE-Bench Verified则达到65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长周期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing benchmarks for AI coding agents, which focus on isolated, single-issue tasks rather than the complex, long-term nature of real-world software evolution. The authors introduce SWE-EVO, a benchmark designed to evaluate agents on multi-step, multi-file software evolution tasks derived from the release notes and version histories of seven mature Python projects. The benchmark includes 48 tasks requiring changes across an average of 21 files, validated by extensive test suites with over 874 tests per instance. Experimental results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on the single-issue SWE-Bench Verified, highlighting the current challenges in sustained, multi-file reasoning for coding agents. The study also introduces Fix Rate as a new metric to measure partial progress in solving these complex tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有AI编码代理基准测试的不足，这些测试主要关注孤立的、单一问题的任务，而非真实软件工程中长期的代码演进过程。SWE-EVO被引入作为评估代理在多步骤、多文件代码演进任务上的基准，这些任务来源于七个成熟的开源Python项目，包含48个经过全面测试套件验证的演进任务，平均涉及21个文件的修改。实验结果表明，即使是先进的模型如GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，远低于单任务SWE-Bench Verified的65%，突显了代理在持续多文件推理方面的困难。</div>
</details>
</div>
<div class="card">
<div class="title">Automated QoR improvement in OpenROAD with coding agents</div>
<div class="meta-line">Authors: Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee</div>
<div class="meta-line">First: 2026-01-09T19:30:02+00:00 · Latest: 2026-01-09T19:30:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在OpenROAD中使用编码代理实现自动化QoR改进</div>
<div class="mono" style="margin-top:8px">EDA开发和创新一直受到专家工程资源稀缺的限制。尽管领先的LLMs在编码和科学推理任务中表现出色，但它们在推动EDA技术发展方面的潜力尚未得到充分验证。我们提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主、基于仓库的编码系统，能够读取OpenROAD，提出研究方向，将其扩展为实现步骤，并提交可执行的代码差异。我们的贡献包括：(i) 一个用于EDA代码修改的闭环LLM框架；(ii) 针对PPA导向改进的OpenROAD任务套件和评估协议；(iii) 需要最少人工监督的端到端演示。在OpenROAD上的实验实现了最高达5.9%的布线线长减少和10.0%的有效时钟周期减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of expert engineering resources in EDA development and explore the potential of large language models (LLMs) in advancing EDA technology. The authors introduce AuDoPEDA, an autonomous coding system that leverages OpenAI models and a Codex-class agent to analyze OpenROAD, suggest research directions, generate implementation steps, and submit executable code changes. Experimental results show that AuDoPEDA achieves up to a 5.9% reduction in routed wirelength and a 10.0% improvement in effective clock period, demonstrating its effectiveness in PPA-oriented improvements.</div>
<div class="mono" style="margin-top:8px">本文针对EDA开发中专家资源稀缺的问题，提出AuDoPEDA系统，该系统基于OpenAI模型和Codex类代理，能够分析OpenROAD，提出研究方向并生成可执行的代码修改。系统采用闭环LLM框架进行代码迭代优化，并通过面向PPA（功耗、性能、面积）改进的任务集进行评估。实验结果显示，AuDoPEDA可实现布线长度最多减少5.9%，有效时钟周期最多缩短10.0%。</div>
</details>
</div>
<div class="card">
<div class="title">From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level</div>
<div class="meta-line">Authors: Jia Li, Yuxin Su, Michael R. Lyu</div>
<div class="meta-line">First: 2026-01-07T09:22:28+00:00 · Latest: 2026-01-09T16:30:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03731v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03731v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从实验室到实际应用：在仓库级别对代理代码推理进行基准测试</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLMs）演进为自主代理，评估仓库级别的推理能力，即在大规模、真实世界、相互依赖的文件系统中保持逻辑一致性，变得至关重要。当前的基准测试通常在孤立的代码片段和黑盒评估之间波动。我们提出了 RepoReason，这是一个基于归纳断言验证的白盒诊断基准。为了消除记忆效应同时保留真实的逻辑深度，我们实现了一个执行驱动的变异框架，利用环境作为语义预言机来重新生成真实状态。此外，我们使用动态程序切片建立了一个细粒度的诊断系统，并通过三个正交指标（$ESV$（读取负载）、$MCL$（模拟深度）、$DFI$（集成宽度））量化推理能力。对前沿模型（如 Claude-4.5-Sonnet、DeepSeek-v3.1-Terminus）的全面评估揭示了一个普遍存在的聚合缺陷，其中集成宽度是主要的认知瓶颈。我们的研究结果为优化下一代代理软件工程提供了细致的白盒见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of evaluating agentic code reasoning at the repository level, as large language models evolve into autonomous agents. It introduces RepoReason, a white-box benchmark that focuses on abductive assertion verification, using an execution-driven mutation framework to generate ground-truth states without relying on memorization. The benchmark employs dynamic program slicing to create a fine-grained diagnostic system, measuring reasoning through three metrics: ESV (reading load), MCL (simulation depth), and DFI (integration width). Evaluations of state-of-the-art models show a significant aggregation deficit, with integration width identified as the main cognitive bottleneck in maintaining logical consistency across large, interdependent codebases.</div>
<div class="mono" style="margin-top:8px">该研究旨在评估代理代码在仓库层面的推理能力，这对于确保大规模、相互依赖文件系统的逻辑一致性至关重要。作者提出了RepoReason，这是一个基于白盒诊断的基准，专注于通过执行驱动的变异框架进行归纳性断言验证，利用环境作为语义预言机生成真实的基准状态，以避免记忆效应并保持逻辑深度。该基准结合动态程序切片技术，构建了细粒度的诊断系统，并通过三个相互独立的指标（ESV：阅读负载，MCL：模拟深度，DFI：集成宽度）量化推理能力。实验结果显示，前沿模型普遍存在聚合缺陷，其中集成宽度是仓库级推理的主要认知瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">CoV: Chain-of-View Prompting for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang</div>
<div class="meta-line">First: 2026-01-08T17:59:42+00:00 · Latest: 2026-01-09T07:20:05+00:00</div>
<div class="meta-line">Comments: Code link https://github.com/ziplab/CoV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05172v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05172v2">PDF</a> · <a href="https://github.com/ziplab/CoV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoV：用于空间推理的视角链提示</div>
<div class="mono" style="margin-top:8px">在三维环境中进行具身问答（EQA）通常需要收集分布在多个视角且部分遮挡的上下文信息。然而，大多数最近的视觉-语言模型（VLMs）受限于固定的、有限的输入视角，这限制了它们在推理时获取与问题相关上下文的能力，并阻碍了复杂的空间推理。我们提出了一种名为Chain-of-View（CoV）的提示方法，这是一种无需训练、在测试时进行推理的框架，通过粗到细的探索过程将VLM转化为主动视角推理器。CoV首先使用视角选择代理来过滤冗余帧并识别与问题对齐的关键视角，然后通过交替进行迭代推理和离散相机操作，进行细粒度视角调整，从底层三维场景表示中获取新的观察结果，直到收集到足够的上下文信息或达到步数预算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of spatial reasoning in embodied question answering (EQA) by proposing a novel test-time reasoning framework called Chain-of-View (CoV) prompting. The motivation stems from the limitation of existing vision-language models (VLMs) that rely on a fixed set of input views, which hinders their ability to gather sufficient context for complex questions. CoV enables active viewpoint exploration through a View Selection agent that identifies relevant anchor views and iteratively adjusts the camera to obtain more detailed observations. Experimental results on OpenEQA show significant improvements, with an average of +11.56% in LLM-Match and up to +13.62% on Qwen3-VL-Flash, demonstrating the effectiveness of question-aligned view selection and open-view search in enhancing spatial reasoning without requiring additional training.</div>
<div class="mono" style="margin-top:8px">本文针对具身问答（EQA）中空间推理的挑战，提出了一种新的测试时推理框架Chain-of-View（CoV）提示。传统视觉-语言模型（VLMs）受限于固定的输入视角，难以获取复杂问题所需的上下文信息。CoV通过粗到细的探索过程，使模型能够主动选择相关视角并迭代调整摄像头以获取更详细的观察信息。在OpenEQA数据集上的实验表明，CoV在LLM-Match指标上平均提升11.56%，其中Qwen3-VL-Flash最大提升达13.62%，且在增加动作步数时表现出测试时扩展性，平均提升2.51%，最高提升3.73%。该方法不依赖额外训练，适用于多种模型。</div>
</details>
</div>
<div class="card">
<div class="title">Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection</div>
<div class="meta-line">Authors: Feihu Jin, Ying Tan</div>
<div class="meta-line">First: 2026-01-09T03:20:54+00:00 · Latest: 2026-01-09T03:20:54+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05501v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05501v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of &quot;beneficial stochasticity&quot; to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Hi-ZFO：通过重要性引导张量选择的分层零阶和一阶LLM微调</div>
<div class="mono" style="margin-top:8px">使用标准一阶（FO）优化方法对大型语言模型（LLMs）进行微调通常会引导训练趋向于尖锐且泛化能力差的极小值点。相反，零阶（ZO）方法在不依赖显式梯度的情况下提供了更强的探索能力，但收敛速度较慢。更关键的是，我们的分析表明，在生成任务中，庞大的输出和搜索空间显著放大了估计方差，使得ZO方法既嘈杂又低效。为了解决这些挑战，我们提出了\textbf{Hi-ZFO}（\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder优化），一个混合框架，旨在结合一阶梯度的精确性与零阶估计的探索能力。Hi-ZFO通过逐层重要性分析对模型进行自适应划分，对关键层应用精确的一阶更新，而对不敏感层则利用零阶优化。值得注意的是，Hi-ZFO中的零阶方法并非仅仅是节省内存的替代方案；它被有意引入作为&quot;有益的随机性&quot;来源，以帮助模型逃离纯一阶优化容易停滞的局部极小值点。在多种生成、数学和代码推理任务中，Hi-ZFO均表现出色，同时显著减少了训练时间。这些结果证明了分层混合优化在LLM微调中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind Hi-ZFO is to overcome the limitations of standard first-order (FO) optimization in LLM fine-tuning, which often leads to sharp and poorly generalizing minima, and the inefficiency of zeroth-order (ZO) methods due to high estimation variance in generative tasks. Hi-ZFO introduces a hybrid framework that combines FO and ZO optimization by adaptively partitioning the model based on layer-wise importance profiling. This allows precise FO updates on critical layers while using ZO optimization for less sensitive ones, introducing beneficial stochasticity to avoid local minima. The method is validated across various tasks, showing improved performance and reduced training time compared to traditional approaches.</div>
<div class="mono" style="margin-top:8px">Hi-ZFO的研究动机是解决在微调大语言模型（LLMs）时，第一阶（FO）优化方法容易收敛到尖锐且泛化能力差的极小值点，而零阶（ZO）方法在生成任务中由于输出空间庞大导致估计方差高，从而变得嘈杂且低效。Hi-ZFO提出了一种混合框架，通过分层重要性分析将模型分块，对关键层采用精确的FO优化，对不敏感层使用ZO优化，以引入有益的随机性，帮助模型逃离局部极小值。实验结果表明，Hi-ZFO在多个任务中均实现了更优的性能，并显著降低了训练时间。</div>
</details>
</div>
<div class="card">
<div class="title">GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</div>
<div class="meta-line">Authors: Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov</div>
<div class="meta-line">First: 2026-01-08T18:59:24+00:00 · Latest: 2026-01-08T18:59:24+00:00</div>
<div class="meta-line">Comments: NVIDIA-Tech Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDPO: 针对多奖励强化学习优化的组奖励解耦归一化策略优化</div>
<div class="mono" style="margin-top:8px">随着语言模型能力的不断提升，用户期望它们不仅能提供准确的响应，还能在各种场景下表现出符合多样化人类偏好的行为。为此，强化学习（RL）流程开始引入多个奖励，每个奖励捕捉不同的偏好，以引导模型向期望的行为发展。然而，近期的研究默认在多奖励设置下使用组相对策略优化（GRPO），而未检验其适用性。本文中我们证明，直接将GRPO应用于归一化不同的rollout奖励组合会导致它们坍缩为相同的优势值，降低训练信号的分辨率，从而导致次优收敛，甚至在某些情况下出现早期训练失败。随后，我们引入了组奖励解耦归一化策略优化（GDPO），这是一种新的策略优化方法，通过解耦个体奖励的归一化，更真实地保留其相对差异，实现更精确的多奖励优化，并显著提升了训练的稳定性。我们在三个任务上将GDPO与GRPO进行比较：工具调用、数学推理和编码推理，评估了正确性指标（准确率、错误率）和约束遵守指标（格式、长度）。在所有设置下，GDPO均优于GRPO，证明了其在多奖励强化学习优化中的有效性和通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-reward reinforcement learning (RL) optimization, where existing methods like Group Relative Policy Optimization (GRPO) fail to maintain distinct reward differences, leading to suboptimal convergence and training instability. The authors propose GDPO, a novel policy optimization approach that decouples the normalization of individual rewards, preserving their relative differences and enhancing training stability. Experimental evaluations on three tasks—tool calling, math reasoning, and coding reasoning—show that GDPO outperforms GRPO in both correctness and constraint adherence metrics, demonstrating its effectiveness in multi-reward RL settings.</div>
<div class="mono" style="margin-top:8px">本文针对多奖励强化学习（RL）优化中的挑战，指出现有方法如Group Relative Policy Optimization（GRPO）在处理不同奖励组合时无法保持其相对差异，导致收敛效果不佳和训练不稳定。作者提出了一种新的策略优化方法GDPO，通过解耦各奖励的归一化过程，更准确地保留其相对差异，从而提升训练稳定性。在工具调用、数学推理和编码推理三个任务上的实验表明，GDPO在正确性指标和约束遵循指标上均优于GRPO，验证了其在多奖励RL优化中的有效性与普适性。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests</div>
<div class="meta-line">Authors: Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang</div>
<div class="meta-line">First: 2026-01-08T12:31:02+00:00 · Latest: 2026-01-08T12:31:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04886v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分析AI编码代理撰写的拉取请求中的消息-代码不一致</div>
<div class="mono" style="margin-top:8px">由AI编码代理生成的拉取请求（PR）描述是向人类审阅者传达代码更改的主要渠道。然而，这些消息与实际更改之间的对齐情况尚未被探索，引发了对AI代理可信度的担忧。为填补这一空白，我们使用PR消息-代码不一致（PR-MCI）分析了五个代理的23,247个代理PR。我们贡献了974个手动标注的PR，发现406个PR（1.7%）表现出高PR-MCI，并识别出八种PR-MCI类型，其中声称未实现更改的描述是最常见问题（45.4%）。统计检验证实，高MCI的PR接受率低51.7%（28.3% vs. 80.0%），且合并时间是普通PR的3.5倍（55.8 vs. 16.0小时）。我们的研究结果表明，不可靠的PR描述会损害对AI代理的信任，强调了需要PR-MCI验证机制和改进PR生成以实现可信的人机协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the issue of message-code inconsistency in pull requests authored by AI coding agents, as PR descriptions are critical for human reviewers to understand code changes. By analyzing 23,247 PRs across five agents, the researchers identified 406 PRs (1.7%) with high PR-MCI, primarily due to descriptions falsely claiming unimplemented changes. The findings show that these high-MCI PRs have significantly lower acceptance rates and longer merge times, indicating that unreliable descriptions hinder trust in AI agents and emphasize the need for improved verification and generation mechanisms to support effective human-AI collaboration.</div>
<div class="mono" style="margin-top:8px">本研究探讨了由AI编码代理撰写的拉取请求（PR）中消息与代码不一致的问题，因为PR描述是人类审阅者理解代码更改的关键渠道。通过对五个代理的23,247个PR进行分析，研究人员发现有406个PR（1.7%）存在高PR-MCI，其中大部分是由于描述错误地声称未实现更改（占45.4%）。实验结果表明，高PR-MCI的PR接受率显著降低（28.3% vs. 80.0%），合并时间也更长（55.8小时 vs. 16小时），这表明不可靠的PR描述会损害对AI代理的信任，并影响协作效率。</div>
</details>
</div>
<div class="card">
<div class="title">AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding</div>
<div class="meta-line">Authors: Aleksei Kondratenko, Mussie Birhane, Houssame E. Hsain, Guido Maciocci</div>
<div class="meta-line">First: 2026-01-08T10:54:32+00:00 · Latest: 2026-01-08T10:54:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04819v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AECV-Bench：在建筑与工程图纸理解上对多模态模型的基准测试</div>
<div class="mono" style="margin-top:8px">AEC图纸通过符号、布局规范和密集标注来编码几何信息和语义信息，但现代多模态和视觉-语言模型是否能可靠地理解这种图形语言仍不清楚。我们提出了AECV-Bench，这是一个通过两个互补的使用案例来评估多模态和视觉-语言模型在真实AEC制品上的基准：(i) 在120份高质量平面图上进行物体计数（门、窗、卧室、卫生间等），以及(ii) 基于图纸的文档问答，涵盖192个问题-答案对，测试文本提取（OCR）、实例计数、空间推理和比较推理能力。物体计数性能通过每字段精确匹配准确率和MAPE结果进行报告，而文档问答性能则通过整体准确率和按类别细分结果进行报告，采用LLM作为评分管道和针对边缘情况的人工裁定。在统一协议下评估一系列最先进的模型，我们观察到一个稳定的性能梯度：OCR和以文本为中心的文档问答表现最佳（准确率高达0.95），空间推理表现中等，而以符号为中心的图纸理解——尤其是门和窗的可靠计数——仍是一个未解决的问题（通常准确率在0.40-0.55之间），存在显著的比例误差。这些结果表明，当前系统在作为文档助手方面表现良好，但缺乏对图纸的稳健理解能力，这促使我们开发领域特定的表示方法和工具增强、人类参与的工作流，以实现高效的AEC自动化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces AECV-Bench, a benchmark designed to evaluate the performance of multimodal and vision-language models in understanding architectural and engineering drawings. The motivation stems from the need to assess whether these models can effectively interpret the complex graphical language used in AEC drawings, which combines geometry, semantics, and dense annotations. The benchmark includes two use cases: object counting in floor plans and drawing-grounded document QA. Results show that OCR and text-based QA achieve high accuracy, while spatial reasoning and symbol-based understanding, particularly for doors and windows, remain challenging with accuracies often between 0.40 and 0.55. These findings highlight the current limitations in drawing comprehension and suggest the need for domain-specific representations and human-in-the-loop workflows to improve AEC automation.</div>
<div class="mono" style="margin-top:8px">本文提出了AECV-Bench，这是一个用于评估多模态和视觉-语言模型在理解和解析建筑与工程图纸方面能力的基准测试。研究动机源于对现代模型是否能可靠地解析包含符号、布局规范和密集注释的AEC图纸的不确定性。该基准包含两个使用场景：一是对120份高质量平面图中的对象（如门、窗、卧室、卫生间）进行计数，二是基于192组问题-答案对的图纸相关文档问答，测试文本提取（OCR）、实例计数、空间推理和比较推理能力。实验结果显示，OCR和以文本为中心的问答准确率较高（最高达0.95），而空间推理和以符号为中心的图纸理解能力较弱，尤其是门和窗的可靠计数仍存在显著挑战，准确率通常在0.40到0.55之间。这些结果表明，当前系统在作为文档助手方面表现良好，但在图纸理解方面存在不足，从而推动了针对领域特定表示和人机协作流程的研究。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
