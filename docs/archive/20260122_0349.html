<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 03:49</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0349</div>
    <div class="row"><div class="card">
<div class="title">Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics</div>
<div class="meta-line">Authors: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li</div>
<div class="meta-line">First: 2026-01-20T14:51:45+00:00 · Latest: 2026-01-20T14:51:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14027v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14027v1">PDF</a> · <a href="https://github.com/project-numina/numina-lean-agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Numina-Lean-Agent：一个开放且通用的代理推理系统用于形式化数学</div>
<div class="mono" style="margin-top:8px">代理系统最近已成为形式化定理证明的主导范式，通过协调多个模型和工具实现了强大的性能。然而，现有方法通常依赖于特定任务的流水线和训练过的形式化证明器，限制了其灵活性和可复现性。在本文中，我们提出了一种范式，即直接使用通用编码代理作为形式化数学推理器。该范式受到以下三点动机的驱动：(1) 通用编码代理为超越证明的多样化推理任务提供了自然的接口；(2) 仅通过替换底层基础模型即可提升性能，而无需训练；(3) MCP 使得灵活扩展和自主调用专用工具成为可能，避免了复杂的设计。基于这一范式，我们引入了 Numina-Lean-Agent，它结合了 Claude Code 和 Numina-Lean-MCP，以实现与 Lean 的自主交互、相关定理的检索、非正式证明和辅助推理工具。使用 Claude Opus 4.5 作为基础模型，Numina-Lean-Agent 在 Putnam 2025 中解决了所有问题（12/12），与最佳闭源系统表现相当。除了基准评估，我们还通过与数学家互动，成功地将 Brascamp-Lieb 定理形式化，进一步展示了其通用性。我们将在 https://github.com/project-numina/numina-lean-agent 发布 Numina-Lean-Agent 和所有解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes Numina-Lean-Agent, an agentic reasoning system for formal mathematics, motivated by the need for a more flexible and reproducible approach compared to existing task-specific pipelines and trained provers. The system leverages a general coding agent, combined with Numina-Lean-MCP, to enable autonomous interaction with Lean, theorem retrieval, and use of informal proving and auxiliary reasoning tools. Experimental results show that Numina-Lean-Agent, using Claude Opus 4.5 as its base model, solves all 12 problems in the Putnam 2025 competition, matching the performance of the best closed-source systems. Additionally, it successfully formalizes the Brascamp-Lieb theorem through interaction with mathematicians, demonstrating its broad applicability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过摒弃任务特定的流水线和训练过的定理证明器，开发一个更灵活且可复现的数学形式化推理系统。所提出的方法利用通用编码代理，结合Claude Code与Numina-Lean-MCP，实现与Lean定理证明器的自主交互、相关定理检索以及非正式证明和辅助推理工具的使用。主要实验结果表明，Numina-Lean-Agent成功解决了Putnam 2025竞赛中的全部12道题目，与最佳闭源系统表现相当，并通过与数学家合作成功形式化了Brascamp-Lieb定理。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍存在分裂：视觉-语言模型（如CLIP）在全局语义对齐方面表现优异，但缺乏空间精度；而自监督方法（如MAE、DINO）能够捕捉复杂的局部结构，但在高层次语义上下文方面存在困难。我们认为这些范式本质上是互补的，可以整合到一个有原则的多任务框架中，并通过密集空间监督进一步增强。我们提出了MTV，一个联合优化视觉-语言对比、自监督和密集空间目标的多任务视觉预训练框架。为减少对人工标注的依赖，我们利用高容量的&quot;专家&quot;模型——如Depth Anything V2和OWLv2——大规模合成密集且结构化的伪标签。除了框架本身，我们还系统地研究了多任务视觉学习的机制，分析了：(i) 每个目标的边际增益，(ii) 任务间的协同效应与干扰，以及(iii) 在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了&quot;两者兼顾&quot;的性能，显著提升了细粒度空间推理能力，而不会损害全局语义理解。我们的发现表明，借助高质量的伪监督，多任务学习是一条通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current visual representation learning approaches by proposing a multi-task framework that combines the strengths of vision-language models and self-supervised methods. The framework, named MTV, jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial learning objectives, using high-capacity expert models to generate pseudo-labels. Experimental results show that MTV achieves superior performance in both fine-grained spatial reasoning and global semantic understanding, demonstrating the effectiveness of integrating these paradigms through principled multi-task learning.</div>
<div class="mono" style="margin-top:8px">本文针对当前视觉表征学习方法的局限性，提出了一种结合视觉-语言模型和自监督方法优势的多任务框架MTV。该框架通过共享主干网络，联合优化视觉-语言对比、自监督和密集空间任务，并利用高容量专家模型生成伪标签以减少人工标注需求。实验结果表明，MTV在细粒度空间推理和全局语义理解方面均表现出色，验证了多任务学习在提升视觉编码器泛化能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories</div>
<div class="meta-line">Authors: Aditya Bharat Soni, Rajat Ghosh, Vaishnavi Bhargava, Valerie Chen, Debojyoti Dutta</div>
<div class="meta-line">First: 2026-01-20T08:10:56+00:00 · Latest: 2026-01-20T08:10:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- &quot;test first, write code later&quot;, and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Tester：在真实仓库中训练开源大语言模型以生成问题复现测试</div>
<div class="mono" style="margin-top:8px">软件测试对于确保软件系统的正确性和可靠性至关重要。从自然语言问题描述中自动生成问题复现测试可以简化根本原因分析，提高开发人员的生产力，促进测试驱动开发——&quot;先测试后编写代码&quot;，并且可用于提升自动化问题解决系统（如编码代理）的效果。现有的方法主要依赖于闭源大语言模型，对开源模型的探索有限。为了解决这一问题，我们提出了SWE-Tester——一种新颖的训练流程，用于训练开源大语言模型生成问题复现测试。首先，我们从2600个开源GitHub仓库中精选出41000个高质量训练实例，并用其训练不同规模和家族的大语言模型。微调后的模型在SWT-Bench Verified上实现了高达10\%的成功率提升和21\%的变更覆盖率提升。进一步分析表明，随着推理时间计算量的增加、数据量的增多和模型规模的扩大，模型表现持续提升。这些结果突显了我们框架在推动该领域开源大语言模型发展方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the generation of issue reproduction tests using open-source large language models (LLMs) rather than closed-source ones. SWE-Tester introduces a novel pipeline that trains open-source LLMs on a curated dataset of 41K instances from 2.6K GitHub repositories. The fine-tuned models demonstrate significant improvements, achieving up to a 10\% increase in success rate and 21\% in change coverage on the SWT-Bench Verified benchmark. These results indicate that training open-source LLMs can effectively enhance the quality and reliability of automated test generation for real-world software development.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升开源大语言模型（LLMs）在从自然语言描述生成问题复现测试方面的能力，这对于软件测试和调试至关重要。提出的方法SWE-Tester引入了一个训练开源LLMs的流水线，利用从2.6K个GitHub仓库中精心整理的41K个实例数据集进行训练。经过微调的模型在SWT-Bench Verified基准上表现出显著提升，成功率达到最高提升10\%，变更覆盖率提升21\%。这些结果突显了在该领域中合理训练的开源LLMs的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis</div>
<div class="meta-line">Authors: Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang, Zhongxue Gan</div>
<div class="meta-line">First: 2026-01-20T06:50:40+00:00 · Latest: 2026-01-20T06:50:40+00:00</div>
<div class="meta-line">Comments: This study has been accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为前时空解耦：面向多模态情感分析的解耦表征学习</div>
<div class="mono" style="margin-top:8px">多模态情感分析融合语言、视觉和听觉模态。主流方法基于模态不变和模态特异性因子分解或复杂融合，仍依赖于时空混合建模。这忽略了时空异质性，导致时空信息不对称，从而限制了性能。因此，我们提出TSDA（行为前时空解耦），在任何模态交互之前，显式地将每个模态解耦为时间动态和空间结构上下文。对于每个模态，时间编码器和空间编码器将信号分别投影到时间空间和空间空间中。因子一致的跨模态对齐随后仅对齐模态间的时间特征与时间对应特征，空间特征与空间对应特征。因子特异性监督和去相关正则化减少了跨因子泄漏，同时保留了互补性。随后，一个门控再耦合模块将对齐的流重新耦合以执行任务。大量实验表明，TSDA优于基线方法。消融分析验证了设计的必要性和可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of existing multimodal sentiment analysis approaches that fail to account for spatiotemporal heterogeneity by integrating linguistic, visual, and acoustic modalities. The proposed method, TSDA, decouples each modality into temporal and spatial components before any interaction, using separate encoders for each. Temporal features are aligned across modalities based on their temporal counterparts, while spatial features are aligned based on their spatial counterparts. Factor-specific supervision and decorrelation regularization are introduced to reduce cross-factor leakage and maintain feature complementarity. A gated recouple module is then used to combine the aligned features for the final task. Experimental results demonstrate that TSDA achieves superior performance compared to existing baselines, with ablation studies confirming the effectiveness of its design.</div>
<div class="mono" style="margin-top:8px">本研究针对现有多模态情感分析方法在处理语言、视觉和听觉模态时未能考虑时空异质性的不足，提出了一种新的方法TSDA。该方法在任何交互之前，将每种模态分解为时序动态和空间结构上下文，并分别使用时序编码器和空间编码器进行处理。随后，通过因子一致的跨模态对齐，将时序特征与跨模态的时序特征对齐，空间特征与跨模态的空间特征对齐。引入因子特定监督和去相关正则化以减少跨因子信息泄露，同时保持特征的互补性。最后，使用门控再耦合模块将对齐后的特征流整合用于任务处理。大量实验表明，TSDA在多个基准数据集上优于现有方法，消融实验进一步验证了其设计的必要性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</div>
<div class="meta-line">Authors: Peter A. Massih, Eric Cosatto</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-19T21:14:34+00:00 · Latest: 2026-01-19T21:14:34+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素级精确推理：用于定量地理空间分析的QVLM架构和SQuID数据集</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLMs）在定量空间推理方面表现不佳，因为其架构会破坏进行计数和测量所需的像素级信息。视觉编码器通过图像块嵌入压缩图像，导致空间索引丢失，无法实现精确的像素级追踪。我们提出了两个贡献以解决这一根本性限制。首先，我们引入了SQuID（卫星定量智能数据集），这是一个包含2000对卫星图像问答对的基准数据集，具有数值范围和分类答案，旨在评估定量空间推理能力。该数据集跨越三个难度层级，标注信息由人类标签及其学习到的变异性自动生成。其次，我们提出了QVLM（定量视觉-语言模型），这是一种代码生成架构，通过将语言理解与视觉分析解耦，保持像素精度。QVLM不将图像编码为嵌入，而是生成可执行代码，首先调用分割模型获取像素级掩码，然后直接在这些掩码上进行操作，从而在整个推理过程中保留空间索引。我们的实验表明，使用GPT-5作为代码生成器的QVLM在SQuID数据集上达到了42.0%的准确率，而使用图像-问题对提示的VLM仅达到28.1%。我们的工作表明，对于定量空间推理任务，架构解耦能够提高定量任务的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance quantitative spatial reasoning in Vision-Language Models (VLMs) by preserving pixel-level information, which is essential for accurate counting and measurements. To achieve this, the authors introduce QVLM, a novel architecture that decouples language understanding from visual analysis, generating executable code to process satellite images through segmentation and direct mask manipulation. They also propose the SQuID dataset, consisting of 2,000 satellite image Question-Answer pairs with numerical and categorical answers, designed to evaluate quantitative reasoning capabilities. Experimental results demonstrate that QVLM using GPT-5 as a coder achieves 42.0% accuracy on SQuID, significantly outperforming traditional VLMs with image-question prompts, which only reach 28.1% accuracy.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前视觉语言模型（VLMs）在定量空间推理中的不足，即图像编码过程中丢失了像素级信息，导致难以进行计数和测量。为此，作者提出了QVLM架构，通过将语言理解与视觉分析解耦，并利用生成的可执行代码处理由分割模型获得的像素级掩码，从而保持空间索引的精确性。同时，他们构建了SQuID数据集，包含2000对卫星图像与问题-答案对，涵盖数值和分类答案，用于评估定量空间推理能力。实验结果表明，使用GPT-5作为代码生成器的QVLM在SQuID上达到42.0%的准确率，显著优于传统VLM在图像-问题对提示下的28.1%准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility</div>
<div class="meta-line">Authors: Nickil Maveli, Antonio Vergari, Shay B. Cohen</div>
<div class="meta-line">First: 2026-01-19T21:09:48+00:00 · Latest: 2026-01-19T21:09:48+00:00</div>
<div class="meta-line">Comments: 32 pages (preprint)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13398v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13398v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型能否压缩（并解压）？通过可逆性评估代码理解和执行</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在代码基准测试中表现出色，但代码执行的往返测试揭示了它们在正向和反向执行中保持一致推理能力方面的局限性。我们提出了RoundTripCodeEval（RTCE），一个包含四个不同代码执行推理任务的全面基准，旨在严格测试往返一致性。RTCE提供了一种无需执行的精确匹配评估方式，检验模型在不同算法和方向上编码与解码操作之间是否保持一致的一一映射关系。我们系统地使用零样本提示、执行轨迹的监督微调以及自我反思机制评估最先进的代码大语言模型（Code-LLMs）。每种方法都带来适度的改进，但均未弥合差距，表明当前LLMs在真正的往返一致性方面存在困难，这表明它们缺乏进行可信代码推理所需的内部一致性。RTCE揭示了多个现有I/O预测、执行推理或往返自然语言基准未涵盖的新颖且未被测量的见解。在论文被接受后，我们将发布代码和数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of large language models (LLMs) to maintain consistent reasoning during code compression and decompression, highlighting limitations in their round-trip execution capabilities. The authors introduce RoundTripCodeEval (RTCE), a benchmark that evaluates code understanding through four distinct tasks focusing on the consistency of encoding and decoding processes. By assessing bijection fidelity without execution, RTCE reveals that current state-of-the-art Code-LLMs show only modest improvements when tested with zero-shot prompting, supervised fine-tuning, and self-reflection mechanisms, indicating a lack of internal coherence necessary for reliable code reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在代码压缩与解压缩过程中保持一致推理能力的问题，重点关注往返一致性。作者提出了RoundTripCodeEval（RTCE）基准，通过四个不同的任务评估代码理解和执行的双向一致性，测试编码与解码过程之间的一一映射保真度。他们使用零样本提示、执行轨迹上的监督微调和自我反思机制对最先进的代码LLM进行了系统评估，发现尽管这些方法带来了一定改进，但均未实现真正的往返一致性，表明当前模型在代码推理方面缺乏必要的内部连贯性。</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models</div>
<div class="meta-line">Authors: Samuel Cyrenius Anderson</div>
<div class="meta-line">First: 2026-01-19T19:53:37+00:00 · Latest: 2026-01-19T19:53:37+00:00</div>
<div class="meta-line">Comments: 34 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -&gt; 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -&gt; 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维的几何学：大规模语言模型中规模如何重构推理</div>
<div class="mono" style="margin-top:8px">规模并不均匀地提升推理能力，而是重构了推理。通过分析四个领域（法律、科学、代码、数学）和两个规模（80亿、700亿参数）下超过25,000条思维链轨迹，我们发现神经网络的扩展定律触发了领域特异的相变，而非统一的能力提升。法律推理经历结晶化：表征维度减少45%（d95: 501 -&gt; 274），轨迹对齐度增加31%，并实现10倍的语义分离。科学和数学推理保持液态——即使参数增加9倍，其几何结构仍保持不变。代码推理形成离散的战略模式格子（轮廓系数：0.13 -&gt; 0.42）。这种几何结构决定了可学习性。我们引入了神经推理算子——从初始到终端隐藏状态的学习映射。在结晶化的法律推理中，我们的算子通过探针解码在未见任务上达到63.6%的准确率，无需遍历中间状态即可预测推理终点。我们进一步识别出一个跨领域和规模的普遍振荡特征（相干度 ~ -0.4），表明注意力层和前馈层通过相反的动力学驱动推理。这些发现表明，思维的成本并非由任务难度决定，而是由流形几何决定——这为推理加速提供了蓝图，前提是拓扑结构允许。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how increasing the scale of large language models (LLMs) affects their reasoning capabilities across different domains. By analyzing over 25,000 chain-of-thought trajectories in Law, Science, Code, and Math with models of 8B and 70B parameters, the research reveals that scaling induces domain-specific phase transitions rather than uniform improvements. Legal reasoning transitions to a crystalline state with reduced representational dimensionality and improved trajectory alignment, while scientific and mathematical reasoning remain in a liquid state, maintaining geometric invariance. Code reasoning forms a lattice structure with enhanced distinctiveness. The study introduces Neural Reasoning Operators, which map initial to terminal hidden states and achieve high accuracy in predicting reasoning outcomes without traversing intermediate steps. A universal oscillatory pattern is also identified, suggesting that attention and feedforward layers contribute to reasoning through opposing dynamics.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）规模扩大对其在不同领域推理能力的影响。通过对法律、科学、代码和数学四个领域中超过25,000条推理轨迹的分析，研究发现模型规模的增加会引发领域特异性的推理结构相变。法律推理表现出结晶化特征，表现为表征维度的减少和轨迹对齐度的提升，而科学和数学推理则保持液态，即使参数增加9倍仍保持几何不变性。代码推理形成了策略模式的离散晶格结构。研究引入了神经推理算子，该算子通过探针解码实现了对推理终点的高精度预测，无需经过中间状态。此外，还发现了一个跨领域和规模的普遍振荡特征，表明注意力层和前馈层通过相反的动力学机制驱动推理。这些发现表明，推理成本并非由任务难度决定，而是由推理空间的几何结构决定，为推理加速提供了蓝图。</div>
</details>
</div>
<div class="card">
<div class="title">CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</div>
<div class="meta-line">Authors: Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</div>
<div class="meta-line">First: 2026-01-19T18:59:44+00:00 · Latest: 2026-01-19T18:59:44+00:00</div>
<div class="meta-line">Comments: Code is available: https://github.com/CausalSpatial/CausalSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13304v1">PDF</a> · <a href="https://github.com/CausalSpatial/CausalSpatial">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer &quot;what-if&quot; questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalSpatial：面向对象因果空间推理的基准测试</div>
<div class="mono" style="margin-top:8px">人类可以观察静态场景并瞬间预测接下来会发生什么——移动这个物体是否会导致碰撞？我们称这种能力为因果空间推理。然而，当前的多模态大语言模型（MLLMs）无法做到这一点，因为它们主要局限于静态空间感知，难以回答三维场景中的“如果……会怎样”类问题。我们引入了CausalSpatial，这是一个诊断基准，用于评估模型在碰撞、兼容性、遮挡和轨迹四个任务中是否能够预测物体运动的后果。结果揭示了一个严重差距：人类得分84%，而GPT-5仅达到54%。为什么MLLMs会失败？我们的分析发现其根本缺陷在于模型过度依赖文本链式推理，而脱离了视觉证据，从而产生流畅但缺乏空间基础的幻觉。为了解决这一问题，我们提出了因果物体世界模型（COW），该框架通过生成假设动态的视频来外部化模拟过程。借助明确的因果视觉线索，COW使模型能够基于物理现实而非语言先验进行推理。我们在此公开了数据集和代码：https://github.com/CausalSpatial/CausalSpatial</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CausalSpatial, a benchmark designed to assess object-centric causal spatial reasoning in models. The motivation stems from the observation that humans can intuitively predict the consequences of object movements in static scenes, a capability current multimodal large language models (MLLMs) lack due to their reliance on static spatial perception. The benchmark evaluates models across four tasks: Collision, Compatibility, Occlusion, and Trajectory, revealing a significant performance gap between humans (84%) and GPT-5 (54%). The analysis suggests that MLLMs fail because they depend too heavily on textual reasoning, which often diverges from visual evidence, leading to spatially ungrounded hallucinations. To address this, the authors propose the Causal Object World model (COW), which simulates hypothetical dynamics by generating videos, allowing models to base their reasoning on explicit visual cues of causality.</div>
<div class="mono" style="margin-top:8px">该研究提出了CausalSpatial基准测试，旨在评估模型在三维场景中预测物体运动后果的能力，涵盖碰撞、兼容性、遮挡和轨迹四个任务。当前的多模态大语言模型（MLLMs）在这些任务上表现不佳，因为它们过度依赖文本推理而缺乏对视觉信息的深入理解，导致预测结果脱离物理现实。基准测试结果显示，人类得分高达84%，而GPT-5仅为54%。为解决这一问题，作者提出了因果物体世界模型（COW），通过生成假设动态的视频来使模型的推理建立在物理现实基础上。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-19T18:48:37+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能来寻找共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力，才能有效地作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的超过600个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为在人类团队中，增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道被模糊、时机不当和不准确的信息堵塞；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the limitations of current coding agents in collaborative settings, highlighting the need for social intelligence in team coordination. CooperBench, a benchmark comprising over 600 collaborative coding tasks across four programming languages and twelve libraries, is introduced to evaluate how well agents can work together. The main experimental results show that state-of-the-art coding agents achieve on average 30% lower success rates when collaborating compared to working individually, indicating the &#x27;curse of coordination.&#x27; The analysis identifies three key challenges: ineffective communication, lack of commitment adherence, and misaligned expectations. The study also observes some emergent coordination behaviors, such as role division and negotiation, in large-scale simulations.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨当前编码代理在协作环境中的局限性，尤其是它们在协调方面的能力不足。作者提出了CooperBench，这是一个包含超过600个协作编码任务的基准测试集，涵盖四种编程语言和十二个库，旨在测试代理如何处理需要协调的冲突特性。实验结果表明，最先进的编码代理在协作时表现显著下降，平均成功率比单独完成任务低30%。这揭示了‘协作诅咒’问题，并指出了三个主要障碍：沟通不畅、承诺偏离以及对他人计划的错误预期。通过大规模模拟，研究还观察到了一些有趣的协作行为，如角色分工和协商。</div>
</details>
</div>
<div class="card">
<div class="title">KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</div>
<div class="meta-line">Authors: Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong</div>
<div class="meta-line">First: 2026-01-19T17:20:16+00:00 · Latest: 2026-01-19T17:20:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13240v1">PDF</a> · <a href="https://github.com/jiangxxxue/KOCO-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&amp;A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KOCO-BENCH：大型语言模型能否在软件开发中利用领域知识？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在通用编程方面表现出色，但在领域特定的软件开发中表现不佳，因此需要领域专门化方法，使LLMs能够学习和利用领域知识和数据。然而，现有的领域特定代码基准无法评估领域专门化方法的有效性，这些基准侧重于评估LLMs已有的知识，而非它们如何获取和应用新知识，缺乏用于开发领域专门化方法的显式知识语料库。为此，我们提出了KOCO-BENCH，这是一个全新的基准，用于评估领域专门化方法在现实软件开发中的效果。KOCO-BENCH包含6个新兴领域、11个软件框架和25个项目，配有精心整理的知识语料库，并包含多粒度的评估任务，包括领域代码生成（从函数级到项目级，配有严格的测试套件）和领域知识理解（通过多项选择题问答）。与以往仅提供测试集供直接评估的基准不同，KOCO-BENCH要求从知识语料库中获取并应用多样化的领域知识（如API、规则、约束等）以解决评估任务。我们的评估结果表明，KOCO-BENCH对最先进的LLMs提出了重大挑战。即使应用了领域专门化方法（如SFT、RAG、kNN-LM），改进仍然有限。表现最好的编码代理Claude Code仅达到34.2%，突显了对更有效的领域专门化方法的迫切需求。我们发布了KOCO-BENCH、评估代码和基线模型，以促进进一步研究，详见https://github.com/jiangxxxue/KOCO-bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of enabling large language models (LLMs) to effectively leverage domain knowledge in software development, as existing benchmarks fail to evaluate how LLMs acquire and apply new knowledge rather than just what they know. KOCO-BENCH is introduced as a new benchmark that includes curated knowledge corpora and multi-granularity tasks such as domain code generation and knowledge understanding. Experimental results show that even with domain specialization techniques like SFT, RAG, and kNN-LM, LLMs perform poorly, with the best-performing model achieving only 34.2% accuracy, indicating the need for more advanced methods in this area.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估大型语言模型（LLMs）在软件开发中利用领域知识的能力。现有基准测试无法有效衡量LLMs如何获取和应用新的领域知识，而仅关注其已有的知识水平。为此，作者提出了KOCO-BENCH，一个包含精心整理的知识语料库和多粒度任务（如领域代码生成和知识理解）的新基准测试。实验结果表明，即使应用了领域专业化方法（如SFT、RAG和kNN-LM），LLMs的表现仍不理想，最佳模型仅达到34.2%的准确率，凸显了开发更有效领域适应方法的迫切需求。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-01-19T13:13:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v1">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即在二维感知之外解释几何、视角和空间关系的能力。尽管最近的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入了Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用3D重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上平均提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使模型能够选择信息性视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是实现多模态代理中更灵活且类似人类的三维推理的一种可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of Think3D is to enhance spatial reasoning capabilities in vision large models (VLMs) by enabling them to interact with 3D space. The framework integrates 3D reconstruction models to generate point clouds and camera poses from images or videos, allowing VLM agents to manipulate space through camera-based operations and switch between ego and global views. This approach transforms spatial reasoning into an interactive 3D chain-of-thought process, significantly improving performance on tasks like BLINK Multi-view and MindCube without additional training, achieving average gains of +7.8% and +4.7%, respectively. Furthermore, reinforcement learning policies enhance smaller models&#x27; ability to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">Think3D的研究动机源于当前视觉大模型（VLMs）在处理真实三维空间推理时的局限性，尽管它们在二维视觉理解方面表现优异。该框架通过利用三维重建模型，从图像或视频中生成点云和相机姿态，使VLM代理能够通过基于相机的操作和视角切换来主动操控三维空间，从而将空间推理转化为交互式的三维思维链过程。在无需额外训练的情况下，Think3D显著提升了GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上提升4.7%。此外，通过强化学习策略，较小的模型在空间探索方面也获得了显著提升，工具使用带来的性能增益从0.7%提升至6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">Knot So Simple: A Minimalistic Environment for Spatial Reasoning</div>
<div class="meta-line">Authors: Zizhao Chen, Yoav Artzi</div>
<div class="meta-line">First: 2025-05-23T15:34:08+00:00 · Latest: 2026-01-18T19:17:38+00:00</div>
<div class="meta-line">Comments: Fix camera ready footer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18028v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18028v3">PDF</a> · <a href="https://github.com/lil-lab/knotgym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Knot So Simple：一个用于空间推理的极简环境</div>
<div class="mono" style="margin-top:8px">我们提出了KnotGym，一个用于复杂空间推理和操作的交互式环境。KnotGym包含一系列以目标为导向的绳子操作任务，具有不同复杂度级别，所有任务均基于纯图像观察进行操作。任务的复杂度沿一个清晰且可量化的轴线定义，基于绳结的交叉点数量，从而形成自然的泛化测试。KnotGym具有简单的观察空间，支持可扩展开发，同时突出了将敏锐感知、空间推理和基于环境的操作整合在一起的核心挑战。我们评估了多种方法，包括基于模型的强化学习、模型预测控制和链式思维推理，并展示了KnotGym所呈现的挑战。KnotGym可在https://github.com/lil-lab/knotgym获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces KnotGym, a minimalistic environment designed to evaluate spatial reasoning and manipulation through complex knot-tying tasks. The environment is built around a clear complexity metric based on the number of knot crossings, enabling a structured progression of task difficulty. By using only image-based observations, KnotGym emphasizes the integration of perception, reasoning, and action in robotic manipulation. The authors assess various approaches, including model-based reinforcement learning, model-predictive control, and chain-of-thought reasoning, demonstrating the challenges these methods face in handling intricate spatial tasks.</div>
<div class="mono" style="margin-top:8px">本研究提出了KnotGym，一个用于评估空间推理与操作的极简环境，通过复杂的绳结任务进行测试。该环境基于绳结交叉点数量构建了一个清晰的复杂度指标，实现了任务难度的有序递进。所有任务仅依赖图像观测，突显了在机器人任务中感知、推理与操作整合的核心挑战。作者评估了多种方法，包括基于模型的强化学习、模型预测控制和链式思维推理，展示了这些方法在解决任务时所面临的困难。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</div>
<div class="meta-line">Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</div>
<div class="meta-line">First: 2025-07-07T18:17:06+00:00 · Latest: 2026-01-18T13:38:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05386v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.05386v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model&#x27;s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT&#x27;s gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化微调自然缓解持续后训练中的遗忘</div>
<div class="mono" style="margin-top:8px">持续后训练（CPT）是一种流行且有效的技术，用于将基础模型（如多模态大语言模型）适应于特定且不断演化的下游任务。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，但CPT中学习范式的根本作用仍被广泛忽视。本文对两种核心的后训练范式——监督微调（SFT）和强化微调（RFT）进行了比较分析，探讨它们在持续后训练过程中对知识保留的影响。我们的实验基于包含七个多样化多模态任务的基准数据集，使用Qwen2.5-VL-7B-Instruct作为持续后训练的基础模型。研究得出两个重要发现：（1）在持续学习下游任务时，SFT会导致先前学习任务的灾难性遗忘，而RFT则能自然保留先前知识，其性能与多任务训练相当。（2）RFT能够保护并增强模型在标准基准（如MMMU和MMLU-Pro）上的通用知识，而SFT则严重削弱模型的通用能力。进一步分析表明，这种稳定性并非主要源于显式的机制，如KL惩罚或思维链推理，而是源于RFT中隐含的正则化机制。我们的理论分析表明，RFT的梯度更新自然受到奖励方差的缩放，作为一种数据依赖的正则化器，能够保护先前获得的知识。最后，我们提出了一种基于rollout的实例过滤算法，以提升RFT的稳定性和效率。我们的全面研究证明了RFT作为持续后训练的稳健范式的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the issue of catastrophic forgetting in continual post-training (CPT) of foundation models, particularly focusing on the effectiveness of different learning paradigms. It compares supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), revealing that SFT leads to significant performance degradation on previously learned tasks, while RFT maintains prior knowledge and achieves performance comparable to multi-task training. The study demonstrates that RFT&#x27;s stability arises from an implicit regularization mechanism tied to reward variance, rather than explicit techniques like KL penalty. Additionally, the authors propose a rollout-based instance filtering algorithm to improve RFT&#x27;s efficiency and robustness in continual learning.</div>
<div class="mono" style="margin-top:8px">本文探讨了基础模型在持续微调过程中出现灾难性遗忘的问题，重点比较了监督微调（SFT）和强化微调（RFT）两种方法的效果。研究发现，SFT在持续学习新任务时会导致先前任务知识的严重丢失，而RFT则能自然保留已有知识，并在性能上接近多任务训练。在七个不同的多模态任务上使用Qwen2.5-VL-7B-Instruct进行实验，结果表明RFT不仅防止遗忘，还能提升模型在标准基准上的通用知识。作者认为这种稳定性源于RFT中与奖励方差相关的隐式正则化机制，而非显式的KL惩罚等方法。最后，他们提出了一种基于rollout的实例过滤算法，以提高RFT在持续学习中的效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jihie Kim, Jean Oh</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-18T11:03:44+00:00</div>
<div class="meta-line">Comments: Corrected author order in metadata; manuscript unchanged</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未找到安全（404）：基于大语言模型的机器人决策中的隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键环境中，人工智能系统的一个错误可能导致生命损失。随着大语言模型（LLMs）在机器人决策中的应用日益广泛，风险的物理维度也在扩大；一个错误的指令可能直接危及人类安全。本文旨在系统评估LLM在即使微小错误也可能导致灾难的场景中的表现。通过一个火灾疏散场景的定性评估，我们识别了基于LLM的决策中的关键失败案例。基于这些案例，我们设计了七个任务用于定量评估，分为：完整信息任务、不完整信息任务和安全导向空间推理（SOSR）任务。完整信息任务使用ASCII地图以减少解释歧义，并将空间推理与视觉处理分离。不完整信息任务要求模型推断缺失的上下文，测试其空间连续性与幻觉之间的区别。SOSR任务使用自然语言评估在生命威胁情境下的安全决策能力。我们对各种LLM和视觉-语言模型（VLMs）在这些任务上的表现进行了基准测试。除了整体表现外，我们还分析了1%失败率的潜在影响，强调了“罕见”错误如何演变为灾难性后果。结果揭示了严重的漏洞：一些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，模型指示机器人向危险区域移动而非紧急出口。我们的研究得出令人深思的结论：当前的大语言模型尚未准备好直接部署在安全关键系统中。在机器人领域，99%的准确率是危险误导的，因为它意味着每100次执行中可能有一次导致灾难性伤害。我们证明了即使是最先进的模型也无法保证安全，完全依赖它们会带来不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the safety risks associated with using Large Language Models (LLMs) in robotics decision-making, particularly in critical scenarios where minor errors can lead to severe consequences. The authors designed seven tasks to evaluate LLMs and Vision-Language Models (VLMs) under different information conditions, including complete, incomplete, and safety-oriented spatial reasoning. Their experiments revealed significant vulnerabilities, with some models failing completely in ASCII navigation tasks and others directing robots toward hazardous areas in simulated fire drills. These results underscore the dangers of relying on high accuracy rates in safety-critical applications, as even a 1% failure rate can translate into catastrophic outcomes.</div>
<div class="mono" style="margin-top:8px">本文探讨了在安全关键型机器人决策中使用大型语言模型（LLMs）所伴随的风险，指出即使是微小的错误也可能导致严重后果。作者设计了七个任务来评估LLMs和视觉语言模型（VLMs）在不同情境下的表现，包括完整信息、不完整信息以及安全导向的空间推理任务。实验结果显示，一些模型在ASCII导航任务中完全失败，而在模拟火灾演练中，部分模型错误地指示机器人向危险区域移动而非安全出口，凸显了在实际安全应用中过度依赖LLMs所带来的潜在危害。</div>
</details>
</div>
<div class="card">
<div class="title">CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks</div>
<div class="meta-line">Authors: Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, Xiangyu Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-07-03T01:35:58+00:00 · Latest: 2026-01-17T19:21:17+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Datasets &amp; Benchmarks Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05269v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05269v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models&#x27; ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs&#x27; code reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoRe: 通过静态分析任务对LLM代码推理能力进行基准测试</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已在软件工程的多个领域得到广泛应用，如代码生成、程序修复和漏洞检测。这些应用需要理解超越表面代码模式的深层次内容：值传播、控制流以及程序元素之间的相互依赖。然而，现有的基准测试主要评估端到端结果，例如代码是否被正确修复或生成，而忽视了模型在程序语义推理方面的能力。本文提出了CORE，一个高质量、经过人工验证的基准测试，旨在评估LLMs在基本静态分析任务上的表现。CORE包含12,553个任务实例，涵盖C/C++、Java和Python编写的程序中的数据依赖、控制依赖和信息流。为确保语义多样性和推理复杂性，我们提出了一种语义感知的多样化采样策略，根据结构覆盖和依赖深度选择目标和任务实例。我们评估了10种主流LLMs，并表明，尽管它们在识别依赖关系方面表现良好，但在需要更深层次语义理解和多步骤推理的任务上仍存在困难。我们进一步进行了定性分析，揭示了诸如复杂控制结构和反向依赖模式等关键挑战，为提升LLMs的代码推理能力提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces CoRe, a benchmark designed to assess the code reasoning capabilities of large language models (LLMs) through static analysis tasks. The motivation stems from the need to evaluate LLMs&#x27; understanding of deeper program semantics, such as value propagation and control flow, which are essential for applications like program repair and vulnerability detection. The benchmark includes 12,553 task instances across C/C++, Java, and Python, and employs a semantics-aware sampling strategy to ensure diversity and complexity. Experimental results show that while LLMs perform well in identifying basic dependencies, they struggle with tasks requiring multi-step semantic reasoning, highlighting challenges such as complex control structures and backward dependencies.</div>
<div class="mono" style="margin-top:8px">本文提出了CoRe基准，旨在通过静态分析任务评估大语言模型（LLMs）的代码推理能力。研究动机源于现有基准主要关注代码生成或修复等端到端结果，而未能充分考察模型对程序语义的理解。CoRe包含12,553个任务实例，涵盖C/C++、Java和Python程序中的数据依赖、控制依赖和信息流。为确保语义多样性和推理复杂性，采用了一种基于语义的多样化采样策略。实验结果表明，尽管LLMs在识别基本依赖方面表现良好，但在需要多步语义推理的任务上仍存在困难，揭示了如复杂控制结构和逆向依赖等关键挑战。</div>
</details>
</div>
<div class="card">
<div class="title">A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation</div>
<div class="meta-line">Authors: Yifei Sun, Yongan Li, A. K. Qin, Sicheng Hou, Tamas Pflanzner</div>
<div class="meta-line">First: 2026-01-16T21:36:04+00:00 · Latest: 2026-01-16T21:36:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11792v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种具有细粒度难度指导的自进化多角色协作框架用于创新数学问题生成</div>
<div class="mono" style="margin-top:8px">数学问题生成（MPG）是智能教育领域的重要研究方向。近年来，大语言模型（LLMs）的快速发展为问题生成任务带来了新的技术方法。尽管现有LLMs可以实现较高的正确率，但通常缺乏创新性且判别能力较差。本文提出创新数学问题生成（IMPG）任务。为了解决IMPG任务，本文提出了一种具有细粒度难度指导的自进化多角色协作框架。首先，构建了一个包含采样器、生成器、评估器、状态机和记忆模块的多角色协作机制，通过自评估和外部反馈的迭代优化确保生成问题的正确性。其次，引入了改进的难度模型以量化难度并提供细粒度指导。我们采用数据驱动的关联引导路径采样（DAPS）算法来增强采样编码的语义合理性。第三，我们构建了HSM3K-CN数据集，包含高质量的高中数学问题。采用多阶段训练流程，包括持续预训练（CPT）、监督微调（SFT）和组相对策略优化（GRPO），以提升基础模型的生成和评估能力。最后，通过知识蒸馏将专家模型的评估能力转移到学徒模型中，实现系统自进化。实验表明，与基线模型相比，我们提出的方法在保持高正确率的同时显著提升了生成问题的创新性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing large language models in generating innovative and discriminative mathematical problems. The authors propose a self-evolving, multi-role collaborative framework that integrates a sampler, generator, evaluator, state machine, and memory to iteratively optimize problem generation. They also introduce an improved difficulty model combined with the data-driven association-guided path sampling algorithm to enhance semantic coherence. The HSM3K-CN dataset is constructed to support training, and a multi-stage pipeline involving continual pre-training, supervised fine-tuning, and group relative policy optimization is used. The method achieves significant improvements in problem innovation while maintaining high correctness rates compared to baseline models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有大语言模型在生成创新性数学问题时的不足，特别是在创新性和区分度方面。作者提出了一种自进化、多角色协作框架，包含采样器、生成器、评估器、状态机和记忆模块，通过自评与外部反馈实现问题生成的迭代优化。引入了改进的难度模型，并结合数据驱动的关联引导路径采样（DAPS）算法提升采样编码的语义合理性。构建了HSM3K-CN数据集，并采用包含持续预训练（CPT）、监督微调（SFT）和群体相对策略优化（GRPO）的多阶段训练流程。实验表明，该方法在保持高正确率的同时显著提升了生成问题的创新性。</div>
</details>
</div>
<div class="card">
<div class="title">SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models</div>
<div class="meta-line">Authors: Turhan Can Kargin, Wojciech Jasiński, Adam Pardyl, Bartosz Zieliński, Marcin Przewięźlikowski</div>
<div class="meta-line">First: 2026-01-16T19:21:02+00:00 · Latest: 2026-01-16T19:21:02+00:00</div>
<div class="meta-line">Comments: Project page is available at https://sparrta.gmum.net/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpaRRTa：用于评估视觉基础模型空间智能的合成基准</div>
<div class="mono" style="margin-top:8px">视觉基础模型（VFMs），如DINO和CLIP，在图像语义理解方面表现出色，但其空间推理能力有限，这限制了它们在具身系统中的应用。因此，近期研究将一些3D任务（如深度估计）纳入VFM的训练中。然而，VFMs在其他空间任务中的表现仍不一致，引发了一个问题：这些模型是否真正具备空间感知能力，还是仅仅过度拟合了特定的3D目标。为了解决这一问题，我们引入了空间关系识别任务（SpaRRTa）基准，用于评估VFMs识别图像中物体相对位置的能力。与传统的关注精确度量预测（如表面法线估计）的3D目标不同，SpaRRTa探索了支撑更高级类人空间理解的基本能力。SpaRRTa生成具有多样场景和完全可控物体布局的任意数量的逼真图像，并提供可自由访问的空间标注。通过评估一系列最先进的VFMs，我们揭示了它们在空间推理能力上的显著差异。通过我们的分析，我们提供了关于现代VFMs中支持或阻碍空间感知机制的见解。我们希望SpaRRTa能成为指导未来空间感知视觉模型发展的有用工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SpaRRTa, a synthetic benchmark designed to evaluate spatial intelligence in Visual Foundation Models (VFMs). The motivation stems from the observation that while VFMs like DINO and CLIP are strong in semantic understanding, they lack robust spatial reasoning capabilities, limiting their use in embodied systems. SpaRRTa focuses on the ability to recognize relative object positions in images, differing from traditional 3D tasks that emphasize metric predictions. The benchmark generates photorealistic images with controllable object arrangements and accessible spatial annotations, revealing significant variations in spatial reasoning performance across state-of-the-art VFMs.</div>
<div class="mono" style="margin-top:8px">本文提出SpaRRTa，这是一个用于评估视觉基础模型（VFMs）空间智能的合成基准。研究动机源于观察到尽管像DINO和CLIP这样的VFMs在语义理解方面表现优异，但它们的空间推理能力有限，限制了其在具身系统中的应用。SpaRRTa通过评估VFMs识别图像中物体相对位置的能力，区别于传统的侧重于精确度量预测的3D任务。该基准生成具有多样场景和可控物体布局的逼真图像，并附带空间标注。实验结果表明，不同VFMs在空间推理能力上存在显著差异，揭示了提升视觉模型空间理解能力的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</div>
<div class="meta-line">Authors: Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo Pérez-Pellitero, Youngkyoon Jang</div>
<div class="meta-line">First: 2026-01-16T17:02:46+00:00 · Latest: 2026-01-16T17:02:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11442v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Map2Thought：通过度量认知地图实现显式的3D空间推理</div>
<div class="mono" style="margin-top:8px">我们提出了Map2Thought框架，使3D视觉语言模型能够进行显式且可解释的空间推理。该框架基于两个关键组件：度量认知地图（Metric-CogMap）和认知链式推理（Cog-CoT）。Metric-CogMap通过将关系推理的离散网格与精确几何理解的连续度量表示相结合，提供统一的空间表示。在Metric-CogMap的基础上，Cog-CoT通过确定性操作进行显式的几何推理，包括向量运算、边界框距离和遮挡感知的外观顺序提示，从而生成基于3D结构的可解释推理轨迹。实验结果表明，Map2Thought实现了可解释的3D理解，在仅使用一半监督数据的情况下达到了59.9%的准确率，接近使用完整数据集训练的60.9%基线。在VSI-Bench数据集上，它在10%、25%和50%的训练子集下分别优于最先进的方法5.3%、4.8%和4.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the interpretability and efficiency of spatial reasoning in 3D Vision-Language Models (VLMs). The proposed framework, Map2Thought, integrates two components: Metric Cognitive Map (Metric-CogMap) for unified spatial representation and Cognitive Chain-of-Thought (Cog-CoT) for explicit geometric reasoning. Metric-CogMap combines a discrete grid for relational reasoning with a continuous metric-scale representation for precise geometry, while Cog-CoT uses deterministic operations like vector calculations, bounding-box distances, and occlusion-aware cues to generate interpretable inference traces. Experimental results demonstrate that Map2Thought achieves 59.9% accuracy with half the supervision, closely matching the 60.9% baseline performance, and outperforms existing methods by 5.3%, 4.8%, and 4.0% on 10%, 25%, and 50% training subsets respectively on the VSI-Bench.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D视觉语言模型（VLMs）中空间推理的可解释性和效率。Map2Thought提出了一种结合度量认知地图（Metric-CogMap）和认知链式推理（Cog-CoT）的框架，以实现显式且结构化的空间理解。Metric-CogMap通过将离散关系网格与连续度量表示相结合，提供统一的空间表征，而Cog-CoT则利用确定性几何操作生成可解释的推理轨迹。实验结果表明，该框架在仅使用一半监督数据的情况下达到59.9%的准确率，接近使用完整数据集训练的60.9%基线性能，并在VSI-Bench数据集的10%、25%和50%训练子集上分别优于现有方法5.3%、4.8%和4.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation</div>
<div class="meta-line">Authors: Joschka Kersting, Michael Rummel, Gesa Benndorf</div>
<div class="meta-line">First: 2025-11-12T08:56:11+00:00 · Latest: 2026-01-16T14:53:55+00:00</div>
<div class="meta-line">Comments: ICIT2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09122v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09122v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向供应商的工业代理：基于RAG增强的LLM用于安全的本地PLC代码生成</div>
<div class="mono" style="margin-top:8px">可编程逻辑控制器使用专有代码方言进行操作，这使得训练代码助手变得具有挑战性。当前的LLM是在大规模代码数据集上训练的，能够直接生成符合IEC 61131-3标准的代码，但它们并不了解特定的功能块或相关的项目代码。此外，像三菱电机这样的公司及其客户并不信任云服务提供商。因此，拥有自己的代码代理是解决这一问题的首选方案。在本研究中，我们提出了一种适用于工业领域的低数据域代码助手解决方案。我们展示了如何在不微调大型模型的情况下，通过微调小型本地模型用于边缘设备，实现高质量的代码生成。我们的工具允许多个AI模型相互竞争，使用推理自动纠正错误，并通过在聊天界面中直接编译代码来验证代码的有效性。我们通过详尽的评估支持我们的方法，包括代码编译统计数据和用户评分。我们发现，通过使用广泛的提示工程和定向检索，基于检索增强生成（RAG）的代码助手可以在低数据域中有效运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of training coding assistants for industrial applications where Programmable Logic Controllers (PLCs) use proprietary code dialects. The authors propose a vendor-aware coding assistant that leverages Retrieval-Augmented Generation (RAG) to enable secure, on-premise code generation without requiring extensive fine-tuning of large models. By using small local models and extensive prompt engineering, the tool allows multiple AI models to compete, reason, and generate valid IEC 61131-3 compatible PLC code. The approach is validated through an evaluation that includes code compilation statistics and user feedback, demonstrating its effectiveness in low-data domains.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决工业可编程逻辑控制器（PLC）使用专有代码方言所带来的编码助手训练难题。现有大型语言模型（LLMs）虽能生成符合IEC 61131-3标准的代码，但缺乏特定领域的知识，且企业如三菱电机及其客户不信任云服务提供商。为此，研究人员开发了一种基于检索增强生成（RAG）技术的厂商感知编码助手，无需对大型模型进行微调，而是通过本地小模型和详尽的提示工程实现高质量代码生成。该工具支持多个AI模型协作，具备推理、自动纠错和代码验证功能，可在聊天界面直接编译代码以检查其有效性。实验结果表明，RAG增强的编码助手在低数据场景下能有效生成符合标准的PLC代码，相关评估包括编译统计数据和用户评分。</div>
</details>
</div>
<div class="card">
<div class="title">OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding</div>
<div class="meta-line">Authors: Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui</div>
<div class="meta-line">First: 2026-01-15T12:36:08+00:00 · Latest: 2026-01-16T02:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10343v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OctoBench：基于仓库的智能编码中对支架感知指令遵循的基准测试</div>
<div class="mono" style="margin-top:8px">现代编码支架使LLMs成为有能力的软件代理，但它们遵循支架指定指令的能力仍被低估，尤其是在约束条件异构且持续跨交互的情况下。为填补这一空白，我们引入了OctoBench，用于评估基于仓库的智能编码中对支架感知指令遵循的能力。OctoBench包含34个环境和217个任务实例，涵盖三种支架类型，并配有7,098个目标检查清单项。为了将任务解决与规则遵循分离，我们提供了一个自动化的观察与评分工具包，可捕捉完整交互轨迹并进行细粒度检查。在八个代表性模型上的实验揭示了任务解决与支架感知合规之间存在系统性差距，强调了需要专门针对异构指令遵循进行训练和评估的重要性。我们发布该基准测试以支持可重复的基准测试，并加速开发更加支架感知的编码代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the ability of large language models (LLMs) to follow scaffold-specified instructions in repository-grounded agentic coding, particularly when dealing with heterogeneous constraints across interactions. The authors introduce OctoBench, a benchmark that includes 34 environments and 217 tasks across three scaffold types, along with 7,098 objective checklist items. They also develop an automated observation-and-scoring toolkit to capture full interaction trajectories and perform fine-grained compliance checks. Experimental results on eight representative models show a systematic gap between task-solving performance and scaffold-aware instruction following, highlighting the need for more targeted training and evaluation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型在基于仓库的代理编码中遵循支架指定指令的能力，特别是在异构约束条件下。作者提出了OctoBench基准测试，包含34个环境和217个任务，涵盖三种支架类型，并配有7098个目标检查清单项。他们还提供了一个自动化观察与评分工具，用于追踪任务执行过程并进行细粒度的合规性检查。实验结果表明，八个代表性模型在任务解决能力和支架意识指令遵循之间存在系统性差距，强调了需要更针对性的训练和评估方法。</div>
</details>
</div>
<div class="card">
<div class="title">MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models</div>
<div class="meta-line">Authors: Muhammad Imran, Chi Lee, Yugyung Lee</div>
<div class="meta-line">First: 2026-01-16T01:18:02+00:00 · Latest: 2026-01-16T01:18:02+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11666v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX&#x27;s potential to enhance trust and transparency in radiological AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MATEX：面向医学视觉-语言模型的多尺度注意力与文本引导可解释性</div>
<div class="mono" style="margin-top:8px">我们引入了MATEX（多尺度注意力与文本引导可解释性），这是一种新颖的框架，通过结合解剖学信息的空间推理来提升医学视觉-语言模型的可解释性。MATEX通过多层注意力展开、文本引导的空间先验以及层间一致性分析，生成精确、稳定且具有临床意义的梯度归因图。通过解决先前方法的关键局限性，如空间不精确、缺乏解剖学基础以及注意力粒度有限等问题，MATEX能够提供更忠实且可解释的模型解释。在MS-CXR数据集上的评估表明，MATEX在空间精度和与专家标注结果的一致性方面均优于最先进的M2IB方法。这些结果突显了MATEX在放射学AI应用中增强信任和透明度的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MATEX is to improve the interpretability of medical vision-language models by addressing their limitations in spatial precision, anatomical grounding, and attention granularity. The framework integrates multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to generate accurate and clinically relevant gradient attribution maps. Experimental results on the MS-CXR dataset demonstrate that MATEX surpasses the state-of-the-art M2IB method in spatial precision and alignment with expert annotations, thereby enhancing the trustworthiness and transparency of radiological AI systems.</div>
<div class="mono" style="margin-top:8px">MATEX的动机是提升医学视觉语言模型的可解释性，解决空间精度不足和缺乏解剖学依据等问题。该框架结合多尺度注意力机制、文本引导的空间先验和层一致性分析，生成精确且具有临床意义的梯度归因图。在MS-CXR数据集上的实验结果表明，MATEX在空间精度和与专家标注的一致性方面均优于当前最先进的M2IB方法，从而增强了放射学AI系统的可信度和透明度。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents</div>
<div class="meta-line">Authors: Himanshu Thakur, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri</div>
<div class="meta-line">First: 2026-01-15T19:33:42+00:00 · Latest: 2026-01-15T19:33:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10820v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team&#x27;s unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team&#x27;s environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过LLM代理受限拓扑规划实现可靠的机器学习特征工程</div>
<div class="mono" style="margin-top:8px">最近代码生成模型的进展为自动化特征工程带来了前所未有的机会，但其在现实世界机器学习团队中的采用仍受到关键挑战的限制：(i) 缺乏能够捕捉生产级特征工程迭代和复杂编码过程的数据集；(ii) 常用编码代理（如CoPilot和Devin）与团队独特工具、代码库、工作流程和实践的集成和个性化有限；(iii) 由于反馈时机不当或不足，导致人机协作效果不佳。我们提出了一种由规划器引导的受限拓扑多代理框架，以多步骤方式为仓库生成代码。该规划器利用团队环境（以图表示）来协调可用代理的调用，生成上下文感知的提示，并利用下游失败来回溯修正上游产物。它可以在关键步骤请求人工干预，确保生成的代码可靠、可维护，并符合团队期望。在我们新构建的内部数据集上，我们的方法在评估指标上分别比手动构建和未规划的流程提升了38%和150%。在实践中，当构建服务于超过1.2亿用户的推荐模型的特征时，我们的方法通过将特征工程周期从三周缩短至一天，实现了实际影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of reliable machine learning feature engineering by introducing a planner-guided, constrained-topology multi-agent framework that automates the code generation process for repositories. The framework uses an LLM-powered planner to integrate with a team&#x27;s environment, represented as a graph, to coordinate agent calls, generate context-aware prompts, and correct upstream artifacts based on downstream failures. It also allows for human intervention at critical steps to ensure code reliability and alignment with team practices. Experimental results on a novel in-house dataset show that the approach improves the evaluation metric by 38% compared to manual methods and 150% compared to unplanned workflows. In real-world applications, it significantly reduces feature engineering cycles from three weeks to one day when building features for recommendation models serving over 120 million users.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将代码生成模型集成到实际机器学习团队中所面临的挑战，包括缺乏捕捉迭代特征工程过程的数据集、代码代理与团队工具和流程的有限集成，以及人机协作效果不佳的问题。提出了一种由规划器引导、受限拓扑结构的多代理框架，利用LLM驱动的规划器根据团队环境图协调代理调用，生成上下文感知的提示，并通过下游失败反向修正上游产物。该框架在关键步骤允许人工干预，以确保生成代码的可靠性与可维护性。在自建数据集上的实验结果显示，与手动构建的流程相比提升了38%，与未规划的流程相比提升了150%，在实际应用中，为服务超过1.2亿用户的推荐模型缩短了特征工程周期，从三周减少到一天。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</div>
<div class="meta-line">Authors: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-10T12:54:04+00:00 · Latest: 2026-01-15T13:22:05+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, accepted to AAAI 2026. Project page:https://github.com/CASIA-IVA-Lab/UrbanNav</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09607v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09607v2">PDF</a> · <a href="https://github.com/CASIA-IVA-Lab/UrbanNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanNav: 从大规模网络人类轨迹中学习语言引导的城市导航</div>
<div class="mono" style="margin-top:8px">使用自然语言指令在复杂的城市环境中导航对具身智能体提出了重大挑战，包括嘈杂的语言指令、模糊的空间参照、多样化的地标以及动态的街道场景。当前的视觉导航方法通常局限于模拟或非街道环境，并且往往依赖于精确的目标格式，如特定坐标或图像。这限制了它们在自主智能体（如最后一公里配送机器人）在陌生城市中导航时的有效性。为了解决这些限制，我们引入了UrbanNav，一个可扩展的框架，用于训练具身智能体在多样化城市环境中遵循自由形式的语言指令。通过利用大规模的城市步行视频，我们开发了一个可扩展的标注流程，将人类导航轨迹与基于真实地标的语言指令对齐。UrbanNav包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，涵盖了广泛的城市场景。我们的模型学习了鲁棒的导航策略，以应对复杂的城市场景，展示了优越的空间推理能力、对嘈杂指令的鲁棒性以及对未见过的城市环境的泛化能力。实验结果表明，UrbanNav显著优于现有方法，突显了大规模网络视频数据在实现具身智能体语言引导的真实城市导航方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UrbanNav addresses the challenges of navigating complex urban environments using natural language instructions for embodied agents. It introduces a scalable framework that trains agents to follow free-form language guidance by leveraging web-scale city walking videos and developing an annotation pipeline that aligns human trajectories with real-world landmarks. The framework includes over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, enabling robust spatial reasoning and generalization. Experimental results demonstrate that UrbanNav significantly outperforms existing methods in handling noisy instructions and diverse urban settings, showing its effectiveness for real-world applications such as autonomous delivery robots.</div>
<div class="mono" style="margin-top:8px">UrbanNav 针对使用自然语言指令在复杂城市环境中导航的挑战，提出了一种可扩展的框架，训练具身智能体遵循自由形式的语言指导。该方法利用大规模的城市步行视频，并开发了一个标注流程，将人类导航轨迹与现实世界地标和语言指令对齐。框架包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，使模型能够学习鲁棒的导航策略。实验结果表明，UrbanNav 在空间推理、处理噪声指令以及泛化到未见过的城市场景方面显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants</div>
<div class="meta-line">Authors: Tsvi Cherny-Shahar, Amiram Yehudai</div>
<div class="meta-line">First: 2026-01-15T06:42:45+00:00 · Latest: 2026-01-15T06:42:45+00:00</div>
<div class="meta-line">Comments: 35 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10112v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.
  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\% and reduces completion time by 53.9\%, yielding a mean 57.8\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\% in accuracy and 69.5\% in efficiency on average, compared to 6.6\% and 46.1\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>仓库智能图：面向大语言模型代码助手的确定性架构图</div>
<div class="mono" style="margin-top:8px">仓库感知的编码代理通常难以恢复构建和测试结构，尤其是在多语言项目中，跨语言依赖关系被编码在异构构建系统和工具中。我们引入了仓库智能图（RIG），这是一种确定性的、有证据支持的架构图，它表示可构建的组件、聚合器、运行器、测试、外部包和包管理器，并通过显式的依赖和覆盖边追溯到具体的构建和测试定义。我们还提出了SPADE，一种确定性的提取器，它从构建和测试工件（目前包括基于CMake文件API和CTest元数据的自动CMake插件）中构建RIG，并将其作为LLM友好的JSON视图暴露出来，代理可以将其视为仓库结构的权威描述。
我们评估了三个商业代理（Claude Code、Cursor、Codex）在八个涵盖从低到高构建导向复杂度的仓库上的表现，包括现实中的MetaFFI项目。每个代理在有和没有RIG上下文的情况下，针对每个仓库回答三十个结构化问题，我们测量了准确率、实际完成时间和效率（每正确答案所需秒数）。在所有仓库和代理中，提供RIG使平均准确率提高了12.2%，完成时间减少了53.9%，从而平均每正确答案所需时间减少了57.8%。在多语言仓库中，收益更大，平均准确率提高了17.7%，效率提高了69.5%，相比之下，单语言仓库的提升分别为6.6%和46.1%。定性分析表明，RIG将失败从结构误解转移到了对正确结构的推理错误上，而罕见的回归则突显了基于图的推理质量仍然是关键因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge faced by repository-aware coding agents in understanding build and test structures, particularly in multilingual projects with complex, heterogeneous build systems. The authors propose the Repository Intelligence Graph (RIG), a deterministic and evidence-based representation of a project&#x27;s components, dependencies, and tests. They also introduce SPADE, an extractor that builds RIG from build and test artifacts, such as CMake and CTest data, and provides it in a format suitable for large language models. Evaluation on three commercial agents across eight repositories shows that using RIG improves mean accuracy by 12.2% and reduces completion time by 53.9%, with more significant gains in multilingual projects.</div>
<div class="mono" style="margin-top:8px">该研究针对代码助手在多语言项目中理解构建和测试结构时面临的挑战，提出了一种确定性的架构表示方法——Repository Intelligence Graph（RIG）。RIG通过显式的依赖和覆盖边，将可构建组件、聚合器、测试用例、外部包及包管理器等元素连接起来，为大型语言模型提供结构化的输入。方法引入了SPADE，一种从构建和测试工件（如CMake和CTest数据）中提取RIG的工具，并以LLM友好的JSON格式呈现。实验结果显示，在八个不同复杂度的仓库中，使用RIG使平均准确率提高了12.2%，完成时间减少了53.9%，效率提升了57.8%。在多语言仓库中，准确率和效率的提升更为显著，分别达到17.7%和69.5%。</div>
</details>
</div>
<div class="card">
<div class="title">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</div>
<div class="meta-line">Authors: Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</div>
<div class="meta-line">First: 2026-01-12T16:26:42+00:00 · Latest: 2026-01-15T03:58:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07695v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes &quot;near-miss&quot; samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑操作符：平滑可验证奖励激活视觉-语言模型的空间推理能力</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在实现精确数值预测以进行3D场景理解时面临关键瓶颈。传统的强化学习（RL）方法主要基于相对排名，常受严重奖励稀疏性和梯度不稳定性困扰，无法有效利用3D物理约束提供的可验证信号。值得注意的是，在标准GRPO框架中，相对归一化会导致&quot;近失&quot;样本（具有小但非零误差的样本）出现优势坍塌。这导致了严重的数据利用瓶颈，即有价值的边界样本在优化过程中被丢弃。为了解决这一问题，我们引入了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA采用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励空间。同时，AP-GRPO整合了绝对标量梯度，以缓解传统相对排名机制中固有的数值信息损失。通过这种方法，我们构建了Numerical3D-50k数据集，包含50,000个可验证的3D子任务。实证结果表明，AP-GRPO在保持更高数据效率的同时，实现了与大规模监督方法相当的性能，有效激活了VLMs中的潜在3D推理能力，而无需对模型架构进行修改。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge that Vision-Language Models (VLMs) face in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning methods, such as GRPO, suffer from reward sparsity and gradient instability due to relative ranking and normalization, which discard valuable boundary samples. To resolve this, the authors propose the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA uses a dynamically parameterized Sigmoid function to generate dense, continuous rewards, while AP-GRPO incorporates absolute scalar gradients to preserve numerical information. The proposed approach is validated using Numerical3D-50k, a dataset of 50,000 verifiable 3D subtasks, and the results show that AP-GRPO matches the performance of large-scale supervised methods with better data efficiency, effectively enhancing the spatial reasoning capabilities of VLMs without altering their architecture.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉-语言模型（VLMs）在3D场景理解中进行精确数值预测所面临的瓶颈问题。传统强化学习方法由于奖励稀疏性和梯度不稳定性，难以有效利用3D物理约束提供的可验证信号。为此，作者提出了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA通过动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励，而AP-GRPO则引入绝对标量梯度以减少传统相对排序机制中的信息损失。实验使用包含50,000个可验证3D子任务的Numerical3D-50k数据集验证了该方法，结果显示AP-GRPO在数据效率上优于大规模监督方法，同时保持了与之相当的性能，并在不修改模型结构的前提下激活了VLMs的潜在3D推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器，其训练方法往往将图像扁平化为1D的图像块序列，丢弃了空间推理所需的2D结构。我们认为这种缺乏空间感知是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the spatial reasoning limitations in vision-language models (VLMs), which are typically trained using contrastive language-image pretraining (CLIP) style image encoders that flatten images into 1D sequences, thereby losing essential 2D spatial structure. The authors propose two approaches to enhance spatial awareness: training image encoders with alternative objectives and incorporating 2D positional encodings. Their experiments demonstrate that these modifications result in improved performance on spatial reasoning benchmarks, highlighting the importance of spatial structure in VLM design for applications like robotics and embodied AI.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）虽然发展迅速，但在捕捉空间关系方面仍存在盲区，这对需要空间定位的应用如机器人和具身AI至关重要。现有VLMs通常采用CLIP风格的图像编码器，将图像扁平化为1D序列，忽略了空间推理所需的2D结构。为解决这一问题，研究者探讨了基于不同训练目标的图像编码器以及2D位置编码的引入，实验结果表明这些架构改进能够提升多个基准测试中的空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Predicting When to Trust Vision-Language Models for Spatial Reasoning</div>
<div class="meta-line">Authors: Muhammad Imran, Yugyung Lee</div>
<div class="meta-line">First: 2026-01-14T22:00:28+00:00 · Latest: 2026-01-14T22:00:28+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11644v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预测何时信任视觉-语言模型进行空间推理</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多模态任务中表现出色，但在空间推理任务中存在系统性错误，仅在基本方向关系任务中达到49%（CLIP）至54%（BLIP-2）的准确率。为了在机器人和自主系统中安全部署，我们需要预测何时信任VLM的空间预测，而不是接受所有输出。我们提出了一种基于视觉的置信度估计框架，通过独立的几何验证（使用目标检测）来验证VLM的预测。与依赖文本自评估的方法不同，我们的方法通过梯度提升融合了四个信号：VLM声明与坐标之间的几何对齐、重叠带来的空间歧义、检测质量以及VLM内部的不确定性。我们在BLIP-2上实现了0.674的AUROC（比文本方法基线提升34.0%），在CLIP上实现了0.583的AUROC（比文本方法基线提升16.1%），并在生成式和分类架构中实现了泛化。我们的框架支持选择性预测：在目标准确率为60%时，BLIP-2的覆盖率达到61.9%，比基线提升2.2倍（27.6%）。特征分析表明，基于视觉的信号贡献了模型重要性的87.4%，而VLM置信度仅贡献12.7%，验证了外部几何验证优于自评估。我们展示了可靠的场景图构建，其中基于置信度的剪枝使精度从52.1%提升至78.3%，同时保留了68.2%的边。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of systematic spatial reasoning failures in Vision-Language Models (VLMs) by proposing a vision-based confidence estimation framework. The method validates VLM predictions through independent geometric verification using object detection, combining four signals—geometric alignment, spatial ambiguity, detection quality, and internal uncertainty—via gradient boosting. Experimental results show significant improvements in confidence prediction performance, achieving 0.674 AUROC on BLIP-2 and 0.583 AUROC on CLIP, with notable gains in coverage and precision when applying confidence-based pruning.</div>
<div class="mono" style="margin-top:8px">本文针对视觉-语言模型（VLMs）在空间推理任务中出现的系统性错误，提出了一种基于视觉的置信度估计框架。该方法通过独立的几何验证（基于目标检测）来验证VLM的预测结果，并融合了四个信号：几何对齐度、空间模糊性、检测质量以及模型内部不确定性，采用梯度提升技术进行整合。实验结果显示，该框架在BLIP-2上实现了0.674的AUROC（比文本基线方法提升34.0%），在CLIP上达到0.583（比文本基线方法提升16.1%）。此外，该框架支持选择性预测，在60%目标准确度下实现61.9%的覆盖率，较基线方法提升2.2倍，并在场景图构建中显著提高精度，同时保留68.2%的边。</div>
</details>
</div>
<div class="card">
<div class="title">MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</div>
<div class="meta-line">Authors: Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong</div>
<div class="meta-line">First: 2026-01-14T21:21:00+00:00 · Latest: 2026-01-14T21:21:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09879v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedVL-SAM2：一种统一的三维医学视觉-语言模型，用于多模态推理和提示驱动的分割</div>
<div class="mono" style="margin-top:8px">近期医学视觉-语言模型（VLMs）的进展在图像级文本中心任务（如报告生成和视觉问答（VQA））中取得了优异表现。然而，在三维医学VLM中实现细粒度视觉定位和体素空间推理仍具挑战性，尤其是在试图在一个通用框架内统一这些能力时。为解决这一问题，我们提出了MedVL-SAM2，这是一种统一的三维医学多模态模型，能够同时支持报告生成、VQA以及包括语义分割、指称分割和交互分割在内的多种分割范式。MedVL-SAM2通过专为三维医学影像设计的统一架构，整合了图像级推理和像素级感知，并引入了基于SAM2的体素分割模块，以实现精确的多粒度空间推理。该模型采用多阶段训练流程：首先在大规模的三维CT图像-文本对语料库上进行预训练，以对齐体素视觉特征与放射学语言嵌入；随后利用一个全面的三维CT分割数据集，联合优化语言理解和分割目标。这种联合训练使得模型能够通过语言、点或框提示进行灵活交互，从而统一高级视觉推理与空间精确定位。我们的统一架构在报告生成、VQA和多种三维分割任务中均实现了最先进的性能。进一步的分析表明，该模型能够提供可靠的三维视觉定位、可控的交互分割以及强大的跨模态推理能力，证明了在统一的三维医学VLM中可以同时实现高级语义推理和精确的三维定位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing 3D medical vision-language models in achieving fine-grained visual grounding and volumetric spatial reasoning. MedVL-SAM2 is proposed as a unified framework that integrates report generation, visual question answering, and multi-paradigm segmentation tasks. The model employs a cohesive architecture tailored for 3D medical imaging and incorporates a SAM2-based volumetric segmentation module to enable precise spatial reasoning. It is trained through a multi-stage pipeline, first pre-trained on 3D CT image-text pairs and then jointly optimized with segmentation and language understanding objectives. This approach allows flexible interaction via language, point, or box prompts and achieves state-of-the-art performance across multiple 3D medical tasks, including reliable visual grounding and robust cross-modal reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有三维医学视觉语言模型在实现细粒度视觉定位和体积空间推理方面的不足。MedVL-SAM2被提出为一个统一框架，能够同时支持报告生成、视觉问答和多种分割任务。该模型采用专为三维医学影像设计的统一架构，并结合基于SAM2的体积分割模块以实现精确的多粒度空间推理。其训练过程采用多阶段流水线，首先在大规模的三维CT图像-文本对语料库上进行预训练，随后在包含分割和语言理解目标的综合三维CT分割数据集上进行联合优化。实验结果表明，MedVL-SAM2在多个任务上达到了最先进的性能，并展示了可靠的三维视觉定位、可控的交互分割以及强大的跨模态推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</div>
<div class="meta-line">Authors: Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</div>
<div class="meta-line">First: 2026-01-14T18:59:59+00:00 · Latest: 2026-01-14T18:59:59+00:00</div>
<div class="meta-line">Comments: Project page: https://jasper0314-huang.github.io/fast-thinkact/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09708v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jasper0314-huang.github.io/fast-thinkact/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fast-ThinkAct: 通过可表述的潜在规划实现高效的视觉-语言-动作推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）任务需要对复杂视觉场景进行推理，并在动态环境中执行适应性动作。尽管近期关于推理VLA的研究表明，显式的思维链（CoT）可以提升泛化能力，但它们由于推理轨迹过长而面临较高的推理延迟。我们提出Fast-ThinkAct，这是一种高效的推理框架，通过可表述的潜在推理实现紧凑且高效的规划。Fast-ThinkAct通过从教师模型中蒸馏学习，利用偏好引导的目标对操作轨迹进行对齐，从而迁移语言和视觉规划能力，用于具身控制。这使得推理增强的策略学习能够有效连接紧凑推理与动作执行。在多种具身操作和推理基准上的广泛实验表明，Fast-ThinkAct在推理延迟上比最先进的推理VLA减少了高达89.3\%，同时保持了有效的长时规划、少样本适应和失败恢复能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the high inference latency in existing Vision-Language-Action (VLA) systems that rely on explicit chain-of-thought reasoning. Fast-ThinkAct introduces an efficient framework that employs verbalizable latent planning to achieve compact yet effective reasoning. The method involves distilling knowledge from a teacher model using a preference-guided objective, enabling the transfer of both linguistic and visual planning capabilities. Experimental results show that Fast-ThinkAct significantly reduces inference latency by up to 89.3% compared to state-of-the-art reasoning VLAs, while maintaining strong performance in long-horizon planning, few-shot adaptation, and failure recovery.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有视觉-语言-动作（VLA）系统中依赖显式链式推理所带来的高推理延迟问题。Fast-ThinkAct 提出了一种高效的框架，通过可语言化的潜在推理实现紧凑而有效的规划，利用教师模型的知识蒸馏，并以偏好引导的目标进行训练。该方法能够将语言和视觉规划能力迁移到具身控制中，提升策略学习的效果，使简洁推理与动作执行有效连接。实验结果表明，Fast-ThinkAct 在多个具身操作与推理基准测试中表现出色，相比最先进的推理 VLA 系统推理延迟降低了高达 89.3%，同时保持了长时规划、少样本适应和故障恢复能力。</div>
</details>
</div>
<div class="card">
<div class="title">Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs</div>
<div class="meta-line">Authors: Rui Zhu, Xin Shen, Shuchen Wu, Chenxi Miao, Xin Yu, Yang Li, Weikang Li, Deguo Xia, Jizhou Huang</div>
<div class="meta-line">First: 2026-01-14T12:24:47+00:00 · Latest: 2026-01-14T12:24:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09430v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09430v1">PDF</a> · <a href="https://github.com/ruiz-nju/Video-MSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Video-MSR：评估多跳空间推理能力的多模态大语言模型基准</div>
<div class="mono" style="margin-top:8px">空间推理已成为多模态大语言模型（MLLMs）的一项关键能力，受到越来越多的关注并迅速发展。然而，现有的基准测试主要集中在单步感知到判断的任务上，对需要复杂视觉-空间逻辑链的场景探索不足。为弥合这一差距，我们引入了Video-MSR，这是首个专门设计用于评估动态视频场景中多跳空间推理（MSR）能力的基准测试。Video-MSR通过四个不同的任务系统地探测MSR能力：受限定位、基于链的参考检索、路径规划和反事实物理推理。我们的基准测试包含3,052个高质量视频实例和4,993个问答对，通过结合先进模型生成与严格的人类验证的可扩展、视觉基础的流程构建。通过对20个最先进的MLLMs进行全面评估，我们发现了显著的局限性，揭示了尽管模型在表层感知上表现出色，但在MSR任务中却出现明显的性能下降，经常在多步推理过程中出现空间迷失和幻觉。为缓解这些不足并增强模型的MSR能力，我们进一步整理了MSR-9K，一个专门的指令微调数据集，并对Qwen-VL进行了微调，在Video-MSR上实现了+7.82%的绝对提升。我们的结果突显了多跳空间指令数据的有效性，并确立了Video-MSR作为未来研究的重要基础。代码和数据将在https://github.com/ruiz-nju/Video-MSR上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of benchmarks for evaluating multi-hop spatial reasoning in Multimodal Large Language Models (MLLMs), which is essential for understanding complex visual-spatial relationships in dynamic video scenarios. The authors introduce Video-MSR, a novel benchmark comprising 3,052 video instances and 4,993 question-answer pairs, designed to assess MSR through four distinct tasks. Evaluation of 20 state-of-the-art MLLMs reveals significant performance drops in multi-step spatial reasoning, with issues such as spatial disorientation and hallucination. To improve MSR capabilities, the researchers curate MSR-9K and fine-tune Qwen-VL, achieving a 7.82% absolute improvement on Video-MSR.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）在动态视频场景中缺乏对多步空间推理能力的评估基准问题，提出了Video-MSR这一首个专门用于评估多跳空间推理（MSR）的基准测试。该基准包含3,052个高质量视频实例和4,993个问答对，通过四个不同的任务系统地探测MSR能力。对20个最先进的MLLMs进行评估后发现，模型在多步空间推理任务中表现显著下降，常出现空间迷失和幻觉等问题。为提升MSR能力，论文还构建了专门的指令微调数据集MSR-9K，并通过在该数据集上微调Qwen-VL，实现了在Video-MSR任务上的7.82%绝对性能提升。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
