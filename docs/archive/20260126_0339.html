<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-26 03:39</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260126_0339</div>
    <div class="row"><div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知</div>
<div class="mono" style="margin-top:8px">大型音频语言模型（LALMs）在语音识别、音频描述和听觉问答任务中最近取得了显著进展。然而，这些模型是否能够感知空间动态，特别是声源的运动，仍不清楚。在本研究中，我们发现当前的ALLMs存在系统性的运动感知缺陷。为了解决这一问题，我们引入了AudioMotionBench，这是首个专门设计用于评估听觉运动理解的基准测试。AudioMotionBench引入了一个受控的问答基准测试，用于评估音频语言模型（LALMs）能否从双耳音频中推断出移动声源的方向和轨迹。全面的定量和定性分析表明，当前模型在可靠识别运动线索或区分方向模式方面存在困难。平均准确率仍低于50\%，突显了听觉空间推理的基本局限性。我们的研究指出了人类与模型在听觉空间推理之间的根本差距，为未来增强音频语言模型的空间认知能力提供了诊断工具和新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of Large Audio-Language Models (LALMs) to perceive auditory motion, a critical aspect of spatial reasoning. The researchers introduce AudioMotionBench, a novel benchmark designed to assess whether these models can infer the direction and trajectory of moving sound sources from binaural audio. Experimental results show that current models perform poorly, with average accuracy below 50%, indicating a significant deficit in understanding spatial dynamics. The findings highlight a fundamental gap between human auditory perception and machine models, offering insights for improving spatial cognition in future audio-language systems.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型音频语言模型（LALMs）在感知听觉运动方面的能力，这是空间推理的关键组成部分，尚未得到充分评估。作者提出了AudioMotionBench，这是首个专门用于评估听觉运动理解的基准测试。实验结果表明，当前模型在识别运动线索和区分方向模式方面表现不佳，平均准确率低于50%，揭示了模型在听觉空间推理方面存在根本性缺陷。这些发现指出了人类听觉空间感知与现有模型之间的基本差距，为未来提升音频语言模型的空间认知能力提供了诊断工具和新见解。</div>
</details>
</div>
<div class="card">
<div class="title">SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</div>
<div class="meta-line">Authors: Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-10T20:04:08+00:00 · Latest: 2026-01-22T14:26:01+00:00</div>
<div class="meta-line">Comments: Conference: NeurIPS 2025 (main)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10046v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimWorld-Robotics：为多模态机器人导航与协作合成逼真且动态的城市环境</div>
<div class="mono" style="margin-top:8px">近年来，基础模型在开发能够根据多模态输入在开放场景中执行多样化任务的通用型机器人方面取得了令人鼓舞的成果。然而，当前的研究主要集中在室内和家庭场景。在本工作中，我们提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境的具身AI模拟平台。SWR基于Unreal Engine 5构建，能够程序化生成包含行人和交通系统的无限逼真城市场景，其在真实感、复杂性和可扩展性方面超越了以往的城市模拟。它还支持多机器人控制与通信。借助这些关键特性，我们构建了两个具有挑战性的机器人基准测试：(1) 多模态指令跟随任务，其中机器人必须在行人和交通存在的情况下，根据视觉-语言导航指令到达目标；(2) 多智能体搜索任务，其中两个机器人必须通过通信协作定位并相遇。与现有基准不同，这两个新基准在真实场景中全面评估了多种关键的机器人能力，包括(1) 多模态指令的语义对齐，(2) 大规模环境中的三维空间推理，(3) 在行人和交通中的安全长距离导航，(4) 多机器人协作，以及(5) 基于环境的通信。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SimWorld-Robotics (SWR), a simulation platform designed to support the development of generalist robotics in complex, large-scale urban environments. The motivation stems from the need for more realistic and dynamic benchmarks to evaluate robot navigation and collaboration capabilities beyond traditional indoor settings. SWR is built on Unreal Engine 5 and generates photorealistic urban scenes with dynamic elements such as pedestrians and traffic systems. The platform enables two key benchmarks: a multimodal instruction-following task and a multi-agent search task, which assess a range of critical robot abilities, including perception, spatial reasoning, navigation, and communication. Experimental results show that current state-of-the-art models, including vision-language models, struggle with these tasks, highlighting the challenges in adapting such models for real-world urban robotics applications.</div>
<div class="mono" style="margin-top:8px">本文提出了SimWorld-Robotics（SWR），一个用于在大规模、逼真的城市环境中开发通用机器人技术的模拟平台。研究动机源于对传统室内场景之外、复杂动态环境中的机器人能力评估与提升的需求。SWR基于Unreal Engine 5构建，能够生成包含行人和交通系统的无限逼真城市场景，具备更高的真实感、复杂度和可扩展性。平台支持多机器人控制与通信，并构建了两个挑战性基准任务：多模态指令跟随任务和多智能体搜索任务。实验结果表明，当前最先进的模型，包括视觉-语言模型，在这些任务中表现不佳，缺乏在开放城市环境中所需的稳健感知、推理和规划能力。</div>
</details>
</div>
<div class="card">
<div class="title">Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</div>
<div class="meta-line">Authors: Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</div>
<div class="meta-line">First: 2025-05-19T16:56:31+00:00 · Latest: 2026-01-22T14:25:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13353v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.13353v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code&#x27;s operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs&#x27; accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval&#x27;s 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知与敏感性：探究语义回忆对长上下文代码推理的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用于理解大型代码库，但它们是通过理解长代码上下文的操作语义还是依赖模式匹配捷径仍不清楚。我们区分了词汇回忆（逐字检索代码）和语义回忆（理解操作语义）。通过对10个最先进的LLMs进行评估，我们发现前沿模型能够实现近乎完美的、位置无关的词汇回忆，但当代码位于长上下文的中心位置时，语义回忆会严重退化。我们引入了语义回忆敏感性这一指标，用于衡量任务是否需要理解代码的操作语义，还是允许使用模式匹配捷径。通过一种新颖的反事实测量方法，我们表明模型在解决现有代码理解基准时严重依赖模式匹配捷径。我们提出了一项新任务SemTrace，其通过不可预测的操作实现高语义回忆敏感性；LLMs的准确性表现出严重的位移效应，当相关代码片段接近输入代码上下文的中间位置时，中位数准确率下降了92.73%，而CRUXEval则为53.36%。我们的研究结果表明，当前的评估方法在长上下文代码理解中严重低估了语义回忆失败的情况。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how large language models (LLMs) handle long code contexts, focusing on whether they rely on pattern matching or truly understand the operational semantics. The research differentiates between lexical recall, which involves retrieving code verbatim, and semantic recall, which requires comprehension of code meaning. By evaluating ten state-of-the-art LLMs, it was found that while models perform well in lexical recall, their semantic recall significantly declines when code is located in the central part of long contexts. The authors introduce a metric called semantic recall sensitivity to assess the necessity of semantic understanding in tasks. Using a counterfactual measurement approach, they demonstrate that models heavily depend on pattern matching shortcuts for existing benchmarks. They propose a new task, SemTrace, designed to test semantic recall sensitivity, and show that LLMs exhibit severe positional accuracy drops, with a median decrease of 92.73% compared to 53.36% in CRUXEval, indicating that current evaluations may not fully capture semantic recall failures in long context code understanding.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）在处理长代码上下文时是否依赖模式匹配还是真正理解代码的操作语义。研究区分了字面回忆（逐字检索代码）和语义回忆（理解代码含义）。通过对十种最先进的LLMs进行评估，发现虽然模型在字面回忆上表现优异，但当代码位于长上下文的中间位置时，其语义回忆能力显著下降。作者引入了语义回忆敏感性这一指标，用于评估任务是否需要语义理解。通过一种新颖的反事实评估方法，他们表明模型在解决现有代码理解基准时高度依赖模式匹配。他们提出了一项新任务SemTrace，强调语义回忆，并发现LLMs的准确率随着代码片段在输入中的位置接近中间而急剧下降，表明当前评估可能低估了长上下文代码理解中的语义回忆失败问题。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</div>
<div class="meta-line">Authors: Pascal Benschop, Justin Dauwels, Jan van Gemert</div>
<div class="meta-line">First: 2026-01-22T09:14:11+00:00 · Latest: 2026-01-22T09:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用合成生成视频评估视觉语言模型的情境与空间感知</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在空间推理方面仍存在脆弱性，当语义依赖于细微的时间或几何线索时。我们引入了一个合成基准测试，用于评估两种互补能力：情境感知（识别互动是否具有伤害性或无害性）和空间感知（追踪谁对谁做了什么，以及推理相对位置和运动）。通过最小化的视频对，我们测试了三个挑战：区分暴力行为与无害活动、跨视角绑定攻击者角色、以及判断细粒度轨迹对齐。尽管我们在无训练设置下评估了近期的VLMs，但该基准适用于任何视频分类模型。结果显示，模型在各项任务中的表现仅略高于随机猜测。一个简单的辅助手段——稳定的颜色线索——部分缓解了攻击者角色的混淆，但并未解决其根本性缺陷。通过发布数据和代码，我们旨在提供可复现的诊断工具，并为轻量级空间先验的探索提供基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to evaluate the situational and spatial awareness capabilities of vision language models (VLMs) by introducing a synthetic video benchmark. The benchmark tests three key challenges: distinguishing harmful from benign interactions, binding roles across different viewpoints, and assessing fine-grained trajectory alignment. The results indicate that current VLMs perform only slightly better than chance in these tasks, suggesting limitations in their ability to process subtle temporal and geometric information. A simple modification, such as stable color cues, helps reduce some role confusion but does not fully address the core issues.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLMs）在空间推理方面的不足，提出一个合成基准测试，通过生成的视频评估情境意识和空间意识。该基准测试包含三个关键挑战：区分有害与无害互动、跨视角绑定角色以及评估细粒度轨迹对齐。实验结果显示，当前VLMs在这些任务上的表现仅略高于随机猜测，表明其在理解微妙的时间和几何线索方面存在脆弱性。简单的修改如稳定颜色提示虽能部分缓解攻击者角色混淆问题，但无法根本解决模型的内在缺陷，提示需要更强大的空间先验知识。</div>
</details>
</div>
<div class="card">
<div class="title">AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</div>
<div class="meta-line">Authors: Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Yingheng Wang, Bram Hoex, Zhicheng Zhong, Tong Xie</div>
<div class="meta-line">First: 2025-10-06T11:17:56+00:00 · Latest: 2026-01-22T05:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04704v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.04704v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AtomWorld：评估大型语言模型在晶体材料中空间推理能力的基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在文本推理方面表现出色，并开始发展空间理解能力，这引发了是否可以将这些能力结合用于复杂、领域特定任务的疑问。这一问题在材料科学等需要深入理解三维原子结构的领域尤为重要。尽管初步研究已成功将LLMs应用于纯晶体生成或坐标理解等任务，但缺乏一个标准化的基准来系统评估其在不同原子结构上的核心推理能力。为了解决这一问题，我们引入了AtomWorld基准，用于在基于晶体学信息文件（CIFs）的任务中评估LLMs。这些任务包括结构编辑、CIF感知和性质引导建模，揭示了一个关键限制：尽管当前模型已建立有希望的基线，但在结构理解和空间推理方面仍持续失败。我们的实验表明，这些模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析和材料洞察中的累积性错误。通过定义这些标准化任务，AtomWorld为推动LLMs向稳健的原子尺度建模发展奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to assess the spatial reasoning capabilities of large language models (LLMs) in the context of crystalline materials, which is crucial for materials science. The authors introduce AtomWorld, a benchmark based on Crystallographic Information Files (CIFs), to evaluate LLMs on tasks such as structural editing, CIF perception, and property-guided modeling. The main experimental results reveal that current models, although showing promise, struggle with structural understanding and spatial reasoning, often making frequent errors in modifying crystal structures and interpreting CIF data, which can lead to significant issues in downstream analysis and material discovery.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型（LLMs）在晶体材料领域的空间推理能力，这对于材料科学至关重要。作者提出了AtomWorld基准，基于晶体结构信息文件（CIFs）来评估LLMs在结构编辑、CIF感知和性质引导建模等任务中的表现。实验结果表明，当前模型尽管显示出一定的潜力，但在理解原子结构和空间推理方面仍存在显著困难，经常在修改结构和解析CIF数据时出现错误，这可能导致后续材料分析中的累积误差。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理为何失败？对GitHub中失败代理拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编码代理现在正在向软件项目提交拉取请求（PRs），不仅作为助手，还作为自主贡献者。随着这些代理贡献在真实仓库中迅速增加，人们对它们在实际中的行为以及为何许多未能被合并仍知之甚少。在本文中，我们对GitHub上五个编码代理提交的33,000个代理撰写的PR进行了大规模研究。（RQ1）我们首先从四个广泛维度定量描述了被合并和未被合并的PR：1）不同任务类型的合并结果，2）代码更改，3）CI构建结果，以及4）评审动态。我们观察到，与文档、CI和构建更新相关的任务合并成功率最高，而性能和错误修复任务表现最差。未被合并的PR通常涉及更大的代码更改，修改更多文件，并且常常无法通过项目的CI/CD流水线验证。（RQ2）为进一步探讨为何某些代理PR未被合并，我们对600个PR进行了定性分析，以构建拒绝模式的分层分类法。这种分析通过揭示定量指标未捕捉到的拒绝原因，如缺乏有意义的评审互动、重复PR、不受欢迎的功能实现以及代理与项目目标的不一致，补充了RQ1的定量发现。我们的研究结果突显了关键的社会技术因素和人机协作因素，这些因素对于提高未来代理工作流的成功率至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of AI coding agents in contributing to software projects through pull requests (PRs) on GitHub. The researchers analyzed 33,000 PRs from five coding agents, identifying patterns in merge success across different task types. They found that documentation, CI, and build-related tasks had the highest merge rates, while performance and bug-fix tasks were the least successful. Not-merged PRs were associated with larger code changes, more files modified, and CI/CD failures. A qualitative analysis of 600 PRs revealed additional reasons for rejection, such as lack of reviewer interaction, duplicate submissions, and misaligned agent behavior, providing insights into the socio-technical challenges in AI-assisted software development.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在通过GitHub提交拉取请求（PR）时的失败情况。通过对五个编码代理提交的33,000个PR进行大规模分析，发现文档、CI和构建更新任务的合并成功率最高，而性能和错误修复任务的合并成功率最低。未被合并的PR通常涉及较大的代码改动、修改更多文件，并且往往无法通过项目的CI/CD流水线验证。对600个PR的定性分析进一步揭示了常见的拒绝原因，如缺乏审阅者互动、重复提交、不受欢迎的功能实现以及代理与项目目标的不一致，为改善未来代理工作流提供了关键的社会技术与人机协作因素的洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</div>
<div class="meta-line">Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</div>
<div class="meta-line">First: 2025-07-07T18:17:06+00:00 · Latest: 2026-01-21T13:37:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05386v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.05386v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model&#x27;s general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT&#x27;s gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化微调自然缓解持续微调中的遗忘</div>
<div class="mono" style="margin-top:8px">持续微调（CPT）是一种流行且有效的技术，用于将基础模型（如多模态大语言模型）适应于特定且不断变化的下游任务。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，但CPT中学习范式的根本作用仍被广泛忽视。本文对两种核心的微调范式——监督微调（SFT）和强化微调（RFT）进行了比较分析，探讨它们在持续微调过程中对知识保留的影响。我们的实验基于包含七个多样化多模态任务的基准数据集，使用Qwen2.5-VL-7B-Instruct作为持续微调的基础模型。研究得出两个重要发现：（1）在持续学习下游任务时，SFT会导致先前学习任务的灾难性遗忘，而RFT则能自然保留先前知识，其性能与多任务训练相当。（2）RFT成功保护并甚至增强了模型在标准基准（如MMMU和MMLU-Pro）上的通用知识，而SFT则严重损害了模型的通用能力。进一步分析表明，这种稳定性并非主要归因于显式的机制，如KL惩罚或思维链推理，而是源于RFT中隐含的正则化机制。我们的理论分析表明，RFT的梯度更新自然受到奖励方差的缩放，作为一种数据依赖的正则化器，其内在机制能够保护先前获得的知识。最后，我们提出了一种基于rollout的实例过滤算法，以提升RFT的稳定性和效率。我们的全面研究展示了RFT作为持续微调的稳健范式的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the issue of catastrophic forgetting in continual post-training (CPT) of foundation models, particularly focusing on the effectiveness of supervised fine-tuning (SFT) versus reinforcement fine-tuning (RFT). The study highlights that SFT results in significant loss of previously learned knowledge when adapting to new downstream tasks, while RFT inherently preserves prior knowledge and maintains performance comparable to multi-task training. Experimental results on seven multimodal tasks using Qwen2.5-VL-7B-Instruct show that RFT not only mitigates forgetting but also enhances general knowledge on standard benchmarks. The authors attribute this stability to an implicit regularization mechanism within RFT, driven by reward variance during gradient updates, rather than explicit techniques like KL penalty. They further propose a rollout-based instance filtering algorithm to improve RFT&#x27;s efficiency and robustness in continual learning scenarios.</div>
<div class="mono" style="margin-top:8px">本文探讨了基础模型在持续微调过程中出现灾难性遗忘的问题，重点比较了监督微调（SFT）和强化微调（RFT）两种学习范式的有效性。研究发现，RFT能够自然地缓解遗忘，保持先前学习的知识，其性能可与多任务训练相媲美。在七个多样化的多模态任务上使用Qwen2.5-VL-7B-Instruct进行实验表明，RFT不仅维持了模型的通用知识，还能在标准基准（如MMMU和MMLU-Pro）上提升表现，而SFT则导致严重的能力下降。论文指出，RFT的稳定性主要源于其隐式的正则化机制，该机制由奖励方差驱动的梯度更新所体现，而非显式的KL惩罚或思维链推理。最后，提出了一种基于rollout的实例过滤算法，以增强RFT的稳定性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</div>
<div class="meta-line">Authors: Xinyi Zheng, Yunze Liu, Chi-Hao Wu, Fan Zhang, Hao Zheng, Wenqi Zhou, Walterio W. Mayol-Cuevas, Junxiao Shen</div>
<div class="meta-line">First: 2026-01-21T11:32:24+00:00 · Latest: 2026-01-21T11:32:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialMem：基于度量锚定和快速检索的统一三维记忆</div>
<div class="mono" style="margin-top:8px">我们提出了SpatialMem，这是一个以记忆为中心的系统，将三维几何、语义和语言统一为一个可查询的表示。从随意捕捉的视角RGB视频出发，SpatialMem重建度量尺度的室内环境，检测结构化的三维锚点（墙壁、门、窗）作为第一层框架，并填充一个分层记忆，其中包含开放词汇的对象节点——将证据片段、视觉嵌入和两层文本描述与三维坐标相链接——以实现紧凑存储和快速检索。这种设计支持对空间关系（如距离、方向、可见性）的可解释推理，并且无需专用传感器即可支持下游任务，如语言引导的导航和对象检索。在三个真实室内场景中的实验表明，SpatialMem在增加杂乱和遮挡的情况下仍能保持强大的锚点-描述级导航完成度和分层检索准确性，为具身空间智能提供了一个高效且可扩展的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpatialMem is motivated by the need for a unified representation of 3D geometry, semantics, and language to enable efficient spatial reasoning and navigation. The system reconstructs metrically scaled indoor environments from egocentric RGB videos and uses structural 3D anchors as a foundational scaffold. It then builds a hierarchical memory by linking open-vocabulary object nodes to 3D coordinates through evidence patches, visual embeddings, and textual descriptions. Experimental results across three real-life indoor scenes show that SpatialMem achieves strong navigation completion and hierarchical retrieval accuracy even under increasing clutter and occlusion, demonstrating its effectiveness for embodied spatial intelligence tasks.</div>
<div class="mono" style="margin-top:8px">SpatialMem的提出源于对统一表示3D几何、语义和语言的需求，以支持空间推理与导航。该系统从第一人称视角的RGB视频中重建度量尺度的室内环境，并利用结构化的3D锚点作为框架，构建一个包含开放词汇对象的分层记忆结构。每个对象通过3D坐标与证据片段、视觉嵌入和文本描述相连接，实现紧凑存储与快速检索。在三个真实室内场景的实验中，结果表明SpatialMem在增加杂乱和遮挡的情况下仍能保持较高的导航完成度和检索准确性，展示了其在具身空间智能任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">3D Space as a Scratchpad for Editable Text-to-Image Generation</div>
<div class="meta-line">Authors: Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</div>
<div class="meta-line">First: 2026-01-21T02:40:19+00:00 · Latest: 2026-01-21T02:40:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://oindrilasaha.github.io/3DScratchpad/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D空间作为可编辑文本到图像生成的草稿纸</div>
<div class="mono" style="margin-top:8px">近期在大型语言模型（LLMs）方面的进展表明，当将中间思维外部化为显式的空间，如思维链轨迹或工具增强推理时，推理能力会得到提升。然而，视觉语言模型（VLMs）缺乏类似的机制来进行空间推理，这限制了它们生成准确反映几何关系、物体身份和构图意图图像的能力。我们引入了空间草稿纸的概念——一种连接语言意图与图像合成的3D推理基底。给定一个文本提示，我们的框架解析主体和背景元素，将它们实例化为可编辑的3D网格，并使用代理场景规划来确定位置、方向和视角选择。最终的3D布局通过保留身份的提示被渲染回图像域，使VLM能够生成空间一致且视觉连贯的输出。与以往基于2D布局的方法不同，我们的方法支持直观的3D编辑，这些编辑能够可靠地传播到最终图像中。在实验上，它在GenAI-Bench上实现了文本对齐的32%提升，证明了显式3D推理在精确可控图像生成中的优势。我们的结果突显了一种新的视觉-语言模型范式，即不仅在语言层面进行推理，也在空间层面进行推理。代码和可视化结果见 https://oindrilasaha.github.io/3DScratchpad/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of visual language models (VLMs) in generating images that accurately reflect spatial relationships and compositional intent by introducing a 3D spatial scratchpad. The proposed framework parses text prompts to extract subjects and background elements, representing them as editable 3D meshes and using agentic scene planning to determine their placement, orientation, and viewpoint. The 3D arrangement is then rendered back into the image domain with identity-preserving cues, enabling spatially consistent and visually coherent outputs. Experimental results on GenAI-Bench show a 32% improvement in text alignment compared to prior 2D layout-based methods, demonstrating the effectiveness of explicit 3D reasoning in enhancing controllability and precision in text-to-image generation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉语言模型（VLMs）在生成图像时难以准确反映几何关系和构图意图的问题。作者提出了一种3D空间scratchpad机制，用于外部化推理过程，使模型能够解析文本提示，生成可编辑的3D网格表示主体和背景元素，并通过代理场景规划确定其位置、方向和视角。最终的3D场景被渲染回图像域，并保留身份信息，从而生成空间一致且视觉连贯的图像。在GenAI-Bench上的实验结果显示，与传统的2D布局方法相比，该方法在文本对齐方面提升了32%，证明了显式的3D推理在提升图像生成的可控性和精确性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics</div>
<div class="meta-line">Authors: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li</div>
<div class="meta-line">First: 2026-01-20T14:51:45+00:00 · Latest: 2026-01-20T14:51:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14027v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14027v1">PDF</a> · <a href="https://github.com/project-numina/numina-lean-agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Numina-Lean-Agent：一个开放且通用的代理推理系统用于形式化数学</div>
<div class="mono" style="margin-top:8px">代理系统最近已成为形式化定理证明的主导范式，通过协调多个模型和工具实现了强大的性能。然而，现有方法通常依赖于任务特定的流水线和训练过的形式化证明器，限制了其灵活性和可复现性。在本文中，我们提出了一种范式，即直接使用通用编码代理作为形式化数学推理器。该范式受到以下三点启发：(1) 通用编码代理为超越证明的多样化推理任务提供了自然的接口；(2) 仅通过替换底层基础模型即可提升性能，而无需训练；(3) MCP支持灵活扩展和自主调用专用工具，避免了复杂的设计。基于这一范式，我们引入了Numina-Lean-Agent，它结合了Claude Code和Numina-Lean-MCP，以实现与Lean的自主交互、相关定理检索、非形式化证明以及辅助推理工具。使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent在Putnam 2025中解决了所有问题（12/12），与最佳闭源系统表现相当。除了基准评估，我们还通过与数学家互动，成功地形式化了Brascamp-Lieb定理，进一步展示了其通用性。我们将在https://github.com/project-numina/numina-lean-agent上发布Numina-Lean-Agent及其所有解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to develop a more flexible and reproducible agentic reasoning system for formal mathematics by moving away from task-specific pipelines and trained formal provers. The proposed method utilizes a general coding agent as a formal math reasoner, leveraging its ability to handle diverse reasoning tasks and enabling performance improvements through model replacement. The key experimental results show that Numina-Lean-Agent, combining Claude Code with Numina-Lean-MCP, successfully solves all 12 problems in the Putnam 2025 competition, matching the best closed-source systems. Additionally, it demonstrates its generality by assisting mathematicians in formalizing the Brascamp-Lieb theorem.</div>
<div class="mono" style="margin-top:8px">本文提出Numina-Lean-Agent，这是一个用于形式数学的代理推理系统。研究动机源于现有定理证明系统依赖任务特定的流程和训练过的证明器，限制了其灵活性和可复现性。该方法利用通用编码代理与Numina-Lean-MCP结合，实现与Lean的自主交互、相关定理检索以及辅助推理工具的使用。实验结果表明，Numina-Lean-Agent使用Claude Opus 4.5作为基础模型，成功解决Putnam 2025竞赛中的全部12道题目，性能与最佳闭源系统相当。此外，通过与数学家的交互，该系统成功形式化了Brascamp-Lieb定理，展示了其广泛适用性。</div>
</details>
</div>
<div class="card">
<div class="title">CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments</div>
<div class="meta-line">Authors: Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li</div>
<div class="meta-line">First: 2026-01-20T13:44:02+00:00 · Latest: 2026-01-20T13:44:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14339v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CityCube：在城市环境中评估视觉-语言模型的跨视角空间推理能力</div>
<div class="mono" style="margin-top:8px">跨视角空间推理对于具身AI至关重要，是复杂环境中空间理解、心理模拟和规划的基础。现有基准主要关注室内或街道场景，忽略了开放城市空间中独特的挑战，这些场景具有丰富的语义、复杂的几何结构和多样的视角变化。为了解决这一问题，我们引入CityCube，这是一个系统性的基准，旨在评估当前视觉-语言模型（VLMs）在城市环境中的跨视角推理能力。CityCube整合了四种视角动态，以模拟相机运动，并涵盖了来自多种平台（如车辆、无人机和卫星）的广泛视角范围。为了全面评估，它包含5,022个精心标注的多视角问答对，按五个认知维度和三种空间关系表达进行分类。对33个VLMs的全面评估表明，其表现与人类存在显著差距：即使是大规模模型也难以超过54.1%的准确率，仍比人类表现低34.2%。相比之下，小型微调后的VLMs准确率超过60.0%，突显了我们基准的重要性。进一步分析表明，VLMs与类人推理之间存在任务相关性和根本性的认知差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces CityCube, a benchmark designed to evaluate cross-view spatial reasoning in vision-language models (VLMs) within urban environments. Motivated by the limitations of existing benchmarks that focus on indoor or street scenes, CityCube incorporates four viewpoint dynamics and multiple platforms to simulate realistic urban view variations. The benchmark includes 5,022 annotated multi-view QA pairs organized into five cognitive dimensions and three spatial relation expressions. Experimental results show that even large-scale VLMs achieve only 54.1% accuracy, significantly below human performance, while small-scale fine-tuned models surpass this threshold, indicating the importance of tailored approaches for urban spatial reasoning.</div>
<div class="mono" style="margin-top:8px">该研究提出了CityCube，这是一个用于评估视觉语言模型（VLMs）在城市环境中跨视角空间推理能力的基准。由于现有基准主要关注室内或街道场景，未能体现开放城市空间中丰富的语义、复杂的几何结构和视角变化带来的挑战，因此本研究构建了CityCube。该基准集包含5,022个精心标注的多视角问答对，涵盖五个认知维度和三种空间关系表达。实验结果显示，即使是大规模的VLMs也只能达到54.1%的准确率，比人类表现低34.2%，而经过微调的小规模模型则超过这一水平，凸显了针对城市空间推理任务进行专门优化的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前的视觉表征学习仍存在分裂：视觉-语言模型（如CLIP）在全局语义对齐方面表现优异，但缺乏空间精度；而自监督方法（如MAE、DINO）能够捕捉复杂的局部结构，但在高层语义上下文方面存在困难。我们认为这些范式本质上是互补的，可以整合到一个有原则的多任务框架中，并通过密集空间监督进一步增强。我们提出了MTV，一个联合优化视觉-语言对比、自监督和密集空间目标的多任务视觉预训练框架。为减少对人工标注的依赖，我们利用高容量的&quot;专家&quot;模型——如Depth Anything V2和OWLv2——大规模合成密集且结构化的伪标签。除了框架本身，我们还系统地研究了多任务视觉学习的机制，分析了：(i) 每个目标的边际增益，(ii) 任务协同与干扰，以及(iii) 在不同数据和模型规模下的扩展行为。我们的结果表明，MTV实现了&quot;两全其美&quot;的性能，在不损害全局语义理解的前提下，显著提升了细粒度空间推理能力。我们的发现表明，借助高质量的伪监督，多任务学习是一条通向更通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current visual representation learning approaches by proposing a multi-task framework that combines the strengths of vision-language models and self-supervised methods. The proposed MTV framework jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial learning objectives, using high-capacity expert models to generate pseudo-labels. Experimental results show that MTV achieves superior performance in both fine-grained spatial reasoning and global semantic understanding, demonstrating the effectiveness of integrating these paradigms with dense spatial supervision.</div>
<div class="mono" style="margin-top:8px">本文旨在解决当前视觉表征学习方法的局限性，提出了一种结合视觉-语言模型和自监督学习的多任务框架。动机来源于观察到视觉-语言模型在全局语义对齐方面表现优异，但缺乏空间精度，而自监督方法能捕捉局部结构却难以处理高层语义上下文。MTV框架通过联合优化视觉-语言对比学习、自监督学习和密集空间监督任务，利用专家模型生成大规模的结构化伪标签。实验结果表明，MTV在细粒度空间推理和全局语义理解方面均表现出色，验证了多任务学习结合高质量伪监督的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories</div>
<div class="meta-line">Authors: Aditya Bharat Soni, Rajat Ghosh, Vaishnavi Bhargava, Valerie Chen, Debojyoti Dutta</div>
<div class="meta-line">First: 2026-01-20T08:10:56+00:00 · Latest: 2026-01-20T08:10:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- &quot;test first, write code later&quot;, and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Tester：在真实仓库中训练开源大语言模型以生成问题复现测试</div>
<div class="mono" style="margin-top:8px">软件测试对于确保软件系统的正确性和可靠性至关重要。从自然语言问题描述中自动生成问题复现测试可以简化根本原因分析，提高开发人员的生产力，促进测试驱动开发——&quot;先测试后编写代码&quot;，并且可用于提升自动问题解决系统（如编码代理）的效果。现有的方法主要依赖于闭源大语言模型，对开源模型的探索有限。为了解决这一问题，我们提出了SWE-Tester——一种新颖的训练流程，用于训练开源大语言模型生成问题复现测试。首先，我们从2600个开源GitHub仓库中精选出41000个高质量的训练实例，并用其训练不同规模和家族的大语言模型。微调后的模型在SWT-Bench Verified上实现了高达10\%的成功率提升和21\%的变更覆盖率提升。进一步分析表明，随着推理时的计算量增加、数据量增多和模型规模扩大，模型表现持续提升。这些结果突显了我们框架在推动该领域开源大语言模型发展方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the effectiveness of automated issue reproduction test generation by leveraging open-source large language models (LLMs) instead of closed-source ones. The authors introduce SWE-Tester, a pipeline that trains open-source LLMs on a curated dataset of 41K instances from 2.6K GitHub repositories. The fine-tuned models demonstrate significant improvements, achieving up to a 10\% increase in success rate and 21\% in change coverage on the SWT-Bench Verified benchmark. These results indicate that increasing model size, inference-time compute, and training data consistently enhances performance in this task.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升开源大语言模型（LLM）在从自然语言描述生成问题复现测试方面的能力，这对于软件测试和调试至关重要。提出的方法SWE-Tester引入了一个训练开源LLM的流程，利用从2.6K个GitHub仓库中精选的41K个实例构建高质量训练数据集。在SWT-Bench Verified上的实验结果表明，微调后的模型在成功率和变更覆盖率方面分别提升了10\%和21\%，展示了该框架在该领域的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis</div>
<div class="meta-line">Authors: Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang, Zhongxue Gan</div>
<div class="meta-line">First: 2026-01-20T06:50:40+00:00 · Latest: 2026-01-20T06:50:40+00:00</div>
<div class="meta-line">Comments: This study has been accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为前时空解耦：面向多模态情感分析的解耦表征学习</div>
<div class="mono" style="margin-top:8px">多模态情感分析融合语言、视觉和听觉模态。主流方法基于模态不变和模态特异性因子分解或复杂融合，仍然依赖于时空混合建模。这忽略了时空异质性，导致时空信息不对称，从而限制了性能。因此，我们提出TSDA（行为前时空解耦），在任何交互之前，显式地将每个模态解耦为时间动态和空间结构上下文。对于每个模态，一个时间编码器和一个空间编码器将信号投影到独立的时间和空间空间中。因子一致性跨模态对齐随后仅将时间特征与跨模态的时间对应特征对齐，仅将空间特征与跨模态的空间对应特征对齐。因子特异性监督和去相关正则化减少了跨因子泄漏，同时保留了互补性。随后，一个门控再耦合模块将对齐的流重新耦合以执行任务。大量实验表明，TSDA优于基线方法。消融分析研究证实了该设计的必要性和可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of existing multimodal sentiment analysis approaches that fail to account for spatiotemporal heterogeneity, leading to information asymmetry and suboptimal performance. The proposed method, TSDA, decouples each modality into temporal dynamics and spatial structural context before any interaction, using separate encoders for each. Temporal features are aligned across modalities based on their temporal counterparts, while spatial features are aligned based on their spatial counterparts. Factor-specific supervision and decorrelation regularization are introduced to reduce cross-factor leakage and maintain feature complementarity. A Gated Recouple module is then used to combine the aligned features for the final task. Extensive experiments demonstrate that TSDA achieves superior performance compared to existing baselines, and ablation studies confirm the effectiveness and interpretability of its design.</div>
<div class="mono" style="margin-top:8px">本研究针对现有多模态情感分析方法未能考虑时空异质性的局限性，提出了一种新的方法TSDA，通过在任何交互之前将每种模态（语言、视觉、听觉）分解为时序动态和空间结构上下文。该方法使用独立的时序和空间编码器将信号映射到各自的时序和空间表示空间，并通过因子一致的跨模态对齐，仅对齐同类型的时序或空间特征。同时，引入因子特定监督和去相关正则化以减少跨因子信息泄露并保持特征互补性。最后，通过门控再耦合模块整合对齐后的特征流用于任务处理。大量实验表明，TSDA在多个基准数据集上优于现有方法，消融实验进一步验证了其设计的必要性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</div>
<div class="meta-line">Authors: Peter A. Massih, Eric Cosatto</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-19T21:14:34+00:00 · Latest: 2026-01-19T21:14:34+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素级精确推理：用于定量地理空间分析的QVLM架构和SQuID数据集</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLMs）在定量空间推理方面表现不佳，因为其架构会破坏进行计数和测量所需的像素级信息。视觉编码器通过补丁嵌入压缩图像，导致空间索引的丢失，从而无法实现精确的像素级追踪。我们提出了两个贡献以解决这一根本性限制。首先，我们引入了SQuID（卫星定量智能数据集），这是一个包含2000对卫星图像问答对的基准数据集，具有数值范围和分类答案，旨在评估定量空间推理能力。该数据集跨越三个难度层级，其注释由人类标签及其学习到的变异性自动生成。其次，我们提出了QVLM（定量视觉-语言模型），这是一种代码生成架构，通过将语言理解与视觉分析解耦，保持像素精度。QVLM不将图像编码为嵌入，而是生成可执行代码，首先调用分割模型获取像素级掩码，然后直接在这些掩码上进行操作，从而在整个推理过程中保留空间索引。我们的实验表明，使用GPT-5作为编码器的QVLM在SQuID数据集上达到了42.0%的准确率，而使用图像-问题对提示的VLM仅达到28.1%。我们的工作表明，对于定量空间推理任务，架构解耦能够提高定量任务的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of current Vision-Language Models (VLMs) in quantitative spatial reasoning, where they lose pixel-level information essential for counting and measurements. The authors propose SQuID, a new benchmark dataset containing 2,000 satellite image Question-Answer pairs with numerical and categorical answers, designed to assess quantitative geospatial understanding. They also introduce QVLM, a novel architecture that separates language understanding from visual analysis by generating executable code that operates on pixel-level masks obtained from segmentation models. Experimental results demonstrate that QVLM using GPT-5 as the coder achieves 42.0% accuracy on SQuID, significantly outperforming traditional VLMs which only reach 28.1% accuracy.</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLMs）在进行定量空间推理时表现不佳，因为其架构在图像编码过程中丢失了像素级信息。为此，作者提出了SQuID数据集，包含2000对卫星图像与问题-答案对，具有数值和分类答案，用于评估定量地理空间理解能力。同时，他们设计了QVLM模型，通过将语言理解和视觉分析解耦，并生成可执行代码来操作由分割模型获得的像素级掩码，从而保留空间索引。实验表明，使用GPT-5作为编码器的QVLM在SQuID数据集上达到42.0%的准确率，显著优于传统VLMs的28.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility</div>
<div class="meta-line">Authors: Nickil Maveli, Antonio Vergari, Shay B. Cohen</div>
<div class="meta-line">First: 2026-01-19T21:09:48+00:00 · Latest: 2026-01-19T21:09:48+00:00</div>
<div class="meta-line">Comments: 32 pages (preprint)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13398v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13398v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型能否压缩（并解压）？通过可逆性评估代码理解和执行</div>
<div class="mono" style="margin-top:8px">大语言模型在代码基准测试中表现出色，但代码的往返执行揭示了它们在正向和反向执行中保持一致推理能力的局限性。我们提出了RoundTripCodeEval（RTCE），一个包含四个不同代码执行推理任务的全面基准，旨在严格测试往返一致性。RTCE提供了一种无需执行的精确匹配评估方式，用于衡量双射保真度，评估模型在不同算法和方向上编码与解码操作之间是否保持一致的一一映射关系。我们系统地使用零样本提示、执行轨迹上的监督微调以及自我反思机制对最先进的代码大语言模型进行评估。每种方法都带来适度的改进，但没有完全弥合差距，这表明当前的大语言模型在真正的往返一致性方面存在困难，表明它们缺乏进行可信代码推理所需的内部一致性。RTCE揭示了若干新的、此前未被测量的见解，这些见解无法通过现有的输入输出预测、执行推理或往返自然语言基准测试捕捉到。在论文被接受后，我们将发布代码和数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of large language models (LLMs) to maintain consistent reasoning during code compression and decompression, highlighting limitations in their round-trip execution capabilities. The authors introduce RoundTripCodeEval (RTCE), a benchmark that evaluates bijection fidelity through four distinct code execution reasoning tasks, focusing on the consistency between encoding and decoding processes. Experimental results show that while zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms yield modest improvements, none of them fully achieve round-trip consistency, suggesting that current LLMs lack the internal coherence necessary for reliable code understanding and execution.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型在代码压缩与解压过程中保持一致推理能力的问题，重点关注往返一致性。作者提出了RoundTripCodeEval（RTCE）基准，通过四个不同的任务评估代码理解和执行，测试编码与解码过程之间的双射保真度。他们采用零样本提示、执行轨迹监督微调和自我反思机制对最先进的代码语言模型进行系统评估，发现尽管这些方法带来了一定改进，但均未实现真正的往返一致性，表明当前模型在实现可靠代码推理方面缺乏内部连贯性。</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models</div>
<div class="meta-line">Authors: Samuel Cyrenius Anderson</div>
<div class="meta-line">First: 2026-01-19T19:53:37+00:00 · Latest: 2026-01-19T19:53:37+00:00</div>
<div class="meta-line">Comments: 34 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -&gt; 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -&gt; 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维的几何学：大规模语言模型中规模如何重构推理</div>
<div class="mono" style="margin-top:8px">规模并不均匀地提升推理能力，而是重构了推理。通过分析四个领域（法律、科学、代码、数学）和两个规模（80亿、700亿参数）下超过25,000个思维链轨迹，我们发现神经网络的规模定律触发了领域特异的相变，而非统一的能力提升。法律推理经历结晶化：表征维度减少了45%（d95: 501 -&gt; 274），轨迹对齐度增加了31%，并实现了10倍的语义分离。科学和数学推理保持液态——即使参数增加9倍，其几何结构仍保持不变。代码推理则形成了一种离散的战略模式格子（轮廓系数：0.13 -&gt; 0.42）。这种几何结构预示了可学习性。我们引入了神经推理算子——从初始到终端隐藏状态的学习映射。在结晶化的法律推理中，我们的算子通过探针解码在未见任务上达到了63.6%的准确率，无需遍历中间状态即可预测推理终点。我们进一步识别出一种跨领域和规模的普遍振荡特征（相干度 ~ -0.4），表明注意力层和前馈层通过相反的动力学驱动推理。这些发现表明，思维的成本并非由任务难度决定，而是由流形几何决定——这为推理加速提供了拓扑结构允许的蓝图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how increasing the scale of large language models (LLMs) affects their reasoning capabilities across different domains. By analyzing over 25,000 chain-of-thought trajectories in Law, Science, Code, and Math with models of 8B and 70B parameters, the research reveals that scaling induces domain-specific phase transitions in the geometric structure of reasoning. Legal reasoning becomes more crystalline, showing reduced dimensionality and improved trajectory alignment, while scientific and mathematical reasoning remains liquid, maintaining geometric invariance. Code reasoning transitions to a lattice structure with enhanced distinctiveness. The authors introduce Neural Reasoning Operators, which map initial to terminal hidden states, achieving high accuracy in predicting reasoning outcomes without traversing intermediate steps. A universal oscillatory pattern is also identified, suggesting that attention and feedforward layers contribute to reasoning through opposing dynamics. These insights provide a framework for understanding how reasoning efficiency is influenced by the geometry of the latent space rather than task complexity.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大语言模型（LLMs）规模扩大对其在不同领域推理能力的影响。通过对法律、科学、代码和数学四个领域中超过25,000条推理轨迹的分析，研究发现规模扩大会引发领域特异性的相变，而非统一的能力提升。法律推理转变为晶体态，表征维度减少45%，轨迹对齐度增加31%，且具有10倍的流形分离能力；科学和数学推理则保持液态，几何结构不变。代码推理形成了具有战略模式的离散晶格结构，聚类性能显著提升。作者提出了神经推理算子，该算子通过探针解码实现了从初始到终端隐藏状态的映射，预测推理终点的准确率达63.6%。此外，研究还发现一个跨领域和规模的普遍振荡特征，表明注意力和前馈层通过相反的动力学机制驱动推理过程。</div>
</details>
</div>
<div class="card">
<div class="title">CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</div>
<div class="meta-line">Authors: Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai, S. Kevin Zhou, Yijun Yang, Alan Yuille, Jieneng Chen</div>
<div class="meta-line">First: 2026-01-19T18:59:44+00:00 · Latest: 2026-01-19T18:59:44+00:00</div>
<div class="meta-line">Comments: Code is available: https://github.com/CausalSpatial/CausalSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13304v1">PDF</a> · <a href="https://github.com/CausalSpatial/CausalSpatial">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer &quot;what-if&quot; questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CausalSpatial：面向对象因果空间推理的基准测试</div>
<div class="mono" style="margin-top:8px">人类可以观察静态场景并瞬间预测接下来会发生什么——移动这个物体是否会导致碰撞？我们称这种能力为因果空间推理。然而，当前的多模态大语言模型（MLLMs）无法做到这一点，因为它们主要局限于静态空间感知，难以回答三维场景中的“假设情景”问题。我们引入了CausalSpatial，这是一个诊断基准，通过四个任务（碰撞、兼容性、遮挡和轨迹）评估模型是否能够预测物体运动的后果。结果揭示了一个严重差距：人类得分84%，而GPT-5仅达到54%。为什么MLLMs会失败？我们的分析发现其根本缺陷在于：模型过度依赖文本链式推理，脱离了视觉证据，从而产生流畅但缺乏空间基础的幻觉。为了解决这一问题，我们提出了因果物体世界模型（COW），该框架通过生成假设动态的视频来外部化模拟过程。借助明确的因果视觉线索，COW使模型能够基于物理现实而非语言先验进行推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces CausalSpatial, a benchmark designed to evaluate the ability of models to reason about the consequences of object motions in 3D scenes, specifically focusing on four tasks: Collision, Compatibility, Occlusion, and Trajectory. Current multimodal large language models (MLLMs) struggle with these tasks due to their reliance on textual reasoning rather than visual grounding, leading to spatially ungrounded hallucinations. The study reveals a significant performance gap between humans (84%) and GPT-5 (54%), and proposes the Causal Object World model (COW) to simulate hypothetical dynamics through video generation, enabling more accurate spatial reasoning grounded in physical reality.</div>
<div class="mono" style="margin-top:8px">该研究提出了CausalSpatial基准，用于评估模型在物体中心因果空间推理方面的能力。动机源于观察到人类能够直观预测静态场景中物体移动后的结果，但当前的多模态大语言模型（MLLMs）在这方面表现不佳，尤其在碰撞预测等任务上。实验结果显示，人类表现达到84%，而GPT-5仅为54%，揭示了模型过度依赖文本推理而非视觉证据的问题。为此，作者提出了因果物体世界模型（COW），通过生成假设动态的视频来模拟，使模型能够基于物理现实而非语言先验进行推理。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-19T18:48:37+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能以找到共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力，才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的超过600个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队形成鲜明对比，因为在人类团队中，增加队友通常会提高生产力。我们的分析揭示了三个关键问题：(1) 通信渠道充斥着模糊、时机不当和不准确的信息；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of enabling AI agents to function effectively as teammates in collaborative coding tasks. It introduces CooperBench, a benchmark consisting of over 600 tasks across 12 libraries and four programming languages, designed to evaluate agents&#x27; coordination abilities. The results show that current coding agents perform significantly worse when working together than when completing tasks individually, with an average 30% drop in success rates. This highlights the &#x27;curse of coordination&#x27; and identifies three main issues: ineffective communication, lack of commitment adherence, and misaligned expectations. The research also observes some emergent coordination behaviors, such as role and resource division, and emphasizes the need for developing social intelligence in AI agents.</div>
<div class="mono" style="margin-top:8px">该研究旨在探讨AI代理在复杂任务中协作所需的社交智能，因为当前的编码代理在协调方面存在困难。为此，研究者引入了CooperBench，这是一个包含12个库、四种编程语言的600多个协作编码任务的基准测试集。实验结果表明，最先进的编码代理在协作执行任务时，成功率平均比单独执行任务低30%，揭示了‘协调的诅咒’。分析指出三个关键问题：沟通不畅、承诺不一致以及对他人计划的错误预期。此外，大规模模拟还观察到一些有趣的协调行为，如角色分工和协商。</div>
</details>
</div>
<div class="card">
<div class="title">KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</div>
<div class="meta-line">Authors: Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong</div>
<div class="meta-line">First: 2026-01-19T17:20:16+00:00 · Latest: 2026-01-19T17:20:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13240v1">PDF</a> · <a href="https://github.com/jiangxxxue/KOCO-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&amp;A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KOCO-BENCH：大型语言模型能否在软件开发中利用领域知识？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在通用编程方面表现出色，但在领域特定的软件开发中存在困难，因此需要领域专门化方法来使LLMs学习和利用领域知识和数据。然而，现有的领域特定代码基准无法评估领域专门化方法的有效性，这些基准侧重于评估LLMs已有的知识，而非其获取和应用新知识的方式，且缺乏用于开发领域专门化方法的显式知识语料库。为此，我们提出了KOCO-BENCH，这是一个新型基准，专门用于在现实世界软件开发中评估领域专门化方法。KOCO-BENCH包含6个新兴领域、11个软件框架和25个项目，配有精心整理的知识语料库，并包含多粒度评估任务，包括从函数级到项目级的领域代码生成（附有严格的测试套件）和领域知识理解（通过多项选择题问答）。与以往仅提供测试集用于直接评估的基准不同，KOCO-BENCH要求从知识语料库中获取并应用多样化的领域知识（如API、规则、约束等）以解决评估任务。我们的评估结果表明，KOCO-BENCH对最先进的LLMs提出了重大挑战。即使应用了领域专门化方法（如SFT、RAG、kNN-LM），改进仍然有限。表现最好的编码代理Claude Code仅达到34.2%，突显了对更有效的领域专门化方法的迫切需求。我们发布了KOCO-BENCH、评估代码和基线模型，以促进进一步研究，详见https://github.com/jiangxxxue/KOCO-bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces KOCO-BENCH, a benchmark designed to evaluate how well large language models (LLMs) can leverage domain knowledge in software development. Existing benchmarks fail to assess the effectiveness of domain specialization methods, which focus on acquiring and applying new knowledge rather than measuring pre-existing knowledge. KOCO-BENCH includes six domains, 11 frameworks, and 25 projects, with curated knowledge corpora and multi-granularity tasks such as code generation and knowledge understanding. Experimental results show that even with domain adaptation techniques like SFT, RAG, and kNN-LM, LLMs perform poorly, with the best-performing coding agent achieving only 34.2%.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大型语言模型（LLMs）在软件开发任务中是否能够有效利用领域知识。为此，作者提出了KOCO-BENCH，这是一个包含六个新兴领域、11个软件框架和25个项目的新基准，配有精心整理的知识语料库及多粒度评估任务。实验结果显示，即使应用了领域专业化技术如SFT、RAG和kNN-LM，当前最先进的LLM在任务上的表现仍有限，最佳编码代理Claude Code的准确率仅为34.2%，凸显了开发更有效领域适应方法的迫切需求。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-01-19T13:13:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v1">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即在二维感知之外解释几何、视角和空间关系的能力。尽管最近的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入了Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用三维重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上平均提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使模型能够选择信息量大的视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是实现多模态代理中更灵活、更接近人类的三维推理的一种可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of Think3D is to enhance spatial reasoning capabilities in vision-language models by enabling them to interact with 3D space. The framework integrates 3D reconstruction models to generate point clouds and camera poses from images or videos, allowing agents to manipulate space through camera-based operations and switch between ego and global views. This interactive 3D reasoning process significantly improves spatial performance on benchmarks like BLINK Multi-view and MindCube without additional training, achieving average gains of +7.8% and +4.7%, respectively. Furthermore, reinforcement learning policies enhance smaller models&#x27; ability to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升视觉大模型（VLMs）的空间推理能力，因为当前的VLMs主要依赖于二维感知，难以进行真正的三维推理。Think3D提出了一种框架，通过集成三维重建模型，从图像或视频中生成点云和相机姿态，使VLM代理能够通过基于相机的操作和视角切换来主动操控空间，将空间推理转化为交互式的三维推理过程。该框架在无需额外训练的情况下显著提升了先进模型如GPT-4.1和Gemini 2.5 Pro的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上提升4.7%。此外，通过强化学习策略，使小型模型能够选择更有信息量的视角，从而将工具使用带来的性能提升从0.7%提高到6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">Knot So Simple: A Minimalistic Environment for Spatial Reasoning</div>
<div class="meta-line">Authors: Zizhao Chen, Yoav Artzi</div>
<div class="meta-line">First: 2025-05-23T15:34:08+00:00 · Latest: 2026-01-18T19:17:38+00:00</div>
<div class="meta-line">Comments: Fix camera ready footer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18028v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.18028v3">PDF</a> · <a href="https://github.com/lil-lab/knotgym">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Knot So Simple：一个用于空间推理的极简环境</div>
<div class="mono" style="margin-top:8px">我们提出了KnotGym，一个用于复杂空间推理和操作的交互式环境。KnotGym包含一系列以目标为导向的绳子操作任务，复杂度各不相同，所有任务均基于纯图像观察进行操作。任务的复杂度沿一个清晰且可量化的轴线定义，基于绳结的交叉点数量，从而形成自然的泛化测试。KnotGym具有简单的观察空间，便于可扩展开发，同时突出了在整合敏锐感知、空间推理和基于环境的操作中面临的核心挑战。我们评估了不同类别的方法，包括基于模型的强化学习、模型预测控制和链式思维推理，并展示了KnotGym所呈现的挑战。KnotGym可在https://github.com/lil-lab/knotgym获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study introduces KnotGym, a minimalistic environment designed to evaluate spatial reasoning and manipulation capabilities through complex knot-tying tasks. The environment is built around a clear complexity metric based on the number of knot crossings, enabling a structured assessment of agents&#x27; abilities to perform goal-oriented rope manipulation using only image observations. Experimental results show that existing methods, including model-based reinforcement learning, model-predictive control, and chain-of-thought reasoning, face significant challenges in handling the intricate spatial and perceptual demands of these tasks.</div>
<div class="mono" style="margin-top:8px">该研究提出了KnotGym，一个用于评估复杂空间推理与操作的最小化环境。该环境通过基于绳结交叉数的清晰复杂度指标设计了一系列目标导向的任务，从而实现难度的自然递进。KnotGym仅使用图像观测，突出了感知、推理与操作在机器人任务中的整合挑战。论文评估了多种方法，包括基于模型的强化学习、模型预测控制和链式思维推理，展示了这些方法在处理复杂空间问题时所面临的困难。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jihie Kim, Jean Oh</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-18T11:03:44+00:00</div>
<div class="meta-line">Comments: Corrected author order in metadata; manuscript unchanged</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未找到安全（404）：基于大语言模型的机器人决策中的隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键环境中，人工智能系统的一个错误可能导致生命损失。随着大语言模型（LLMs）在机器人决策中的应用日益广泛，风险的物理维度也在扩大；一个错误的指令可能直接危及人类安全。本文旨在系统评估LLMs在即使微小错误也可能导致灾难的场景中的表现。通过一个火灾疏散场景的定性评估，我们识别了基于LLM的决策中的关键失败案例。基于这些案例，我们设计了七个任务用于定量评估，分为：完整信息任务、不完整信息任务和安全导向空间推理（SOSR）任务。完整信息任务使用ASCII地图以减少解释歧义，并将空间推理与视觉处理隔离。不完整信息任务要求模型推断缺失的上下文，测试其空间连续性与幻觉之间的区别。SOSR任务使用自然语言评估在生命威胁情境下的安全决策能力。我们对各种LLMs和视觉-语言模型（VLMs）在这些任务上的表现进行了基准测试。除了整体表现外，我们还分析了1%失败率的潜在影响，强调&quot;罕见&quot;错误如何演变为灾难性后果。结果揭示了严重的漏洞：一些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，模型指示机器人向危险区域移动而非紧急出口。我们的研究得出令人深思的结论：当前的LLMs尚未准备好直接部署在安全关键系统中。在机器人领域，99%的准确率是危险的误导，因为它意味着每100次执行中可能有一次导致灾难性伤害。我们证明了即使是最先进的模型也无法保证安全，完全依赖它们会带来不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the risks associated with using Large Language Models (LLMs) in safety-critical robotics decision-making, emphasizing that even minor errors can lead to severe consequences. The authors designed seven tasks to evaluate LLMs and Vision-Language Models (VLMs) in different scenarios, including Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). The results show that several models failed completely in ASCII navigation tasks, and in a simulated fire drill, some models directed robots toward hazardous areas instead of emergency exits, highlighting the dangers of relying on LLMs for real-world safety applications.</div>
<div class="mono" style="margin-top:8px">本文探讨了在安全关键场景中使用大型语言模型（LLMs）进行机器人决策所存在的潜在风险，特别是在轻微错误可能导致严重后果的情况下。作者设计了七个任务来评估LLMs和视觉语言模型（VLMs）的可靠性，包括完整信息、不完整信息以及安全导向空间推理（SOSR）任务，以测试其在不同情境下的表现。实验结果表明，一些模型在ASCII导航任务中完全失败，而在模拟火灾演练中，部分模型错误地指示机器人向危险区域移动而非安全出口，揭示了即使1%的错误率也可能导致灾难性后果的严重隐患。</div>
</details>
</div>
<div class="card">
<div class="title">CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks</div>
<div class="meta-line">Authors: Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, Xiangyu Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-07-03T01:35:58+00:00 · Latest: 2026-01-17T19:21:17+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Datasets &amp; Benchmarks Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05269v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.05269v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models&#x27; ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs&#x27; code reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoRe: 通过静态分析任务对LLM代码推理能力进行基准测试</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已在软件工程的多个领域得到广泛应用，如代码生成、程序修复和漏洞检测。这些应用需要理解超越表面代码模式的深层次内容：值传播、控制流以及程序元素之间的相互依赖。然而，现有的基准测试主要评估端到端结果，例如代码是否被正确修复或生成，而忽略了模型在程序语义推理方面的能力。本文提出了CORE，一个高质量、经过人工验证的基准测试，旨在评估LLMs在基本静态分析任务上的表现。CORE包含12,553个任务实例，涵盖C/C++、Java和Python编写的程序中的数据依赖、控制依赖和信息流。为确保语义多样性和推理复杂性，我们提出了一种语义感知的多样化采样策略，基于结构覆盖和依赖深度选择目标和任务实例。我们评估了10种主流LLMs，并发现尽管它们在识别依赖关系方面表现良好，但在需要更深层次语义理解和多步骤推理的任务上仍存在困难。我们进一步进行了定性分析，揭示了诸如复杂控制结构和反向依赖模式等关键挑战，为提升LLMs的代码推理能力提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces CoRe, a benchmark designed to assess the code reasoning capabilities of large language models (LLMs) by focusing on static analysis tasks that require understanding of value propagation, control flow, and interdependence between program elements. The benchmark includes 12,553 task instances across C/C++, Java, and Python, and employs a semantics-aware sampling strategy to ensure diversity and complexity in the tasks. Evaluation of ten mainstream LLMs shows that while they perform well in identifying basic dependencies, they struggle with more complex semantic reasoning and multi-step tasks, highlighting key challenges such as handling backward dependencies and intricate control structures.</div>
<div class="mono" style="margin-top:8px">本文提出了CoRe基准，旨在通过静态分析任务评估大型语言模型（LLMs）的代码推理能力，这些任务需要理解值传播、控制流和程序元素间的依赖关系。该基准涵盖C/C++、Java和Python共12,553个任务实例，并采用语义感知的多样化采样策略以确保任务的多样性和复杂性。对十种主流LLMs的评估表明，尽管它们在识别基本依赖关系方面表现良好，但在需要深入语义理解和多步推理的任务中仍存在显著困难。</div>
</details>
</div>
<div class="card">
<div class="title">A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation</div>
<div class="meta-line">Authors: Yifei Sun, Yongan Li, A. K. Qin, Sicheng Hou, Tamas Pflanzner</div>
<div class="meta-line">First: 2026-01-16T21:36:04+00:00 · Latest: 2026-01-16T21:36:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11792v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种具有细粒度难度指导的自进化多角色协作框架用于创新数学问题生成</div>
<div class="mono" style="margin-top:8px">数学问题生成（MPG）是智能教育领域的重要研究方向。近年来，大语言模型（LLMs）的快速发展为问题生成任务带来了新的技术方法。尽管现有LLMs可以实现较高的正确率，但通常缺乏创新性且判别能力较差。本文提出创新数学问题生成（IMPG）任务。为了解决IMPG任务，本文提出了一种具有细粒度难度指导的自进化多角色协作框架。首先，构建了一个包含采样器、生成器、评估器、状态机和记忆模块的多角色协作机制，通过自评估和外部反馈的迭代优化确保生成问题的正确性。其次，引入了改进的难度模型以量化难度并提供细粒度指导。我们采用数据驱动的关联引导路径采样（DAPS）算法来增强采样编码的语义合理性。第三，我们构建了HSM3K-CN数据集，包含高质量的高中数学问题。采用多阶段训练流程，包括持续预训练（CPT）、监督微调（SFT）和组相对策略优化（GRPO），以提升基础模型的生成和评估能力。最后，通过蒸馏将专家模型的评估能力转移到学徒模型中，实现系统的自进化。实验表明，与基线模型相比，我们提出的方法在保持高正确率的同时显著提升了生成问题的创新性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing large language models in generating innovative and discriminative mathematical problems. The authors propose a self-evolving, multi-role collaborative framework that integrates a sampler, generator, evaluator, state machine, and memory to iteratively optimize problem generation. They also introduce an improved difficulty model combined with the data-driven association-guided path sampling (DAPS) algorithm to enhance semantic rationality. The HSM3K-CN dataset, consisting of high-quality high school math problems, is used for training with a multi-stage pipeline including continual pre-training, supervised fine-tuning, and group relative policy optimization. The method achieves system self-evolution through knowledge distillation from an expert model to an apprentice model, resulting in significantly improved innovation while maintaining high correctness rates in generated problems.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有大语言模型在生成创新性数学问题时缺乏创造力和判别能力的问题。作者提出了一种自进化、多角色协作框架，包含采样器、生成器、评估器、状态机和记忆模块，通过自我评估和外部反馈实现问题生成的迭代优化。他们引入了改进的难度模型和数据驱动的关联引导路径采样（DAPS）算法以增强语义合理性。构建了HSM3K-CN数据集，并采用包含持续预训练（CPT）、监督微调（SFT）和群体相对策略优化（GRPO）的多阶段训练流程。实验结果表明，该方法在保持高正确率的同时，显著提升了生成问题的创新性。</div>
</details>
</div>
<div class="card">
<div class="title">SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models</div>
<div class="meta-line">Authors: Turhan Can Kargin, Wojciech Jasiński, Adam Pardyl, Bartosz Zieliński, Marcin Przewięźlikowski</div>
<div class="meta-line">First: 2026-01-16T19:21:02+00:00 · Latest: 2026-01-16T19:21:02+00:00</div>
<div class="meta-line">Comments: Project page is available at https://sparrta.gmum.net/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpaRRTa：用于评估视觉基础模型空间智能的合成基准</div>
<div class="mono" style="margin-top:8px">视觉基础模型（VFMs），如DINO和CLIP，在图像语义理解方面表现出色，但其空间推理能力有限，这限制了它们在具身系统中的应用。因此，近期研究将一些3D任务（如深度估计）纳入VFM的训练中。然而，VFM在其他空间任务中的表现仍不一致，引发了一个问题：这些模型是否真正具备空间感知能力，还是仅仅过拟合了特定的3D目标。为了解决这一问题，我们引入了空间关系识别任务（SpaRRTa）基准，用于评估VFMs识别图像中物体相对位置的能力。与传统的关注精确度量预测（如表面法线估计）的3D目标不同，SpaRRTa探索了支撑更高级类人空间理解的基本能力。SpaRRTa生成具有多样场景和完全可控物体布局的任意数量的逼真图像，并提供可自由访问的空间标注。通过评估一系列最先进的VFMs，我们揭示了它们在空间推理能力上的显著差异。通过我们的分析，我们提供了关于现代VFMs中支持或阻碍空间感知机制的见解。我们希望SpaRRTa能成为指导未来空间感知视觉模型开发的有用工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SpaRRTa, a synthetic benchmark designed to assess spatial intelligence in Visual Foundation Models (VFMs). The motivation stems from the observation that while VFMs like DINO and CLIP are strong in semantic understanding, they lack robust spatial reasoning abilities, limiting their use in embodied systems. SpaRRTa evaluates the ability of VFMs to recognize relative object positions in images, differing from traditional 3D tasks that focus on metric predictions. The benchmark generates photorealistic images with controllable object arrangements and accessible spatial annotations, revealing significant variations in spatial reasoning performance among state-of-the-art models.</div>
<div class="mono" style="margin-top:8px">本研究提出了SpaRRTa，这是一个用于评估视觉基础模型（VFMs）空间智能的合成基准。研究动机源于观察到尽管像DINO和CLIP这样的VFMs在语义理解方面表现出色，但它们在空间推理能力上存在不足，限制了其在具身系统中的应用。SpaRRTa通过评估VFMs识别图像中物体相对位置的能力，区别于传统的侧重于精确度量预测的3D任务。该基准生成具有多样场景和可控物体布局的逼真图像，并附带可自由访问的空间标注。实验结果表明，不同先进视觉模型在空间推理能力上存在显著差异，揭示了当前模型在空间感知方面的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</div>
<div class="meta-line">Authors: Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo Pérez-Pellitero, Youngkyoon Jang</div>
<div class="meta-line">First: 2026-01-16T17:02:46+00:00 · Latest: 2026-01-16T17:02:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11442v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Map2Thought：通过度量认知地图实现显式的3D空间推理</div>
<div class="mono" style="margin-top:8px">我们提出了Map2Thought框架，该框架使3D视觉语言模型能够进行显式且可解释的空间推理。该框架基于两个关键组件：度量认知地图（Metric-CogMap）和认知链式推理（Cog-CoT）。Metric-CogMap通过将关系推理的离散网格与精确几何理解的连续度量表示相结合，提供统一的空间表示。在Metric-CogMap的基础上，Cog-CoT通过确定性操作进行显式的几何推理，包括向量运算、边界框距离和遮挡感知的外观顺序提示，从而生成基于3D结构的可解释推理轨迹。实验结果表明，Map2Thought实现了可解释的3D理解，在仅使用一半监督数据的情况下达到了59.9%的准确率，接近使用完整数据集训练的60.9%基线。在VSI-Bench数据集上，它在10%、25%和50%的训练子集下分别优于最先进的方法5.3%、4.8%和4.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the interpretability and efficiency of spatial reasoning in 3D Vision-Language Models (VLMs). Map2Thought introduces a framework combining Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT) to enable explicit 3D understanding. Metric-CogMap integrates discrete relational grids with continuous metric representations, while Cog-CoT applies deterministic geometric operations to generate interpretable reasoning traces. The framework achieves 59.9% accuracy with half the supervision, closely matching the 60.9% baseline, and outperforms existing methods by 5.3%, 4.8%, and 4.0% on 10%, 25%, and 50% training subsets of VSI-Bench.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D视觉语言模型（VLMs）中空间推理的可解释性和效率。Map2Thought提出了一种结合度量认知图（Metric-CogMap）和认知链式推理（Cog-CoT）的框架，以实现显式的3D理解。Metric-CogMap通过将离散的关系网格与连续的度量尺度几何表示相结合，统一了空间表示。Cog-CoT则通过向量运算、边界框距离和遮挡感知的外观顺序等确定性操作进行几何推理，生成可解释的推理轨迹。实验结果表明，Map2Thought在仅使用一半监督数据的情况下达到59.9%的准确率，接近60.9%的基线表现，并在VSI-Bench的10%、25%和50%训练子集上分别优于现有方法5.3%、4.8%和4.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation</div>
<div class="meta-line">Authors: Joschka Kersting, Michael Rummel, Gesa Benndorf</div>
<div class="meta-line">First: 2025-11-12T08:56:11+00:00 · Latest: 2026-01-16T14:53:55+00:00</div>
<div class="meta-line">Comments: ICIT2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09122v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09122v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向供应商的工业代理：基于RAG增强的LLM用于安全的本地PLC代码生成</div>
<div class="mono" style="margin-top:8px">可编程逻辑控制器使用专有代码方言进行操作，这使得训练代码助手变得具有挑战性。当前的LLM是在大规模代码数据集上训练的，能够直接生成符合IEC 61131-3标准的代码，但它们并不了解特定的功能块或相关的项目代码。此外，像三菱电机这样的公司及其客户并不信任云服务提供商。因此，拥有自己的代码代理是解决这一问题的首选方案。在本研究中，我们提出了一种适用于工业领域的低数据域代码助手解决方案。我们展示了如何在不微调大型模型的情况下，通过微调小型本地模型用于边缘设备，实现高质量的代码生成。我们的工具允许多个AI模型相互竞争，使用推理进行自动纠错，并通过在聊天界面中直接编译代码来验证代码的有效性。我们通过详尽的评估支持我们的方法，包括代码编译统计数据和用户评分。我们发现，通过广泛的提示工程和定向检索，基于检索增强生成（RAG）的代码助手可以在低数据域中有效运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of generating secure and accurate PLC code for industrial applications using proprietary code dialects, which are not well-supported by existing large language models. The proposed method employs a Retrieval-Augmented Generation (RAG) approach, combining extensive prompt engineering with directed retrieval to enable high-quality code generation without requiring large-scale fine-tuning. The main experimental results demonstrate that this vendor-aware coding assistant can produce valid IEC 61131-3 compatible code and effectively handle specific function blocks and project contexts, even in low-data scenarios, through competitive AI model interactions and automatic bug correction.</div>
<div class="mono" style="margin-top:8px">本研究针对工业应用中使用专有代码方言的可编程逻辑控制器（PLC）训练编码助手的挑战。作者提出了一种基于检索增强生成（RAG）的供应商感知编码助手，能够在不进行大规模模型微调的情况下实现安全的本地代码生成。通过使用小型本地模型和详尽的提示工程，该工具支持多个AI模型协作、推理和自动修复错误，并在聊天界面中直接编译代码以验证其有效性。评估结果显示，该方法在低数据工业编码环境中能够生成高质量代码，获得用户好评，并具有良好的编译统计数据。</div>
</details>
</div>
<div class="card">
<div class="title">OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding</div>
<div class="meta-line">Authors: Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui</div>
<div class="meta-line">First: 2026-01-15T12:36:08+00:00 · Latest: 2026-01-16T02:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10343v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OctoBench：基于仓库的智能编码中对支架感知指令遵循的基准测试</div>
<div class="mono" style="margin-top:8px">现代编码支架使LLMs成为有能力的软件代理，但它们遵循支架指定指令的能力仍被低估，尤其是在约束条件异构且持续跨交互的情况下。为填补这一空白，我们引入了OctoBench，用于评估基于仓库的智能编码中对支架感知指令遵循的能力。OctoBench包含34个环境和217个任务实例，涵盖三种支架类型，并配有7,098个目标检查清单项。为了将任务解决与规则遵循分离，我们提供了一个自动化的观察与评分工具包，用于捕捉完整交互轨迹并进行细粒度检查。在八个代表性模型上的实验揭示了任务解决与支架感知合规之间存在系统性差距，强调了需要专门针对异构指令遵循进行训练和评估的重要性。我们发布该基准测试以支持可重复的基准测试，并加速开发更加支架感知的编码代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the ability of large language models (LLMs) to follow scaffold-specified instructions in repository-grounded agentic coding, particularly under heterogeneous constraints. The authors introduce OctoBench, a benchmark that includes 34 environments and 217 tasks across three scaffold types, along with 7,098 objective checklist items. They also provide an automated observation-and-scoring toolkit to capture full interaction trajectories and perform fine-grained compliance checks. Experimental results on eight representative models show a systematic gap between task-solving performance and scaffold-aware instruction following, highlighting the need for more targeted training and evaluation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大型语言模型（LLMs）在基于仓库的代理编码中遵循支架指定指令的能力，特别是在异构约束条件下。作者提出了OctoBench基准，包含34个环境和217个任务，涵盖三种支架类型，并配有7098个目标检查项。他们还开发了一个自动化观察与评分工具包，用于捕捉完整的交互轨迹并进行细粒度的合规性检查。在八个代表性模型上的实验结果显示，任务解决能力和支架感知指令遵循之间存在系统性差距，强调了需要更针对性的训练和评估方法。</div>
</details>
</div>
<div class="card">
<div class="title">MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models</div>
<div class="meta-line">Authors: Muhammad Imran, Chi Lee, Yugyung Lee</div>
<div class="meta-line">First: 2026-01-16T01:18:02+00:00 · Latest: 2026-01-16T01:18:02+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11666v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX&#x27;s potential to enhance trust and transparency in radiological AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MATEX：面向医学视觉-语言模型的多尺度注意力与文本引导可解释性</div>
<div class="mono" style="margin-top:8px">我们引入了MATEX（多尺度注意力与文本引导可解释性），这是一种新颖的框架，通过结合解剖学引导的空间推理来提升医学视觉-语言模型的可解释性。MATEX通过多层注意力展开、文本引导的空间先验以及层间一致性分析，生成精确、稳定且具有临床意义的梯度归因图。通过解决现有方法的关键局限性，如空间不精确、缺乏解剖学基础以及注意力粒度有限等问题，MATEX能够提供更忠实且可解释的模型解释。在MS-CXR数据集上的评估表明，MATEX在空间精度和与专家标注结果的一致性方面均优于最先进的M2IB方法。这些结果突显了MATEX在增强放射学AI应用中的信任和透明度方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of MATEX is to improve the interpretability of medical vision-language models by addressing limitations such as spatial imprecision and lack of anatomical grounding. The framework integrates multi-scale attention mechanisms, text-guided spatial priors, and layer consistency analysis to generate accurate and clinically relevant gradient attribution maps. Experimental results on the MS-CXR dataset demonstrate that MATEX achieves superior spatial precision and alignment with expert annotations compared to the state-of-the-art M2IB approach, thereby enhancing the trustworthiness and transparency of radiological AI systems.</div>
<div class="mono" style="margin-top:8px">MATEX的提出旨在提升医学视觉语言模型的可解释性，解决其在空间精度、解剖学基础和注意力粒度方面的不足。该框架结合多层注意力展开、文本引导的空间先验和层一致性分析，生成精确且具有临床意义的梯度归因图。在MS-CXR数据集上的实验结果表明，MATEX在空间精度和与专家标注的一致性方面均优于当前最先进的M2IB方法，从而增强了放射学AI应用的可信度和透明度。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
