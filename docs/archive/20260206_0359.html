<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 03:59</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0359</div>
    <div class="row"><div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：通过视觉语言模型将任务规划接地为空间感知的自中心动作以实现人形机器人应用</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人具有根本性挑战，因为这需要在部分信息观测和动态变化环境中紧密集成感知、移动和操作。此外，还需在不同类型子任务之间稳健地进行转换。为应对这些挑战，我们提出了一种新的任务——EgoActing，该任务要求将高层指令直接接地为各种精确且具有空间感知的人形动作。我们进一步通过引入EgoActor，一个统一且可扩展的视觉语言模型（VLM），来实例化该任务。EgoActor能够预测移动基元（如行走、转向、侧移、高度变化）、头部运动、操作指令和人机交互，从而实时协调感知与执行。我们利用来自现实世界演示的广泛监督，结合空间推理问答和模拟环境演示，使EgoActor能够做出稳健且上下文感知的决策，并在1秒内进行流畅的动作推理，适用于8B和4B参数模型。在模拟和现实环境中的大量评估表明，EgoActor有效连接了抽象的任务规划和具体的运动执行，并能在多样任务和未见过的环境中进行泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enable humanoid robots to perform complex task planning in real-world environments with limited perceptual information and dynamic changes. To achieve this, the authors introduce EgoActing, a task that directly maps high-level instructions to spatially aware actions such as locomotion, head movement, and manipulation. They propose EgoActor, a unified vision-language model capable of predicting these actions using RGB-only egocentric data from real-world and simulated demonstrations, as well as spatial reasoning tasks. Experimental results show that EgoActor achieves robust and context-aware decision-making, with fast action inference under 1 second, and successfully bridges abstract planning with concrete execution across diverse tasks and environments.</div>
<div class="mono" style="margin-top:8px">人形机器人在现实环境中的部署面临巨大挑战，因为需要在部分信息观察和动态变化的环境中整合感知、移动和操作。为此，本文提出EgoActing任务，旨在将高层指令直接映射到具有空间感知的人形动作。所提出的方法EgoActor是一个统一的视觉-语言模型，能够预测多种移动原语、头部运动、操作指令以及人机交互行为。它通过现实世界演示、空间推理问答和模拟环境数据进行广泛训练，从而实现上下文感知的决策和快速动作推理。实验结果表明，EgoActor能够有效连接抽象任务规划与具体运动执行，并在多种任务和未见过的环境中表现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-02-04T12:38:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v2">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即超越二维感知，解释几何、视角和空间关系的能力。尽管近期的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用从图像或视频中恢复点云和相机姿态的三维重建模型，Think3D使代理能够通过基于相机的操作和自主/全局视角切换，主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上平均提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使模型能够选择信息性视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是一种实现多模态代理中更灵活、更接近人类的三维推理的可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance spatial reasoning capabilities in vision large models (VLMs), which are currently limited to 2D perception and struggle with genuine 3D understanding. Think3D introduces a framework that enables VLM agents to interact with 3D space by integrating 3D reconstruction models to generate point clouds and camera poses from images or videos. This allows agents to perform camera-based operations and switch between ego and global views, simulating a 3D chain-of-thought process. The framework significantly improves spatial reasoning performance without additional training, achieving average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. Furthermore, reinforcement learning policies enhance smaller models&#x27; ability to select informative viewpoints, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">Think3D的动机是通过让视觉语言模型与三维空间互动来提升其空间推理能力。该框架结合了三维重建模型，从图像或视频中生成点云和相机姿态，使智能体能够通过基于相机的操作和视角切换来主动操控空间。这种交互式的三维推理过程在无需额外训练的情况下显著提升了模型在BLINK Multi-view和MindCube等基准测试中的表现，平均提升分别为+7.8%和+4.7%。此外，通过强化学习策略，较小的模型在选择信息性视角方面获益显著，工具使用带来的性能提升从+0.7%提高到+6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</div>
<div class="meta-line">Authors: Mingcong Lei, Honghao Cai, Yuyuan Yang, Yimou Wu, Jinke Ren, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Shaohan Jiang, Ge Wang, Junyuan Tan, Zhenglin Wan, Zheng Li, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han</div>
<div class="meta-line">First: 2025-08-02T15:39:42+00:00 · Latest: 2026-02-04T12:10:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01415v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.01415v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied intelligence aims to enable robots to learn, reason, and generalize robustly across complex real-world environments. However, existing approaches often struggle with partial observability, fragmented spatial reasoning, and inefficient integration of heterogeneous memories, limiting their capacity for long-horizon adaptation. To address this, we introduce RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory within a parallelized architecture for efficient long-horizon planning and interactive learning. Its core innovations are a dynamic spatial knowledge graph for scalable, consistent memory updates and a closed-loop planner with a critic module for adaptive decision-making. Extensive experiments on EmbodiedBench show that RoboMemory, instantiated with Qwen2.5-VL-72B-Ins, improves the average success rate by 26.5% over its strong baseline and even surpasses the closed-source SOTA, Claude-3.5-Sonnet. Real-world trials further confirm its capability for cumulative learning, with performance consistently improving over repeated tasks. Our results position RoboMemory as a scalable foundation for memory-augmented embodied agents, bridging insights from cognitive neuroscience with practical robotic autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMemory：一种受大脑启发的多记忆代理框架，用于物理具身系统中的交互式环境学习</div>
<div class="mono" style="margin-top:8px">具身智能旨在使机器人能够在复杂的真实世界环境中学习、推理并稳健地泛化。然而，现有方法常面临部分可观测性、碎片化空间推理以及异质记忆整合效率低等问题，限制了其长时序适应能力。为了解决这些问题，我们引入了RoboMemory，这是一种受大脑启发的框架，通过并行化架构将空间、时间、事件和语义记忆统一起来，以实现高效的长时序规划和交互式学习。其核心创新包括用于可扩展且一致记忆更新的动态空间知识图谱，以及带有批评模块的闭环规划器，以实现自适应决策。在EmbodiedBench上的大量实验表明，RoboMemory在Qwen2.5-VL-72B-Ins实例化后，其平均成功率比其强基线提高了26.5%，甚至超越了闭源的SOTA模型Claude-3.5-Sonnet。现实世界试验进一步验证了其累积学习能力，性能在重复任务中持续提升。我们的结果表明，RoboMemory为记忆增强的具身代理提供了一个可扩展的基础，将认知神经科学的见解与实际的机器人自主性相结合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboMemory is introduced to enhance embodied intelligence by addressing challenges such as partial observability, fragmented spatial reasoning, and inefficient memory integration in physical robots. The framework integrates spatial, temporal, episodic, and semantic memory into a parallelized architecture, featuring a dynamic spatial knowledge graph for scalable memory updates and a closed-loop planner with a critic module for adaptive decision-making. Experimental results on EmbodiedBench demonstrate that RoboMemory, when implemented with Qwen2.5-VL-72B-Ins, achieves a 26.5% improvement in average success rate compared to a strong baseline and outperforms the closed-source state-of-the-art model, Claude-3.5-Sonnet. Real-world trials further validate its ability to support cumulative learning through repeated task execution.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决部分可观测性、空间推理碎片化以及异质记忆整合效率低等问题，提升机器人在复杂现实环境中的适应能力和学习能力。RoboMemory是一个受大脑启发的多记忆代理框架，将空间、时间、事件和语义记忆统一在一个并行架构中，以支持高效的长时规划和交互式学习。该框架引入了动态空间知识图，实现可扩展且一致的记忆更新，并采用带有批评模块的闭环规划器进行自适应决策。在EmbodiedBench上的实验表明，当使用Qwen2.5-VL-72B-Ins实现时，RoboMemory在平均成功率上比强基线模型提升了26.5%，并超越了闭源的最先进模型Claude-3.5-Sonnet。实际应用测试进一步验证了其累积学习的能力，性能在重复任务中持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">MapCoder-Lite: Distilling Multi-Agent Coding into a Single Small LLM</div>
<div class="meta-line">Authors: Woongkyu Lee, Junhee Cho, Jungwook Choi</div>
<div class="meta-line">First: 2025-09-22T08:19:11+00:00 · Latest: 2026-02-04T07:25:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17489v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17489v2">PDF</a> · <a href="https://github.com/aiha-lab/MapCoder-Lite">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale (&gt;30B) models or collapse when downsized to small open-source models. We present MapCoder-Lite, a framework for distilling the complex reasoning of large, multi-agent coding systems into a single 7B model. Our contribution is a novel, three-pillar methodology that synergistically generates, refines, and encodes multi-agent knowledge: (i) pass-based trajectory distillation from strong LLMs fixes format fragility in retrieval and reduces failures in debugging, (ii) supervisor-guided correction with global feedback strengthens planning and coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests shows that MapCoder-Lite more than doubles xCodeEval accuracy (from 13.2% to 28.3%), eliminates all format failures, while reducing GPU memory and token-generation time by 4x compared to a 32B model. It also achieves over 10% gains on simpler coding benchmarks, demonstrating broad improvements beyond competitive programming. These results demonstrate that careful agent-wise fine-tuning unleashes high-quality multi-agent coding on a small language model. Our code is publicly available at https://github.com/aiha-lab/MapCoder-Lite.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapCoder-Lite：将多智能体编码知识蒸馏到单一小型LLM中</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已经从单功能任务扩展到能够处理编程竞赛问题的代码生成，但现有的多智能体解决方案要么依赖昂贵的超大规模（&gt;30B）模型，要么在缩小到小型开源模型时失效。我们提出了MapCoder-Lite，一个将大型多智能体编码系统的复杂推理蒸馏到单一7B模型的框架。我们的贡献是一种新颖的三支柱方法，协同生成、优化和编码多智能体知识：(i) 基于传递的轨迹蒸馏从强LLM中修复检索中的格式脆弱性并减少调试失败；(ii) 带有全局反馈的监督引导修正增强了规划和编码智能体；(iii) 智能体级的LoRA微调实现了内存高效的专门化。在xCodeEval、APPS和CodeContests上的全面评估表明，MapCoder-Lite的xCodeEval准确率超过原有水平两倍以上（从13.2%提升至28.3%），消除了所有格式错误，同时相比32B模型减少了4倍的GPU内存和token生成时间。它还在更简单的编码基准测试中实现了超过10%的性能提升，展示了其在编程竞赛之外的广泛改进。这些结果表明，仔细的智能体级微调可以在小型语言模型上释放高质量的多智能体编码能力。我们的代码可在https://github.com/aiha-lab/MapCoder-Lite上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MapCoder-Lite is to enable high-quality multi-agent coding capabilities within a single small language model, overcoming the limitations of existing approaches that either require expensive large-scale models or fail when scaled down. The framework employs a three-pillar methodology: pass-based trajectory distillation from strong LLMs to address format issues, supervisor-guided correction with global feedback to enhance planning and coding agents, and agent-wise LoRA fine-tuning for memory-efficient specialization. Experimental results on xCodeEval, APPS, and CodeContests show that MapCoder-Lite significantly improves performance, achieving over 28% accuracy on xCodeEval, eliminating all format failures, and reducing GPU memory and token-generation time by 4x compared to a 32B model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是实现小语言模型上的高质量多智能体编程能力，克服现有解决方案需要昂贵大模型或在缩小规模后失效的问题。MapCoder-Lite提出了一种三支柱方法：从强大LLM中进行基于轮次的轨迹蒸馏以解决格式脆弱性和调试失败，通过监督者引导的全局反馈增强规划和编码智能体，以及基于智能体的LoRA微调以实现内存高效的专门化。在xCodeEval、APPS和CodeContests上的实验结果表明，MapCoder-Lite在xCodeEval上准确率超过28%，消除了所有格式错误，并将GPU内存和生成令牌时间减少了4倍，相比32B模型有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization</div>
<div class="meta-line">Authors: Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing</div>
<div class="meta-line">First: 2025-09-15T20:36:19+00:00 · Latest: 2026-02-04T05:16:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12434v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.12434v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues. A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs. While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS. Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents. To bridge this gap, we introduce EntroPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting. EntroPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses. We validate EntroPO by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches. On the SWEBENCH leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with EntroPO ranks 1st on SWEBENCH-LITE and 4th on SWEBENCH-VERIFIED on the open-weight leaderboard, surpassed only by models with over 10x more parameters(e.g., &gt;$350B).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过熵增强的多轮偏好优化构建编码代理</div>
<div class="mono" style="margin-top:8px">软件工程为大型语言模型（LLMs）提出了复杂且多步骤的挑战，需要在大规模代码库上进行推理并协调使用工具。这些任务的难度在诸如SWE-bench等基准测试中得到了体现，当前LLMs仍难以解决现实问题。一种有前景的方法是测试时扩展（TTS），但其效果高度依赖于模型输出的多样性。虽然标准对齐方法如直接偏好优化（DPO）和卡尼曼-特弗斯基优化（KTO）在将模型输出与人类偏好对齐方面有效，但这一过程可能会降低多样性，从而限制TTS的效果。此外，现有的偏好优化算法通常设计用于单轮任务，无法充分应对交互式编码代理所需的多轮推理和工具集成的复杂性。为弥合这一差距，我们引入了EntroPO，这是一种熵增强框架，将现有的偏好优化算法适应到多轮、工具辅助的环境中。EntroPO通过显式保留策略熵来增强偏好目标，并将学习过程推广到多轮交互的优化，而非单轮响应。我们通过微调来自不同家族和规模（最高达106B参数）的多样化模型来验证EntroPO。为了最大化TTS带来的性能提升，我们进一步提出了一种混合的最佳轨迹选择方案，结合了学习的验证器模型与无模型方法。在SWEBENCH排行榜上，我们的方法在开放权重模型中取得了新的最先进结果。使用EntroPO训练的30B参数模型在开放权重排行榜上分别位列SWEBENCH-LITE第一和SWEBENCH-VERIFIED第四，仅被参数量超过其10倍的模型（例如，&gt;350B参数）超越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of improving Large Language Models&#x27; (LLMs) performance in complex software engineering tasks that require multi-step reasoning and tool coordination. It proposes EntroPO, an entropy-enhanced framework that modifies existing preference optimization methods to support multi-turn interactions, thereby preserving diversity in model outputs and enhancing test-time scaling effectiveness. Experimental results on the SWEBENCH benchmark demonstrate that EntroPO achieves state-of-the-art performance among open-weight models, with a 30B parameter model ranking first on SWEBENCH-LITE and fourth on SWEBENCH-VERIFIED, outperforming many other models despite having fewer parameters than top-performing ones.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型语言模型（LLMs）在需要多步骤推理和工具协作的复杂软件工程任务中的性能问题。作者提出了EntroPO框架，通过增强熵值来改进现有偏好优化方法，使其适用于多轮交互场景，从而在保持策略多样性的同时提升测试时扩展效果。在SWEBENCH基准测试中，实验结果表明EntroPO在开放权重模型中取得了最先进的性能，其中30B参数的模型在SWEBENCH-LITE上排名第一，在SWEBENCH-VERIFIED上排名第四，优于其他开放模型，但不及参数量超过其10倍的模型。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-02-04T04:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot;-a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal audio scene analysis (ASA). To bridge this gap, we first introduce a hierarchical framework for audio scene analysis. Guided by this framework, we introduce a system that enables large audio-language models (LALMs) to understand and reason about the complex acoustic world.
  Our system endows LALMs with universal spatial understanding through four key innovations: (1) A scalable simulation pipeline that synthesizes high-quality First-Order-Ambisonics(FOA) data; (2) A unified model framework that integrates universal spatial encoding with a dense hybrid projection mechanism to bridge the modality gap; (3) A progressive training curriculum that evolves from representation alignment to reinforcement learning-based reasoning; and (4) A comprehensive benchmark for audio scene analysis (ASA) designed to rigorously evaluate atomic perception, relational integration, and cognitive reasoning capabilities, on which our model demonstrates comparatively strong capability for spatial understanding. Our work provides a clear pathway for leveraging the powerful reasoning abilities of LALMs towards holistic ASA, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：在大型音频语言模型中实现空间理解</div>
<div class="mono" style="margin-top:8px">现有的大型音频语言模型将世界视为&quot;单声道&quot;，即忽略关键的空间维度（&quot;在哪里&quot;）的单一音频流，这无法满足通用音频场景分析（ASA）的需求。为弥合这一差距，我们首先提出了一种分层的音频场景分析框架。基于该框架，我们引入了一种系统，使大型音频语言模型（LALMs）能够理解和推理复杂的声学世界。
我们的系统通过四项关键创新赋予LALMs普遍的空间理解能力：(1) 一个可扩展的模拟管道，用于合成高质量的首阶全向声学（FOA）数据；(2) 一个统一的模型框架，将普遍空间编码与密集混合投影机制结合，以弥合模态间的差距；(3) 一种渐进式训练课程，从表示对齐逐步发展到基于强化学习的推理；(4) 一个全面的音频场景分析（ASA）基准测试，旨在严格评估基本感知、关系整合和认知推理能力，我们的模型在该基准上表现出较强的对空间的理解能力。我们的工作为利用LALMs强大的推理能力实现全面的ASA提供了清晰的路径，从&quot;单声道&quot;语义识别推进到空间智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing large audio-language models (LALMs) that perceive the world as a single audio stream, neglecting the spatial dimension essential for universal audio scene analysis. The authors propose a hierarchical framework and a system that enhances LALMs with spatial understanding through four innovations: a scalable FOA data simulation pipeline, a unified model integrating spatial encoding with hybrid projection, a progressive training curriculum from alignment to reasoning, and a comprehensive ASA benchmark. The model demonstrates strong performance in spatial perception, relational integration, and cognitive reasoning tasks, marking a significant step toward spatially intelligent audio scene analysis.</div>
<div class="mono" style="margin-top:8px">本文针对现有大型音频语言模型（LALMs）将世界视为单一音频流、忽略空间维度这一限制，提出了一种基于分层框架的系统，以增强LALMs对复杂声学环境的理解与推理能力。该系统通过四项创新实现：可扩展的FOA数据模拟流水线、融合空间编码与密集混合投影机制的统一模型框架、从表征对齐到基于强化学习的推理的渐进式训练课程，以及全面的音频场景分析（ASA）基准测试。实验结果表明，该模型在空间理解任务中表现出色，显著优于现有方法在原子感知、关系整合和认知推理方面的能力。</div>
</details>
</div>
<div class="card">
<div class="title">CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning</div>
<div class="meta-line">Authors: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le</div>
<div class="meta-line">First: 2025-05-31T23:32:01+00:00 · Latest: 2026-02-03T23:34:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00750v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00750v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codesense-bench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about code semantics is essential for enhancing code LLMs&#x27; abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limits models&#x27; capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CodeSense：面向代码语义推理的现实世界基准和数据集</div>
<div class="mono" style="margin-top:8px">理解并推理代码语义对于增强代码大语言模型（LLMs）解决现实世界软件工程（SE）任务的能力至关重要。尽管已有多个代码推理基准，但大多数依赖合成数据集或教育编程问题，且主要关注输入/输出预测等粗粒度推理任务，这限制了它们在实际软件工程场景中评估LLMs的有效性。为弥合这一差距，我们提出了CodeSense，这是首个提供一系列针对现实代码软件工程语义推理的细粒度任务的基准。我们从现实世界仓库中收集了Python、C和Java软件项目，执行了这些仓库中的测试，收集了其执行轨迹，并构建了一个用于细粒度语义推理任务的基准数据集。随后，我们在最先进的LLMs上进行了全面评估。我们的结果表明，模型在处理细粒度推理任务时存在明显的性能差距。尽管链式推理和上下文学习等提示技术有所帮助，但LLMs缺乏代码语义，从根本上限制了其代码推理能力。除了数据集、基准和评估，我们的工作还产生了一个执行追踪框架和工具集，使得收集细粒度软件工程推理任务的基准数据变得容易，为未来基准构建和模型微调提供了坚实的基础。我们的代码和数据位于https://codesense-bench.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the ability of code large language models (LLMs) to perform real-world software engineering tasks by enhancing their semantic reasoning capabilities. The authors introduce CodeSense, a new benchmark and dataset that includes fine-grained code reasoning tasks derived from real-world software projects in Python, C, and Java. They collected execution traces from real repositories and built a ground truth dataset for these tasks. Evaluation on state-of-the-art LLMs revealed a significant performance gap in handling fine-grained reasoning, indicating that current models lack deep understanding of code semantics. Prompting techniques like chain-of-thought and in-context learning provided some improvement, but the fundamental issue remains. The work also provides an execution tracing framework and toolset for future benchmarking and model training.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决现有基准测试在粗粒度推理任务上的局限性，提升代码大语言模型在实际软件工程任务中的表现。CodeSense 提出了一种新的基准测试和数据集，包含从真实代码仓库中提取的 Python、C 和 Java 项目中的细粒度代码推理任务，并利用执行轨迹构建了精确的语义推理基准数据。实验结果表明，尽管使用了链式思维和上下文学习等提示技术，当前模型在处理细粒度推理任务时仍存在明显性能差距，揭示了代码模型在语义理解方面的不足。此外，本工作还开发了一个执行追踪框架和工具集，便于收集细粒度软件工程推理任务的基准数据，为未来基准构建和模型微调提供了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents</div>
<div class="meta-line">Authors: Bhanu Prakash Vangala, Ali Adibifar, Ashish Gehani, Tanu Malik</div>
<div class="meta-line">First: 2025-12-26T21:17:22+00:00 · Latest: 2026-02-03T22:46:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22387v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI生成的代码不可复现（暂且如此）：基于大语言模型的编码代理依赖差距实证研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为编码代理的兴起承诺加速软件开发，但其对生成代码可复现性的影响仍鲜有研究。本文通过实证研究探讨LLM生成的代码是否可以在仅使用操作系统包和模型指定依赖的干净环境中成功执行。我们评估了三个最先进的LLM编码代理（Claude Code、OpenAI Codex和Gemini）在Python、JavaScript和Java三种语言中，由100个标准化提示生成的300个项目。我们引入了一个三层依赖框架（区分声明依赖、工作依赖和运行时依赖），以量化代码执行的可复现性。研究结果表明，只有68.3%的项目能够直接运行，不同语言之间存在显著差异（Python为89.2%，Java为44.0%）。我们还发现，从声明依赖到实际运行时依赖的平均扩展率为13.5倍，揭示了大量隐藏依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reproducibility of code generated by large language models (LLMs) when executed in a clean environment. The research is motivated by the growing use of LLM-based coding agents in software development and the lack of understanding regarding their impact on code execution reliability. The authors evaluate three advanced coding agents—Claude Code, OpenAI Codex, and Gemini—across 300 projects created from 100 standardized prompts in Python, JavaScript, and Java. They introduce a three-layer dependency framework to categorize claimed, working, and runtime dependencies. The results indicate that only 68.3% of the projects can be executed successfully without additional setup, with notable differences across programming languages. Python projects had the highest success rate at 89.2%, while Java projects had the lowest at 44.0%. Additionally, the average number of runtime dependencies expanded 13.5 times compared to the declared ones, highlighting the presence of significant hidden dependencies.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）生成代码在干净环境中的可执行性问题。研究动机源于LLM编码代理在软件开发中的广泛应用，以及对其生成代码执行可靠性影响的了解不足。研究人员评估了三个先进的编码代理——Claude Code、OpenAI Codex 和 Gemini——在Python、JavaScript和Java三种语言中，基于100个标准化提示生成的300个项目。他们引入了一个三层依赖框架，用于区分声明依赖、工作依赖和运行时依赖。主要实验结果表明，仅有68.3%的项目无需额外配置即可成功运行，不同编程语言之间存在显著差异。此外，运行时依赖的数量平均比声明依赖增加了13.5倍，揭示了大量隐藏依赖的存在。</div>
</details>
</div>
<div class="card">
<div class="title">FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation</div>
<div class="meta-line">Authors: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Mingjie Zhan, Hongsheng Li</div>
<div class="meta-line">First: 2026-02-03T18:01:34+00:00 · Latest: 2026-02-03T18:01:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03798v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03798v1">PDF</a> · <a href="https://github.com/mnluzimu/FullStack-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FullStack-Agent: 通过面向开发的测试和仓库反向翻译增强代理式全栈网页编码</div>
<div class="mono" style="margin-top:8px">帮助非专家用户开发复杂的交互式网站已成为LLM驱动代码代理的热门任务。然而，现有的代码代理往往只生成前端网页，用华丽的视觉效果掩盖了实际全栈数据处理和存储能力的不足。值得注意的是，构建生产级全栈网页应用远比仅生成前端网页更具挑战性，需要对数据流进行细致控制，全面理解不断更新的包和依赖关系，并准确定位代码库中的隐晦错误。为了解决这些困难，我们引入了FullStack-Agent，这是一个统一的代理系统，用于全栈代理式编码，包含三个部分：(1) FullStack-Dev，一个具备强大规划、代码编辑、代码库导航和错误定位能力的多代理框架；(2) FullStack-Learn，一种创新的数据扩展和自我提升方法，通过反向翻译爬取和合成的网站仓库来提升FullStack-Dev的核心LLM；(3) FullStack-Bench，一个全面的基准测试系统，系统性地测试生成网站的前端、后端和数据库功能。我们的FullStack-Dev在前端、后端和数据库测试用例上分别优于之前最先进的方法8.7%、38.2%和15.9%。此外，FullStack-Learn通过自我提升，使一个30B模型在三个测试用例集上的性能分别提升了9.7%、9.5%和2.8%，证明了我们方法的有效性。代码已发布在https://github.com/mnluzimu/FullStack-Agent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing code agents that focus only on frontend development, neglecting the complexities of full-stack data processing and storage. FullStack-Agent introduces a three-part system: FullStack-Dev, a multi-agent framework with advanced planning, code editing, and bug localization capabilities; FullStack-Learn, a self-improving method that enhances the backbone LLM through repository back-translation; and FullStack-Bench, a benchmark for evaluating frontend, backend, and database functionalities. The results show that FullStack-Dev improves performance by 8.7%, 38.2%, and 15.9% on frontend, backend, and database tasks respectively, while FullStack-Learn boosts the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the same test sets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有代码代理仅关注前端开发、忽视全栈应用复杂性的不足。FullStack-Agent引入了一个统一的代理系统，包含三个部分：FullStack-Dev用于多代理编码，具备规划、代码编辑和代码库导航与错误定位能力；FullStack-Learn通过反向翻译仓库实现模型自我提升；FullStack-Bench用于全面测试生成网站的前端、后端和数据库功能。实验结果显示，FullStack-Dev在前端、后端和数据库测试用例上的表现分别提升了8.7%、38.2%和15.9%，而FullStack-Learn使30B模型在相同测试集上的性能分别提升了9.7%、9.5%和2.8%，验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?</div>
<div class="meta-line">Authors: Azmine Toushik Wasi, Wahid Faisal, Abdur Rahman, Mahfuz Ahmed Anik, Munem Shahriar, Mohsin Mahmud Topu, Sadia Tasnim Meem, Rahatun Nesa Priti, Sabrina Afroz Mitu, Md. Iqramul Hoque, Shahriyar Zaman Ridoy, Mohammed Eunus Ali, Majd Hawasly, Mohammad Raza, Md Rizwan Parvez</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:52:02+00:00 · Latest: 2026-02-03T17:52:02+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. 92 Pages. 42 Figures and 29 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03916v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spatialab-reasoning.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs&#x27; spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth &amp; Occlusion, Orientation, Size &amp; Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs&#x27; spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatiaLab：视觉-语言模型能否在真实环境中进行空间推理？</div>
<div class="mono" style="margin-top:8px">空间推理是人类认知的基本组成部分，但仍然是当前视觉-语言模型（VLMs）的一大挑战。以往的工作主要依赖于合成或LLM生成的环境，任务设计和类似谜题的设置有限，未能捕捉到VLMs在现实世界中遇到的复杂性、视觉噪声和多样的空间关系。为了解决这一问题，我们引入了SpatiaLab，这是一个用于评估VLMs在现实、无约束情境下空间推理能力的全面基准。SpatiaLab包含六个主要类别：相对位置、深度与遮挡、方向、大小与比例、空间导航和3D几何，每个类别下有五个子类别，共计30种不同的任务类型。每个子类别至少包含25个问题，每个主类别至少包含200个问题，支持多项选择和开放式评估。在多种最先进的VLMs上进行的实验表明，其空间推理能力与人类存在显著差距。在多项选择设置中，InternVL3.5-72B的准确率为54.93%，而人类为87.57%。在开放式的设置中，所有模型的性能下降约10-25%，其中GPT-5-mini得分最高，为40.93%，而人类为64.93%。这些结果突显了处理复杂空间关系、深度感知、导航和3D几何方面的关键局限性。通过提供一个多样化的现实评估框架，SpatiaLab揭示了推进VLMs空间推理能力的关键挑战和机遇，为未来研究提供了指导基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of spatial reasoning in vision-language models (VLMs) by introducing SpatiaLab, a new benchmark designed to evaluate their performance in realistic, unconstrained environments. Unlike previous studies that relied on synthetic or LLM-generated scenarios, SpatiaLab includes 1,400 visual question-answer pairs across six categories, each with multiple subcategories, enabling comprehensive assessment of spatial understanding. Experimental results show that even state-of-the-art models like InternVL3.5-72B and GPT-5-mini significantly underperform compared to humans, achieving 54.93% and 40.93% accuracy in multiple-choice settings, respectively, while human performance remains at 87.57%. The findings emphasize the need for improved spatial reasoning capabilities in VLMs, particularly in handling depth perception, navigation, and 3D geometry.</div>
<div class="mono" style="margin-top:8px">本文针对视觉语言模型（VLMs）在现实环境中进行空间推理的挑战，提出了SpatiaLab这一基准测试，用于评估其在真实、无约束场景下的表现。与以往依赖合成或LLM生成数据的研究不同，SpatiaLab包含1,400个视觉问答对，涵盖六个主要类别和30个子类别，涉及多样的空间关系。实验结果显示，即使是最先进的模型如InternVL3.5-72B，在多项选择任务中也仅达到54.93%的准确率，远低于人类的87.57%；在开放式任务中，所有模型的性能均下降约10-25%，其中GPT-5-mini表现最佳，为40.93%，仍低于人类的64.93%。这些结果突显了当前VLMs在处理复杂空间关系、深度感知、导航和三维几何方面的局限性，并强调了构建更强大、与人类认知对齐的空间推理模型的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</div>
<div class="meta-line">Authors: Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao</div>
<div class="meta-line">First: 2025-03-17T09:24:26+00:00 · Latest: 2026-02-03T13:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12968v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiPMB：基于优化泊松多伯努利滤波的3D多目标跟踪增强方法</div>
<div class="mono" style="margin-top:8px">准确的3D多目标跟踪（MOT）对自动驾驶至关重要，因为它能够实现复杂环境下的鲁棒感知、导航和规划。尽管基于深度学习的解决方案在3D MOT任务中表现出色，但基于模型的方法因其简单性、可解释性和数据效率仍然具有吸引力。传统基于模型的跟踪器通常依赖于随机向量贝叶斯滤波器，在基于检测的跟踪（TBD）框架中存在局限，主要由于启发式的数据关联和轨迹管理方案。相比之下，基于随机有限集（RFS）的贝叶斯滤波方法在理论上处理了目标的出生、生存和消亡过程，从而增强了可解释性和参数调整能力。本文提出了一种新的RFS-based 3D MOT方法OptiPMB，该方法在基于检测的跟踪框架中采用优化的泊松多伯努利（PMB）滤波器，并结合了多项关键创新设计。具体而言，我们提出了一种测量驱动的混合自适应出生模型以提高轨迹初始化效果，采用自适应检测概率参数以有效维护被遮挡目标的轨迹，并优化密度剪枝和轨迹提取模块以进一步提升整体跟踪性能。在nuScenes和KITTI数据集上的大量评估表明，OptiPMB在跟踪精度上优于现有最先进的方法，从而为基于模型的3D MOT建立了新的基准，并为未来基于RFS的跟踪器研究提供了有价值的参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the accuracy of 3D multi-object tracking (MOT) in autonomous driving by combining the advantages of model-based approaches with the performance of deep learning methods. OptiPMB introduces an optimized Poisson multi-Bernoulli (PMB) filter within the tracking-by-detection (TBD) framework, incorporating innovations such as a measurement-driven hybrid adaptive birth model, adaptive detection probability parameters, and optimized density pruning and track extraction modules. Experimental results on the nuScenes and KITTI datasets demonstrate that OptiPMB outperforms existing state-of-the-art methods in tracking accuracy, setting a new benchmark for model-based 3D MOT.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过结合模型化方法与深度学习方法的优势，提升自动驾驶中3D多目标跟踪（MOT）的准确性。OptiPMB提出了一种优化的泊松多伯努利（PMB）滤波器，基于跟踪-检测（TBD）框架，并引入了创新设计，包括测量驱动的混合自适应出生模型、自适应检测概率参数以及优化的密度剪枝和轨迹提取模块。在nuScenes和KITTI数据集上的实验结果表明，OptiPMB在跟踪精度上优于现有最先进的方法，为模型化3D MOT设定了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">CP-Agent: Agentic Constraint Programming</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2025-08-10T19:59:01+00:00 · Latest: 2026-02-03T12:13:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver&#x27;s feedback, and refining constraint models based on execution results.
  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CP-Agent：代理约束编程</div>
<div class="mono" style="margin-top:8px">将自然语言翻译为形式化约束模型需要对问题领域和建模框架有专业知识。为探索代理工作流的有效性，我们提出了CP-Agent，这是一个使用ReAct框架并带有持久IPython内核的Python编码代理。我们将相关的领域知识作为少于50行的项目提示提供。该算法通过迭代执行代码、观察求解器的反馈，并根据执行结果改进约束模型来工作。我们在CP-Bench的101个约束编程问题上评估了CP-Agent。我们对基准进行了小幅修改，以解决问题说明中的系统性歧义和真实模型中的错误。在澄清后的基准上，CP-Agent在所有101个问题上均达到完美准确率。我们的实验表明，最小的指导优于详细的程序结构。我们的实验还表明，显式的任务管理工具对集中建模任务可能有正面和负面的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the effectiveness of agentic workflows in translating natural language into formal constraint models. CP-Agent, a Python coding agent, is proposed that integrates the ReAct framework with a persistent IPython kernel, using concise domain knowledge as a project prompt. The agent iteratively executes code, observes solver feedback, and refines constraint models based on execution outcomes. Evaluated on 101 constraint programming problems from CP-Bench, CP-Agent achieves perfect accuracy after minor adjustments to the benchmark to resolve ambiguities and errors. The results indicate that minimal guidance surpasses detailed procedural scaffolding, and explicit task management tools can have mixed impacts on modeling tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索将自然语言转化为形式化约束模型的代理工作流的有效性。CP-Agent是一个使用ReAct框架并配备持久IPython内核的Python编码代理，其操作由不超过50行的项目提示引导。该代理通过迭代执行代码、观察求解器反馈并根据执行结果优化约束模型来工作。在对CP-Bench的101个约束编程问题进行评估时，对基准进行了小幅调整以解决问题描述中的系统性歧义和真实模型中的错误，结果显示CP-Agent在所有问题上均达到完美准确率。实验表明，最小的指导可能优于详细的程序框架，而显式的任务管理工具对专注建模任务的影响可能是正负参半的。</div>
</details>
</div>
<div class="card">
<div class="title">InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation</div>
<div class="meta-line">Authors: Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu, Yanyong Zhang</div>
<div class="meta-line">First: 2026-02-03T08:22:13+00:00 · Latest: 2026-02-03T08:22:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03242v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://shanpoyang654.github.io/InstaDrive/page.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#x27;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InstaDrive：面向实例感知的驾驶世界模型用于真实且一致的视频生成</div>
<div class="mono" style="margin-top:8px">自动驾驶依赖于在高质量、大规模多视角驾驶视频上训练的鲁棒模型。虽然世界模型为生成真实驾驶视频提供了一种成本效益高的解决方案，但它们在保持实例级时间一致性和空间几何保真度方面存在困难。为了解决这些挑战，我们提出了InstaDrive，一个通过两项关键进展提升驾驶视频真实性的新框架：(1) 实例流引导器，从帧中提取并传播实例特征以强制时间一致性，保持实例身份随时间不变。(2) 空间几何对齐器，提升空间推理能力，确保实例定位精确，并显式建模遮挡层次。通过引入这些实例感知机制，InstaDrive实现了最先进的视频生成质量，并在nuScenes数据集上增强了下游自动驾驶任务的性能。此外，我们利用CARLA的自动驾驶功能，在多样化的地图和区域上程序化和随机地模拟罕见但安全关键的驾驶场景，从而实现对自动驾驶系统的严格安全评估。我们的项目页面是https://shanpoyang654.github.io/InstaDrive/page.html。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InstaDrive was developed to improve the realism and consistency of driving video generation, which is crucial for training autonomous driving systems. The framework introduces two key components: an Instance Flow Guider that maintains temporal consistency by propagating instance features across frames, and a Spatial Geometric Aligner that enhances spatial reasoning and accurately models occlusion hierarchies. Experimental results on the nuScenes dataset show that InstaDrive achieves state-of-the-art performance in video generation quality and improves the effectiveness of downstream autonomous driving tasks.</div>
<div class="mono" style="margin-top:8px">InstaDrive 是为提升世界模型生成的驾驶视频的现实感和一致性而设计的，这对自动驾驶系统的训练至关重要。该框架引入了两个关键组件：实例流引导器，通过在帧间传播实例特征来维持时间一致性，并保持实例身份；以及空间几何对齐器，通过增强空间推理能力，确保实例的精确位置并显式建模遮挡层次。这些实例感知机制显著提升了视频生成质量，并在 nuScenes 数据集上的下游自动驾驶任务中表现出色。此外，利用 CARLA 的自动驾驶功能，可以对稀有但关键的驾驶场景进行程序化和随机化模拟，从而支持自动驾驶系统的严格安全评估。</div>
</details>
</div>
<div class="card">
<div class="title">GeoResponder: Towards Building Geospatial LLMs for Time-Critical Disaster Response</div>
<div class="meta-line">Authors: Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</div>
<div class="meta-line">First: 2025-09-18T09:46:55+00:00 · Latest: 2026-02-03T07:50:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19354v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19354v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models excel at linguistic tasks but lack the inner geospatial capabilities needed for time-critical disaster response, where reasoning about road networks, continuous coordinates, and access to essential infrastructure such as hospitals, shelters, and pharmacies is vital. We introduce GeoResponder, a framework that instills robust spatial reasoning through a scaffolded instruction-tuning curriculum. By stratifying geospatial learning into different cognitive layers, we effectively anchor semantic knowledge to the continuous coordinate manifold and enforce the internalization of spatial axioms. Extensive evaluations across four topologically distinct cities and diverse tasks demonstrate that GeoResponder significantly outperforms both state-of-the-art foundation models and domain-specific baselines. These results suggest that LLMs can begin to internalize and generalize geospatial structures, pointing toward the future development of language models capable of supporting disaster response needs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoResponder：构建面向时效性灾害响应的地理空间大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型在语言任务上表现出色，但在时效性灾害响应中缺乏必要的地理空间能力，其中对道路网络、连续坐标以及医院、避难所和药店等关键基础设施的访问推理至关重要。我们引入GeoResponder框架，通过结构化的指令微调课程来增强模型的强空间推理能力。通过将地理空间学习分层为不同的认知层次，我们有效地将语义知识锚定在连续坐标流形上，并强制模型内化空间公理。在四个拓扑结构不同的城市和多种任务上的广泛评估表明，GeoResponder在性能上显著优于最先进的基础模型和领域专用基线。这些结果表明，大语言模型开始能够内化和泛化地理空间结构，预示着未来能够支持灾害响应需求的语言模型的发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GeoResponder is to enhance the geospatial reasoning capabilities of large language models (LLMs) for time-critical disaster response scenarios. The framework employs a scaffolded instruction-tuning curriculum to embed robust spatial reasoning into LLMs, stratifying geospatial learning into distinct cognitive layers to anchor semantic knowledge to continuous coordinates and enforce spatial axioms. Experimental results across four topologically distinct cities and various tasks show that GeoResponder significantly outperforms both state-of-the-art foundation models and domain-specific baselines, indicating that LLMs can internalize and generalize geospatial structures for practical applications.</div>
<div class="mono" style="margin-top:8px">GeoResponder的动机是提升大型语言模型在时间敏感的灾害响应场景中的地理空间推理能力。该框架通过分层的指令微调课程，嵌入强大的空间推理能力，将地理空间学习划分为不同的认知层次，以将语义知识锚定到连续坐标系并内化空间公理。在四个拓扑结构不同的城市和多种任务上的实验结果表明，GeoResponder显著优于当前最先进的基础模型和领域专用基线，表明LLM可以内化并泛化地理空间结构，为实际的灾害响应应用提供支持。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-03T07:43:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正日益受到低质量合成数据和数量扩展收益递减的限制。此外，以数量为中心的扩展方法在轨迹数据利用上存在早期瓶颈。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，用于代码代理，通过多样性而非原始数据量来提升性能。在固定训练预算下，增加轨迹多样性带来的性能提升比增加轨迹数量更大，从而改善代理训练的性能-成本权衡。TDScaling集成了四项创新：（1）一种捕捉真实服务逻辑依赖的业务聚类机制；（2）一种基于蓝图的多代理范式，确保轨迹的一致性；（3）一种自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成向长尾场景发展，以防止模式崩溃；（4）一种沙箱化的代码工具，以减轻内在编码能力的灾难性遗忘。在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得显著提升。我们计划在发表后公开完整的代码库和合成数据集（包括30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of code large language models (LLMs) as they transition into tool-interactive agents, where performance is constrained by low-quality synthetic data and the diminishing returns of increasing data quantity. The authors propose TDScaling, a data synthesis framework that enhances performance through trajectory diversity rather than volume. TDScaling introduces four key innovations: a Business Cluster mechanism to capture logical dependencies, a blueprint-driven multi-agent paradigm for trajectory coherence, an adaptive evolution mechanism guided by entropy metrics to avoid mode collapse, and a sandboxed code tool to preserve coding capabilities. Experimental results on various benchmarks and code agent tasks show that TDScaling significantly improves both tool-use generalization and coding proficiency, offering a better performance-cost trade-off compared to traditional scaling methods.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）在演变为工具交互代理时面临的挑战，如合成数据质量低和数量扩展带来的边际效益递减问题。提出TDScaling框架，通过提升轨迹多样性而非单纯增加数据量来增强性能。该框架包含四项创新：业务聚类机制以捕捉真实服务的逻辑依赖关系，蓝图驱动的多代理范式确保轨迹的一致性，基于领域熵、推理模式熵和累积动作复杂度的自适应演化机制防止模式坍塌，以及一个沙盒环境的代码工具以保留模型的原生编码能力。在多个基准测试和代码代理任务上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得显著提升，实现了更好的性能与成本平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</div>
<div class="meta-line">Authors: Sherman Wong, Zhenting Qi, Zhaodong Wang, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Wenlin Chen, Yilun Du, Minlan Yu, Ying Zhang</div>
<div class="meta-line">First: 2025-12-11T08:05:58+00:00 · Latest: 2026-02-03T05:01:47+00:00</div>
<div class="meta-line">Comments: The latest version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10398v6">Abs</a> · <a href="https://arxiv.org/pdf/2512.10398v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world software engineering tasks require coding agents that can operate on massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK supports a unified orchestrator with advanced context management for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the construction, evaluation, and refinement of agents through a build-test-improve cycle, enabling rapid agent development on new tasks and tool stacks. Instantiated on the Confucius SDK using the meta-agent, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a Resolve@1 of 59%, exceeding prior research baselines as well as commercial results, under identical repositories, model backends, and tool access.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>孔子代码代理：适用于真实代码库的可扩展代理框架</div>
<div class="mono" style="margin-top:8px">现实中的软件工程任务需要能够处理大规模仓库、支持长时序会话并可靠协调复杂工具链的代码代理。现有的研究级代码代理虽然具有透明性，但在扩展到更重的生产级工作负载时表现不佳，而生产级系统虽然在实际性能上表现优异，但可扩展性、可解释性和可控性有限。我们引入了孔子代码代理（CCA），这是一个能够在大规模代码库中运行的软件工程代理。CCA基于孔子SDK构建，这是一个围绕代理体验（AX）、用户体验（UX）和开发者体验（DX）三个互补视角设计的代理开发平台。该SDK支持统一的编排器，具备高级上下文管理以实现长上下文推理，支持跨会话的持续学习的持久笔记系统，以及用于可靠工具使用的模块化扩展系统。此外，我们还引入了一个元代理，通过构建-测试-改进的循环自动完成代理的构建、评估和优化，从而实现对新任务和工具栈的快速代理开发。基于孔子SDK和元代理实例化的CCA在现实中的软件工程任务中表现出色。在SWE-Bench-Pro上，CCA在相同仓库、模型后端和工具访问条件下，实现了59%的Resolve@1，超过了先前的研究基准以及商业结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the Confucius Code Agent (CCA) is to address the limitations of existing coding agents, which either lack scalability for real-world codebases or offer poor extensibility and interpretability. CCA is built using the Confucius SDK, which provides a structured platform for agent development through three perspectives: Agent Experience, User Experience, and Developer Experience. The SDK includes features such as a unified orchestrator with advanced context management, a persistent note-taking system for continual learning, and a modular extension system for reliable tool integration. The meta-agent framework automates the build-test-improve cycle, enabling efficient agent development. Experimental results show that CCA achieves a Resolve@1 score of 59% on SWE-Bench-Pro, outperforming previous research and commercial systems under identical conditions.</div>
<div class="mono" style="margin-top:8px">开发Confucius Code Agent（CCA）的动机是解决现有编码代理在实际代码库中的可扩展性不足以及灵活性和可解释性有限的问题。CCA基于Confucius SDK构建，该SDK通过三个视角（代理体验、用户体验和开发者体验）提供结构化的代理开发平台。SDK包含统一的编排器以支持长上下文推理、持久的笔记系统以实现跨会话持续学习，以及模块化的扩展系统以确保可靠工具集成。引入的元代理框架通过构建-测试-改进的循环自动化代理开发过程。在SWE-Bench-Pro上，CCA实现了59%的Resolve@1得分，优于以往的研究和商业系统在实际软件工程任务中的表现。</div>
</details>
</div>
<div class="card">
<div class="title">IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</div>
<div class="meta-line">Authors: Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T03:39:31+00:00 · Latest: 2026-02-03T03:39:31+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03060v1">PDF</a> · <a href="https://github.com/FireRedTeam/IVC-Prune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVC-Prune: 揭示LVLMs中隐含的视觉坐标以实现视觉标记剪枝</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在多个任务中表现出色。然而，处理高分辨率视觉输入时，其推理成本却很高。尽管视觉标记剪枝已成为一种有前景的解决方案，但现有方法主要关注语义相关性，往往丢弃对空间推理至关重要的标记。我们通过一个新颖的洞察来解决这一问题，即\emph{LVLMs如何处理空间推理}。具体而言，我们发现LVLMs通过旋转位置嵌入（Rotary Position Embeddings, RoPE）隐式地建立视觉坐标系统，其中特定的标记位置作为\textbf{隐含视觉坐标}（IVC tokens），对空间推理至关重要。基于这一洞察，我们提出\textbf{IVC-Prune}，一种无需训练、基于提示的剪枝策略，保留IVC标记和语义相关的前景标记。IVC标记通过理论分析RoPE的数学性质来识别，目标是那些其旋转矩阵近似单位矩阵或$90^\circ$旋转矩阵的位置。前景标记则通过一个稳健的两阶段过程来识别：首先发现语义种子，然后通过值向量相似性进行上下文优化。在四个代表性LVLMs和二十个多样化基准上的广泛评估表明，IVC-Prune可将视觉标记减少约50\%，同时保持$\geq$ 99\%的原始性能，并在多个基准上实现性能提升。源代码可在https://github.com/FireRedTeam/IVC-Prune获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high inference cost in Large Vision-Language Models (LVLMs) when handling high-resolution visual inputs. The authors propose IVC-Prune, a training-free and prompt-aware token pruning method that identifies and retains tokens crucial for spatial reasoning. They reveal that LVLMs implicitly use Rotary Position Embeddings (RoPE) to form visual coordinate systems, with specific token positions acting as implicit visual coordinates (IVC tokens). By combining theoretical analysis of RoPE&#x27;s mathematical properties with a two-stage process for identifying semantically relevant tokens, IVC-Prune achieves a 50% reduction in visual tokens while preserving over 99% of the original model performance and improving results on several benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对大型视觉语言模型（LVLMs）在处理高分辨率视觉输入时的高推理成本问题。作者提出了一种无需训练且基于提示的视觉标记剪枝方法IVC-Prune，该方法识别并保留对空间推理至关重要的标记。研究发现，LVLMs通过旋转位置嵌入（RoPE）隐式构建视觉坐标系统，其中特定位置的标记作为隐式视觉坐标（IVC tokens）。通过结合RoPE的数学性质分析与两阶段的前景标记选择过程，IVC-Prune实现了约50%的视觉标记减少，同时保持原始性能的99%以上，并在多个基准测试中取得性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</div>
<div class="meta-line">Authors: Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</div>
<div class="meta-line">First: 2026-02-03T03:10:27+00:00 · Latest: 2026-02-03T03:10:27+00:00</div>
<div class="meta-line">Comments: In Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03045v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03045v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>绘制前请澄清：面向稳健文本到CAD生成的主动代理</div>
<div class="mono" style="margin-top:8px">大型语言模型最近使文本到CAD系统成为可能，这些系统可以从自然语言提示中合成参数化CAD程序（例如CadQuery）。然而在实际应用中，几何描述可能不够具体或内部不一致：关键尺寸可能缺失，约束可能冲突。现有的微调模型倾向于被动地遵循用户指令，并在文本模糊时虚构尺寸。为了解决这一问题，我们提出了一种名为ProCAD的主动代理框架，用于文本到CadQuery的生成，该框架在代码合成之前解决规格问题。我们的框架结合了一个主动澄清代理，它仅在必要时审核提示并提出有针对性的澄清问题，以生成自洽的规格，以及一个CAD编码代理，它将规格转换为可执行的CadQuery程序。我们通过一个精心整理的高质量文本到CadQuery数据集对编码代理进行微调，并通过代理式监督微调（SFT）在澄清轨迹上训练澄清代理。实验表明，主动澄清显著提高了对模糊提示的鲁棒性，同时保持了较低的交互开销。ProCAD在包括Claude Sonnet 4.5在内的前沿闭源模型上表现更优，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。我们的代码和数据集将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating accurate CAD models from ambiguous natural language descriptions, which often lack critical dimensions or contain conflicting constraints. It introduces ProCAD, a proactive agentic framework that combines a clarifying agent to resolve specification issues before code synthesis and a CAD coding agent trained on a high-quality dataset to translate the clarified specification into an executable CadQuery program. Experimental results demonstrate that ProCAD significantly enhances robustness to ambiguous inputs, achieving a 79.9% reduction in mean Chamfer distance and a 4.8% to 0.9% decrease in invalidity ratio compared to existing closed-source models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升文本到CAD系统的鲁棒性，以应对自然语言提示中几何描述不足或不一致的问题。提出的方法ProCAD是一个主动代理框架，包含一个澄清代理和一个CAD编码代理。澄清代理审计提示并仅在必要时提出针对性问题，以生成自洽的规范；编码代理则在高质量数据集上进行微调，将规范转换为可执行的CadQuery程序。实验结果表明，ProCAD在处理模糊提示方面显著提升了鲁棒性，与现有闭源模型相比，平均Chamfer距离减少了79.9%，无效率从4.8%降至0.9%。</div>
</details>
</div>
<div class="card">
<div class="title">CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability</div>
<div class="meta-line">Authors: Xianzhen Luo, Jingyuan Zhang, Shiqi Zhou, Rain Huang, Chuan Xiao, Qingfu Zhu, Zhiyuan Ma, Xing Yue, Yang Yue, Wencong Zeng, Wanxiang Che</div>
<div class="meta-line">First: 2026-02-03T02:27:16+00:00 · Latest: 2026-02-03T02:27:16+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03012v1">PDF</a> · <a href="https://github.com/livecvebench/CVE-Factory">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CVE-Factory：为代码安全漏洞实现专家级代理任务的扩展</div>
<div class="mono" style="margin-top:8px">评估和提升代码代理的安全能力需要高质量且可执行的漏洞任务。然而，现有工作依赖于成本高昂且难以扩展的手动复现，并且数据分布过时。为了解决这些问题，我们提出了CVE-Factory，这是首个能够将稀疏的CVE元数据自动转换为可执行代理任务的多代理框架，实现专家级质量。与人类专家复现的交叉验证表明，CVE-Factory在解决方案正确性和环境保真度方面分别达到95\%和96\%。它还被评估在最新的现实漏洞上，验证成功率达到66.2\%。这种自动化实现了两个下游贡献。首先，我们构建了LiveCVEBench，这是一个持续更新的基准，包含190个任务，涵盖14种语言和153个仓库，捕捉包括AI工具漏洞在内的新兴威胁。其次，我们合成超过1,000个可执行的训练环境，这是代码安全领域首次实现代理任务的大规模扩展。经过微调的Qwen3-32B在LiveCVEBench上的表现从5.3\%提升至35.8\%，超越了Claude 4.5 Sonnet，其提升效果也推广到Terminal Bench（12.5\%到31.3\%）。我们开源了CVE-Factory、LiveCVEBench、Abacus-cve（微调模型）、训练数据集和排行榜。所有资源均可在https://github.com/livecvebench/CVE-Factory 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the security evaluation and improvement of code agents by providing high-quality, executable vulnerability tasks. CVE-Factory is introduced as a multi-agent framework that automatically transforms sparse CVE metadata into fully executable agentic tasks, achieving expert-level quality. Experimental results show that CVE-Factory achieves 95% solution correctness and 96% environment fidelity when validated against human expert reproductions, and it achieves a 66.2% verified success rate on the latest realistic vulnerabilities. The framework also enables the creation of LiveCVEBench, a continuously updated benchmark with 190 tasks across 14 languages and 153 repositories, and synthesizes over 1,000 training environments. Fine-tuned Qwen3-32B significantly improves performance on LiveCVEBench, reaching 35.8% from 5.3%, outperforming Claude 4.5 Sonnet, with performance gains also observed on Terminal Bench.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过提供高质量、可执行的漏洞任务来提升代码代理的安全评估能力。为此，作者提出了CVE-Factory，这是一个首个能够将稀疏的CVE元数据自动转换为完整可执行代理任务的多代理框架。该框架通过与人工专家任务的交叉验证，达到了95%的解决方案正确性和96%的环境保真度。在最新的真实漏洞上测试，CVE-Factory实现了66.2%的验证成功率。自动化方法带来了两个重要贡献：构建了持续更新的LiveCVEBench基准，包含14种语言和153个仓库的190个任务，涵盖新兴威胁如AI工具漏洞；并合成了超过1000个可执行的训练环境。对Qwen3-32B进行微调后，其在LiveCVEBench上的表现从5.3%提升至35.8%，超越了Claude 4.5 Sonnet，且在Terminal Bench上也表现出显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing</div>
<div class="meta-line">Authors: Saeid Sheikhi</div>
<div class="meta-line">First: 2026-02-02T21:44:01+00:00 · Latest: 2026-02-02T21:44:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02842v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟链：一种面向大型语言模型的双模式推理框架，具有动态问题路由</div>
<div class="mono" style="margin-top:8px">我们提出了模拟链（CoS），一种新颖的双模式推理框架，该框架通过动态路由将问题发送到大型语言模型（LLMs）中的专用推理策略。与现有的统一提示方法不同，CoS采用三种不同的推理模式：(1) 用于数学问题的计算流程与自洽性，(2) 用于空间推理的符号状态跟踪与JSON表示，(3) 用于多跳推理的混合事实提取。通过在GSM8K、StrategyQA和bAbI基准上使用四个最先进的模型（Gemma-3 27B、LLaMA-3.1 8B、Mistral 7B和Qwen-2.5 14B）进行综合评估，我们证明CoS在GSM8K上达到71.5%的准确率（绝对提升1.0%），在StrategyQA上达到90.0%（提升2.5%），在bAbI上达到19.0%（相对提升65.2%），优于最强的基线方法。分析表明，针对特定问题选择合适的推理模式至关重要，当正确应用于数学问题时，计算模式可达到81.2%的准确率，而错误路由则导致0%的准确率。我们提供了模式选择、状态跟踪和答案提取的详细算法，确立了CoS作为一种无需额外训练即可提升LLM推理能力的有效方法。与Self-Consistency相比，该框架在准确率和效率之间提供了更优的权衡，计算成本降低了54%，同时实现了相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the reasoning capabilities of Large Language Models (LLMs) by addressing their limitations in handling diverse problem types through specialized strategies. Chain of Simulation (CoS) introduces a dual-mode reasoning framework that dynamically routes problems to three distinct modes: computational flow for mathematical problems, symbolic state tracking for spatial reasoning, and hybrid fact-extraction for multi-hop inference. Experimental results on GSM8K, StrategyQA, and bAbI benchmarks show that CoS improves accuracy by 1.0%, 2.5%, and 65.2% respectively compared to the strongest baselines, with computational mode achieving 81.2% accuracy when correctly applied. The framework also achieves comparable performance at 54% lower computational cost than Self-Consistency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决大型语言模型（LLMs）在处理不同类型问题时的局限性，提升其推理能力。Chain of Simulation（CoS）提出了一种双模式推理框架，动态地将问题路由到专门的推理策略，包括用于数学问题的计算流程、用于空间推理的符号状态跟踪以及用于多跳推理的混合事实提取。在GSM8K、StrategyQA和bAbI基准测试中，CoS分别比最强基线模型提升了1.0%、2.5%和65.2%的准确率，其中计算模式在正确应用时达到81.2%的准确率。该框架在准确率和效率之间提供了更优的平衡，相比Self-Consistency方法，在计算成本降低54%的情况下实现了相近的性能表现。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-02-02T19:55:32+00:00</div>
<div class="meta-line">Comments: 21 main pages, 6 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该比闭源系统具有根本性的优势：它们可以专门针对私有代码库，直接在权重中编码特定仓库的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，能够快速且低成本地创建专门针对私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法和代码）模型中实现了最先进的结果，同时其性能与前沿的开放权重模型（如Devstral-Small-2）相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到同等性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数万个轨迹。结合成本效益，这使得专门针对私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过20万个合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和干扰因素进行了详细分析。总体而言，我们相信这项工作将大大加速开放编码代理的研究，并展示能够专门针对私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this paper is to enable open-source coding agents to effectively specialize to private codebases, leveraging their open-weight nature. The authors propose SERA, which employs a method called Soft Verified Generation (SVG) to efficiently generate synthetic training data from a single code repository, allowing for the creation of specialized agents without the need for expensive reinforcement learning or synthetic data methods. The main experimental results show that SERA achieves state-of-the-art performance among fully open-source models and matches the performance of advanced open-weight models like Devstral-Small-2, while being 26x cheaper than reinforcement learning and 57x cheaper than prior synthetic data approaches to reach equivalent performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是实现对私有代码库的开放权重编码代理的实用化定制，这在过去由于高昂的训练成本和复杂性而难以实现。作者提出了SERA方法，利用监督微调（SFT）训练编码代理，在完全开源模型中取得了最先进的性能，与先进的开放权重模型如Devstral-Small-2表现相当。SERA相比强化学习和之前合成数据方法在达到相似性能时成本低26倍和57倍。此外，Soft Verified Generation（SVG）技术可以从单个代码库生成数千条合成轨迹，支持对私有代码库的定制化以及更大规模代码库的分析，从而揭示训练编码代理的扩展规律和影响因素。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将现实世界可验证环境扩展到百万级</div>
<div class="mono" style="margin-top:8px">我们提出了SWE-Universe，这是一个可扩展且高效的框架，用于从GitHub拉取请求（PRs）自动构建现实世界软件工程（SWE）可验证环境。为了解决自动构建中普遍存在的挑战，如低生产率、弱验证器和高昂成本，我们的框架采用了一个由高效自定义训练模型驱动的构建代理。该代理通过迭代自验证和循环内黑客检测，确保高保真、可验证任务的可靠生成。通过这种方法，我们将现实世界多语言SWE环境的数量扩展到百万级别（807,693）。我们通过大规模代理中间训练和强化学习展示了这些环境的深远价值。最后，我们将该技术应用于Qwen3-Max-Thinking，并在SWE-Bench Verified上取得了75.3%的成绩。我们的工作为推进下一代代码代理提供了关键资源和稳健的方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the challenges of automatically building high-quality, verifiable software engineering environments at scale. The framework introduces a building agent based on a custom-trained model, which performs iterative self-verification and in-loop hacking detection to ensure the reliability and fidelity of generated tasks. Through this approach, the study successfully constructs over 800,000 real-world multilingual SWE environments and validates their effectiveness by applying them to Qwen3-Max-Thinking, achieving a 75.3% score on SWE-Bench Verified.</div>
<div class="mono" style="margin-top:8px">SWE-Universe的动机是解决现有方法在自动构建现实软件工程环境时存在的问题，如生产率低、验证能力弱和成本高昂。该框架引入了一个由定制模型驱动的构建代理，通过迭代自验证和循环内黑客检测，确保生成高保真且可验证的任务。该方法成功将多语言软件工程环境的数量扩展到超过80万个实例，并在应用于Qwen3-Max-Thinking后，在SWE-Bench Verified基准测试中取得了75.3%的得分，展示了其有效性和对下一代编码代理发展的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image</div>
<div class="meta-line">Authors: Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik</div>
<div class="meta-line">First: 2025-09-02T13:17:11+00:00 · Latest: 2026-02-02T16:22:10+00:00</div>
<div class="meta-line">Comments: Project page is at https://learn-to-steer-paper.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02295v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://learn-to-steer-paper.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model&#x27;s internal representations.
  We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model&#x27;s cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces in the cross-attention maps, rather than learning true spatial patterns. We solve this by augmenting our training data with samples generated using prompts with incorrect relation words, which encourages the classifier to avoid linguistic shortcuts and learn spatial patterns from the attention maps. Our method dramatically improves spatial accuracy: from 20% to 61% on FLUX.1-dev and from 7% to 54% on SD2.1 across standard benchmarks. It also generalizes to multiple relations with significantly improved accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于文本到图像推理时优化的数据驱动损失函数</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型可以生成令人惊叹的视觉效果，但在儿童看来微不足道的任务上却常常失败，例如将狗放在泰迪熊的右侧而不是左侧。当组合变得更加不寻常时，例如将长颈鹿放在飞机上方，这些失败会更加明显。现有方法通过模型微调或使用手工设计的次优损失函数在推理时进行优化，以解决这些空间推理问题。我们提出的方法是直接从模型的内部表示中学习这些目标，而不是强加我们对空间编码的假设。我们引入了Learn-to-Steer，这是一个新颖的框架，用于学习数据驱动的推理时优化目标，而非手工设计。我们的关键洞察是训练一个轻量级分类器，从扩散模型的交叉注意力图中解码空间关系，然后在推理过程中将该分类器作为学习得到的损失函数进行部署。训练此类分类器面临一个令人惊讶的挑战：它们可能通过检测交叉注意力图中的语言痕迹来走捷径，而不是学习真正的空间模式。我们通过在训练数据中加入使用错误关系词提示生成的样本，来解决这一问题，这促使分类器避免语言捷径，并从注意力图中学习空间模式。我们的方法显著提升了空间准确性：在FLUX.1-dev上从20%提升至61%，在SD2.1上从7%提升至54%。它还能够显著提升准确率地推广到多种关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of spatial reasoning failures in text-to-image diffusion models, which struggle with simple tasks such as positioning objects relative to each other. The authors propose Learn-to-Steer, a framework that learns data-driven loss functions for inference-time optimization by training a lightweight classifier to decode spatial relationships from cross-attention maps. To prevent the classifier from relying on linguistic cues, the training data is augmented with prompts containing incorrect relation words. The method significantly improves spatial accuracy, achieving 61% on FLUX.1-dev and 54% on SD2.1 across standard benchmarks, demonstrating better generalization to multiple spatial relations.</div>
<div class="mono" style="margin-top:8px">本文针对文本到图像扩散模型在空间推理任务中的失败问题，如难以正确放置物体（如狗在泰迪熊右侧而非左侧），提出了一种新的方法。作者提出了Learn-to-Steer框架，通过训练一个轻量级分类器来从扩散模型的交叉注意力图中解码空间关系，并将该分类器作为学习得到的损失函数用于推理阶段。为防止分类器依赖语言线索，训练数据中加入了包含错误关系词的提示生成样本。实验结果表明，该方法显著提升了空间准确性，在FLUX.1-dev上达到61%，在SD2.1上达到54%，并在多种空间关系任务中表现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">OmniCode: A Benchmark for Evaluating Software Engineering Agents</div>
<div class="meta-line">Authors: Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta</div>
<div class="meta-line">First: 2026-02-02T16:04:10+00:00 · Latest: 2026-02-02T16:04:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02262v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02262v1">PDF</a> · <a href="https://github.com/seal-research/OmniCode">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniCode：评估软件工程代理的基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型的编码代理正在重新定义现实世界软件的开发方式。为了推动对更优编码代理的研究，我们需要能够严格评估此类代理执行各种软件工程任务能力的挑战性基准。然而，像HumanEval和SWE-Bench这样的流行编码基准主要关注狭窄范围的任务，如编程竞赛和补丁生成。实际上，软件工程师在现实世界软件开发中需要处理更广泛的任务。为了解决这一差距，我们提出了OmniCode，一个包含超越代码或补丁生成的更广泛和多样化任务类别的新型软件工程基准。总体而言，OmniCode包含1794个任务，涵盖三种编程语言（Python、Java和C++）以及四个关键类别：错误修复、测试生成、代码审查修复和风格修复。与以往的软件工程基准相比，OmniCode的任务（1）经过人工验证以消除定义不清的问题，（2）合成构建或最近整理以避免数据泄露问题，从而提供了一种从有限现实数据中合成生成多样化软件任务的新框架。我们使用流行的代理框架如SWE-Agent对OmniCode进行了评估，结果显示虽然它们在Python错误修复方面表现良好，但在测试生成等任务以及C++和Java等语言中表现不足。例如，在Java测试生成任务中，SWE-Agent使用DeepSeek-V3.1最多只能达到20.9%的准确率。OmniCode旨在成为一种稳健的基准，推动开发能够在软件开发不同方面表现优异的代理。代码和数据可在https://github.com/seal-research/OmniCode获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind OmniCode is to provide a comprehensive benchmark for evaluating the capabilities of LLM-powered software engineering agents beyond narrow tasks like code generation or patch creation. The proposed benchmark includes 1794 tasks across Python, Java, and C++, covering four categories: bug fixing, test generation, code review fixing, and style fixing. These tasks are manually validated and synthetically generated to avoid data leakage, offering a more realistic assessment of agent performance. Evaluation with existing frameworks such as SWE-Agent reveals significant performance gaps, particularly in test generation for Java, where SWE-Agent achieves only 20.9% accuracy using DeepSeek-V3.1, highlighting the need for more robust and versatile coding agents.</div>
<div class="mono" style="margin-top:8px">提出OmniCode的动机是为LLM驱动的编码代理提供一个全面的评估基准，以应对现实软件开发中更广泛的任务需求。该基准包含除代码或补丁生成之外的多个任务类别，如错误修复、测试生成、代码审查修复和风格调整，覆盖Python、Java和C++三种编程语言。OmniCode任务通过人工验证和合成生成，避免了数据泄露问题，确保了评估的严谨性和多样性。实验结果显示，现有框架如SWE-Agent在Python错误修复任务中表现良好，但在如Java测试生成等任务上表现不佳，仅达到20.9%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</div>
<div class="meta-line">Authors: Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</div>
<div class="meta-line">First: 2025-05-23T11:48:48+00:00 · Latest: 2026-02-02T13:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17779v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.17779v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U2-BENCH：在超声理解上对大型视觉-语言模型进行基准测试</div>
<div class="mono" style="margin-top:8px">超声是一种广泛使用的成像方式，对全球医疗至关重要，但由于其图像质量在操作者、噪声和解剖结构上的差异，其解读仍具挑战性。尽管大型视觉-语言模型（LVLMs）在自然和医学领域展示了令人印象深刻的多模态能力，但它们在超声方面的表现仍鲜有研究。我们引入了U2-BENCH，这是首个全面评估LVLMs在超声理解上的基准，涵盖分类、检测、回归和文本生成等任务。U2-BENCH汇集了7,241个案例，涉及15个解剖区域，并在50个超声应用场景中定义了8个临床启发式任务，如诊断、视图识别、病灶定位、临床价值评估和报告生成。我们评估了23个最先进的LVLMs，包括开源和闭源、通用型和医学专用型模型。我们的结果表明，在图像分类任务上表现强劲，但在空间推理和临床语言生成方面仍存在持续挑战。U2-BENCH建立了一个严格且统一的测试平台，用于评估和加速医学超声成像这一独特多模态领域的LVLM研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the lack of evaluation frameworks for large vision-language models (LVLMs) in the domain of medical ultrasound imaging, which is essential for global healthcare but challenging due to image variability and noise. U2-BENCH is introduced as the first comprehensive benchmark that assesses LVLMs across multiple tasks including classification, detection, regression, and text generation. It includes 7,241 cases from 15 anatomical regions and covers 8 clinically relevant tasks in 50 application scenarios. The evaluation of 23 state-of-the-art LVLMs shows strong performance in image-level classification but highlights persistent issues in spatial reasoning and clinical language generation.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在超声理解领域缺乏对大型视觉语言模型（LVLMs）的评估框架的问题。U2-BENCH作为首个全面评估LVLMs在超声理解方面能力的基准，涵盖了分类、检测、回归和文本生成等任务。该基准包含来自15个解剖区域的7241个案例，并覆盖了50种超声应用场景，涉及8项临床启发式任务。对23个最先进的LVLMs进行评估结果显示，在图像级分类任务中表现优异，但在空间推理和临床语言生成方面仍存在持续挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models</div>
<div class="meta-line">Authors: Cristian Sbrolli, Matteo Matteucci, Toshihiko Yamasaki</div>
<div class="meta-line">First: 2026-02-02T12:39:39+00:00 · Latest: 2026-02-02T12:39:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02043v1">PDF</a> · <a href="https://huggingface.co/AutoComp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing &quot;a red cube and a blue sphere&quot; with &quot;a blue cube and a red sphere&quot;. Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., &quot;a monitor to the left of a bicycle on a white background&quot;) and LLM-generated Contextual captions (e.g., &quot;In a brightly lit photography studio, a monitor is positioned to the left of a bicycle&quot;), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel &quot;Confusion Benchmark&quot; reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Auto-Comp：用于对比视觉-语言模型可扩展组合探查的自动化流水线</div>
<div class="mono" style="margin-top:8px">现代视觉-语言模型（VLMs）在组合推理方面存在关键缺陷，常常将 &quot;一个红色立方体和一个蓝色球体&quot; 与 &quot;一个蓝色立方体和一个红色球体&quot; 混淆。分离这些失败的视觉和语言根源是实现稳健评估的基本挑战。为了实现细粒度、可控的分析，我们引入了 Auto-Comp，这是一个完全自动化且合成的流水线，用于生成可扩展的基准测试。其可控性是解构和隔离不同推理能力的关键。Auto-Comp 从最小化描述（例如：&quot;一个显示器位于白色背景上的自行车左侧&quot;）和大语言模型生成的上下文描述（例如：&quot;在一个明亮的摄影工作室中，显示器位于自行车左侧&quot;）生成配对图像，从而允许进行受控的 A/B 测试，以区分核心绑定能力与视觉-语言复杂性。我们在新的基准测试上评估了 20 个 VLMs，发现 CLIP 和 SigLIP 模型家族在颜色绑定和空间关系方面均存在普遍的组合失败。关键的是，我们提出的新型 &quot;混淆基准&quot; 揭示了一个比简单属性交换更深层次的缺陷：模型对低熵值的干扰项（例如：重复的物体或颜色）高度敏感，表明其组合失败不仅限于已知的词袋模型限制。我们发现了一个令人惊讶的权衡：视觉-语言上下文虽然提供了全局场景线索，有助于空间推理，但同时通过引入视觉杂乱而阻碍了局部属性绑定。我们发布了 Auto-Comp 流水线，以促进未来基准测试的创建，并提供了我们生成的所有基准测试（https://huggingface.co/AutoComp）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the critical issue of compositional reasoning in Vision-Language Models (VLMs), where models often fail to distinguish between attribute swaps such as &#x27;a red cube and a blue sphere&#x27; and &#x27;a blue cube and a red sphere&#x27;. To enable detailed and controllable analysis, the authors propose Auto-Comp, an automated pipeline that generates synthetic benchmarks with paired images and captions. By conducting A/B tests on these benchmarks, they evaluate 20 VLMs and find that both CLIP and SigLIP models exhibit universal compositional failures. Their &#x27;Confusion Benchmark&#x27; highlights that models are particularly vulnerable to low-entropy distractors, indicating that their issues go beyond simple bag-of-words limitations. The study also identifies a trade-off between visio-linguistic context aiding spatial reasoning and hindering local attribute binding.</div>
<div class="mono" style="margin-top:8px">本文针对视觉语言模型（VLMs）在组合推理中的关键缺陷进行研究，即模型常混淆如&quot;一个红色立方体和一个蓝色球体&quot;与&quot;一个蓝色立方体和一个红色球体&quot;等属性组合。为实现细粒度且可控的分析，作者提出了Auto-Comp，一个自动化生成合成基准的管道，通过将最小图像与上下文描述配对，进行A/B测试以区分核心绑定能力与视觉语言复杂性。对20个VLM的评估表明，CLIP和SigLIP模型家族均存在普遍的组合推理失败。研究发现，模型对低熵干扰项（如重复对象或颜色）高度敏感，说明其组合推理错误不仅限于简单的属性交换。此外，研究揭示了视觉语言上下文与属性绑定之间的权衡关系：全局场景线索有助于空间推理，但可能通过视觉干扰影响局部属性理解。</div>
</details>
</div>
<div class="card">
<div class="title">Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models</div>
<div class="meta-line">Authors: Wenhui Tan, Fiorenzo Parascandolo, Enver Sangineto, Jianzhong Ju, Zhenbo Luo, Qian Cao, Rita Cucchiara, Ruihua Song, Jian Luan</div>
<div class="meta-line">First: 2026-02-02T06:12:33+00:00 · Latest: 2026-02-02T06:12:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01698v1">PDF</a> · <a href="https://GitHub.com/Xiaomi-Research/LED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在微调后恢复探索：用于大型推理模型的潜在探索解码</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）通过强化学习（RL）微调后最近在数学和代码推理方面表现出色。然而，我们发现现代推理微调会导致意外的探索崩溃：基于温度的采样不再提升pass@$n$准确率。实证研究表明，微调后的LRMs最后一层的后验熵显著降低，而中间层的熵则相对较高。基于这种熵的不对称性，我们提出了潜在探索解码（LED），这是一种深度条件解码策略。LED通过累积求和聚合中间后验，并选择熵最大的深度配置作为探索候选。在多个推理基准和模型上，无需额外训练或参数，LED始终能将pass@1和pass@16准确率分别提升0.61和1.03个百分点。项目页面：https://GitHub.com/Xiaomi-Research/LED</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of exploration collapse in Large Reasoning Models (LRMs) after reinforcement learning post-training, where temperature-based sampling fails to improve pass@$n$ accuracy. The authors observe that post-trained LRMs show a significant reduction in entropy in the final layer, while intermediate layers maintain higher entropy. To resolve this, they introduce Latent Exploration Decoding (LED), a decoding strategy that leverages cumulative sum of intermediate posteriors to select depth configurations with maximal entropy as exploration candidates. Experimental results demonstrate that LED improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points respectively, across various reasoning benchmarks and models, without requiring additional training or parameters.</div>
<div class="mono" style="margin-top:8px">本文针对大型推理模型（LRMs）在强化学习微调后出现的探索坍塌问题，即基于温度的采样无法提升pass@$n$准确率。作者提出了一种名为Latent Exploration Decoding（LED）的解码策略，利用微调模型中中间层与最终层熵值不对称的现象。LED通过累积求和的方式聚合中间层后验分布，并选择熵值最高的深度配置作为探索候选。实验表明，在多个推理基准测试中，LED在不增加额外训练或参数的情况下，能够稳定提升pass@1和pass@16的准确率，分别提高0.61和1.03个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-02T05:17:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v1">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在单个问题的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念导向任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ProjDevBench is to address the limitations of current coding agent evaluations, which primarily focus on bug fixing rather than end-to-end project development. The benchmark introduces a comprehensive evaluation framework that combines Online Judge testing with LLM-assisted code review to assess coding agents on system architecture design, functional correctness, and iterative solution refinement. It includes 20 programming problems across 8 categories, evaluating six agents based on different LLM backends. The main experimental results show an overall acceptance rate of 27.38%, indicating that while agents can handle basic functionality and data structures, they face significant challenges in complex system design, time complexity optimization, and resource management.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，现有评估主要集中在单个问题的修复上，而忽略了完整的端到端项目开发。ProjDevBench提出一个基准，通过提供项目需求并评估生成的代码仓库，从系统架构设计、功能正确性和迭代解决方案优化三个方面对编码代理进行测试。该基准涵盖8个类别中的20个编程问题，包括概念性任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。实验结果显示，尽管代理在基础功能和数据结构方面表现良好，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难，整体接受率为27.38%。</div>
</details>
</div>
<div class="card">
<div class="title">From Perception to Action: Spatial AI Agents and World Models</div>
<div class="meta-line">Authors: Gloria Felicia, Nolan Bryant, Handi Putra, Ayaan Gazali, Eliel Lobo, Esteban Rojas</div>
<div class="meta-line">First: 2026-02-02T05:00:55+00:00 · Latest: 2026-02-02T05:00:55+00:00</div>
<div class="meta-line">Comments: 61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01644v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从感知到行动：空间AI代理与世界模型</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型已成为代理推理和规划的主流方法，但它们在符号域的成功并不容易直接应用于物理世界。空间智能，即感知三维结构、推理物体关系并在物理约束下行动的能力，是一种与之正交的重要能力，对具身代理尤为关键。现有的综述文章要么单独讨论代理架构，要么单独探讨空间领域，没有提供一个统一的框架来连接这些互补的能力。本文填补了这一空白。通过全面回顾2000多篇论文，引用了742篇顶级会议和期刊的文章，我们提出了一种统一的三轴分类体系，将代理能力与空间任务在不同尺度上联系起来。关键的是，我们区分了空间锚定（对几何和物理的度量理解）与符号锚定（将图像与文本关联），认为仅凭感知并不能赋予代理能力。我们的分析揭示了三个关键发现，对应这三个轴：（1）分层记忆系统（能力轴）对于长视野空间任务至关重要。（2）图神经网络与语言模型的集成（任务轴）是结构化空间推理的一种有前景的方法。（3）世界模型（尺度轴）对于在微观到宏观空间尺度上安全部署至关重要。最后，我们识别出六个重大挑战，并概述了未来研究的方向，包括需要统一的评估框架以标准化跨领域的评估。该分类体系为整合分散的研究努力奠定了基础，并有助于在机器人、自动驾驶和地理空间智能等领域实现下一代空间感知的自主系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating agentic reasoning with spatial intelligence to enable embodied agents to operate effectively in the physical world. It proposes a unified three-axis taxonomy that connects agentic capabilities with spatial tasks across different scales, distinguishing between spatial grounding and symbolic grounding. The key findings include the importance of hierarchical memory systems for long-term spatial tasks, the potential of GNN-LLM integration for structured spatial reasoning, and the necessity of world models for safe deployment across various spatial scales. The study highlights the need for a standardized evaluation framework to assess cross-domain performance and aims to unify fragmented research efforts in spatial AI.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将代理推理与空间智能结合以实现物理世界中具身智能体有效操作的挑战。它提出了一种统一的三轴分类法，将代理能力与空间任务在不同尺度上联系起来，区分了空间锚定与符号锚定。主要发现包括：分层记忆系统对长期空间任务的重要性，GNN-LLM融合在结构化空间推理中的潜力，以及世界模型在微至宏观空间尺度安全部署中的必要性。文章还指出了标准化评估框架的迫切需求，并提出了未来研究的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Autoregressive-Diffusion Collaborative Thoughts</div>
<div class="meta-line">Authors: Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu</div>
<div class="meta-line">First: 2026-02-02T03:54:15+00:00 · Latest: 2026-02-02T03:54:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自回归-扩散协同思维的推理</div>
<div class="mono" style="margin-top:8px">自回归模型和扩散模型代表两种互补的生成范式。自回归模型擅长于序列规划和约束组合，但在需要显式空间或物理基础的任务上表现不佳。相比之下，扩散模型通过高维生成捕捉丰富的空间结构，但缺乏满足复杂多阶段约束或可靠识别和纠正错误所需的逐步逻辑控制。我们引入了协同思维（Collaborative Thoughts），这是一种统一的协同框架，使自回归模型和扩散模型能够通过闭环交互共同进行推理和生成。在协同思维中，自回归模型执行结构化规划和约束管理，扩散模型将这些约束实例化为中间的视觉思维，而基于视觉的批评模块则评估这些视觉思维是否满足预期的结构和物理要求。这种反馈随后用于迭代优化后续的规划和生成步骤，从而减少跨模态的误差传播。重要的是，无论任务是自回归问答还是基于扩散的视觉生成，协同思维都使用相同的协同循环。通过代表性示例，我们展示了协同思维如何提高空间推理的可靠性以及生成的可控性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of both autoregressive and diffusion models in handling tasks that require spatial reasoning and multi-stage constraint satisfaction. The authors propose Collaborative Thoughts, a framework that integrates autoregressive models for structured planning and constraint management with diffusion models for generating rich spatial outputs. A vision-based critic module evaluates the generated visual thoughts, providing feedback to refine the planning and generation process iteratively. Experimental results demonstrate that this collaborative approach enhances the reliability of spatial reasoning and improves the controllability of generation across different tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决自回归模型和扩散模型在处理需要空间推理和多阶段约束满足的任务时的局限性。提出的方法Collaborative Thoughts将这两种范式整合到一个统一框架中，其中自回归模型负责结构化规划和约束管理，扩散模型则生成中间的视觉表示。一个基于视觉的批评模块评估这些视觉输出是否符合结构和物理要求，并提供反馈以迭代地优化规划和生成过程。实验结果表明，该协作方法提高了空间推理的可靠性，并增强了生成的可控性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
