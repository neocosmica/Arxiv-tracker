<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 03:58</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0358</div>
    <div class="row"><div class="card">
<div class="title">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</div>
<div class="meta-line">Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</div>
<div class="meta-line">First: 2026-02-05T18:59:55+00:00 · Latest: 2026-02-05T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06041v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06041v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视角描述预测相机姿态以实现空间推理</div>
<div class="mono" style="margin-top:8px">多图像空间推理对于当前的多模态大语言模型（MLLMs）仍然是一个具有挑战性的任务。虽然单视角感知本质上是2D的，但跨视角推理需要在不同视角之间建立一致的场景理解。特别地，我们研究视角转换（perspective taking），其中模型必须从多视角观察中构建一致的3D理解，并据此在新的语言指定视角下进行推理。我们提出了CAMCUE，这是一个姿态感知的多图像框架，它将相机姿态作为跨视角融合和新视角推理的显式几何锚点。CAMCUE将每个视角的姿态注入到视觉标记中，将自然语言视角描述锚定到目标相机姿态，并合成姿态条件下的想象目标视角以支持回答。为了支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例将多视角图像和姿态与多样化的目标视角描述和视角转换问题配对。我们还在测试集中包含了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确率提升了9.06%，并且能够从自然语言视角描述中预测目标姿态，其旋转准确率超过90%（在20°以内），平移准确率在0.5误差阈值内。这种直接的锚定避免了昂贵的测试时搜索和匹配过程，将推理时间从每个示例256.6秒减少到1.45秒，从而实现了快速、交互式的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance spatial reasoning capabilities in multimodal large language models (MLLMs) by enabling them to predict camera poses from perspective descriptions. The proposed method, CAMCUE, introduces a pose-aware framework that explicitly uses camera pose as a geometric anchor for cross-view fusion and novel-view reasoning. It integrates per-view camera poses into visual tokens, maps natural-language viewpoint descriptions to target poses, and synthesizes imagined target views to support reasoning. The main experimental results show that CAMCUE improves overall accuracy by 9.06% and achieves over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold, significantly reducing inference time from 256.6s to 1.45s per example.</div>
<div class="mono" style="margin-top:8px">本文针对当前多模态大语言模型（MLLMs）在多图像空间推理中的挑战，特别是难以从多个视角构建连贯的3D场景理解。提出的方法CAMCUE是一个基于相机姿态的框架，通过将相机姿态作为跨视角融合的几何锚点，支持多视角推理。该方法将每视角的相机姿态注入视觉标记中，将自然语言视角描述映射到目标姿态，并合成基于姿态的想象目标视角以辅助推理。为评估该方法，构建了包含27,668个训练样本和508个测试样本的CAMCUE-DATA数据集，其中包含多视角图像、姿态以及多样化的视角描述和视角转换问题。实验结果显示，CAMCUE整体准确率提升了9.06%，在旋转误差小于20°和位移误差小于0.5的条件下，旋转准确率超过90%，推理时间从256.6秒大幅降低至1.45秒每样本。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-05T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v1">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型（MLLMs）在空间推理方面取得了进展，越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式融合，这常常导致语义与几何的不匹配以及冗余信号。我们提出了GeoThinker框架，将整合范式从被动融合转向主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控进行校准，该门控机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面设定了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力和显著提升的空间感知能力，包括具身指称和自动驾驶。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), where geometric information is often misaligned with semantic content and leads to redundant signals. The proposed GeoThinker framework introduces an active perception approach by enabling the model to selectively retrieve geometric evidence based on its internal reasoning needs. It achieves this through Spatial-Grounded Fusion at specific Vision-Language Model (VLM) layers, using frame-strict cross-attention and Importance Gating to focus on task-relevant structures. Experimental results on the VSI-Bench show that GeoThinker achieves a peak score of 72.6, outperforming existing methods and demonstrating strong generalization across complex scenarios such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）在空间推理任务中被动整合几何信息的不足，指出其常导致语义与几何信息不对齐以及冗余信号的问题。提出GeoThinker框架，采用主动感知方法，使模型能够根据内部推理需求选择性地检索几何证据。该方法通过在特定VLM层应用空间基础融合，结合帧严格交叉注意力和重要性门控机制，聚焦于任务相关的结构。在VSI-Bench上的实验结果显示，GeoThinker取得了72.6的最高峰值得分，表明其在复杂场景如具身指称和自动驾驶中具有更强的空间感知能力和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-05T17:10:26+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cioutn.github.io/context-bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：面向编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估基准。ContextBench包含来自8种编程语言、66个仓库的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四种前沿的LLM和五个编码代理。我们的结果表明，复杂的代理框架在上下文检索方面仅带来边际收益（&quot;编码代理的苦涩教训&quot;），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有端到端基准，揭示了问题解决过程。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。数据和代码可在 https://cioutn.github.io/context-bench/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of current evaluations of coding agents, which primarily focus on final task success without considering the context retrieval process. ContextBench introduces a process-oriented benchmark that includes 1,136 issue-resolution tasks across eight programming languages, each with human-annotated gold contexts. The authors developed an automated evaluation framework to track agent behavior and assess context recall, precision, and efficiency during problem-solving. Experimental results reveal that advanced agent scaffolding provides only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between explored and used context. These findings highlight the importance of evaluating intermediate context usage in coding agents.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，即主要关注最终任务的成功率，而忽视了代码上下文的检索与使用过程。ContextBench 提出一个面向过程的评估基准，包含 1,136 个跨八种编程语言的代码问题解决任务，每个任务均附有人工标注的黄金上下文。作者构建了自动化评估框架，用于追踪代理行为并评估上下文的召回率、精确率和效率。实验结果表明，高级代理结构在上下文检索方面仅带来小幅提升，LLMs 更倾向于召回而非精确，且存在显著的探索与使用上下文之间的差距。这些发现强调了在软件任务中引入中间上下文指标的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution</div>
<div class="meta-line">Authors: Philipp D. Siedler</div>
<div class="meta-line">First: 2025-05-21T22:00:20+00:00 · Latest: 2026-02-05T16:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16048v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.16048v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPhyR：基于拓扑优化的材料分布空间-物理推理基准数据集</div>
<div class="mono" style="margin-top:8px">我们引入了一个新颖的数据集，用于基于拓扑优化方法评估大型语言模型（LLM）的物理和空间推理能力。该方法用于在给定载荷和支撑条件下计算设计空间内的最优材料分布。在该数据集中，LLM会接收到诸如2D边界、施加的力和支撑等条件，并需推理出相应的最优材料分布。数据集包含多种任务，从填充部分结构中的掩码区域到预测完整的材料分布。解决这些任务需要在给定约束下理解力的流动和所需的材料分布，而无需访问仿真工具或显式的物理模型，从而挑战模型对结构稳定性和空间组织的推理能力。我们的数据集旨在评估二维环境下的空间和物理推理能力，为传统的语言和逻辑基准提供补充视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SPhyR, a novel dataset aimed at evaluating the spatial and physical reasoning capabilities of Large Language Models (LLMs) in material distribution tasks. The dataset is based on topology optimization, where models are given 2D boundary conditions, applied forces, and supports, and must infer the optimal material layout without simulation tools or explicit physical models. The tasks include predicting complete material distributions and filling in masked regions of partial structures, requiring an understanding of force flow and structural stability. Experimental results show that LLMs can perform these tasks with varying degrees of accuracy, highlighting the potential and limitations of current models in spatial-physical reasoning.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为SPhyR的新数据集，旨在评估大型语言模型（LLM）在材料分布任务中的空间与物理推理能力。该数据集基于拓扑优化方法，为LLM提供二维边界条件、施加的力和支撑条件，要求其在不依赖仿真工具或显式物理模型的情况下推断最优材料布局。实验结果显示，模型能够预测完整的材料分布并填充部分结构中的掩码区域，表明其具备在二维环境下对力流和结构稳定性进行推理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非自我中心感知者：通过帧实例化将非自我中心推理与自我中心视觉先验分离</div>
<div class="mono" style="margin-top:8px">随着对空间基础任务（如视觉-语言导航/动作）的需求增加，视觉-语言模型（VLMs）中的非自我中心感知能力正受到越来越多的关注。然而，VLMs在需要显式视角转换的非自我中心空间查询上仍表现脆弱，答案依赖于目标中心的推理框架，而非观察到的相机视角。因此，我们引入了Allocentric Perceiver，这是一种无需训练的策略，通过使用现成的几何专家从一张或多张图像中恢复度量3D状态，然后实例化一个与指令语义意图对齐的查询条件非自我中心参考框架。通过确定性地将重建的几何结构转换到目标框架，并使用结构化的、基于几何的表示提示主干VLM，Allocentric Perceiver将心理旋转从隐式推理转移到显式计算。我们在多个主干家族上评估Allocentric Perceiver在空间推理基准测试中的表现，观察到在非自我中心任务中取得了稳定且显著的提升（约10%），同时保持了强大的自我中心性能，并超越了空间感知微调模型以及最先进的开源和专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing demand for spatially grounded tasks like Vision-Language Navigation and Action has highlighted the limitations of Vision-Language Models (VLMs) in handling allocentric spatial queries that require perspective shifts. To address this, the paper introduces Allocentric Perceiver, a training-free method that reconstructs metric 3D states from images using geometric experts and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s intent. This approach transforms the reconstructed geometry into the target frame deterministically and prompts the VLM with structured, geometry-based representations, shifting mental rotation from implicit to explicit computation. Experimental results show consistent and significant improvements ($\sim$10%) on allocentric tasks while preserving strong egocentric performance, outperforming both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div>
<div class="mono" style="margin-top:8px">随着对空间感知任务如视觉语言导航/动作的需求增加，视觉语言模型（VLMs）在处理需要视角转换的分配中心空间查询时表现出局限性。为此，本文提出了一种无需训练的策略 Allocentric Perceiver，该方法利用现成的几何专家从一张或多张图像中恢复度量3D状态，并根据指令的语义意图实例化一个查询条件下的分配中心参考系。通过将重建的几何信息确定性地转换为目标视角，并使用结构化的几何感知表示来提示主干VLM，该方法将心理旋转从隐式推理转移到显式计算。实验结果表明，在分配中心任务上取得了显著提升（约10%），同时保持了良好的自中心性能，优于空间感知微调模型及当前最先进的开源和专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?</div>
<div class="meta-line">Authors: Yikun Zong, Cheston Tan</div>
<div class="meta-line">First: 2026-02-05T11:49:30+00:00 · Latest: 2026-02-05T11:49:30+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05570v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TangramSR：视觉-语言模型能否在连续几何空间中进行推理？</div>
<div class="mono" style="margin-top:8px">人类通过心理旋转、迭代优化和视觉反馈等认知过程，在空间推理任务（如七巧板拼图）中表现出色。受人类通过试错、观察和修正解决七巧板问题的启发，我们设计了一个框架来建模这些人类认知机制。然而，对五种代表性视觉-语言模型（VLMs）的全面实验揭示了在连续几何推理中的系统性失败：单块任务的平均IoU仅为0.41，两块组合任务则降至0.23，远低于人类表现，儿童能够成功完成七巧板任务。本文解决了一个自改进AI的基本挑战：模型能否在测试时通过迭代优化预测结果而无需参数更新？我们引入了一种测试时自优化框架，结合上下文学习（ICL）与奖励引导的反馈循环，灵感来源于人类认知过程。我们的无训练验证-优化智能体应用递归优化循环，基于几何一致性反馈迭代优化预测结果，在中等三角形案例中实现了IoU从0.63到0.932的显著提升。这表明，通过ICL和奖励循环引入人类启发的迭代优化机制，可以显著增强视觉-语言模型的几何推理能力，推动自改进AI在连续空间领域从理论走向实践。我们的工作可通过此匿名链接获取：https://anonymous.4open.science/r/TangramVLM-F582/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the ability of Vision-Language Models (VLMs) to perform continuous geometric reasoning, inspired by human spatial reasoning in tasks like Tangram puzzle solving. The authors propose a test-time self-refinement framework that integrates in-context learning with reward-guided feedback loops to mimic human iterative refinement processes. Experimental results show that their verifier-refiner agent significantly improves geometric reasoning performance, achieving an IoU of 0.932 on medium-triangle cases from an initial 0.63, without requiring model retraining. These findings highlight the potential of human-inspired mechanisms in enhancing VLMs&#x27; spatial reasoning capabilities in continuous domains.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉语言模型（VLMs）在连续几何推理任务中的表现，受人类解决类似拼图任务（如七巧板）的启发。作者提出了一种测试时的自精炼框架，结合上下文学习与奖励引导的反馈循环，以模拟人类的迭代精炼过程。实验结果显示，该训练无关的验证-精炼代理在中等三角形案例中显著提升了几何推理性能，从初始的0.63 IoU提升至0.932，而无需模型再训练。这些结果表明，引入人类启发的机制可以有效增强VLMs在空间任务中的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Imagine a City: CityGenAgent for Procedural 3D City Generation</div>
<div class="meta-line">Authors: Zishan Liu, Zecong Tang, RuoCheng Wu, Xinzhe Zheng, Jingyu Hu, Ka-Hei Hui, Haoran Xie, Bo Dai, Zhengzhe Liu</div>
<div class="meta-line">First: 2026-02-05T06:36:03+00:00 · Latest: 2026-02-05T06:36:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05362v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05362v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models&#x27; generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>想象一座城市：用于程序化3D城市生成的CityGenAgent</div>
<div class="mono" style="margin-top:8px">交互式3D城市的自动化生成是自动驾驶、虚拟现实和具身智能等广泛应用中的关键挑战。尽管生成模型和程序化技术的最新进展提高了城市生成的逼真度，但现有方法在高保真资产创建、可控性和操作性方面仍存在困难。在本工作中，我们引入了CityGenAgent，这是一个由自然语言驱动的框架，用于高质量3D城市的分层程序化生成。我们的方法将城市生成分解为两个可解释的组成部分：Block Program和Building Program。为确保结构正确性和语义对齐，我们采用了一种两阶段学习策略：(1) 监督微调（SFT）。我们训练BlockGen和BuildingGen生成符合模式约束的有效程序，包括无自相交的多边形和完整的字段；(2) 强化学习（RL）。我们设计了空间对齐奖励（Spatial Alignment Reward）以增强空间推理能力，并设计了视觉一致性奖励（Visual Consistency Reward）以弥合文本描述与视觉模态之间的差距。得益于程序和模型的泛化能力，CityGenAgent支持自然语言编辑和操作。全面的评估表明，与现有方法相比，CityGenAgent在语义对齐、视觉质量和可控性方面表现更优，为可扩展的3D城市生成奠定了坚实的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to address the challenges in generating high-fidelity, controllable, and semantically aligned 3D cities for applications such as autonomous driving and virtual reality. CityGenAgent introduces a natural language-driven framework that decomposes city generation into two interpretable components: Block Program and Building Program. The framework employs a two-stage learning strategy, starting with Supervised Fine-Tuning to generate valid programs adhering to schema constraints, followed by Reinforcement Learning with custom rewards for spatial alignment and visual consistency. Experimental results show that CityGenAgent outperforms existing methods in semantic alignment, visual quality, and controllability, providing a robust solution for scalable 3D city generation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有方法在生成高保真度、可控性3D城市时的不足。CityGenAgent提出了一种基于自然语言驱动的框架，通过将城市生成过程分解为Block Program和Building Program两个可解释的组件实现分层程序化生成。该框架采用两阶段学习策略，首先通过监督微调确保生成程序符合结构约束，然后利用强化学习结合空间对齐奖励和视觉一致性奖励提升空间推理能力。实验结果表明，CityGenAgent在语义对齐、视觉质量和可控性方面优于现有方法，为可扩展的3D城市生成奠定了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking</div>
<div class="meta-line">Authors: Bishoy Galoaa, Xiangyu Bai, Utsav Nandi, Sai Siddhartha Vivek Dhir Rangoju, Somaieh Amraee, Sarah Ostadabbas</div>
<div class="meta-line">First: 2026-02-04T20:44:16+00:00 · Latest: 2026-02-04T20:44:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniTrack：用于多目标跟踪的可微分图表示学习</div>
<div class="mono" style="margin-top:8px">我们提出了UniTrack，这是一种即插即用的图论损失函数，旨在通过统一的可微分学习直接优化跟踪特定目标，从而显著提升多目标跟踪（MOT）性能。与之前需要重新设计跟踪架构的图基MOT方法不同，UniTrack提供了一个通用的训练目标，将检测精度、身份保持和时空一致性整合到一个端到端可训练的损失函数中，无需架构修改即可无缝集成到现有MOT系统中。通过可微分图表示学习，UniTrack使网络能够学习跨帧的运动连续性和身份关系的整体表示。我们在多种跟踪模型和多个具有挑战性的基准上验证了UniTrack，展示了其在所有测试架构和数据集（包括Trackformer、MOTR、FairMOT、ByteTrack、GTR和MOTE）上的一致性能提升。广泛评估表明，在具有挑战性的基准上，身份切换减少了最多53%，IDF1提高了12%，其中GTR在SportsMOT上实现了9.7%的MOTA性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniTrack introduces a differentiable graph-theoretic loss function aimed at improving multi-object tracking performance by directly optimizing tracking-specific objectives. It integrates detection accuracy, identity preservation, and spatiotemporal consistency into a unified end-to-end trainable framework, allowing for seamless integration with existing MOT systems without altering their architecture. The method enables networks to learn holistic motion and identity relationships across frames through differentiable graph representation learning. Experimental results across multiple MOT models and benchmarks show significant improvements, including up to 53% reduction in identity switches and 12% IDF1 gains, with GTR achieving a 9.7% increase in MOTA on SportsMOT.</div>
<div class="mono" style="margin-top:8px">UniTrack 提出了一种可微分的图论损失函数，旨在通过直接优化跟踪特定目标来提升多目标跟踪性能。该方法将检测精度、身份保持和时空一致性整合到一个统一的端到端可训练框架中，允许在不修改现有跟踪系统架构的情况下无缝集成。通过可微分图表示学习，网络能够学习跨帧的运动连续性和身份关系的整体表示。实验结果表明，在多种跟踪模型和基准测试中均取得显著提升，包括身份切换减少高达53%，IDF1提升12%，其中GTR在SportsMOT数据集上实现了9.7%的MOTA性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：通过视觉语言模型将任务规划接地为空间感知的自中心动作以实现人形机器人应用</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人本质上具有挑战性，因为这需要在部分信息观测和动态变化环境中紧密集成感知、移动和操作。此外，还需要在不同类型子任务之间稳健地进行转换。为了解决这些挑战，我们提出了一种新的任务——EgoActing，该任务要求将高层指令直接接地为各种精确且具有空间感知的人形动作。我们进一步通过引入EgoActor，一个统一且可扩展的视觉语言模型（VLM），来实例化这一任务。EgoActor能够预测移动基元（如行走、转向、侧移、高度变化）、头部运动、操作指令以及人机交互，从而实时协调感知与执行。我们利用来自现实世界演示的广泛监督，包括仅RGB的自中心数据、空间推理问答以及模拟环境演示，使EgoActor能够做出稳健且上下文感知的决策，并在1秒内进行流畅的动作推理。在模拟和现实环境中的大量评估表明，EgoActor能够有效连接抽象的任务规划与具体的运动执行，同时在多样任务和未见过的环境中实现泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The deployment of humanoid robots in real-world environments is challenging due to the need for integrating perception, locomotion, and manipulation under partial observations and dynamic conditions. To address this, the paper introduces EgoActing, a task that directly maps high-level instructions to spatially aware actions. EgoActor, a unified vision-language model, is designed to predict various locomotion primitives, head movements, and manipulation commands, enabling real-time coordination of perception and execution. The model is trained using real-world demonstrations, spatial reasoning tasks, and simulated data, allowing it to make context-aware decisions and perform fast action inference. Evaluations in both simulated and real environments show that EgoActor effectively connects abstract task planning with concrete motor execution and generalizes well across different tasks and settings.</div>
<div class="mono" style="margin-top:8px">在真实环境中部署人形机器人面临诸多挑战，需要在部分信息观测和动态变化的环境下整合感知、移动和操作能力。为解决这些问题，本文提出了EgoActing任务，该任务直接将高层指令映射到精确且具有空间感知的人形动作。论文引入了EgoActor，这是一个统一且可扩展的视觉-语言模型，能够预测如移动、转向、侧移和高度变化等动作，以及头部运动和操作指令。在模拟和真实环境中的广泛评估表明，EgoActor能够有效协调感知与执行，实现快速（小于1秒）且上下文感知的动作推理，具备良好的任务泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Think3D: Thinking with Space for Spatial Reasoning</div>
<div class="meta-line">Authors: Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang, Yijiang Li, Binghao Ran, Fuxi Zhang, Zhuohan Sun, Zhenfei Yin, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2026-01-19T13:13:54+00:00 · Latest: 2026-02-04T12:38:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13029v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13029v2">PDF</a> · <a href="https://github.com/zhangzaibin/spagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Think3D：以空间思维进行空间推理</div>
<div class="mono" style="margin-top:8px">理解并推理物理世界需要空间智能：即超越二维感知，解释几何、视角和空间关系的能力。尽管最近的视觉大模型（VLMs）在视觉理解方面表现出色，但它们本质上仍是二维感知器，难以进行真正的三维推理。我们引入了Think3D框架，使VLM代理能够通过三维空间进行思考。通过利用三维重建模型，从图像或视频中恢复点云和相机姿态，Think3D使代理能够通过基于相机的操作和自主/全局视角切换，主动操控空间，将空间推理转化为交互式的三维思维链过程。无需额外训练，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube上平均提升7.8%，在VSI-Bench上提升4.7%。我们进一步表明，对于难以进行空间探索的小型模型，通过强化学习策略使其能够选择信息性视角和操作，可显著提升其性能。借助强化学习，工具使用带来的性能提升从+0.7%增加到+6.8%。我们的研究结果表明，无需训练的工具增强型空间探索是一种实现多模态代理中更灵活、更接近人类的三维推理的可行路径，为多模态智能开辟了新的维度。代码和模型权重已发布在https://github.com/zhangzaibin/spagent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance spatial reasoning capabilities in vision large models (VLMs), which are currently limited to 2D perception and struggle with genuine 3D understanding. Think3D introduces a framework that enables VLM agents to interact with 3D space by integrating 3D reconstruction models to generate point clouds and camera poses from images or videos. This allows agents to perform camera-based operations and switch between ego and global views, effectively transforming spatial reasoning into an interactive 3D chain-of-thought process. The framework significantly improves spatial reasoning performance without additional training, achieving average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. Furthermore, it demonstrates that smaller models can benefit from reinforcement learning policies that guide viewpoint and operation selection, increasing the benefit from tool usage from +0.7% to +6.8%.</div>
<div class="mono" style="margin-top:8px">Think3D的研究动机是提升视觉大模型（VLMs）的空间推理能力，因为这些模型本质上是2D感知器，缺乏真正的3D理解。该框架通过结合3D重建模型，从图像或视频中生成点云和相机姿态，使智能体能够通过基于相机的操作和视角切换来交互式地操控3D空间。实验结果表明，在BLINK Multi-view和MindCube等任务中，Think3D在无需额外训练的情况下显著提升了空间推理性能，平均提升7.8%；同时，结合强化学习策略后，小型模型的空间推理能力也得到显著增强，工具使用带来的提升从0.7%增加到6.8%。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</div>
<div class="meta-line">Authors: Mingcong Lei, Honghao Cai, Yuyuan Yang, Yimou Wu, Jinke Ren, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Shaohan Jiang, Ge Wang, Junyuan Tan, Zhenglin Wan, Zheng Li, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han</div>
<div class="meta-line">First: 2025-08-02T15:39:42+00:00 · Latest: 2026-02-04T12:10:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01415v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.01415v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied intelligence aims to enable robots to learn, reason, and generalize robustly across complex real-world environments. However, existing approaches often struggle with partial observability, fragmented spatial reasoning, and inefficient integration of heterogeneous memories, limiting their capacity for long-horizon adaptation. To address this, we introduce RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory within a parallelized architecture for efficient long-horizon planning and interactive learning. Its core innovations are a dynamic spatial knowledge graph for scalable, consistent memory updates and a closed-loop planner with a critic module for adaptive decision-making. Extensive experiments on EmbodiedBench show that RoboMemory, instantiated with Qwen2.5-VL-72B-Ins, improves the average success rate by 26.5% over its strong baseline and even surpasses the closed-source SOTA, Claude-3.5-Sonnet. Real-world trials further confirm its capability for cumulative learning, with performance consistently improving over repeated tasks. Our results position RoboMemory as a scalable foundation for memory-augmented embodied agents, bridging insights from cognitive neuroscience with practical robotic autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMemory：一种受大脑启发的多记忆智能体框架，用于物理具身系统中的交互式环境学习</div>
<div class="mono" style="margin-top:8px">具身智能旨在使机器人能够在复杂的真实环境中进行学习、推理和稳健的泛化。然而，现有方法常面临部分可观测性、碎片化空间推理以及异构记忆整合效率低的问题，限制了其长时序适应能力。为解决这些问题，我们引入了RoboMemory，这是一种受大脑启发的框架，通过并行化架构将空间、时间、事件和语义记忆统一，以实现高效的长时序规划和交互式学习。其核心创新包括用于可扩展且一致记忆更新的动态空间知识图谱，以及带有批评模块的闭环规划器，以实现自适应决策。在EmbodiedBench上的大量实验表明，RoboMemory在Qwen2.5-VL-72B-Ins实例化后，其平均成功率比其强基线提高了26.5%，甚至超过了闭源的SOTA模型Claude-3.5-Sonnet。现实世界试验进一步验证了其累积学习能力，性能在重复任务中持续提升。我们的结果表明，RoboMemory为记忆增强的具身智能体提供了一个可扩展的基础，将认知神经科学的见解与实际的机器人自主性相结合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboMemory is introduced to enhance embodied intelligence by addressing challenges such as partial observability, fragmented spatial reasoning, and inefficient memory integration in physical robots. The framework integrates spatial, temporal, episodic, and semantic memory within a parallelized architecture, featuring a dynamic spatial knowledge graph for scalable memory updates and a closed-loop planner with a critic module for adaptive decision-making. Experimental results on EmbodiedBench demonstrate that RoboMemory, when implemented with Qwen2.5-VL-72B-Ins, achieves a 26.5% improvement in average success rate compared to a strong baseline and outperforms the closed-source state-of-the-art model, Claude-3.5-Sonnet. Real-world trials further validate its ability to support cumulative learning through repeated task execution.</div>
<div class="mono" style="margin-top:8px">RoboMemory旨在通过解决机器人在部分可观测性、空间推理和异构记忆整合方面的挑战，提升具身智能。该框架将空间、时间、事件和语义记忆统一于并行架构中，核心创新包括用于可扩展记忆更新的动态空间知识图谱和包含批评模块的闭环规划器。在EmbodiedBench上的实验表明，使用Qwen2.5-VL-72B-Ins实现的RoboMemory相比强基线模型平均成功率提升了26.5%，并超越了闭源的最先进模型Claude-3.5-Sonnet。实际应用测试进一步验证了其在重复任务中的累积学习能力，性能持续提升。</div>
</details>
</div>
<div class="card">
<div class="title">MapCoder-Lite: Distilling Multi-Agent Coding into a Single Small LLM</div>
<div class="meta-line">Authors: Woongkyu Lee, Junhee Cho, Jungwook Choi</div>
<div class="meta-line">First: 2025-09-22T08:19:11+00:00 · Latest: 2026-02-04T07:25:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17489v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17489v2">PDF</a> · <a href="https://github.com/aiha-lab/MapCoder-Lite">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale (&gt;30B) models or collapse when downsized to small open-source models. We present MapCoder-Lite, a framework for distilling the complex reasoning of large, multi-agent coding systems into a single 7B model. Our contribution is a novel, three-pillar methodology that synergistically generates, refines, and encodes multi-agent knowledge: (i) pass-based trajectory distillation from strong LLMs fixes format fragility in retrieval and reduces failures in debugging, (ii) supervisor-guided correction with global feedback strengthens planning and coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests shows that MapCoder-Lite more than doubles xCodeEval accuracy (from 13.2% to 28.3%), eliminates all format failures, while reducing GPU memory and token-generation time by 4x compared to a 32B model. It also achieves over 10% gains on simpler coding benchmarks, demonstrating broad improvements beyond competitive programming. These results demonstrate that careful agent-wise fine-tuning unleashes high-quality multi-agent coding on a small language model. Our code is publicly available at https://github.com/aiha-lab/MapCoder-Lite.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapCoder-Lite：将多智能体编程蒸馏到单一小型大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）已经从单功能任务扩展到具有竞争力的编程问题，但现有的多智能体解决方案要么依赖昂贵的大型模型（&gt;30B参数），要么在缩小到小型开源模型时失效。我们提出了MapCoder-Lite，一个将大型多智能体编程系统的复杂推理蒸馏到单一7B参数模型的框架。我们的贡献是一种新颖的三支柱方法，协同生成、优化和编码多智能体知识：(i) 基于传递的轨迹蒸馏从强大的LLMs中修复检索中的格式脆弱性并减少调试失败；(ii) 带有全局反馈的监督引导修正增强了规划和编码智能体；(iii) 智能体级的LoRA微调实现了内存高效的专门化。在xCodeEval、APPS和CodeContests上的全面评估表明，MapCoder-Lite的xCodeEval准确率超过原来的两倍（从13.2%提升至28.3%），消除了所有格式错误，同时相比32B模型减少了4倍的GPU内存和令牌生成时间。它还在更简单的编程基准测试中实现了超过10%的提升，展示了在编程竞赛之外的广泛改进。这些结果表明，仔细的智能体级微调可以在小型语言模型上释放高质量的多智能体编程能力。我们的代码可在https://github.com/aiha-lab/MapCoder-Lite上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing multi-agent coding systems, which either require expensive large-scale models or fail when scaled down. MapCoder-Lite proposes a framework that distills the complex reasoning of such systems into a single 7B parameter model. It employs a three-pillar methodology: pass-based trajectory distillation to improve retrieval and debugging, supervisor-guided correction with global feedback to enhance planning and coding, and agent-wise LoRA fine-tuning for memory-efficient specialization. Experimental results on xCodeEval, APPS, and CodeContests show that MapCoder-Lite significantly improves accuracy, achieving over 28% on xCodeEval, eliminates all format failures, and reduces GPU memory and token-generation time by 4x compared to a 32B model.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有多智能体编码系统存在的问题，这些系统要么需要昂贵的大规模模型，要么在缩小规模后无法正常运行。MapCoder-Lite提出了一种三支柱方法，将多智能体编码能力蒸馏到单个7B参数模型中，包括基于传递的轨迹蒸馏、监督者引导的全局反馈修正以及智能体级别的LoRA微调。在xCodeEval、APPS和CodeContests等数据集上的实验结果表明，MapCoder-Lite在xCodeEval上的准确率从13.2%提升至28.3%，同时将GPU内存使用和生成令牌时间减少了4倍。</div>
</details>
</div>
<div class="card">
<div class="title">Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization</div>
<div class="meta-line">Authors: Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing</div>
<div class="meta-line">First: 2025-09-15T20:36:19+00:00 · Latest: 2026-02-04T05:16:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12434v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.12434v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues. A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs. While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS. Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents. To bridge this gap, we introduce EntroPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting. EntroPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses. We validate EntroPO by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches. On the SWEBENCH leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with EntroPO ranks 1st on SWEBENCH-LITE and 4th on SWEBENCH-VERIFIED on the open-weight leaderboard, surpassed only by models with over 10x more parameters(e.g., &gt;$350B).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过熵增强的多轮偏好优化构建编码代理</div>
<div class="mono" style="margin-top:8px">软件工程为大型语言模型（LLMs）提出了复杂且多步骤的挑战，需要在大型代码库上进行推理并协调使用工具。这些任务的难度在诸如SWE-bench等基准测试中得到了体现，其中当前的LLMs仍然难以解决现实问题。一种有前景的方法是测试时扩展（TTS），但其效果高度依赖于模型输出的多样性。虽然标准对齐方法如直接偏好优化（DPO）和卡尼曼-特沃斯基优化（KTO）在将模型输出与人类偏好对齐方面是有效的，但这一过程可能会降低多样性，从而限制TTS的效果。此外，现有的偏好优化算法通常设计用于单轮任务，无法充分应对交互式编码代理所需的多轮推理和工具集成的复杂性。为弥合这一差距，我们引入了EntroPO，这是一种熵增强框架，将现有的偏好优化算法适应到多轮、工具辅助的环境中。EntroPO通过显式保留策略熵来增强偏好目标，并将学习过程推广到多轮交互的优化，而非单轮响应。我们通过微调来自不同家族和规模（最高达106B参数）的多样化模型来验证EntroPO。为了最大化TTS带来的性能提升，我们进一步提出了一种混合最佳轨迹选择方案，结合了学习的验证器模型与无模型方法。在SWEBENCH排行榜上，我们的方法在开放权重模型中建立了新的最先进结果。使用EntroPO训练的30B参数模型在开放权重排行榜上分别在SWEBENCH-LITE和SWEBENCH-VERIFIED中排名第一和第四，仅被参数量超过其10倍的模型（例如，&gt;350B参数）超越。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current Large Language Models (LLMs) in handling complex, multi-step coding tasks by introducing EntroPO, an entropy-enhanced multi-turn preference optimization framework. The motivation stems from the challenges faced by LLMs in real-world software engineering scenarios, such as those in the SWE-bench benchmark, where they struggle with reasoning over large codebases and tool integration. EntroPO improves upon existing methods like DPO and KTO by preserving policy entropy and optimizing over multi-turn interactions, thereby enhancing the effectiveness of test-time scaling. The framework is validated by fine-tuning various models, and the results show that a 30B parameter model trained with EntroPO achieves top rankings on the SWEBENCH leaderboard, outperforming other open-weight models and being surpassed only by much larger parameter models.</div>
<div class="mono" style="margin-top:8px">本文针对当前大语言模型（LLMs）在处理复杂多步骤编码任务时的局限性，提出了EntroPO框架，该框架通过增强熵值来优化多轮偏好。研究动机源于LLMs在现实编码场景中仍面临挑战，如SWE-bench基准测试所示，同时现有对齐方法在提升模型与人类偏好一致性时可能降低输出多样性。EntroPO通过保留策略熵并优化多轮交互，改进了传统偏好优化方法，使其适用于需要工具协作和多步推理的编码代理。实验结果表明，该框架在SWEBENCH排行榜上取得了最佳效果，其中30B参数模型在SWEBENCH-LITE和SWEBENCH-VERIFIED中分别位列第一和第四，仅低于参数量超过10倍的模型。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</div>
<div class="meta-line">Authors: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</div>
<div class="meta-line">First: 2026-01-06T11:54:47+00:00 · Latest: 2026-02-04T04:36:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing large audio-language models perceive the world as &quot;mono&quot;-a single stream of audio that ignores the critical spatial dimension (&quot;where&quot;) required for universal audio scene analysis (ASA). To bridge this gap, we first introduce a hierarchical framework for audio scene analysis. Guided by this framework, we introduce a system that enables large audio-language models (LALMs) to understand and reason about the complex acoustic world.
  Our system endows LALMs with universal spatial understanding through four key innovations: (1) A scalable simulation pipeline that synthesizes high-quality First-Order-Ambisonics(FOA) data; (2) A unified model framework that integrates universal spatial encoding with a dense hybrid projection mechanism to bridge the modality gap; (3) A progressive training curriculum that evolves from representation alignment to reinforcement learning-based reasoning; and (4) A comprehensive benchmark for audio scene analysis (ASA) designed to rigorously evaluate atomic perception, relational integration, and cognitive reasoning capabilities, on which our model demonstrates comparatively strong capability for spatial understanding. Our work provides a clear pathway for leveraging the powerful reasoning abilities of LALMs towards holistic ASA, advancing from &quot;mono&quot; semantic recognition to spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界并非单声道：在大型音频语言模型中实现空间理解</div>
<div class="mono" style="margin-top:8px">现有的大型音频语言模型将世界视为&quot;单声道&quot;，即忽略对通用音频场景分析（ASA）至关重要的空间维度（&quot;在哪里&quot;）。为弥合这一差距，我们首先提出了一种用于音频场景分析的分层框架。基于该框架，我们引入了一种系统，使大型音频语言模型（LALMs）能够理解和推理复杂的声学世界。
我们的系统通过四项关键创新赋予LALMs普遍的空间理解能力：(1) 一个可扩展的模拟流水线，合成高质量的单阶全向声学（FOA）数据；(2) 一个统一的模型框架，将普遍空间编码与密集混合投影机制结合，以弥合模态差距；(3) 一种渐进式训练课程，从表示对齐逐步发展到基于强化学习的推理；(4) 一个全面的音频场景分析（ASA）基准测试，旨在严格评估基本感知、关系整合和认知推理能力，我们的模型在该基准上表现出较强的对空间的理解能力。我们的工作为利用LALMs强大的推理能力实现全面的ASA提供了清晰的路径，从&quot;单声道&quot;语义识别推进到空间智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of existing large audio-language models (LALMs) that perceive the world as a single audio stream, neglecting the spatial dimension essential for universal audio scene analysis (ASA). The authors propose a hierarchical framework and a system that enhances LALMs with spatial understanding through four innovations: a scalable FOA data simulation pipeline, a unified model integrating spatial encoding with hybrid projection, a progressive training curriculum from alignment to reasoning, and a comprehensive ASA benchmark. The model demonstrates strong performance in spatial perception, relational integration, and cognitive reasoning, marking a significant step toward spatial intelligence in audio-language models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升大型音频语言模型（LALMs）的空间理解能力，因为当前模型将音频视为单一流，忽略了音频场景分析（ASA）中至关重要的空间维度。提出的方法引入了一个分层的音频场景分析框架，并构建了一个系统，包含四项关键创新：可扩展的FOA数据模拟流水线、融合空间编码与混合投影的统一模型框架、从表征对齐到基于强化学习的推理的渐进式训练课程，以及一个全面的音频场景分析基准测试。实验结果表明，该模型在空间理解任务中表现出色，显著提升了在原子感知、关系整合和认知推理方面的能力。</div>
</details>
</div>
<div class="card">
<div class="title">CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning</div>
<div class="meta-line">Authors: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le</div>
<div class="meta-line">First: 2025-05-31T23:32:01+00:00 · Latest: 2026-02-03T23:34:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00750v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00750v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codesense-bench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning about code semantics is essential for enhancing code LLMs&#x27; abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limits models&#x27; capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CodeSense：面向代码语义推理的现实世界基准和数据集</div>
<div class="mono" style="margin-top:8px">理解并推理代码语义对于增强代码大语言模型（LLMs）解决现实世界软件工程（SE）任务的能力至关重要。尽管已有多个代码推理基准，但大多数依赖合成数据集或教育编程问题，且主要关注输入/输出预测等粗粒度推理任务，这限制了它们在实际软件工程场景中评估LLMs的有效性。为弥合这一差距，我们提出了CodeSense，这是首个提供一系列针对现实代码软件工程的细粒度代码推理任务的基准。我们从现实世界仓库中收集了Python、C和Java软件项目，执行了这些仓库中的测试，收集了其执行轨迹，并构建了一个用于细粒度语义推理任务的基准数据集。随后，我们在最先进的LLMs上进行了全面评估。我们的结果表明，模型在处理细粒度推理任务时存在明显的性能差距。尽管链式思维和上下文学习等提示技术有所帮助，但LLMs中缺乏代码语义从根本上限制了其代码推理能力。除了数据集、基准和评估，我们的工作还产生了一个执行追踪框架和工具集，使得收集细粒度SE推理任务的基准数据变得容易，为未来基准构建和模型微调提供了坚实的基础。我们的代码和数据位于https://codesense-bench.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the ability of code large language models (LLMs) to perform real-world software engineering tasks by enhancing their understanding of code semantics. CodeSense introduces a new benchmark and dataset that includes fine-grained code reasoning tasks derived from real-world code repositories, addressing the limitations of existing benchmarks that rely on synthetic or educational data. The dataset was constructed by collecting execution traces from real Python, C, and Java projects, and the evaluation on state-of-the-art LLMs reveals a significant performance gap in handling fine-grained reasoning tasks, indicating that current models lack sufficient semantic understanding of code.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升代码大语言模型在实际软件工程任务中的表现，因为现有基准主要关注粗粒度的代码推理任务，难以有效评估模型在真实场景中的能力。CodeSense提出了首个包含真实世界代码细粒度推理任务的基准和数据集，涵盖Python、C和Java项目。通过执行代码并收集执行轨迹，构建了用于细粒度语义推理的基准数据集。对当前最先进的LLMs进行综合评估发现，模型在处理细粒度推理任务时存在明显性能差距，表明其缺乏对代码语义的深入理解。尽管链式推理和上下文学习等提示技术有所改善，但LLMs本身对代码语义的缺失仍是根本限制。此外，本研究还提供了一个执行轨迹框架和工具集，便于未来构建基准和进行模型训练。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents</div>
<div class="meta-line">Authors: Bhanu Prakash Vangala, Ali Adibifar, Ashish Gehani, Tanu Malik</div>
<div class="meta-line">First: 2025-12-26T21:17:22+00:00 · Latest: 2026-02-03T22:46:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22387v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22387v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI生成的代码不可复现（暂且如此）：基于大语言模型的编码代理依赖差距实证研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为编码代理的兴起承诺加速软件开发，但其对生成代码可复现性的影响仍鲜有研究。本文通过实证研究探讨LLM生成的代码是否可以在仅使用操作系统包和模型指定依赖的干净环境中成功执行。我们评估了三个最先进的LLM编码代理（Claude Code、OpenAI Codex和Gemini）在Python、JavaScript和Java三种语言下，由100个标准化提示生成的300个项目。我们引入了一个三层依赖框架（区分声明依赖、工作依赖和运行时依赖），以量化代码执行的可复现性。研究结果显示，仅有68.3%的项目能够直接运行，不同语言之间存在显著差异（Python为89.2%，Java为44.0%）。我们还发现，从声明依赖到实际运行时依赖的平均扩展率为13.5倍，揭示了大量隐藏依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reproducibility of code generated by large language models (LLMs) when used as coding agents. The motivation stems from the growing use of LLMs in software development and the need to understand their impact on code execution reliability. The researchers evaluate three LLM-based coding agents—Claude Code, OpenAI Codex, and Gemini—on 300 projects across Python, JavaScript, and Java. They introduce a three-layer dependency framework to categorize claimed, working, and runtime dependencies. The main findings reveal that only 68.3% of the generated projects can be executed successfully in a clean environment, with notable differences across programming languages. Additionally, the average number of runtime dependencies expands 13.5 times compared to the declared ones, highlighting the presence of significant hidden dependencies.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）生成代码在干净环境中的可执行性问题。研究动机源于LLM编码代理在软件开发中的广泛应用，以及对其生成代码可重复性影响的了解不足。作者评估了三个先进的编码代理——Claude Code、OpenAI Codex和Gemini——在Python、JavaScript和Java三种语言中，基于100个标准化提示生成的300个项目。他们引入了一个三层依赖框架，用于区分声明依赖、工作依赖和运行时依赖。结果显示，仅有68.3%的项目无需额外配置即可成功运行，不同编程语言之间存在显著差异。此外，运行时依赖的数量平均是声明依赖的13.5倍，揭示了大量隐藏的依赖关系。</div>
</details>
</div>
<div class="card">
<div class="title">FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation</div>
<div class="meta-line">Authors: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Mingjie Zhan, Hongsheng Li</div>
<div class="meta-line">First: 2026-02-03T18:01:34+00:00 · Latest: 2026-02-03T18:01:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03798v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03798v1">PDF</a> · <a href="https://github.com/mnluzimu/FullStack-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FullStack-Agent: 通过面向开发的测试和仓库反向翻译增强代理式全栈网页编码</div>
<div class="mono" style="margin-top:8px">帮助非专家用户开发复杂的交互式网站已成为LLM驱动代码代理的热门任务。然而，现有的代码代理往往只生成前端网页，用华丽的视觉效果掩盖了实际全栈数据处理和存储的缺失。值得注意的是，构建生产级的全栈网页应用远比仅生成前端网页更具挑战性，需要对数据流进行细致控制，全面理解不断更新的包和依赖关系，并准确定位代码库中模糊的错误。为了解决这些困难，我们引入了FullStack-Agent，这是一个统一的代理系统，用于全栈代理式编码，包含三个部分：(1) FullStack-Dev，一个具备强大规划、代码编辑、代码库导航和错误定位能力的多代理框架；(2) FullStack-Learn，一种创新的数据扩展和自我提升方法，通过反向翻译爬取和合成的网站仓库来提升FullStack-Dev的核心LLM；(3) FullStack-Bench，一个全面的基准测试系统，系统性地测试生成网站的前端、后端和数据库功能。我们的FullStack-Dev在前端、后端和数据库测试用例上分别优于之前最先进的方法8.7%、38.2%和15.9%。此外，FullStack-Learn通过自我提升，使一个30B模型在三个测试用例集上的性能分别提升了9.7%、9.5%和2.8%，证明了我们方法的有效性。代码已发布在https://github.com/mnluzimu/FullStack-Agent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing code agents that focus only on frontend development, neglecting the complexities of full-stack web applications. FullStack-Agent introduces a three-component system: FullStack-Dev, a multi-agent framework with advanced planning, code editing, and bug localization capabilities; FullStack-Learn, a self-improving method that enhances the backbone LLM through repository back-translation; and FullStack-Bench, a benchmark for evaluating frontend, backend, and database functionalities. The results show that FullStack-Dev improves performance by 8.7%, 38.2%, and 15.9% on frontend, backend, and database tasks respectively, while FullStack-Learn boosts a 30B model&#x27;s performance by 9.7%, 9.5%, and 2.8% on the same test sets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有代码代理仅关注前端开发、忽略全栈应用构建复杂性的不足。提出的FullStack-Agent系统包含三个部分：FullStack-Dev，一个具备强大规划、代码编辑和代码库导航与错误定位能力的多代理框架；FullStack-Learn，一种通过爬取和合成网站仓库进行反向翻译的自改进方法，用于提升FullStack-Dev的核心大语言模型；以及FullStack-Bench，一个全面评估生成网站前端、后端和数据库功能的基准测试。实验结果显示，FullStack-Dev在前端、后端和数据库任务上的表现分别提升了8.7%、38.2%和15.9%，而FullStack-Learn使30B模型在三项任务上的性能分别提高了9.7%、9.5%和2.8%，验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?</div>
<div class="meta-line">Authors: Azmine Toushik Wasi, Wahid Faisal, Abdur Rahman, Mahfuz Ahmed Anik, Munem Shahriar, Mohsin Mahmud Topu, Sadia Tasnim Meem, Rahatun Nesa Priti, Sabrina Afroz Mitu, Md. Iqramul Hoque, Shahriyar Zaman Ridoy, Mohammed Eunus Ali, Majd Hawasly, Mohammad Raza, Md Rizwan Parvez</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:52:02+00:00 · Latest: 2026-02-03T17:52:02+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. 92 Pages. 42 Figures and 29 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03916v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spatialab-reasoning.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs&#x27; spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth &amp; Occlusion, Orientation, Size &amp; Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs&#x27; spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatiaLab：视觉语言模型能否在真实环境中进行空间推理？</div>
<div class="mono" style="margin-top:8px">空间推理是人类认知的基本组成部分，但仍然是当前视觉语言模型（VLMs）面临的主要挑战。以往的工作主要依赖于合成或LLM生成的环境，任务设计和类似谜题的设置有限，未能捕捉到VLMs在现实世界中遇到的复杂性、视觉噪声和多样的空间关系。为了解决这一问题，我们引入了SpatiaLab，这是一个用于评估VLMs在现实、无约束情境下空间推理能力的全面基准。SpatiaLab包含六个主要类别：相对位置、深度与遮挡、方向、大小与比例、空间导航和3D几何，每个类别下有五个子类别，共计30种不同的任务类型。每个子类别至少包含25个问题，每个主类别至少包含200个问题，支持多项选择和开放式评估。在多种最先进的VLMs上进行的实验表明，其空间推理能力与人类存在显著差距。在多项选择设置中，InternVL3.5-72B的准确率为54.93%，而人类为87.57%。在开放式的设置中，所有模型的性能下降约10-25%，其中GPT-5-mini得分最高，为40.93%，而人类为64.93%。这些结果突显了处理复杂空间关系、深度感知、导航和3D几何方面的关键局限性。通过提供一个多样化的现实评估框架，SpatiaLab揭示了推进VLMs空间推理能力的关键挑战和机遇，为未来研究提供了指导基准。SpatiaLab可在以下网址获取：https://spatialab-reasoning.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the spatial reasoning capabilities of vision-language models (VLMs) in real-world scenarios, highlighting their limitations compared to human performance. The researchers developed SpatiaLab, a benchmark consisting of 1,400 visual question-answer pairs across six categories and 30 task types, designed to reflect the complexity and diversity of spatial relationships encountered in natural environments. Experimental results show that even advanced models like InternVL3.5-72B achieve only 54.93% accuracy in multiple-choice tasks, significantly lower than human performance at 87.57%, while open-ended tasks see an even greater performance drop, with GPT-5-mini reaching 40.93% versus 64.93% for humans. These findings underscore the need for improved spatial reasoning in VLMs and provide a valuable benchmark for future research.</div>
<div class="mono" style="margin-top:8px">该研究探讨了视觉语言模型（VLMs）在现实场景中的空间推理能力，揭示了其与人类表现之间的显著差距。为此，作者提出了SpatiaLab，一个包含1400个视觉问答对的基准测试，涵盖六个主要类别及多个子任务。实验结果显示，即使是先进的模型如InternVL3.5-72B，在多项选择设置中也仅达到54.93%的准确率，远低于人类的87.57%。在开放式任务中，所有模型的表现均下降了10-25%，其中GPT-5-mini表现最佳，得分为40.93%。这些结果突显了VLMs在处理复杂空间关系、深度感知、导航和三维几何方面的重要局限性，并为未来研究提供了有价值的评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</div>
<div class="meta-line">Authors: Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao</div>
<div class="meta-line">First: 2025-03-17T09:24:26+00:00 · Latest: 2026-02-03T13:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12968v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiPMB：基于优化泊松多伯努利滤波的3D多目标跟踪增强</div>
<div class="mono" style="margin-top:8px">准确的3D多目标跟踪（MOT）对自动驾驶至关重要，因为它能够实现复杂环境下的鲁棒感知、导航和规划。尽管基于深度学习的解决方案在3D MOT任务中表现出色，但基于模型的方法因其简单性、可解释性和数据效率仍然具有吸引力。传统的基于模型的跟踪器通常依赖于随机向量贝叶斯滤波器，在基于检测的跟踪（TBD）框架中运行，但由于启发式的数据关联和轨迹管理方案而存在局限性。相比之下，基于随机有限集（RFS）的贝叶斯滤波方法在理论上处理了目标的生成、存活和消失，从而增强了可解释性和参数调整能力。本文提出了一种新的基于RFS的3D MOT方法OptiPMB，该方法在基于检测的跟踪框架中采用优化的泊松多伯努利（PMB）滤波器，并结合了多项关键创新设计。具体而言，我们提出了一种测量驱动的混合自适应生成模型以提高轨迹初始化效果，采用自适应检测概率参数以有效维护被遮挡目标的轨迹，并优化了密度剪枝和轨迹提取模块以进一步提升整体跟踪性能。在nuScenes和KITTI数据集上的大量评估表明，OptiPMB在跟踪精度上优于现有最先进的方法，从而为基于模型的3D MOT建立了新的基准，并为未来基于RFS的跟踪器研究提供了有价值的参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the accuracy of 3D multi-object tracking (MOT) in autonomous driving by leveraging the advantages of model-based approaches over deep learning-based ones, such as simplicity, interpretability, and data efficiency. OptiPMB introduces an optimized Poisson multi-Bernoulli (PMB) filter within the tracking-by-detection (TBD) framework, incorporating innovations like a measurement-driven hybrid adaptive birth model, adaptive detection probability parameters, and optimized density pruning and track extraction modules. Experimental results on the nuScenes and KITTI datasets demonstrate that OptiPMB achieves superior tracking accuracy compared to existing state-of-the-art methods, setting a new benchmark for model-based 3D MOT.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用模型化方法在自动驾驶中提升三维多目标跟踪（MOT）的准确性，相较于深度学习方法，模型化方法具有简单、可解释和数据效率高的优势。OptiPMB在跟踪-检测（TBD）框架内引入了优化的泊松多伯努利（PMB）滤波器，并结合了测量驱动的混合自适应出生模型、自适应检测概率参数以及优化的密度剪枝和轨迹提取模块。在nuScenes和KITTI数据集上的实验结果表明，OptiPMB在跟踪精度上优于现有最先进的方法，为模型化三维MOT设定了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">CP-Agent: Agentic Constraint Programming</div>
<div class="meta-line">Authors: Stefan Szeider</div>
<div class="meta-line">First: 2025-08-10T19:59:01+00:00 · Latest: 2026-02-03T12:13:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The translation of natural language to formal constraint models requires expertise in the problem domain and modeling frameworks. To explore the effectiveness of agentic workflows, we propose CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel. We provide the relevant domain knowledge as a project prompt of under 50 lines. The algorithm works by iteratively executing code, observing the solver&#x27;s feedback, and refining constraint models based on execution results.
  We evaluate CP-Agent on 101 constraint programming problems from CP-Bench. We made minor changes to the benchmark to address systematic ambiguities in the problem specifications and errors in the ground-truth models. On the clarified benchmark, CP-Agent achieves perfect accuracy on all 101 problems. Our experiments show that minimal guidance outperforms detailed procedural scaffolding. Our experiments also show that explicit task management tools can have both positive and negative effects on focused modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CP-Agent：代理驱动的约束编程</div>
<div class="mono" style="margin-top:8px">将自然语言翻译为形式化约束模型需要对问题领域和建模框架有专业知识。为探索代理工作流的有效性，我们提出了CP-Agent，这是一个使用ReAct框架并带有持久IPython内核的Python编码代理。我们将相关的领域知识作为项目提示提供，长度不超过50行。该算法通过迭代执行代码、观察求解器的反馈，并根据执行结果改进约束模型来工作。我们在CP-Bench的101个约束编程问题上评估了CP-Agent。我们对基准进行了小幅修改，以解决问题说明中的系统性歧义和真实模型中的错误。在澄清后的基准上，CP-Agent在所有101个问题上均实现了完美准确率。我们的实验表明，最小的指导优于详细的程序框架。我们的实验还表明，显式的任务管理工具对集中建模任务可能有正面和负面的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the effectiveness of agentic workflows in translating natural language into formal constraint models. CP-Agent, a Python coding agent, is proposed that integrates the ReAct framework with a persistent IPython kernel, using concise domain knowledge provided as a project prompt. The agent iteratively executes code, observes solver feedback, and refines constraint models based on execution outcomes. Evaluation on 101 constraint programming problems from CP-Bench, with minor adjustments to address ambiguities and errors, shows that CP-Agent achieves perfect accuracy across all problems. The results indicate that minimal guidance can outperform detailed procedural scaffolding, while explicit task management tools may have mixed impacts on focused modeling tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探讨代理工作流在将自然语言转化为形式化约束模型中的有效性。为此，提出了一种名为CP-Agent的Python编码代理，其结合了ReAct框架与持久化的IPython内核，并使用不超过50行的项目提示提供相关领域知识。该代理通过迭代执行代码、观察求解器反馈并根据执行结果优化约束模型来工作。在对CP-Bench的101个约束编程问题进行评估后，通过微调基准以解决问题描述中的系统性歧义和真实模型中的错误，CP-Agent在修正后的基准上实现了全部问题的完美准确率。实验结果表明，最小限度的指导可能优于详细的程序框架，而显式的任务管理工具对专注建模任务的影响可能是正负参半的。</div>
</details>
</div>
<div class="card">
<div class="title">InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation</div>
<div class="meta-line">Authors: Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu, Yanyong Zhang</div>
<div class="meta-line">First: 2026-02-03T08:22:13+00:00 · Latest: 2026-02-03T08:22:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03242v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://shanpoyang654.github.io/InstaDrive/page.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#x27;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InstaDrive：面向实例的驾驶世界模型用于真实且一致的视频生成</div>
<div class="mono" style="margin-top:8px">自动驾驶依赖于在高质量、大规模多视角驾驶视频上训练的鲁棒模型。虽然世界模型为生成真实驾驶视频提供了一种成本效益高的解决方案，但它们在保持实例级时间一致性和空间几何保真度方面存在困难。为了解决这些挑战，我们提出了InstaDrive，一个通过两项关键进展提升驾驶视频真实性的新框架：(1) 实例流引导器，从帧中提取并传播实例特征以强制时间一致性，保持实例身份随时间不变。(2) 空间几何对齐器，提升空间推理能力，确保实例定位精确，并显式建模遮挡层次。通过引入这些实例感知机制，InstaDrive实现了最先进的视频生成质量，并在nuScenes数据集上增强了下游自动驾驶任务的性能。此外，我们利用CARLA的自动驾驶功能，在多样化的地图和区域上程序化和随机地模拟罕见但安全关键的驾驶场景，从而实现对自动驾驶系统的严格安全评估。我们的项目页面是https://shanpoyang654.github.io/InstaDrive/page.html。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InstaDrive was developed to improve the realism and consistency of driving videos generated by world models, which are essential for training autonomous driving systems. The framework introduces two key components: the Instance Flow Guider, which maintains temporal consistency by propagating instance features across frames, and the Spatial Geometric Aligner, which enhances spatial accuracy and models occlusion hierarchies. These instance-aware mechanisms lead to state-of-the-art video generation quality and improve performance in downstream autonomous driving tasks on the nuScenes dataset. Additionally, the use of CARLA&#x27;s autopilot enables the simulation of rare and safety-critical driving scenarios for thorough evaluation.</div>
<div class="mono" style="margin-top:8px">InstaDrive 是为了提升驾驶视频生成的逼真度和一致性而开发的，这对自动驾驶系统的训练至关重要。该框架引入了两个关键组件：实例流引导器，通过在帧间传播实例特征来维持时间一致性，并保持实例身份；以及空间几何对齐器，增强空间推理能力，确保实例的精确定位并显式建模遮挡层次。在 nuScenes 数据集上的实验结果表明，InstaDrive 在视频生成质量上达到了最先进的水平，并提升了下游自动驾驶任务的效果。</div>
</details>
</div>
<div class="card">
<div class="title">GeoResponder: Towards Building Geospatial LLMs for Time-Critical Disaster Response</div>
<div class="meta-line">Authors: Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</div>
<div class="meta-line">First: 2025-09-18T09:46:55+00:00 · Latest: 2026-02-03T07:50:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19354v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19354v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models excel at linguistic tasks but lack the inner geospatial capabilities needed for time-critical disaster response, where reasoning about road networks, continuous coordinates, and access to essential infrastructure such as hospitals, shelters, and pharmacies is vital. We introduce GeoResponder, a framework that instills robust spatial reasoning through a scaffolded instruction-tuning curriculum. By stratifying geospatial learning into different cognitive layers, we effectively anchor semantic knowledge to the continuous coordinate manifold and enforce the internalization of spatial axioms. Extensive evaluations across four topologically distinct cities and diverse tasks demonstrate that GeoResponder significantly outperforms both state-of-the-art foundation models and domain-specific baselines. These results suggest that LLMs can begin to internalize and generalize geospatial structures, pointing toward the future development of language models capable of supporting disaster response needs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoResponder：构建用于时间关键型灾害响应的地理空间大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型在语言任务上表现出色，但在时间关键型灾害响应中缺乏必要的地理空间能力，其中对道路网络、连续坐标以及医院、避难所和药店等关键基础设施的推理至关重要。我们引入了GeoResponder框架，通过结构化的指令微调课程来增强其强大的空间推理能力。通过将地理空间学习分层为不同的认知层次，我们有效地将语义知识锚定在连续坐标空间中，并强制模型内化空间公理。在四个拓扑结构不同的城市和多种任务上的广泛评估表明，GeoResponder在性能上显著优于最先进的基础模型和领域特定基线。这些结果表明，大语言模型开始能够内化和泛化地理空间结构，预示着未来能够支持灾害响应需求的语言模型的发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GeoResponder addresses the limitations of large language models (LLMs) in handling geospatial tasks critical for disaster response. The framework introduces a scaffolded instruction-tuning curriculum to enhance spatial reasoning capabilities, enabling LLMs to understand and operate on continuous coordinates and essential infrastructure. Evaluations across four topologically distinct cities show that GeoResponder outperforms both state-of-the-art foundation models and domain-specific baselines, indicating that LLMs can effectively internalize and generalize geospatial structures for real-world applications.</div>
<div class="mono" style="margin-top:8px">开发GeoResponder的动机是提升大型语言模型（LLM）在时间敏感的灾害响应场景中的地理空间推理能力。该框架通过结构化的指令微调课程，将强大的空间理解能力嵌入到LLM中，使其能够处理道路网络、连续坐标以及医院、避难所和药店等关键基础设施的访问问题。在四个拓扑结构不同的城市和多种任务上的广泛实验表明，GeoResponder在性能上显著优于最先进的基础模型和领域专用基线，表明LLM可以内化并泛化地理空间结构，为实际应用提供支持。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-03T07:43:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展带来的边际效益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，通过提升多样性而非原始数据量来扩展代码代理的性能。在固定训练预算下，增加轨迹多样性所带来的性能提升比增加轨迹数量更大，从而优化了代理训练的性能成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，用于捕捉真实服务中的逻辑依赖关系；（2）基于蓝图的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙盒环境中的代码工具，以减轻内在编码能力的灾难性遗忘。我们在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在工具使用泛化能力和内在编码能力方面均取得了显著提升。我们计划在论文发表后公开完整的代码库和合成数据集（包含30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of code large language models (LLMs) when evolving into tool-interactive agents, particularly the diminishing returns of increasing data quantity and the early bottlenecks in trajectory-based training. It introduces TDScaling, a data synthesis framework that enhances performance by focusing on trajectory diversity rather than volume. The framework incorporates four innovations: capturing logical dependencies with a Business Cluster mechanism, ensuring trajectory coherence through a blueprint-driven multi-agent paradigm, preventing mode collapse with an adaptive evolution mechanism based on entropy metrics, and preserving coding capabilities with a sandboxed code tool. Experimental results on multiple benchmarks show that TDScaling significantly improves both tool-use generalization and coding proficiency.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）在演变为工具交互代理时所面临的合成数据质量低和数量扩展收益递减的问题，提出了TDScaling框架，通过提升轨迹多样性而非单纯增加轨迹数量来增强性能。TDScaling包含四个创新点：业务聚类机制以捕捉实际服务的逻辑依赖关系，蓝图驱动的多代理范式确保轨迹一致性，基于领域熵、推理模式熵和累积动作复杂度的自适应演化机制防止模式坍塌，以及一个沙盒环境的代码工具以保留内在编码能力。在多个通用工具使用基准（如BFCL、tau^2-Bench）和代码代理任务（如RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在提升工具使用泛化能力和编码能力方面均取得显著成效，相比传统的数量扩展方法提供了更优的性能与成本平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</div>
<div class="meta-line">Authors: Sherman Wong, Zhenting Qi, Zhaodong Wang, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Wenlin Chen, Yilun Du, Minlan Yu, Ying Zhang</div>
<div class="meta-line">First: 2025-12-11T08:05:58+00:00 · Latest: 2026-02-03T05:01:47+00:00</div>
<div class="meta-line">Comments: The latest version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10398v6">Abs</a> · <a href="https://arxiv.org/pdf/2512.10398v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world software engineering tasks require coding agents that can operate on massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade coding agents offer transparency but struggle when scaled to heavier, production-level workloads, while production-grade systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK supports a unified orchestrator with advanced context management for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the construction, evaluation, and refinement of agents through a build-test-improve cycle, enabling rapid agent development on new tasks and tool stacks. Instantiated on the Confucius SDK using the meta-agent, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a Resolve@1 of 59%, exceeding prior research baselines as well as commercial results, under identical repositories, model backends, and tool access.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>孔子代码代理：面向真实代码库的可扩展代理框架</div>
<div class="mono" style="margin-top:8px">现实中的软件工程任务需要能够处理大规模仓库、支持长时序会话并可靠协调复杂工具链的代码代理。现有的研究级代码代理虽然透明，但在扩展到更重的生产级工作负载时表现不佳，而生产级系统虽然在实际性能上表现优异，但扩展性、可解释性和可控性有限。我们引入了孔子代码代理（CCA），这是一个能够在大规模代码库中运行的软件工程代理。CCA基于孔子SDK构建，这是一个围绕代理体验（AX）、用户体验（UX）和开发者体验（DX）三个互补视角设计的代理开发平台。该SDK支持统一的编排器，具备高级上下文管理以实现长上下文推理，支持跨会话的持续学习的持久笔记系统，以及模块化的扩展系统以确保可靠的工具使用。此外，我们还引入了一个元代理，通过构建-测试-改进的循环自动完成代理的构建、评估和优化，从而实现对新任务和工具栈的快速代理开发。在孔子SDK上使用元代理实例化的CCA在现实中的软件工程任务中表现出色。在SWE-Bench-Pro上，CCA在相同仓库、模型后端和工具访问条件下，实现了59%的Resolve@1，超过了先前的研究基准和商业结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the Confucius Code Agent (CCA) is to address the limitations of existing coding agents, which either lack scalability for real-world codebases or offer limited flexibility and interpretability. CCA is built using the Confucius SDK, which provides a structured platform for agent development with features like long-context reasoning, cross-session learning, and modular tool integration. The main experimental results show that CCA achieves a Resolve@1 score of 59% on SWE-Bench-Pro, outperforming both prior research and commercial systems under the same conditions.</div>
<div class="mono" style="margin-top:8px">开发Confucius Code Agent（CCA）的动机是解决现有编码代理在实际代码库中的可扩展性不足以及灵活性和可解释性有限的问题。CCA基于Confucius SDK构建，该SDK为代理开发提供了结构化的平台，具备长上下文推理、跨会话学习和模块化工具集成等功能。实验结果显示，CCA在SWE-Bench-Pro上实现了59%的Resolve@1得分，优于以往的研究和商业系统，在相同条件下表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</div>
<div class="meta-line">Authors: Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T03:39:31+00:00 · Latest: 2026-02-03T03:39:31+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03060v1">PDF</a> · <a href="https://github.com/FireRedTeam/IVC-Prune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVC-Prune: 揭示LVLMs中用于视觉标记剪枝的隐式视觉坐标</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（LVLMs）在多个任务中表现出色。然而，处理高分辨率视觉输入时，其推理成本却很高。尽管视觉标记剪枝已成为一种有前景的解决方案，但现有方法主要关注语义相关性，往往丢弃对空间推理至关重要的标记。我们通过一个新颖的洞察来解决这一问题，即\emph{LVLMs如何处理空间推理}。具体而言，我们发现LVLMs通过旋转位置嵌入（RoPE）隐式地建立视觉坐标系统，其中特定的标记位置作为\textbf{隐式视觉坐标}（IVC tokens），对空间推理至关重要。基于这一洞察，我们提出\textbf{IVC-Prune}，一种无需训练、基于提示的剪枝策略，保留IVC标记和语义相关的前景标记。IVC标记通过理论分析RoPE的数学性质来识别，目标是那些其旋转矩阵近似单位矩阵或$90^\circ$旋转矩阵的位置。前景标记则通过一个稳健的两阶段过程进行识别：首先发现语义种子，然后通过值向量相似性进行上下文优化。在四个代表性LVLMs和二十个多样化基准上的广泛评估表明，IVC-Prune可将视觉标记减少约50\%，同时保持$\geq$ 99\%的原始性能，甚至在多个基准上实现性能提升。源代码可在https://github.com/FireRedTeam/IVC-Prune获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high inference cost in Large Vision-Language Models (LVLMs) when handling high-resolution visual inputs. The authors propose IVC-Prune, a training-free and prompt-aware token pruning method that identifies and retains tokens critical for spatial reasoning. They reveal that LVLMs implicitly use Rotary Position Embeddings (RoPE) to form visual coordinate systems, with specific token positions acting as implicit visual coordinates (IVC tokens). By theoretically analyzing RoPE&#x27;s mathematical properties and using a two-stage process for foreground token selection, IVC-Prune achieves a 50% reduction in visual tokens while preserving over 99% of the original performance and improving results on several benchmarks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决大型视觉-语言模型（LVLMs）在处理高分辨率视觉输入时的高推理成本问题。作者提出了一种无需训练且基于提示的视觉标记剪枝方法IVC-Prune，通过分析Rotary Position Embeddings（RoPE）的数学特性，识别出对空间推理至关重要的隐式视觉坐标（IVC tokens）。此外，该方法还采用两阶段流程来筛选语义相关的前景标记。实验结果表明，在四个代表性LVLMs和二十个基准测试中，IVC-Prune可将视觉标记数量减少约50%，同时保持原性能的99%以上，并在多个基准测试中实现性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</div>
<div class="meta-line">Authors: Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</div>
<div class="meta-line">First: 2026-02-03T03:10:27+00:00 · Latest: 2026-02-03T03:10:27+00:00</div>
<div class="meta-line">Comments: In Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03045v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03045v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>绘制前请澄清：面向稳健文本到CAD生成的主动代理</div>
<div class="mono" style="margin-top:8px">大型语言模型最近使文本到CAD系统成为可能，这些系统能够从自然语言提示中合成参数化CAD程序（例如CadQuery）。然而在实际应用中，几何描述可能不够具体或内部不一致：关键尺寸可能缺失，约束可能冲突。现有的微调模型倾向于被动地遵循用户指令，并在文本模糊时凭空捏造尺寸。为了解决这一问题，我们提出了一种名为ProCAD的主动代理框架，用于文本到CadQuery的生成，该框架在代码合成之前解决规格问题。我们的框架结合了一个主动澄清代理，它仅在必要时审核提示并提出有针对性的澄清问题，以生成自洽的规格，以及一个CAD编码代理，它将规格转换为可执行的CadQuery程序。我们通过一个精心挑选的高质量文本到CadQuery数据集对编码代理进行微调，并通过代理式监督微调（SFT）在澄清轨迹上训练澄清代理。实验表明，主动澄清显著提高了对模糊提示的鲁棒性，同时保持了较低的交互开销。ProCAD在包括Claude Sonnet 4.5在内的前沿闭源模型上表现更优，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。我们的代码和数据集将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating accurate CAD models from natural language prompts, where geometric descriptions may be under-specified or inconsistent. It introduces ProCAD, a proactive agentic framework that first resolves specification issues through a clarifying agent, which identifies ambiguities and asks targeted questions to ensure self-consistency, before generating CAD code with a coding agent. The coding agent is fine-tuned on a high-quality dataset, while the clarifying agent is trained using agentic SFT on clarification trajectories. Experimental results demonstrate that ProCAD significantly improves robustness to ambiguous inputs, achieving a 79.9% reduction in mean Chamfer distance and a 4.8% to 0.9% decrease in invalidity ratio compared to state-of-the-art closed-source models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升文本到CAD系统的鲁棒性，以应对自然语言提示中几何描述不足或不一致的问题。提出的方法ProCAD是一个主动代理框架，包含一个澄清代理和一个CAD编码代理。澄清代理审计提示内容，并在必要时提出针对性问题以确保规格的一致性，而编码代理则将该规格转换为可执行的CadQuery程序。实验结果表明，ProCAD在性能上优于现有模型，将平均Chamfer距离降低了79.9%，并将无效率从4.8%降至0.9%。</div>
</details>
</div>
<div class="card">
<div class="title">CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability</div>
<div class="meta-line">Authors: Xianzhen Luo, Jingyuan Zhang, Shiqi Zhou, Rain Huang, Chuan Xiao, Qingfu Zhu, Zhiyuan Ma, Xing Yue, Yang Yue, Wencong Zeng, Wanxiang Che</div>
<div class="meta-line">First: 2026-02-03T02:27:16+00:00 · Latest: 2026-02-03T02:27:16+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03012v1">PDF</a> · <a href="https://github.com/livecvebench/CVE-Factory">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CVE-Factory：为代码安全漏洞实现专家级代理任务的扩展</div>
<div class="mono" style="margin-top:8px">评估和提升代码代理的安全能力需要高质量且可执行的漏洞任务。然而，现有工作依赖于成本高昂且难以扩展的手动复现，并且数据分布过时。为了解决这些问题，我们提出了CVE-Factory，这是首个能够将稀疏的CVE元数据自动转换为可执行代理任务的多代理框架，实现专家级质量。与人类专家复现的交叉验证表明，CVE-Factory在解决方案正确性和环境保真度方面分别达到95\%和96\%，证实了其专家级质量。它还被评估在最新的现实漏洞上，验证成功率达到66.2\%。这种自动化实现了两个下游贡献。首先，我们构建了LiveCVEBench，这是一个持续更新的基准，包含190个任务，涵盖14种语言和153个仓库，捕捉包括AI工具漏洞在内的新兴威胁。其次，我们合成超过1,000个可执行的训练环境，这是代码安全领域首次实现代理任务的大规模扩展。经过微调的Qwen3-32B在LiveCVEBench上的表现从5.3\%提升至35.8\%，超越了Claude 4.5 Sonnet，且性能提升在Terminal Bench上也得到验证（从12.5\%提升至31.3\%）。我们开源了CVE-Factory、LiveCVEBench、Abacus-cve（微调模型）、训练数据集和排行榜。所有资源均可在https://github.com/livecvebench/CVE-Factory 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the security evaluation of code agents by providing high-quality, executable vulnerability tasks. CVE-Factory introduces a multi-agent framework that automatically transforms sparse CVE metadata into fully executable agentic tasks, achieving expert-level quality. Experimental results show that CVE-Factory attains 95% solution correctness and 96% environment fidelity when validated against human expert reproductions, and it achieves a 66.2% verified success rate on recent vulnerabilities. The framework also enables the creation of LiveCVEBench, a benchmark with 190 tasks across 14 languages and 153 repositories, and synthesizes over 1,000 training environments, significantly improving the performance of fine-tuned Qwen3-32B on LiveCVEBench and Terminal Bench.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过提供高质量、可执行的漏洞任务来提升代码代理的安全评估与改进能力。为解决现有方法中手动操作成本高、数据分布过时的问题，作者提出了CVE-Factory，这是一个首个能够将稀疏的CVE元数据自动转换为完整可执行代理任务的多代理框架。通过与人类专家再现的对比验证，CVE-Factory达到了95%的解决方案正确性和96%的环境保真度，证明了其专家级质量。同时，它在最新的现实漏洞上实现了66.2%的验证成功率。该自动化方法带来了两个重要贡献：构建了持续更新的LiveCVEBench基准，包含14种语言和153个仓库的190个任务，涵盖新兴威胁如AI工具漏洞；并合成了超过1000个可执行的训练环境，实现了代码安全领域代理任务的大规模扩展。对Qwen3-32B的微调使其在LiveCVEBench上的表现从5.3%提升至35.8%，超越了Claude 4.5 Sonnet，且结果在Terminal Bench上也具有可推广性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing</div>
<div class="meta-line">Authors: Saeid Sheikhi</div>
<div class="meta-line">First: 2026-02-02T21:44:01+00:00 · Latest: 2026-02-02T21:44:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02842v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟链：一种面向大语言模型的双模式推理框架，具有动态问题路由</div>
<div class="mono" style="margin-top:8px">我们提出了模拟链（CoS），一种新颖的双模式推理框架，该框架通过动态路由将问题发送到大语言模型（LLMs）中的专用推理策略。与现有的统一提示方法不同，CoS采用三种不同的推理模式：(1) 用于数学问题的计算流程与自洽性，(2) 用于空间推理的符号状态跟踪与JSON表示，(3) 用于多跳推理的混合事实提取。通过在GSM8K、StrategyQA和bAbI基准上对四个最先进的模型（Gemma-3 27B、LLaMA-3.1 8B、Mistral 7B和Qwen-2.5 14B）进行综合评估，我们证明CoS在GSM8K上达到71.5%的准确率（绝对提升1.0%），在StrategyQA上达到90.0%（提升2.5%），在bAbI上达到19.0%（相对提升65.2%），优于最强的基线方法。分析表明，针对特定问题选择合适的推理模式至关重要，当正确应用于数学问题时，计算模式可达到81.2%的准确率，而错误路由则导致0%的准确率。我们提供了模式选择、状态跟踪和答案提取的详细算法，确立了CoS作为一种无需额外训练即可提升LLM推理能力的有效方法。与Self-Consistency相比，该框架在准确率和效率之间提供了更优的权衡，计算成本降低了54%，同时实现了相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the reasoning capabilities of Large Language Models (LLMs) by leveraging problem-specific strategies. Chain of Simulation (CoS) introduces a dual-mode reasoning framework that dynamically routes problems to three specialized modes: computational flow for mathematical problems, symbolic state tracking for spatial reasoning, and hybrid fact-extraction for multi-hop inference. Experimental results on GSM8K, StrategyQA, and bAbI benchmarks show that CoS improves accuracy by 1.0%, 2.5%, and 65.2% respectively compared to the strongest baselines, with computational mode achieving 81.2% accuracy when correctly applied. The framework also offers better efficiency than Self-Consistency, achieving similar performance at 54% lower computational cost.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Chain of Simulation（CoS）的双模式推理框架，旨在通过动态路由问题到专门的推理策略来提升大语言模型（LLM）在不同推理任务上的表现。该框架包含三种不同的模式：用于数学问题的计算流程模式、用于空间推理的符号状态跟踪模式以及用于多跳推理的混合事实提取模式。在GSM8K、StrategyQA和bAbI基准测试中，CoS分别达到了71.5%、90.0%和19.0%的准确率，相比现有方法有显著提升。分析表明，准确的模式选择至关重要，当计算模式正确应用于数学问题时，准确率可达81.2%，而错误路由则导致准确率为0%。此外，CoS在准确率与效率之间提供了更优的平衡，相比Self-Consistency方法，在计算成本降低54%的情况下实现了相近的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-02-02T19:55:32+00:00</div>
<div class="meta-line">Comments: 21 main pages, 6 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该在封闭源系统上具有根本性的优势：它们可以专门针对私有代码库，直接在权重中编码特定于仓库的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，能够快速且低成本地创建专门针对私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法和代码）模型中实现了最先进的结果，同时其性能与前沿的开放权重模型（如Devstral-Small-2）相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到等效性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数以千计的轨迹。结合成本效益，这使得专门针对私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过20万个合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和混淆因素进行了详细分析。总体而言，我们相信我们的工作将大大加速开放编码代理的研究，并展示能够专门针对私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to enable the practical specialization of open-weight coding agents to private codebases, which has been previously limited by high training costs and complexity. The authors propose SERA, an efficient method that leverages supervised fine-tuning (SFT) to train coding agents with state-of-the-art performance on open-source models, matching the capabilities of advanced open-weight models like Devstral-Small-2. Their method, Soft Verified Generation (SVG), generates thousands of synthetic trajectories from a single code repository, significantly reducing the cost compared to reinforcement learning and prior synthetic data approaches, achieving 26x and 57x cost savings respectively. Additionally, they apply SVG to a larger corpus, producing over 200,000 trajectories for further analysis of scaling laws and training factors.</div>
<div class="mono" style="margin-top:8px">本研究的动机是实现对私有代码库的开放权重编码代理的实用化专业化，这在过去由于高昂的训练成本和复杂性而受到限制。作者提出了SERA方法，通过仅使用监督微调（SFT）训练编码代理，实现了开放源码模型中的最先进结果，并与先进的开放权重模型如Devstral-Small-2表现相当。其方法Soft Verified Generation（SVG）能够从单个代码库生成数千条合成轨迹，显著降低了训练成本，相比强化学习和之前合成数据方法分别降低了26倍和57倍。此外，他们将SVG应用于更大的代码库集合，生成超过20万个合成轨迹，用于深入分析编码代理的扩展规律和训练因素。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Universe: Scale Real-World Verifiable Environments to Millions</div>
<div class="meta-line">Authors: Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao, Ruisheng Cao, Jiaxi Yang, Jiawei Chen, Mingze Li, Zeyao Ma, Hao Ge, Zongmeng Zhang, Zeyu Cui, Dayiheng Liu, Jingren Zhou, Jianling Sun, Junyang Lin, Binyuan Hui</div>
<div class="meta-line">First: 2026-02-02T17:20:30+00:00 · Latest: 2026-02-02T17:20:30+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Universe：将现实世界可验证环境扩展到百万级</div>
<div class="mono" style="margin-top:8px">我们提出了SWE-Universe，这是一个可扩展且高效的框架，用于从GitHub拉取请求（PRs）自动构建现实世界软件工程（SWE）可验证环境。为了解决自动构建中普遍存在的挑战，如低生产率、弱验证器和高昂成本，我们的框架采用了一个由高效自定义训练模型驱动的构建代理。该代理通过迭代自验证和循环内黑客检测，确保高保真、可验证任务的可靠生成。使用该方法，我们将现实世界多语言SWE环境的数量扩展到百万级（807,693）。我们通过大规模代理中间训练和强化学习展示了这些环境的深远价值。最后，我们将该技术应用于Qwen3-Max-Thinking，并在SWE-Bench Verified上取得了75.3%的得分。我们的工作为推进下一代编码代理提供了关键资源和稳健的方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-Universe is to address the limitations of existing methods in automatically constructing high-quality, verifiable software engineering environments, including low production yield, weak verification capabilities, and high costs. The framework introduces a building agent trained with a custom model to iteratively self-verify and detect hacking during the environment creation process, ensuring reliable and high-fidelity task generation. The method successfully scales the construction of real-world multilingual SWE environments to over 800,000 instances, and the experimental results show that applying this technique to Qwen3-Max-Thinking achieves a 75.3% score on the SWE-Bench Verified benchmark, demonstrating its effectiveness and value.</div>
<div class="mono" style="margin-top:8px">SWE-Universe的提出旨在解决自动构建高质量、可验证的软件工程环境所面临的挑战。该框架引入了一个基于自定义训练模型的构建代理，通过迭代自验证和循环内黑客检测确保生成任务的可靠性和真实性。实验结果表明，该方法成功将现实世界中的多语言软件工程环境扩展到超过80万个实例，并在应用于Qwen3-Max-Thinking时，在SWE-Bench Verified基准测试中取得了75.3%的得分，展示了其在提升编码代理能力方面的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
