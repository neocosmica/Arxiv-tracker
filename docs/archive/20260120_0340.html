<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-20 03:40</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260120_0340</div>
    <div class="row"><div class="card">
<div class="title">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</div>
<div class="meta-line">Authors: Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo Pérez-Pellitero, Youngkyoon Jang</div>
<div class="meta-line">First: 2026-01-16T17:02:46+00:00 · Latest: 2026-01-16T17:02:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11442v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Map2Thought：通过度量认知地图实现显式的3D空间推理</div>
<div class="mono" style="margin-top:8px">我们提出了Map2Thought框架，使3D视觉语言模型能够进行显式且可解释的空间推理。该框架基于两个关键组件：度量认知地图（Metric-CogMap）和认知链式推理（Cog-CoT）。Metric-CogMap通过将关系推理的离散网格与精确几何理解的连续度量表示相结合，提供统一的空间表示。在Metric-CogMap的基础上，Cog-CoT通过确定性操作进行显式的几何推理，包括向量运算、边界框距离和遮挡感知的外观顺序提示，从而生成基于3D结构的可解释推理轨迹。实验结果表明，Map2Thought实现了可解释的3D理解，在仅使用一半监督数据的情况下达到了59.9%的准确率，接近使用完整数据集训练的60.9%基线。在VSI-Bench上，它在10%、25%和50%的训练子集下分别优于最先进的方法5.3%、4.8%和4.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the interpretability and efficiency of spatial reasoning in 3D Vision-Language Models (VLMs). The proposed framework, Map2Thought, integrates two components: Metric Cognitive Map (Metric-CogMap) for unified spatial representation and Cognitive Chain-of-Thought (Cog-CoT) for explicit geometric reasoning. Metric-CogMap combines a discrete grid for relational reasoning with a continuous metric-scale representation for precise geometry, while Cog-CoT uses deterministic operations such as vector calculations, bounding-box distances, and occlusion-aware appearance order to generate interpretable inference traces. Experimental results demonstrate that Map2Thought achieves 59.9% accuracy with half the supervision, closely matching the 60.9% baseline performance, and outperforms existing methods by 5.3%, 4.8%, and 4.0% on 10%, 25%, and 50% training subsets respectively on the VSI-Bench.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D视觉语言模型（VLMs）中空间推理的可解释性和效率。提出的框架Map2Thought包含两个核心组件：用于统一空间表示的度量认知地图（Metric-CogMap）和进行显式几何推理的认知链式推理（Cog-CoT）。Metric-CogMap通过结合离散网格用于关系推理与连续度量表示用于精确几何理解，实现空间信息的整合；Cog-CoT则利用向量运算、边界框距离和遮挡感知的外观顺序等确定性操作生成可解释的推理轨迹。实验结果表明，Map2Thought在仅使用一半监督数据的情况下达到59.9%的准确率，接近使用完整数据集训练的60.9%基线性能，并在VSI-Bench数据集的10%、25%和50%训练子集上分别优于现有最先进方法5.3%、4.8%和4.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation</div>
<div class="meta-line">Authors: Joschka Kersting, Michael Rummel, Gesa Benndorf</div>
<div class="meta-line">First: 2025-11-12T08:56:11+00:00 · Latest: 2026-01-16T14:53:55+00:00</div>
<div class="meta-line">Comments: ICIT2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09122v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09122v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向供应商的工业代理：基于RAG增强的LLM用于安全的本地PLC代码生成</div>
<div class="mono" style="margin-top:8px">可编程逻辑控制器使用专有代码方言进行操作，这使得训练代码助手变得具有挑战性。当前的LLM是在大规模代码数据集上训练的，能够直接生成符合IEC 61131-3标准的代码，但它们并不了解特定的功能块或相关的项目代码。此外，像三菱电机这样的公司及其客户并不信任云服务提供商。因此，拥有自己的代码代理是解决这一问题的首选方案。在本研究中，我们提出了一种适用于工业领域的低数据域代码助手解决方案。我们展示了如何在不微调大型模型的情况下，通过微调小型本地模型用于边缘设备，实现高质量的代码生成。我们的工具让多个AI模型相互竞争，利用推理自动纠正错误，并通过在聊天界面中直接编译代码来验证代码的有效性。我们通过详尽的评估来支持我们的方法，包括代码编译统计数据和用户评分。我们发现，通过使用广泛的提示工程和定向检索，基于检索增强生成（RAG）的代码助手可以在低数据域中有效运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of training coding assistants for industrial Programmable Logic Controllers (PLCs), which use proprietary code dialects. The authors propose a vendor-aware coding assistant that leverages Retrieval-Augmented Generation (RAG) to enable secure, on-premise code generation without relying on large-scale fine-tuning. Their approach involves using small local models fine-tuned for edge device deployment and integrating multiple AI models to enhance reasoning, bug correction, and code validation through direct compilation in the chat interface. The extensive evaluation demonstrates that their RAG-supported assistant achieves high-quality code generation even in low-data domains, supported by code compilation statistics and user feedback.</div>
<div class="mono" style="margin-top:8px">本研究针对工业可编程逻辑控制器（PLC）的编码助手训练难题，因其使用专有代码方言而难以直接应用现有大型语言模型（LLMs）。为解决现有LLMs缺乏领域特定知识且无法在本地环境中获得信任的问题，研究人员开发了一种基于检索增强生成（RAG）的编码助手，结合详尽的提示工程和定向检索实现低数据领域的高质量PLC代码生成。该方法无需对大型模型进行微调，而是采用小型本地模型进行边缘部署，并支持多个AI模型协作、推理、自动纠错和代码编译验证。实验结果表明，该RAG增强的编码助手能够在低数据环境下有效生成有效且准确的代码，通过编译统计和用户评分得到验证。</div>
</details>
</div>
<div class="card">
<div class="title">OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding</div>
<div class="meta-line">Authors: Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui</div>
<div class="meta-line">First: 2026-01-15T12:36:08+00:00 · Latest: 2026-01-16T02:10:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10343v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10343v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OctoBench：基于仓库的代理编码中对 Scaffold-Aware 指令遵循的基准测试</div>
<div class="mono" style="margin-top:8px">现代编码 Scaffold 将 LLM 转化为有能力的软件代理，但其遵循 Scaffold 指定指令的能力仍被低估，尤其是在约束条件异构且持续跨交互的情况下。为填补这一空白，我们引入了 OctoBench，用于评估基于仓库的代理编码中对 Scaffold-Aware 指令遵循的能力。OctoBench 包含 34 个环境和 217 个任务实例，涵盖三种 Scaffold 类型，并配有 7,098 个目标检查清单项。为了将任务解决与规则遵循分离，我们提供了一个自动化的观察与评分工具包，能够捕捉完整交互轨迹并进行细粒度检查。在八个代表性模型上的实验揭示了任务解决与 Scaffold-Aware 遵循之间存在系统性差距，强调了需要专门针对异构指令遵循进行训练和评估的重要性。我们发布该基准测试以支持可重复的基准测试，并加速开发更具备 Scaffold-Aware 能力的编码代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate the ability of large language models (LLMs) to follow scaffold-specified instructions in repository-grounded agentic coding, particularly under heterogeneous constraints. The authors introduce OctoBench, a benchmark that includes 34 environments and 217 tasks across three scaffold types, along with 7,098 objective checklist items. They also develop an automated observation-and-scoring toolkit to track task execution and perform fine-grained compliance checks. Experimental results on eight representative models show a systematic discrepancy between task-solving performance and scaffold-aware instruction following, highlighting the need for more targeted training and evaluation methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估大语言模型（LLMs）在基于仓库的代理编码中遵循支架指定指令的能力，特别是在约束条件多样且持续存在的场景下。作者提出了OctoBench基准测试，包含34个环境和217个任务，涵盖三种支架类型，并配有7098个目标检查项。他们开发了一个自动化观察与评分工具，用于追踪任务执行过程并进行细粒度的合规性检查。在八个代表性模型上的实验结果显示，任务解决能力和支架感知指令遵循之间存在系统性差距，强调了需要更针对性的训练和评估方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents</div>
<div class="meta-line">Authors: Himanshu Thakur, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri</div>
<div class="meta-line">First: 2026-01-15T19:33:42+00:00 · Latest: 2026-01-15T19:33:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10820v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team&#x27;s unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team&#x27;s environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过LLM代理受限拓扑规划实现可靠的机器学习特征工程</div>
<div class="mono" style="margin-top:8px">最近代码生成模型的进展为自动化特征工程带来了前所未有的机会，但其在现实世界机器学习团队中的采用仍受到关键挑战的限制：(i) 缺乏能够捕捉生产级特征工程迭代和复杂编码过程的数据集；(ii) 常用编码代理（如CoPilot和Devin）与团队独特工具、代码库、工作流程和实践的集成和个性化有限；(iii) 由于反馈时机不当或不足，导致人机协作效果不佳。我们通过一个由规划器引导、受限拓扑的多代理框架来解决这些问题，该框架以多步骤方式为仓库生成代码。LLM驱动的规划器利用团队环境（以图表示），协调可用代理的调用，生成上下文感知的提示，并利用下游失败来回溯修正上游产物。它可以在关键步骤请求人工干预，确保生成的代码可靠、可维护，并符合团队期望。在我们新构建的内部数据集上，我们的方法在评估指标上分别比手动构建和未规划的流程提升了38%和150%。在实践中，当构建服务于超过1.2亿用户的推荐模型的特征时，我们的方法通过将特征工程周期从三周缩短至一天，实现了实际影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in automating feature engineering for machine learning teams using large language models (LLMs). The motivation stems from the lack of datasets capturing iterative coding processes, limited integration of coding agents with team-specific environments, and poor human-AI collaboration. The proposed method introduces a planner-guided, constrained-topology multi-agent framework that generates code in a multi-step process, leveraging a team&#x27;s environment as a graph to orchestrate agent calls, create context-aware prompts, and correct upstream artifacts based on downstream failures. It also allows for human intervention at critical stages to ensure reliability and alignment with team practices. Experimental results on an in-house dataset show a 38% improvement over manually crafted workflows and a 150% improvement over unplanned ones. In practice, the approach significantly reduces feature engineering cycles from three weeks to one day when applied to recommendation models serving over 120 million users.</div>
<div class="mono" style="margin-top:8px">本文针对机器学习中自动化特征工程面临的挑战，提出了一种由规划器引导、基于受限拓扑结构的多智能体框架，该框架结合了LLM驱动的规划与现有编码智能体。该方法利用团队环境的图表示来协调智能体调用，生成上下文感知的提示，并根据下游失败情况回溯修正上游步骤。同时，它允许在关键阶段引入人工干预，以确保生成的代码可靠且符合团队实践。在一项新型内部数据集上的实验表明，该方法在评估指标上比手动流程提升了38%，比未规划流程提升了150%。在实际应用中，该方法显著缩短了特征工程周期，将服务1.2亿用户的推荐模型的特征构建时间从三周减少到一天。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</div>
<div class="meta-line">Authors: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-10T12:54:04+00:00 · Latest: 2026-01-15T13:22:05+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, accepted to AAAI 2026. Project page:https://github.com/CASIA-IVA-Lab/UrbanNav</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09607v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09607v2">PDF</a> · <a href="https://github.com/CASIA-IVA-Lab/UrbanNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanNav: 从大规模网络人类轨迹中学习语言引导的城市导航</div>
<div class="mono" style="margin-top:8px">使用自然语言指令在复杂的城市环境中导航对具身智能体提出了重大挑战，包括嘈杂的语言指令、模糊的空间参照、多样化的地标以及动态的街道场景。当前的视觉导航方法通常局限于模拟或非街道环境，并且往往依赖于精确的目标格式，如特定坐标或图像。这限制了它们在自主智能体（如最后一公里配送机器人）在陌生城市中导航时的有效性。为了解决这些限制，我们引入了UrbanNav，一个可扩展的框架，用于训练具身智能体在多样化城市环境中遵循自由形式的语言指令。通过利用大规模的城市步行视频，我们开发了一个可扩展的标注流程，将人类导航轨迹与基于真实地标的语言指令对齐。UrbanNav包含超过1,500小时的导航数据和300万个指令-轨迹-地标三元组，涵盖了广泛的城市场景。我们的模型学习了鲁棒的导航策略，以应对复杂的城市场景，展示了优越的空间推理能力、对嘈杂指令的鲁棒性以及对未见过的城市环境的泛化能力。实验结果表明，UrbanNav显著优于现有方法，突显了大规模网络视频数据在实现具身智能体语言引导的真实城市导航方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UrbanNav addresses the challenges of navigating complex urban environments using natural language instructions by introducing a scalable framework that trains embodied agents to follow free-form language guidance. The method leverages web-scale city walking videos and develops an annotation pipeline to align human trajectories with real-world landmarks and language instructions. The framework includes over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, enabling the model to learn robust navigation policies. Experimental results demonstrate that UrbanNav outperforms existing methods in spatial reasoning, handling noisy instructions, and generalizing to new urban settings.</div>
<div class="mono" style="margin-top:8px">UrbanNav旨在解决基于自然语言指令在复杂城市环境中导航的挑战，为具身智能体提供更有效的解决方案。该框架利用大规模的城市步行视频，构建了一个可扩展的标注流程，将人类导航轨迹与现实地标相关的语言指令对齐。框架包含超过1500小时的导航数据和300万个指令-轨迹-地标三元组，覆盖了广泛的城市场景。实验结果表明，UrbanNav在处理噪声指令和多样化城市环境方面显著优于现有方法，突显了大规模真实世界数据在语言引导导航中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants</div>
<div class="meta-line">Authors: Tsvi Cherny-Shahar, Amiram Yehudai</div>
<div class="meta-line">First: 2026-01-15T06:42:45+00:00 · Latest: 2026-01-15T06:42:45+00:00</div>
<div class="meta-line">Comments: 35 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10112v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.
  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\% and reduces completion time by 53.9\%, yielding a mean 57.8\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\% in accuracy and 69.5\% in efficiency on average, compared to 6.6\% and 46.1\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge faced by repository-aware coding agents in understanding build and test structures, particularly in multilingual projects with complex, heterogeneous build systems. The authors propose the Repository Intelligence Graph (RIG), a deterministic and evidence-based representation of a project&#x27;s components, dependencies, and tests. They also introduce SPADE, an extractor that builds RIG from build and test artifacts, such as CMake and CTest data, and provides it in a JSON format suitable for LLMs. Evaluation on eight repositories with three commercial agents shows that using RIG improves mean accuracy by 12.2% and reduces completion time by 53.9%, with more significant gains in multilingual projects.</div>
<div class="mono" style="margin-top:8px">本文针对多语言项目中构建和测试结构理解的挑战，提出了Repository Intelligence Graph（RIG）这一确定性的架构图，用于明确表示可构建组件、测试用例和依赖关系。作者还开发了SPADE工具，该工具从CMake和CTest的构建产物中提取RIG，并以LLM友好的JSON格式呈现。实验评估了三个商业编码代理在八个不同复杂度的仓库上的表现，结果显示使用RIG可使平均准确率提升12.2%，完成时间减少53.9%，在多语言仓库中提升效果更为显著，平均准确率提高17.7%，效率提升69.5%。定性分析表明，RIG有助于将错误从结构误解转移到推理错误，但图结构推理质量仍是关键影响因素。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jean Oh, Jihie Kim</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-15T05:09:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全未找到（404）：基于大语言模型的机器人决策中的隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键环境中，人工智能系统的一个错误可能导致生命损失。随着大语言模型（LLMs）在机器人决策中的应用日益广泛，风险的物理维度也在扩大；一个错误的指令可能直接危及人类安全。本文旨在系统评估LLMs在即使微小错误也可能导致灾难的场景中的表现。通过一个火灾疏散场景的定性评估，我们识别了基于LLM的决策中的关键失败案例。基于这些案例，我们设计了七个定量评估任务，分为：完整信息任务、不完整信息任务和安全导向空间推理（SOSR）任务。完整信息任务使用ASCII地图以最小化解释歧义，并将空间推理与视觉处理分离。不完整信息任务要求模型推断缺失的上下文，测试其对空间连续性的理解与避免幻觉的能力。SOSR任务使用自然语言评估模型在生命威胁情境下的安全决策能力。我们对多种LLMs和视觉语言模型（VLMs）在这些任务上的表现进行了基准测试。除了整体表现外，我们还分析了1%失败率的潜在影响，强调了“罕见”错误如何演变为灾难性后果。结果揭示了严重的漏洞：一些模型在ASCII导航任务中成功率为0%，而在模拟火灾演练中，模型指示机器人向危险区域移动而非紧急出口。我们的研究得出令人深思的结论：当前的LLMs尚不适用于直接部署在安全关键系统中。在机器人领域，99%的准确率是危险的误导，因为它意味着每100次执行中可能有一次导致灾难性伤害。我们证明了即使是最先进的模型也无法保证安全，完全依赖它们会带来不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the risks associated with using Large Language Models (LLMs) in safety-critical robotics decision-making, emphasizing that even minor errors can lead to severe consequences. The authors designed seven tasks to evaluate LLMs and Vision-Language Models (VLMs) in different scenarios, including complete and incomplete information settings, as well as safety-oriented spatial reasoning. Their experiments revealed critical vulnerabilities, such as a 0% success rate in ASCII navigation tasks and models directing robots toward hazardous areas during a simulated fire drill. These findings underscore the inappropriateness of deploying current LLMs in high-stakes environments without further safety improvements.</div>
<div class="mono" style="margin-top:8px">本文探讨了在安全关键场景中使用大型语言模型（LLMs）进行机器人决策所存在的潜在风险，特别是在轻微错误可能造成严重后果的情况下。作者设计了七个任务来评估LLMs和视觉-语言模型（VLMs）在不同情境下的表现，包括完整信息、不完整信息以及安全导向的空间推理。实验结果显示了模型的重大缺陷，例如某些模型在ASCII导航任务中完全失败，另一些则在模拟火灾演练中指示机器人向危险区域移动而非安全出口。这些结果表明，即使1%的错误率也可能导致灾难性后果，当前LLMs尚不适用于直接部署在安全关键系统中。</div>
</details>
</div>
<div class="card">
<div class="title">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</div>
<div class="meta-line">Authors: Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</div>
<div class="meta-line">First: 2026-01-12T16:26:42+00:00 · Latest: 2026-01-15T03:58:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07695v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes &quot;near-miss&quot; samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑操作符：平滑可验证奖励激活视觉-语言模型的空间推理能力</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在实现精确数值预测以进行3D场景理解时面临关键瓶颈。传统的强化学习（RL）方法主要基于相对排序，常常受到严重奖励稀疏性和梯度不稳定性的影响，无法有效利用3D物理约束提供的可验证信号。值得注意的是，在标准GRPO框架中，相对归一化会导致&quot;近失&quot;样本（具有小但非零误差的样本）出现优势坍塌。这导致了严重的数据利用瓶颈，其中有价值的边界样本在优化过程中被丢弃。为了解决这一问题，我们引入了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA采用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励空间。同时，AP-GRPO整合了绝对标量梯度以缓解传统相对排序机制中固有的数值信息损失。通过利用这种方法，我们构建了Numerical3D-50k数据集，包含50,000个可验证的3D子任务。实证结果表明，AP-GRPO在保持更高数据效率的同时，实现了与大规模监督方法相当的性能，有效激活了VLMs中的潜在3D推理能力，而无需对模型架构进行修改。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge that Vision-Language Models (VLMs) face in achieving accurate numerical predictions for 3D scene understanding. Traditional reinforcement learning methods, such as GRPO, suffer from reward sparsity and gradient instability due to relative ranking mechanisms, which discard valuable boundary samples. To overcome these limitations, the authors propose the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA uses a dynamically parameterized Sigmoid function to generate dense, continuous rewards, while AP-GRPO incorporates absolute scalar gradients to preserve numerical information. The proposed approach is validated using Numerical3D-50k, a dataset of 50,000 verifiable 3D subtasks, and the results show that AP-GRPO achieves performance comparable to large-scale supervised methods with better data efficiency, effectively enhancing the spatial reasoning capabilities of VLMs without altering their architecture.</div>
<div class="mono" style="margin-top:8px">本文旨在解决视觉-语言模型（VLMs）在3D场景理解中进行精确数值预测所面临的瓶颈问题。传统强化学习方法如GRPO由于相对归一化导致奖励稀疏和梯度不稳定，从而丢弃了有价值的边界样本。为此，作者提出了平滑数值奖励激活（SNRA）操作符和绝对保持GRPO（AP-GRPO）框架。SNRA利用动态参数化的Sigmoid函数将原始反馈转换为密集的连续奖励，而AP-GRPO则通过引入绝对标量梯度来保留数值信息。实验使用包含50,000个可验证3D子任务的Numerical3D-50k数据集验证了该方法，结果显示AP-GRPO在数据效率上优于传统方法，且性能与大规模监督方法相当，有效激活了VLMs中潜在的3D推理能力，而无需对模型架构进行修改。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器进行构建，其训练方法往往将图像扁平化为1D的patch序列，丢弃了空间推理所需的2D结构。我们认为这种缺乏空间感知是VLM设计中缺失的一个维度，也是需要空间定位的应用（如机器人和具身AI）的瓶颈。为了解决这一问题，我们研究了（i）采用替代目标训练的图像编码器，以及（ii）2D位置编码。实验表明，这些架构选择可以在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the spatial reasoning limitations in vision-language models (VLMs), which are typically trained using contrastive language-image pretraining (CLIP) style image encoders that flatten images into 1D sequences, neglecting their 2D structure. The authors propose two approaches to enhance spatial awareness: training image encoders with alternative objectives and incorporating 2D positional encodings. Experimental results demonstrate that these modifications improve spatial reasoning performance across multiple benchmarks, highlighting their potential to advance applications in robotics and embodied AI.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）虽然发展迅速，但其在捕捉空间关系方面仍存在不足，这限制了其在需要空间定位的应用中的表现。本文探讨了两种改进方法：使用不同训练目标的图像编码器以及引入二维位置编码。实验结果表明，这些架构上的改进能够提升模型在多个基准测试中的空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</div>
<div class="meta-line">Authors: Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong</div>
<div class="meta-line">First: 2026-01-14T21:21:00+00:00 · Latest: 2026-01-14T21:21:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09879v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedVL-SAM2：一种统一的三维医学视觉-语言模型，用于多模态推理和提示驱动的分割</div>
<div class="mono" style="margin-top:8px">近年来，医学视觉-语言模型（VLMs）在图像级文本中心任务如报告生成和视觉问答（VQA）上取得了显著进展。然而，在三维医学VLM中实现细粒度视觉定位和体素空间推理仍具挑战性，尤其是在试图在一个通用框架内统一这些能力时。为了解决这一挑战，我们提出了MedVL-SAM2，这是一种统一的三维医学多模态模型，能够同时支持报告生成、VQA以及包括语义分割、指称分割和交互分割在内的多种分割范式。MedVL-SAM2通过专为三维医学影像设计的统一架构，将图像级推理与像素级感知相结合，并引入基于SAM2的体素分割模块，以实现精确的多粒度空间推理。该模型采用多阶段训练流程：首先在大规模的三维CT图像-文本对语料库上进行预训练，以对齐体素视觉特征与放射学语言嵌入；随后利用一个全面的三维CT分割数据集，联合优化语言理解和分割目标。这种联合训练使得模型能够通过语言、点或框提示进行灵活交互，从而统一高级视觉推理与空间精确定位。我们的统一架构在报告生成、VQA以及多种三维分割任务中均实现了最先进的性能。进一步的广泛分析表明，该模型能够提供可靠的三维视觉定位、可控的交互分割以及强大的跨模态推理能力，证明了在统一的三维医学VLM中可以同时实现高级语义推理和精确的三维定位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MedVL-SAM2 is to overcome the limitations of existing 3D medical vision-language models in achieving fine-grained visual grounding and volumetric spatial reasoning. The model employs a unified architecture that integrates image-level reasoning with pixel-level perception, specifically designed for 3D medical imaging. It incorporates a SAM2-based volumetric segmentation module and is trained through a multi-stage pipeline, first on a large-scale 3D CT image-text corpus for alignment of visual and language features, then on a comprehensive segmentation dataset for joint optimization. This approach enables flexible interaction via language, point, or box prompts and achieves state-of-the-art performance in report generation, VQA, and 3D segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有三维医学视觉语言模型在实现细粒度视觉定位和体积空间推理方面的不足。MedVL-SAM2被提出为一个统一框架，集成了报告生成、视觉问答和多种分割任务。该模型采用专为三维医学影像设计的统一架构，并引入基于SAM2的体积分割模块以实现多粒度空间推理。其训练过程采用多阶段流程，首先在大量三维CT图像-文本对上进行预训练，随后结合分割和语言理解目标进行联合优化。实验结果表明，MedVL-SAM2在多个任务上达到了最先进的性能，并展示了可靠的三维视觉定位、可控的交互分割以及强大的跨模态推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</div>
<div class="meta-line">Authors: Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</div>
<div class="meta-line">First: 2026-01-14T18:59:59+00:00 · Latest: 2026-01-14T18:59:59+00:00</div>
<div class="meta-line">Comments: Project page: https://jasper0314-huang.github.io/fast-thinkact/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09708v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jasper0314-huang.github.io/fast-thinkact/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fast-ThinkAct: 通过可表述的潜在规划实现高效的视觉-语言-动作推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）任务需要对复杂的视觉场景进行推理，并在动态环境中执行适应性动作。尽管近期关于推理VLA的研究表明，显式的思维链（CoT）可以提升泛化能力，但它们由于推理轨迹过长而面临高推理延迟的问题。我们提出Fast-ThinkAct，这是一种高效的推理框架，通过可表述的潜在推理实现紧凑且高效的规划。Fast-ThinkAct通过从教师模型中蒸馏学习，利用偏好引导的目标对语言和视觉规划能力进行迁移，以实现具身控制的推理增强策略学习。这使得紧凑的推理能够有效连接到动作执行。在多种具身操作和推理基准测试中进行的广泛实验表明，Fast-ThinkAct在推理延迟上比最先进的推理VLA模型减少了高达89.3\%，同时保持了有效的长时规划、少样本适应和失败恢复能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the high inference latency in existing Vision-Language-Action (VLA) systems that rely on explicit chain-of-thought reasoning. Fast-ThinkAct introduces an efficient framework that performs compact yet effective planning through verbalizable latent reasoning, distilling knowledge from a teacher model guided by a preference-based objective. This approach enables the transfer of both linguistic and visual planning capabilities, enhancing policy learning for embodied control. Experimental results across various VLA benchmarks show that Fast-ThinkAct achieves significant improvements in inference speed, reducing latency by up to 89.3% compared to state-of-the-art methods, while maintaining performance in long-horizon planning, few-shot adaptation, and failure recovery.</div>
<div class="mono" style="margin-top:8px">Fast-ThinkAct 的研究动机是解决现有视觉-语言-动作（VLA）系统中依赖显式链式推理所带来的高推理延迟问题。该框架提出了一种可语言化的潜在规划方法，通过从教师模型中蒸馏知识实现高效且紧凑的推理。该方法由偏好引导的目标驱动，以对齐语言和视觉规划能力，用于具身控制。实验结果表明，Fast-ThinkAct 相比最先进的推理 VLA 系统，推理延迟最多可减少 89.3%，同时保持了长时规划、少样本适应和故障恢复的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs</div>
<div class="meta-line">Authors: Rui Zhu, Xin Shen, Shuchen Wu, Chenxi Miao, Xin Yu, Yang Li, Weikang Li, Deguo Xia, Jizhou Huang</div>
<div class="meta-line">First: 2026-01-14T12:24:47+00:00 · Latest: 2026-01-14T12:24:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09430v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09430v1">PDF</a> · <a href="https://github.com/ruiz-nju/Video-MSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Video-MSR：评估多跳空间推理能力的多模态大语言模型基准</div>
<div class="mono" style="margin-top:8px">空间推理已成为多模态大语言模型（MLLMs）的关键能力，受到越来越多的关注并迅速发展。然而，现有基准主要集中在单步感知到判断的任务上，对需要复杂视觉空间逻辑链的场景探索不足。为弥合这一差距，我们引入了Video-MSR，这是首个专门设计用于评估动态视频场景中多跳空间推理（MSR）能力的基准。Video-MSR通过四个不同的任务系统地探测MSR能力：受限定位、基于链的参考检索、路线规划和反事实物理推断。我们的基准包含3,052个高质量视频实例和4,993个问答对，通过结合先进模型生成与严格人工验证的可扩展、视觉基础的流水线构建。通过对20个最先进的MLLMs进行全面评估，我们发现了显著的局限性，揭示了尽管模型在表层感知上表现出色，但在MSR任务中却出现明显的性能下降，经常在多步推理过程中出现空间迷失和幻觉。为缓解这些不足并增强模型的MSR能力，我们进一步整理了MSR-9K，一个专门的指令微调数据集，并对Qwen-VL进行了微调，在Video-MSR上实现了+7.82%的绝对提升。我们的结果突显了多跳空间指令数据的有效性，并确立了Video-MSR作为未来研究的重要基础。代码和数据将在https://github.com/ruiz-nju/Video-MSR上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of benchmarks for evaluating multi-hop spatial reasoning in MLLMs, particularly in dynamic video environments. The authors introduce Video-MSR, a novel benchmark with four tasks designed to assess complex visual-spatial logical reasoning. The dataset includes 3,052 videos and 4,993 question-answer pairs, created through a scalable and visually-grounded pipeline. Evaluation on 20 state-of-the-art MLLMs shows significant performance drops in multi-step spatial reasoning tasks, indicating limitations in handling spatial disorientation and hallucinations. To improve these capabilities, the paper curates MSR-9K and fine-tunes Qwen-VL, achieving a 7.82% absolute improvement on Video-MSR.</div>
<div class="mono" style="margin-top:8px">本文旨在填补对多跳空间推理能力评估的空白，提出了Video-MSR基准，专门用于在动态视频场景中评估多模态大语言模型（MLLMs）的多跳空间推理（MSR）能力。该基准包含四个任务：受限定位、基于链的参考检索、路径规划和反事实物理推断，共涵盖3,052个高质量视频实例和4,993对问题-答案对，通过可扩展且视觉接地的生成流程构建。对20个最先进的MLLMs进行评估发现，模型在多跳推理任务中表现显著下降，常出现空间迷失和幻觉问题。为提升模型的MSR能力，作者进一步构建了专门的指令微调数据集MSR-9K，并对Qwen-VL进行微调，实现了在Video-MSR上7.82%的绝对性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction</div>
<div class="meta-line">Authors: Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang</div>
<div class="meta-line">First: 2026-01-14T08:45:07+00:00 · Latest: 2026-01-14T08:45:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09285v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs&#x27; high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强大型语言模型在金属有机框架结构预测中的空间推理能力</div>
<div class="mono" style="margin-top:8px">金属有机框架（MOFs）是一种具有广泛应用（如碳捕集和药物输送）的多孔晶体材料，但其三维结构的准确预测仍然是一个重大挑战。尽管大型语言模型（LLMs）在生成晶体方面展现出潜力，但其在MOFs中的应用受到MOFs高原子复杂性的限制。受深度生成模型中块状范式成功启发，我们通过引入MOF-LLM，首个专门针对块级MOF结构预测的大型语言模型框架，开创了该领域LLMs的应用。为了有效利用LLMs进行这种模块化组装任务，我们的训练范式结合了空间感知的持续预训练（CPT）、结构监督微调（SFT）以及基于匹配的强化学习（RL）。通过引入显式的空间先验并利用软自适应策略优化（SAPO）优化结构稳定性，我们的方法显著增强了Qwen-3 8B模型的空间推理能力，从而实现准确的MOF结构预测。全面的实验表明，MOF-LLM在性能上优于基于去噪和基于LLM的最先进方法，并展现出优越的采样效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The prediction of 3D structures for metal-organic frameworks (MOFs) is challenging due to their high atomic complexity. To address this, the study introduces MOF-LLM, the first large language model (LLM) framework tailored for block-level MOF structure prediction. The approach combines spatial-aware continual pre-training, structural supervised fine-tuning, and matching-driven reinforcement learning, with an emphasis on incorporating explicit spatial priors and optimizing structural stability through Soft Adaptive Policy Optimization (SAPO). Experimental results show that MOF-LLM achieves better performance than existing denoising-based and LLM-based methods, demonstrating improved accuracy and sampling efficiency in predicting MOF structures.</div>
<div class="mono" style="margin-top:8px">金属有机框架（MOFs）作为一种具有广泛应用的多孔晶体材料，其三维结构预测仍面临巨大挑战，尤其在于其高原子复杂性。为此，本文提出MOF-LLM，这是首个专门用于MOF结构预测的大型语言模型（LLM）框架。该模型通过结合空间感知的持续预训练、结构监督微调以及基于匹配的强化学习进行训练，并利用显式的空间先验和结构稳定性优化方法（SAPO）提升模型的空间推理能力。实验结果表明，MOF-LLM在性能上优于现有的去噪方法和LLM方法，并展现出更高的采样效率。</div>
</details>
</div>
<div class="card">
<div class="title">DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos</div>
<div class="meta-line">Authors: Jiajun Chen, Jing Xiao, Shaohan Cao, Yuming Zhu, Liang Liao, Jun Pan, Mi Wang</div>
<div class="meta-line">First: 2026-01-14T07:22:44+00:00 · Latest: 2026-01-14T07:22:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeTracker：非稳定卫星视频中的运动解耦车辆检测与跟踪</div>
<div class="mono" style="margin-top:8px">卫星视频能够持续观测地表动态，但在非稳定条件下，多目标跟踪（MOT）面临显著挑战，尤其是平台抖动和微小目标弱外观共同导致跟踪性能下降。为了解决这一问题，我们提出DeTracker，一个专门针对非稳定卫星视频的联合检测与跟踪框架。DeTracker引入了一个全局-局部运动解耦（GLMD）模块，通过全局对齐和局部优化显式分离卫星平台运动与真实目标运动，从而提升轨迹稳定性与运动估计精度。此外，我们还开发了一个时序依赖特征金字塔（TDFP）模块，用于跨帧时序特征融合，增强微小目标表示的连续性和可区分性。我们进一步构建了一个新的基准数据集SDM-Car-SU，模拟多方向和多速度平台运动，以实现对不同运动扰动下跟踪鲁棒性的系统评估。在模拟和真实非稳定卫星视频上的大量实验表明，DeTracker显著优于现有方法，在SDM-Car-SU数据集上达到61.1%的MOTA，在真实卫星视频数据上达到47.3%的MOTA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeTracker is proposed to address the challenges of multi-object tracking in unstabilized satellite videos, where platform jitter and weak object appearance hinder performance. The method employs a Global--Local Motion Decoupling module to separate satellite platform motion from object motion, improving trajectory stability and motion estimation. It also introduces a Temporal Dependency Feature Pyramid module for cross-frame feature fusion, enhancing the continuity and discriminability of object representations. Experimental results on a newly created benchmark dataset and real-world videos show that DeTracker achieves significantly better performance, with MOTA scores of 61.1% and 47.3%, respectively, outperforming existing approaches.</div>
<div class="mono" style="margin-top:8px">DeTracker 是为了解决未稳定卫星视频中多目标跟踪的挑战而提出的，其中平台抖动和小目标外观弱化影响了跟踪性能。该框架引入了全局-局部运动解耦模块，通过全局对齐和局部优化分离平台运动与目标运动，从而提升轨迹稳定性和运动估计精度。同时，它还设计了时序依赖特征金字塔模块，用于跨帧特征融合，增强小目标表示的连续性和可区分性。在新构建的基准数据集 SDM-Car-SU 和真实卫星视频上的实验表明，DeTracker 在 MOTA 指标上分别达到 61.1% 和 47.3%，显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents</div>
<div class="meta-line">Authors: Yuchong Xie, Zesen Liu, Mingyu Luo, Zhixiang Zhang, Kaikai Zhang, Yuanyuan Yuan, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She</div>
<div class="meta-line">First: 2025-10-27T07:04:08+00:00 · Latest: 2026-01-14T07:07:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23675v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23675v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern coding agents integrated into IDEs orchestrate powerful tools and high-privilege system access, creating a high-stakes attack surface. Prior work on Indirect Prompt Injection (IPI) is mainly query-specific, requiring particular user queries as triggers and leading to poor generalizability. We propose query-agnostic IPI, a new attack paradigm that reliably executes malicious payloads under arbitrary user queries. Our key insight is that malicious payloads should leverage the invariant prompt context (i.e., system prompt and tool descriptions) rather than variant user queries. We present QueryIPI, an automated framework that uses tool descriptions as optimizable payloads and refines them via iterative, prompt-based blackbox optimization. QueryIPI leverages system invariants for initial seed generation aligned with agent conventions, and iterative reflection to resolve instruction-following failures and safety refusals. Experiments on five simulated agents show that QueryIPI achieves up to 87% success rate, outperforming the best baseline (50%). Crucially, generated malicious descriptions transfer to real-world coding agents, highlighting a practical security risk.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueryIPI：面向编码代理的无查询依赖间接提示注入</div>
<div class="mono" style="margin-top:8px">现代集成到IDE中的编码代理协调强大的工具和高权限系统访问，形成了高风险的攻击面。先前关于间接提示注入（IPI）的工作主要依赖特定查询，需要特定用户查询作为触发条件，导致泛化能力较差。我们提出了一种无查询依赖的IPI新攻击范式，能够在任意用户查询下可靠地执行恶意负载。我们的核心洞察是，恶意负载应利用不变的提示上下文（即系统提示和工具描述），而非变化的用户查询。我们提出了QueryIPI，一个自动化框架，利用工具描述作为可优化的负载，并通过迭代的基于提示的黑盒优化进行精炼。QueryIPI利用系统不变量生成符合代理惯例的初始种子，并通过迭代反思来解决指令遵循失败和安全拒绝问题。在五个模拟代理上的实验表明，QueryIPI的成功率高达87%，优于最佳基线（50%）。关键的是，生成的恶意描述可迁移至现实中的编码代理，突显了实际的安全风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing Indirect Prompt Injection (IPI) attacks, which are highly dependent on specific user queries and lack generalizability. QueryIPI introduces a query-agnostic approach by focusing on the invariant prompt context, such as system prompts and tool descriptions, to reliably execute malicious payloads regardless of the user&#x27;s input. The framework employs an automated method that optimizes tool descriptions as payloads and iteratively refines them through blackbox optimization. Experimental results on five simulated coding agents demonstrate that QueryIPI achieves an 87% success rate, significantly outperforming the best baseline of 50%, and shows that the generated malicious payloads can transfer to real-world coding agents, indicating a serious security vulnerability.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有间接提示注入（IPI）攻击方法的局限性，这些方法通常依赖特定用户查询，泛化能力较差。所提出的方法QueryIPI采用一种与查询无关的攻击范式，通过利用不变的提示上下文，如系统提示和工具描述，构建恶意负载。该框架通过迭代的黑盒优化和反思机制，有效绕过安全机制。实验结果显示，在五个模拟代理上，QueryIPI的成功率高达87%，远超现有最佳基线的50%。更重要的是，生成的恶意描述能够迁移到实际的编码代理中，突显了其潜在的安全风险。</div>
</details>
</div>
<div class="card">
<div class="title">CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation</div>
<div class="meta-line">Authors: Haodi Yao, Fenghua He, Ning Hao, Yao Su</div>
<div class="meta-line">First: 2026-01-14T07:03:01+00:00 · Latest: 2026-01-14T07:03:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIDD：用于高效且具有判别力的局部特征表示的跨层独立可变形描述</div>
<div class="mono" style="margin-top:8px">稳健的局部特征表示对于空间智能任务（如机器人导航和增强现实）至关重要。建立可靠的对应关系需要提供高判别力和计算效率的描述符。为了解决这一问题，我们引入了跨层独立可变形描述（CLIDD），一种通过直接从独立特征层次结构中采样来实现卓越区分度的方法。该方法利用可学习偏移量，在不同尺度上捕捉细粒度的结构细节，同时避免统一密集表示的计算负担。为了确保实时性能，我们实施了一种硬件感知的内核融合策略，以最大化推理吞吐量。此外，我们开发了一个可扩展框架，将轻量级架构与结合度量学习和知识蒸馏的训练协议相结合。该方案生成了一系列针对不同部署约束优化的模型变体。广泛评估表明，我们的方法在匹配精度和计算效率方面均表现出色。具体而言，超紧凑变体在参数量仅为0.004M的情况下，达到了与SuperPoint相当的精度，实现了模型大小减少99.7%。此外，我们的高性能配置在边缘设备上超过了200 FPS，同时优于所有当前最先进的方法，包括高容量的DINOv2框架。这些结果表明，CLIDD在计算开销最小的情况下实现了高精度的局部特征匹配，为实时空间智能任务提供了一种稳健且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop efficient and discriminative local feature representations for spatial intelligence tasks like robot navigation and augmented reality. The proposed method, CLIDD, introduces a cross-layer independent deformable description approach that samples directly from independent feature hierarchies, using learnable offsets to capture structural details across scales without the computational cost of dense representations. A hardware-aware kernel fusion strategy and a scalable framework combining lightweight architectures with metric learning and knowledge distillation are implemented. Experimental results show that the ultra-compact variant of CLIDD achieves 99.7% smaller model size than SuperPoint with comparable precision, while the high-performance version outperforms state-of-the-art methods like DINOv2-based frameworks and exceeds 200 FPS on edge devices.</div>
<div class="mono" style="margin-top:8px">本研究的动机是为机器人导航和增强现实等空间智能任务开发高效且具有判别力的局部特征表示。所提出的方法CLIDD通过从独立特征层次直接采样，结合可学习偏移量捕捉多尺度结构细节，实现优越的区分性，同时避免统一密集表示的计算负担。为了确保实时性能，采用了一种面向硬件的内核融合策略以最大化推理吞吐量，并构建了一个可扩展框架，将轻量级架构与结合度量学习和知识蒸馏的训练协议相结合，生成适用于不同部署约束的模型变体。实验结果表明，最紧凑的变体在参数量上仅为SuperPoint的0.004M，模型体积减少99.7%，同时保持了相当的精度；高性能配置则超越了当前所有最先进的方法，包括高容量的DINOv2框架，并在边缘设备上实现了超过200 FPS的处理速度。</div>
</details>
</div>
<div class="card">
<div class="title">MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences</div>
<div class="meta-line">Authors: Qihao Wang, Ziming Cheng, Shuo Zhang, Fan Liu, Rui Xu, Heng Lian, Kunyi Wang, Xiaoming Yu, Jianghao Yin, Sen Hu, Yue Hu, Shaolei Zhang, Yanbing Liu, Ronghao Chen, Huacan Wang</div>
<div class="meta-line">First: 2026-01-11T06:41:26+00:00 · Latest: 2026-01-13T14:48:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06789v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06789v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a &quot;closed-world&quot; limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemGovern：通过学习受约束的人类经验提升代码代理</div>
<div class="mono" style="margin-top:8px">尽管自主软件工程（SWE）代理正在重塑编程范式，但它们目前受到“封闭世界”限制：它们尝试从头解决 bug 或仅使用局部上下文，而忽略了 GitHub 等平台上大量可用的历史人类经验。然而，现实世界中的问题跟踪数据具有无结构和碎片化的特性，这阻碍了对这些开放世界经验的访问。本文中，我们引入了 MemGovern，这是一个框架，旨在治理和转换原始 GitHub 数据为代理可用的经验记忆。MemGovern 采用经验治理方法，将人类经验转换为代理友好的经验卡片，并引入了一种代理经验搜索策略，使代理能够基于逻辑检索人类专业知识。通过生成 135,000 个受约束的经验卡片，MemGovern 实现了显著的性能提升，在 SWE-bench 验证中提高了 4.65% 的解决率。作为一种插件方法，MemGovern 为代理友好的记忆基础设施提供了解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind MemGovern is to address the closed-world limitation of autonomous software engineering agents, which fail to leverage the vast amount of historical human experience available on platforms like GitHub. The framework introduces a method to govern and transform raw GitHub data into structured, agent-friendly experience cards through experience governance and an agentic experience search strategy. The main experimental results show that MemGovern significantly improves agent performance, achieving a 4.65% increase in resolution rates on the SWE-bench Verified dataset by utilizing these governed experience cards.</div>
<div class="mono" style="margin-top:8px">MemGovern的动机是解决自主软件工程代理的封闭世界限制，即它们无法有效利用GitHub等平台上丰富的历史人类经验。该框架提出了一种方法，通过治理和转换原始GitHub数据，生成结构化、可操作的体验记忆卡片。MemGovern还引入了一种代理驱动的体验搜索策略，实现基于逻辑的人类专业知识检索。实验结果表明，通过生成135000个治理后的体验卡片，MemGovern在SWE-bench Verified数据集上的解决率提升了4.65%。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation</div>
<div class="meta-line">Authors: Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang</div>
<div class="meta-line">First: 2026-01-13T10:31:09+00:00 · Latest: 2026-01-13T10:31:09+00:00</div>
<div class="meta-line">Comments: 2nd International Conference on Drones and Unmanned Systems (DAUS&#x27; 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08412v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08412v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于思维链引导的混合蒸馏方法用于边缘无人机控制代码生成</div>
<div class="mono" style="margin-top:8px">随着大语言模型在代码生成任务中展现出巨大潜力，其在资源受限的无人机平台上用于 onboard 控制的应用已成为重要的研究方向。然而，大模型的高资源消耗与无人机平台对实时性和轻量化的要求之间存在显著矛盾。本文提出一种集成方法，结合知识蒸馏、思维链引导和监督微调，用于无人机多 SDK 控制任务，旨在高效地将复杂推理和代码生成能力转移到更小的模型中。首先，构建一个涵盖多种主流无人机 SDK 的高质量数据集，包含指令-代码-推理链，并引入反事实负样本进行数据增强，引导模型从指令解析到代码生成学习端到端的逻辑。其次，利用通过 QLoRA 量化后的 DeepSeek-Coder-V2-Lite 作为教师模型，基于混合黑盒与白盒蒸馏策略生成高质量的思维链软标签。这些软标签与硬标签结合，使用加权交叉熵损失函数，将复杂推理能力转移到较小的学生模型中。最后，通过针对无人机控制场景优化的提示调优工程，提升了模型在 SDK 类型识别和函数调用匹配等核心任务上的性能。实验结果表明，蒸馏后的轻量模型在保持高代码生成准确率的同时，显著提升了部署和推理效率，有效验证了我们方法在实现精确且轻量化的无人机智能控制方面的可行性与优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying large language models to edge-drone control code generation, where resource constraints and real-time performance are critical. The proposed method integrates knowledge distillation, chain-of-thought guidance, and supervised fine-tuning to transfer complex reasoning and code generation capabilities to smaller models. A high-quality dataset with instruction-code-reasoning chains and counterfactual negative samples is constructed, and a hybrid distillation strategy is used to generate soft labels. The model is further optimized through prompt tuning for UAV control tasks. Experimental results show that the lightweight model achieves high code generation accuracy while significantly improving deployment and inference efficiency, validating the effectiveness of the approach.</div>
<div class="mono" style="margin-top:8px">本文针对将大语言模型应用于资源受限的无人机控制代码生成任务中的挑战，提出了结合知识蒸馏、思维链引导和监督微调的混合方法，以将复杂的推理与代码生成能力迁移至轻量模型。研究构建了一个包含多种主流无人机SDK指令-代码-推理链的高质量数据集，并利用量化后的教师模型生成高质量的软标签。实验结果表明，所提出的轻量模型在保持高代码生成准确率的同时，显著提升了部署和推理效率，验证了该方法在实现精确且轻量级无人机智能控制方面的可行性与优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models</div>
<div class="meta-line">Authors: Shaoxiu Wei, Mingchao Liang, Florian Meyer</div>
<div class="meta-line">First: 2025-06-22T18:15:08+00:00 · Latest: 2026-01-13T08:40:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.18124v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.18124v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiobject tracking (MOT) is an important task in applications including autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT methods are model-based and combine sequential Bayesian estimation with data association and an object birth model. More recent methods are fully data-driven and rely on the training of neural networks. Both approaches offer distinct advantages in specific settings. In particular, model-based methods are generally applicable across a wide range of scenarios, whereas data-driven MOT achieves superior performance in scenarios where abundant labeled data for training is available. A natural thought is whether a general framework can integrate the two approaches. This paper introduces a hybrid method that utilizes neural networks to enhance specific aspects of the statistical model in Bayesian MOT that have been identified as overly simplistic. By doing so, the performance of the prediction and update steps of Bayesian MOT is improved. To ensure tractable computation, our framework uses belief propagation to avoid high-dimensional operations combined with sequential Monte Carlo methods to perform low-dimensional operations efficiently. The resulting method combines the flexibility and robustness of model-based approaches with the capability to learn complex information from data of neural networks. We evaluate the performance of the proposed method based on the nuScenes autonomous driving dataset and demonstrate that it has state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经增强运动和观测模型的贝叶斯多目标跟踪</div>
<div class="mono" style="margin-top:8px">多目标跟踪（MOT）在自动驾驶、海洋科学和航空航天监控等应用中具有重要意义。传统MOT方法是基于模型的，结合了序贯贝叶斯估计、数据关联和目标生成模型。近年来的方法则是完全数据驱动的，依赖于神经网络的训练。两种方法在特定场景下各有优势。具体而言，基于模型的方法通常适用于广泛场景，而数据驱动的MOT方法在有大量标注训练数据的场景中表现更优。一个自然的想法是，是否可以构建一个通用框架来融合这两种方法。本文提出了一种混合方法，利用神经网络增强贝叶斯MOT中被认定为过于简化的统计模型特定部分。通过这种方式，提升了贝叶斯MOT预测和更新步骤的性能。为了确保计算的可行性，我们的框架采用信念传播来避免高维运算，并结合序贯蒙特卡洛方法高效地执行低维运算。所得到的方法结合了基于模型方法的灵活性和鲁棒性，以及神经网络从数据中学习复杂信息的能力。我们基于nuScenes自动驾驶数据集评估了所提方法的性能，并证明其具有最先进的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multiobject tracking (MOT) by proposing a hybrid approach that combines the strengths of model-based and data-driven methods. The motivation stems from the limitations of traditional Bayesian MOT, which often relies on overly simplistic motion and measurement models. The method enhances these models using neural networks to capture more complex patterns, while maintaining the tractability of Bayesian inference through belief propagation and sequential Monte Carlo techniques. Experimental results on the nuScenes dataset show that the proposed approach achieves state-of-the-art performance, effectively integrating model-based robustness with data-driven flexibility.</div>
<div class="mono" style="margin-top:8px">本文针对多目标跟踪（MOT）任务提出了一种混合方法，结合了模型驱动和数据驱动方法的优势。研究动机源于传统贝叶斯MOT方法中运动和观测模型过于简单，以及数据驱动方法对大量标注数据的依赖。该方法通过神经网络增强贝叶斯MOT中的统计模型，以提升预测和更新步骤的准确性。为确保计算可行性，框架采用信念传播避免高维运算，并结合序贯蒙特卡洛方法高效处理低维操作。在nuScenes数据集上的实验结果表明，该方法在MOT任务中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</div>
<div class="meta-line">Authors: Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang</div>
<div class="meta-line">First: 2025-03-29T04:51:50+00:00 · Latest: 2026-01-12T04:43:21+00:00</div>
<div class="meta-line">Comments: Project page: https://logosroboticsgroup.github.io/SPAR/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.22976v7">Abs</a> · <a href="https://arxiv.org/pdf/2503.22976v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://logosroboticsgroup.github.io/SPAR/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从二维世界到三维空间：训练视觉-语言模型以感知和推理三维场景</div>
<div class="mono" style="margin-top:8px">尽管最近在LVLMs（视觉-语言模型）方面取得了进展，它们在空间感知方面仍存在困难，这限制了其对复杂三维场景的推理能力。与之前将3D表示纳入模型以提升空间理解的方法不同，我们旨在通过利用空间相关的图像数据来释放VLMs（视觉语言模型）的潜力。为此，我们引入了一种基于具有3D真实标签场景数据的新型2D空间数据生成与标注流程。该流程能够创建从基础感知任务到更复杂推理任务的多样化空间任务。借助这一流程，我们构建了SPAR-7M，一个从多个公开数据集中生成的大型数据集，涵盖数千个场景。此外，我们还引入了SPAR-Bench，一个旨在比现有空间基准更全面评估空间能力的基准，支持单视角和多视角输入。在SPAR-7M和大规模2D数据集上进行训练使我们的模型在2D空间基准上达到了最先进的性能。进一步在3D任务特定数据集上微调，取得了具有竞争力的结果，突显了我们数据集在增强空间推理方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of vision-language models (VLMs) in spatial perception, which hinders their ability to reason about complex 3D scenes. The authors propose a novel 2D spatial data generation and annotation pipeline based on scene data with 3D ground-truth, enabling the creation of diverse spatial tasks. They construct SPAR-7M, a large-scale dataset derived from multiple public datasets, and introduce SPAR-Bench, a benchmark for comprehensive spatial evaluation. Training on these datasets achieves state-of-the-art performance on 2D spatial benchmarks, and further fine-tuning on 3D task-specific data shows competitive results, highlighting the effectiveness of their approach in improving spatial reasoning.</div>
<div class="mono" style="margin-top:8px">本文针对视觉-语言模型（VLMs）在空间感知方面的不足，指出其限制了对复杂三维场景的推理能力。作者提出了一种基于三维真实数据的新型二维空间数据生成与标注流程，能够创建从基础感知到复杂推理任务的多样化空间任务。利用该流程构建了SPAR-7M这一大规模数据集，并设计了SPAR-Bench基准，以更全面地评估空间能力。在这些数据集上训练的模型在二维空间基准测试中达到了最先进的性能，而在三维特定任务上的微调也取得了有竞争力的结果，证明了该方法在提升空间推理能力方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests</div>
<div class="meta-line">Authors: Jie Wu, Haoling Li, Xin Zhang, Jiani Guo, Jane Luo, Steven Liu, Yangyu Huang, Ruihang Chu, Scarlett Li, Yujiu Yang</div>
<div class="meta-line">First: 2026-01-11T15:22:33+00:00 · Latest: 2026-01-11T15:22:33+00:00</div>
<div class="meta-line">Comments: Project: https://github.com/JieWu02/X-Coder</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06953v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06953v1">PDF</a> · <a href="https://github.com/JieWu02/X-Coder">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Coder：通过完全合成任务、解决方案和测试提升编程竞赛能力</div>
<div class="mono" style="margin-top:8px">由于编程竞赛对推理能力和逻辑复杂度有极高的要求，这对代码大语言模型（Code LLMs）提出了巨大挑战。然而，当前的Code LLMs仍然严重依赖真实世界数据，这限制了其可扩展性。本文探索了一种完全合成的方法：通过完全生成的任务、解决方案和测试用例来训练Code LLMs，从而在不依赖真实世界数据的情况下增强代码推理模型。为此，我们利用基于特征的合成方法，提出了一种名为SynthSmith的新数据合成流水线。SynthSmith在生成多样化且具有挑战性的任务，以及经过验证的解决方案和测试用例方面展现出巨大潜力，支持监督微调和强化学习。基于所提出的合成SFT和RL数据集，我们引入了X-Coder模型系列，在LiveCodeBench v5和v6上分别实现了62.9 avg@8和55.8的显著通过率，尽管其参数量仅为7B，仍优于DeepCoder-14B-Preview和AReal-boba2-14B。深入分析表明，扩展定律在我们的合成数据集上成立，我们探讨了哪些维度更适合扩展。我们进一步提供了以代码为中心的强化学习的见解，并通过详细的消融实验和分析，突出了影响性能的关键因素。我们的研究结果表明，扩展高质量合成数据并采用分阶段训练可以显著提升代码推理能力，同时减少对真实世界编程数据的依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of training Code LLMs for competitive programming, which requires strong reasoning and logical complexity. The authors propose a fully synthetic approach using a novel data synthesis pipeline called SynthSmith, which generates diverse tasks, solutions, and test cases. The X-Coder model series, trained on these synthetic datasets, achieves a notable performance with 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, surpassing models with larger parameters. The study also investigates scaling laws and provides insights into code-centric reinforcement learning through detailed ablation analysis.</div>
<div class="mono" style="margin-top:8px">本文针对代码大模型在编程竞赛中的训练难题，提出了一种完全合成的方法。作者设计了一个名为SynthSmith的新数据合成管道，用于生成多样且具有挑战性的编程任务、解决方案和测试用例。基于这些合成数据，X-Coder模型系列在LiveCodeBench v5和v6上分别取得了62.9 avg@8和55.8的显著表现，超越了参数更大的DeepCoder-14B-Preview和AReal-boba2-14B模型。研究还探讨了合成数据的扩展规律，并通过详细的消融实验分析了代码强化学习中的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Code Reasoning for Software Engineering Tasks: A Survey and A Call to Action</div>
<div class="meta-line">Authors: Saurabh Pujar, Ira Ceka, Irene Manotas, Gail Kaiser, Baishakhi Ray, Shyam Ramji</div>
<div class="meta-line">First: 2025-06-16T19:18:09+00:00 · Latest: 2026-01-11T14:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.13932v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.13932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. Their performance on certain tasks can be further enhanced by incorporating test-time reasoning techniques. These inference-time advances have been adopted into the code domain, enabling complex software engineering (SWE) tasks such as code generation, test generation and issue resolution. However, the impact of different reasoning techniques on code-centric SWE tasks has not been systematically explored. In this work, we survey code reasoning techniques that underpin these capabilities, with a focus on test-time compute and inference-time reasoning paradigms. We examine a variety of code-specific reasoning methods and progressively build up to SWE agents, which combine planning, tool use, and multi-step interaction. We also compare the impact of different techniques on coding tasks, highlighting their relative importance and outlining open challenges and future research directions. Our contributions are: (1) to the best of our knowledge, the first dedicated survey of code reasoning for SWE tasks, highlighting overarching reasoning strategies, hybrid methods, and agentic approaches; (2) a taxonomy of inference-time techniques used to drive code reasoning, accompanied by a curated set of under-explored benchmarks with high potential for SWE evaluation; (3) a comparative analysis of reasoning design patterns across commonly used models and benchmarks; and (4) a synthesis of gaps in current methods and evaluation practices, identifying under-explored areas and concrete opportunities for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向软件工程任务的代码推理：综述与行动呼吁</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的兴起在各种自然语言任务中带来了显著的性能提升。通过引入测试时推理技术，某些任务的性能可以进一步增强。这些推理技术的进步已被应用于代码领域，从而支持诸如代码生成、测试生成和问题解决等复杂的软件工程（SWE）任务。然而，不同推理技术对以代码为中心的SWE任务的影响尚未被系统地研究。在本工作中，我们综述了支撑这些能力的代码推理技术，重点探讨了测试时计算和推理时推理范式。我们分析了多种针对代码的推理方法，并逐步构建到结合规划、工具使用和多步交互的SWE代理。此外，我们还比较了不同技术对编码任务的影响，突出了其相对重要性，并指出了开放性挑战和未来研究方向。我们的贡献包括：(1) 据我们所知，这是首个专门针对SWE任务的代码推理综述，强调了总体推理策略、混合方法和代理方法；(2) 提供了一种推理时技术的分类体系，附带一组尚未充分研究的基准测试，具有很高的SWE评估潜力；(3) 对常用模型和基准测试中推理设计模式进行了比较分析；(4) 综合了当前方法和评估实践中的不足，识别了尚未充分探索的领域和未来研究的具体机会。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for a systematic understanding of how reasoning techniques affect code-centric software engineering tasks. It surveys existing code reasoning methods, particularly those involving test-time computation and inference-time reasoning, and explores their application in complex tasks like code generation and issue resolution. The study also evaluates the effectiveness of various reasoning strategies across different models and benchmarks, identifying key gaps and proposing future research directions.</div>
<div class="mono" style="margin-top:8px">本文旨在系统探讨推理技术对代码相关软件工程任务的影响。文章调研了支撑这些能力的代码推理方法，重点分析了测试时计算和推理时推理范式，并探讨了其在代码生成、测试生成和问题解决等复杂任务中的应用。研究比较了不同推理策略在多种模型和基准上的效果，指出了当前方法和评估实践中的不足，并提出了未来研究的方向，强调了尚未充分探索的领域和改进机会。</div>
</details>
</div>
<div class="card">
<div class="title">Training Versatile Coding Agents in Synthetic Environments</div>
<div class="meta-line">Authors: Yiqi Zhu, Apurva Gandhi, Graham Neubig</div>
<div class="meta-line">First: 2025-12-13T07:02:28+00:00 · Latest: 2026-01-11T10:24:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12216v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12216v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在合成环境中训练多功能编码代理</div>
<div class="mono" style="margin-top:8px">先前关于训练软件工程代理的研究工作主要利用现有的资源，如 GitHub 仓库中的问题，来构建软件工程任务及相应的测试套件。这些方法面临两个关键限制：(1) 依赖于现有的 GitHub 仓库，灵活性有限；(2) 主要关注于问题解决任务，限制了其在软件工程师需处理的更广泛任务类型中的适用性。为克服这些挑战，我们引入了 SWE-Playground，这是一种新颖的生成环境和轨迹的管道，支持多功能编码代理的训练。与之前的方法不同，SWE-Playground 通过强大的语言模型和代理从头开始合成生成项目和任务，消除了对外部数据源的依赖。这使我们能够处理更广泛的编码任务，例如通过生成单元测试来复现问题，以及从头实现库。我们在三个不同的基准上展示了这种方法的有效性，结果表明 SWE-Playground 生成的轨迹具有密集的训练信号，使代理能够在显著更少的轨迹数量下达到与之前工作相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing methods for training software engineering agents, which rely on pre-existing GitHub repositories and focus narrowly on issue resolution tasks. The authors propose SWE-Playground, a synthetic environment generation pipeline that creates projects and tasks from scratch using strong language models and agents, thereby reducing dependency on external data sources. Experimental results on three benchmarks show that SWE-Playground generates trajectories with rich training signals, allowing agents to achieve comparable performance with far fewer trajectories than previous approaches.</div>
<div class="mono" style="margin-top:8px">本文针对现有软件工程代理训练方法的局限性进行探讨，这些方法依赖于现有的GitHub仓库并主要集中在问题解决任务上。作者提出了SWE-Playground，这是一个能够从头生成项目和任务的合成环境生成管道，利用强大的语言模型和代理，从而减少对外部数据的依赖。在三个基准测试上的实验结果表明，SWE-Playground生成的轨迹具有丰富的训练信号，使得代理能够在远少于以往方法的轨迹数量下达到相当的性能水平。</div>
</details>
</div>
<div class="card">
<div class="title">LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Yuhua Zhu, Wenhui Zhao</div>
<div class="meta-line">First: 2026-01-10T12:18:12+00:00 · Latest: 2026-01-10T12:18:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06550v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06550v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMTrack：基于多模态大语言模型的语义多目标跟踪</div>
<div class="mono" style="margin-top:8px">传统多目标跟踪（MOT）系统在定位和关联方面取得了显著精度，有效回答了\textit{where}和\textit{who}的问题。然而，它们通常作为自闭式观察者，能够追踪几何路径，却无法理解目标行为背后的语义\textit{what}和\textit{why}。为弥合几何感知与认知推理之间的差距，我们提出了\textbf{LLMTrack}，一种用于语义多目标跟踪（SMOT）的新型端到端框架。我们采用仿生设计哲学，将强大的定位能力与深层次的理解能力分离，使用Grounding DINO作为视觉模块，LLaVA-OneVision多模态大模型作为认知模块。我们引入了时空融合模块，聚合实例级交互特征和视频级上下文信息，使大语言模型（LLM）能够理解复杂轨迹。此外，我们设计了一种渐进式的三阶段训练策略，通过LoRA实现视觉对齐、时间微调和语义注入，以高效地将大规模模型适配到跟踪领域。在BenSMOT基准上的大量实验表明，LLMTrack实现了最先进的性能，在实例描述、交互识别和视频摘要方面显著优于现有方法，同时保持了良好的跟踪稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Traditional Multi-Object Tracking (MOT) systems excel in localization and association but lack semantic understanding of object behaviors. To address this limitation, LLMTrack introduces a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT) that separates strong localization from deep reasoning. The framework employs Grounding DINO for visual perception and LLaVA-OneVision as the multimodal reasoning component, integrating a Spatio-Temporal Fusion Module to enhance trajectory comprehension. A progressive three-stage training strategy, including Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA, is used to adapt the model effectively. Experimental results on the BenSMOT benchmark show that LLMTrack achieves state-of-the-art performance, outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining stable tracking.</div>
<div class="mono" style="margin-top:8px">LLMTrack旨在解决传统多目标跟踪（MOT）系统在定位和关联方面的高精度，但缺乏对目标行为语义理解的不足。该框架通过将定位与语义推理解耦，采用Grounding DINO进行视觉感知，利用LLaVA-OneVision作为多模态推理组件。引入了时空融合模块，将实例级交互特征与视频级上下文相结合，提升大语言模型对复杂轨迹的理解能力。模型采用渐进式的三阶段训练策略：视觉对齐、时间微调和通过LoRA进行的语义注入，以高效适配跟踪领域。在BenSMOT基准上的实验表明，LLMTrack在实例描述、交互识别和视频摘要任务中显著优于现有方法，同时保持了稳定的跟踪性能。</div>
</details>
</div>
<div class="card">
<div class="title">3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</div>
<div class="meta-line">Authors: Hao Tang, Ting Huang, Zeyu Zhang</div>
<div class="meta-line">First: 2026-01-10T09:13:10+00:00 · Latest: 2026-01-10T09:13:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06496v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06496v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/3DCoCav2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D CoCa v2：基于测试时搜索的对比学习框架用于可泛化的空间智能</div>
<div class="mono" style="margin-top:8px">空间智能指的是在三维环境中感知、推理和描述物体及其关系的能力，是具身感知和场景理解的基础。3D字幕生成旨在用自然语言描述三维场景；然而，由于点云的稀疏性和不规则性，以及现有字幕生成器在不同环境（包括室内外三维场景）中存在弱锚定和有限的分布外（OOD）泛化能力，该任务仍面临挑战。为解决这一问题，我们提出了3D CoCa v2，这是一个可泛化的三维字幕生成框架，将对比视觉-语言学习与三维字幕生成统一起来，并通过测试时搜索（TTS）进一步提升鲁棒性，而无需更新字幕生成器的参数。3D CoCa v2基于冻结的CLIP语义先验，结合了空间感知的三维场景编码器和一个与对比学习和字幕生成目标联合优化的多模态解码器，避免使用外部检测器或手工设计的提议。在推理阶段，TTS生成多样化的字幕候选，并利用紧凑的场景摘要进行奖励引导选择。实验结果表明，在ScanRefer和Nr3D数据集上，3D CoCa v2相比3D CoCa分别提升了+1.50和+1.61的CIDEr@0.5IoU，在TOD3Cap的零样本OOD评估中提升了+3.8的CIDEr@0.25。代码将在https://github.com/AIGeeksGroup/3DCoCav2上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance the generalizability of 3D captioning models across diverse environments, particularly addressing the limitations of existing captioners in terms of weak grounding and limited out-of-distribution performance. 3D CoCa v2 introduces a framework that combines contrastive vision-language learning with 3D caption generation, utilizing a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder, and a multimodal decoder jointly optimized for both contrastive and captioning tasks. The method further incorporates test-time search (TTS) to generate diverse caption candidates and select the best one using a compact scene summary, without modifying the model parameters. Experimental results demonstrate significant improvements over the previous version, 3D CoCa, with increases of +1.50 CIDEr@0.5IoU on ScanRefer, +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot out-of-distribution evaluation on TOD3Cap.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升3D场景描述模型在不同环境下的泛化能力，解决点云稀疏性和不规则性带来的挑战，以及现有描述模型在跨环境中的弱语义关联和有限的分布外（OOD）泛化能力。3D CoCa v2提出了一种结合对比学习与3D描述生成的框架，采用冻结的CLIP语义先验、具有空间感知能力的3D场景编码器以及联合优化的多模态解码器。模型通过测试时搜索（TTS）增强鲁棒性，生成多样化的描述候选并基于场景摘要进行奖励引导选择。实验结果显示，该模型在ScanRefer上CIDEr@0.5IoU提升1.50，在Nr3D上提升1.61，并在TOD3Cap的零样本OOD评估中CIDEr@0.25提升3.8。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-09T20:17:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长周期软件演进场景中对编码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI编码代理基准测试主要关注孤立的、单一问题的任务，例如修复错误或实现小功能。然而，现实中的软件工程本质上是一项长周期的工作：开发者必须理解高层需求，协调修改多个文件，并在多次迭代中演进代码库同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长周期软件演进挑战上的基准测试。该基准测试基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而相比之下在单一问题的SWE-Bench Verified上解决率为65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长周期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SWE-EVO is to address the limitations of existing benchmarks that focus on isolated coding tasks, by evaluating AI coding agents in long-horizon software evolution scenarios. The benchmark is constructed using release notes and version histories from seven open-source Python projects, creating 48 tasks that require multi-step modifications across an average of 21 files. Experimental results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on single-issue benchmarks, highlighting the difficulty agents face in sustained, multi-file reasoning. A new metric, Fix Rate, is introduced to measure partial progress in solving these complex tasks.</div>
<div class="mono" style="margin-top:8px">本研究提出了SWE-EVO基准，用于评估AI编码代理在长期软件演进场景中的表现，这类场景比单一任务更复杂且更贴近现实。SWE-EVO基于七个成熟Python项目的发布说明和版本历史构建，包含48个需要跨平均21个文件进行多步骤修改的任务，并通过广泛的测试套件进行验证。实验结果显示，即使是像GPT-5与OpenHands这样的先进模型，在SWE-EVO上的解决率也只有21%，远低于单一任务SWE-Bench Verified的65%，表明当前代理在持续多文件推理方面存在显著差距。</div>
</details>
</div>
<div class="card">
<div class="title">Automated QoR improvement in OpenROAD with coding agents</div>
<div class="meta-line">Authors: Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee</div>
<div class="meta-line">First: 2026-01-09T19:30:02+00:00 · Latest: 2026-01-09T19:30:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在OpenROAD中使用编码代理实现自动化QoR改进</div>
<div class="mono" style="margin-top:8px">EDA开发和创新一直受到专家工程资源稀缺的限制。尽管领先的LLMs在编码和科学推理任务中表现出色，但它们在推动EDA技术发展方面的潜力尚未得到充分验证。我们提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主、基于仓库的编码系统，能够读取OpenROAD，提出研究方向，将其扩展为实现步骤，并提交可执行的代码差异。我们的贡献包括：(i) 一个用于EDA代码修改的闭环LLM框架；(ii) 针对PPA导向改进的OpenROAD任务套件和评估协议；以及 (iii) 需要最少人工监督的端到端演示。在OpenROAD上的实验实现了最高达5.9%的布线线长减少和10.0%的有效时钟周期减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of expert engineering resources in EDA development and explore the potential of large language models (LLMs) in advancing EDA technology. The authors introduce AuDoPEDA, an autonomous coding system that leverages OpenAI models and a Codex-class agent to analyze OpenROAD, propose research directions, generate implementation steps, and submit executable code changes. Experimental results on OpenROAD show that the system achieves up to a 5.9% reduction in routed wirelength and a 10.0% improvement in effective clock period, demonstrating its effectiveness in PPA-oriented improvements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决EDA开发中专家工程资源稀缺的问题，并探索大语言模型在推动EDA技术发展方面的潜力。作者提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主编码系统，能够分析OpenROAD，提出研究方向，生成实现步骤，并提交可执行的代码修改。实验结果表明，AuDoPEDA在面向PPA的代码优化中实现了最多5.9%的布线线长减少和10.0%的有效时钟周期提升。</div>
</details>
</div>
<div class="card">
<div class="title">From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level</div>
<div class="meta-line">Authors: Jia Li, Yuxin Su, Michael R. Lyu</div>
<div class="meta-line">First: 2026-01-07T09:22:28+00:00 · Latest: 2026-01-09T16:30:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03731v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03731v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从实验室到实际应用：在仓库级别对代理代码推理进行基准测试</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLMs）演变为自主代理，评估仓库级别的推理能力——即在大规模、真实世界且相互依赖的文件系统中保持逻辑一致性——变得至关重要。当前的基准测试通常在孤立的代码片段和黑盒评估之间波动。我们提出了 RepoReason，这是一个以归纳性断言验证为核心的白盒诊断基准。为了消除记忆效应同时保留真实的逻辑深度，我们实现了一个执行驱动的变异框架，利用环境作为语义预言机来重新生成真实状态。此外，我们通过动态程序切片建立了一个细粒度的诊断系统，并通过三个正交指标（$ESV$（读取负载）、$MCL$（模拟深度）、$DFI$（集成宽度））量化推理能力。对前沿模型（如 Claude-4.5-Sonnet、DeepSeek-v3.1-Terminus）的全面评估揭示了一个普遍存在的聚合缺陷，其中集成宽度是主要的认知瓶颈。我们的研究结果为优化下一代代理软件工程提供了细致的白盒见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of evaluating agentic code reasoning at the repository level, which is essential for ensuring logical consistency across large, interdependent file systems. RepoReason introduces a white-box diagnostic benchmark based on abductive assertion verification, employing an execution-driven mutation framework that uses the environment as a semantic oracle to generate ground-truth states. This approach eliminates memorization while maintaining logical depth. The research also implements a fine-grained diagnostic system using dynamic program slicing, measuring reasoning through three metrics: ESV (reading load), MCL (simulation depth), and DFI (integration width). Experimental results on state-of-the-art models show a significant aggregation deficit, with integration width identified as the main cognitive bottleneck.</div>
<div class="mono" style="margin-top:8px">该研究旨在评估代理代码推理在仓库级别的表现，这对于确保大规模、真实世界且相互依赖的文件系统中的逻辑一致性至关重要。作者提出了RepoReason，这是一个基于归纳性断言验证的白盒基准，采用执行驱动的变异框架生成真实状态，避免依赖记忆。他们还构建了一个细粒度的诊断系统，利用动态程序切片，通过三个相互独立的指标（ESV：阅读负载，MCL：模拟深度，DFI：集成宽度）量化推理能力。对前沿模型的评估表明存在普遍的聚合缺陷，其中集成宽度是主要的认知瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">CoV: Chain-of-View Prompting for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang</div>
<div class="meta-line">First: 2026-01-08T17:59:42+00:00 · Latest: 2026-01-09T07:20:05+00:00</div>
<div class="meta-line">Comments: Code link https://github.com/ziplab/CoV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05172v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05172v2">PDF</a> · <a href="https://github.com/ziplab/CoV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoV：用于空间推理的视角链提示</div>
<div class="mono" style="margin-top:8px">在三维环境中进行具身问答（EQA）通常需要收集分布在多个视角且部分遮挡的上下文信息。然而，大多数最近的视觉-语言模型（VLMs）受限于固定的、有限的输入视角，这限制了它们在推理时获取与问题相关上下文的能力，并阻碍了复杂的空间推理。我们提出了一种名为Chain-of-View（CoV）的提示方法，这是一种无需训练、在测试时进行推理的框架，通过粗到细的探索过程将VLM转化为主动视角推理器。CoV首先使用视角选择代理过滤冗余帧并识别与问题对齐的关键视角，然后通过交替进行迭代推理和离散相机操作，进行精细的视角调整，从底层三维场景表示中获取新的观察结果，直到收集到足够的上下文信息或达到步骤预算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Embodied question answering in 3D environments often requires gathering context from multiple viewpoints, but existing vision-language models are limited by fixed input views, hindering spatial reasoning. To address this, the paper introduces Chain-of-View (CoV) prompting, a test-time reasoning framework that enables models to actively explore viewpoints by first selecting relevant ones and then adjusting views iteratively. Experimental results on OpenEQA show that CoV improves LLM-Match by an average of 11.56%, with up to 13.62% gain on Qwen3-VL-Flash, and demonstrates test-time scaling with additional improvements when increasing the action budget. The method is model-agnostic and does not require training, making it a promising approach for enhancing spatial reasoning in 3D EQA tasks.</div>
<div class="mono" style="margin-top:8px">该论文针对具身问答（EQA）中需要从多个视角获取分布且部分遮挡的上下文信息的挑战，提出了一种无需训练的推理框架Chain-of-View（CoV）。CoV通过粗到细的视角探索过程，使视觉语言模型（VLM）能够在推理时动态选择和调整视角。首先，使用视角选择代理过滤冗余帧并识别与问题对齐的关键视角，随后通过迭代推理与离散相机动作相结合的方式，从3D场景表示中获取新观察，直至收集到足够上下文或达到动作预算。在OpenEQA数据集上，CoV在四个主流VLM上平均提升了11.56%的LLM-Match指标，其中Qwen3-VL-Flash最高提升达13.62%。该方法还表现出推理时的扩展性，随着动作预算的增加，平均提升进一步达到2.51%，在Gemini-2.5-Flash上达到3.73%的峰值。在ScanQA和SQA3D数据集上，CoV也展现出优异的性能，表明其作为模型无关策略在提升3D EQA中的空间推理能力方面具有显著效果。</div>
</details>
</div>
<div class="card">
<div class="title">Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection</div>
<div class="meta-line">Authors: Feihu Jin, Ying Tan</div>
<div class="meta-line">First: 2026-01-09T03:20:54+00:00 · Latest: 2026-01-09T03:20:54+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05501v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05501v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of &quot;beneficial stochasticity&quot; to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Hi-ZFO：通过重要性引导张量选择的分层零阶和一阶LLM微调</div>
<div class="mono" style="margin-top:8px">使用标准一阶（FO）优化方法对大型语言模型（LLMs）进行微调通常会引导训练趋向于尖锐且泛化能力差的极小值点。相反，零阶（ZO）方法在不依赖显式梯度的情况下提供了更强的探索能力，但收敛速度较慢。更关键的是，我们的分析表明，在生成任务中，庞大的输出和搜索空间显著放大了估计方差，使得ZO方法既嘈杂又低效。为了解决这些挑战，我们提出了\textbf{Hi-ZFO}（\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization），一个混合框架，旨在结合一阶梯度的精确性和零阶估计的探索能力。Hi-ZFO通过逐层重要性分析对模型进行自适应划分，对关键层应用精确的一阶更新，而对不敏感层则利用零阶优化。值得注意的是，Hi-ZFO中的零阶方法并非仅仅是节省内存的替代方案；它被有意引入作为&quot;有益的随机性&quot;来源，以帮助模型逃离纯一阶优化容易停滞的局部极小值点。在多种生成、数学和代码推理任务中验证，Hi-ZFO始终能够实现优越的性能，同时显著减少训练时间。这些结果证明了分层混合优化在LLM微调中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind Hi-ZFO is to overcome the limitations of standard first-order optimization in LLM fine-tuning, which often leads to sharp, poorly generalizing minima, and the inefficiency of zeroth-order methods due to high estimation variance in generative tasks. Hi-ZFO introduces a hybrid framework that combines first-order gradient updates with zeroth-order optimization, using layer-wise importance profiling to adaptively allocate computational resources. The method applies precise first-order updates to critical layers and utilizes zeroth-order optimization for less sensitive ones, introducing beneficial stochasticity to avoid local minima. Experimental results across various tasks show that Hi-ZFO achieves better performance with significantly reduced training time compared to pure first-order or zeroth-order approaches.</div>
<div class="mono" style="margin-top:8px">Hi-ZFO的研究动机源于标准一阶优化在微调大语言模型时容易导致尖锐且泛化能力差的极小值点，而零阶方法在生成任务中由于估计方差高而效率低下。Hi-ZFO提出了一种混合框架，结合了一阶和零阶优化方法，通过层间重要性分析对模型进行分层处理，对关键层应用一阶优化，对不敏感层使用零阶优化。该方法有意将零阶优化作为有益的随机性来源，帮助模型逃离纯一阶优化容易陷入的局部极小值，从而在多种任务中实现了更优的性能和更短的训练时间。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
