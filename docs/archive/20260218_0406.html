<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-18 04:06</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260218_0406</div>
    <div class="row"><div class="card">
<div class="title">Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery</div>
<div class="meta-line">Authors: Chandrakanth Gudavalli, Tajuddin Manhar Mohammed, Abhay Yadav, Ananth Vishnu Bhaskar, Hardik Prajapati, Cheng Peng, Rama Chellappa, Shivkumar Chandrasekaran, B. S. Manjunath</div>
<div class="meta-line">First: 2026-02-16T17:06:54+00:00 · Latest: 2026-02-16T17:06:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14929v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14929v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Wrivinder：面向将地面图像定位到卫星图像上的空间智能</div>
<div class="mono" style="margin-top:8px">将地面图像与地理注册的卫星地图对齐对于制图、导航和态势感知至关重要，但在视角差异较大或GPS不可靠的情况下仍具有挑战性。我们引入了Wrivinder，这是一个零样本、几何驱动的框架，通过聚合多张地面照片来重建一致的3D场景，并将其与上方的卫星图像对齐。Wrivinder结合了SfM重建、3D高斯点绘制、语义定位以及基于单目深度的度量线索，生成一个稳定的顶视渲染，可以直接与卫星上下文匹配，实现度量准确的相机地理定位。为了支持该任务的系统性评估（该任务缺乏合适的基准数据集），我们还发布了MC-Sat，这是一个经过整理的数据集，连接了多视角地面图像与地理注册的卫星瓦片，涵盖多种户外环境。Wrivinder和MC-Sat共同为研究无需配对监督的几何中心跨视角对齐提供了首个全面的基准和测试平台。在零样本实验中，Wrivinder在密集和大范围场景中均实现了低于30米的地理定位精度，突显了基于几何的聚合方法在稳健地面到卫星定位中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the challenge of aligning ground-level images with satellite maps in scenarios with large viewpoint differences or unreliable GPS. Wrivinder, a zero-shot framework, leverages geometry-driven methods such as SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth estimation to create a consistent 3D scene from multiple ground photographs, enabling accurate camera geo-localization on satellite imagery. The main experimental results show that Wrivinder achieves sub-30 meter geolocation accuracy in both dense and large-area scenes, demonstrating the effectiveness of geometry-based aggregation for cross-view alignment without paired supervision.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在大视角差异或GPS不可靠的情况下，将地面图像与卫星地图对齐的挑战。提出了一种名为Wrivinder的零样本、几何驱动框架，通过多张地面照片重建一致的3D场景，并将其与高空卫星图像对齐。该框架结合了SfM重建、3D高斯点云、语义定位和单目深度估计等方法，生成稳定的顶视渲染图像，用于精确的相机地理定位。为支持该任务的系统性评估，研究者还发布了MC-Sat数据集，该数据集链接了多视角地面图像与地理注册的卫星瓦片。实验结果显示，Wrivinder在密集和大面积场景中均实现了低于30米的地理定位精度，突显了基于几何的聚合方法在地面到卫星定位中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Haichao Zhu, Zhaorui Yang, Qian Zhang</div>
<div class="meta-line">First: 2026-02-16T02:26:59+00:00 · Latest: 2026-02-16T02:26:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14409v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14409v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习提出，几何处理：一种高效的模块化空间推理框架</div>
<div class="mono" style="margin-top:8px">空间感知旨在从视觉观测中估计相机运动和场景结构，这一问题传统上通过几何建模和物理一致性约束来解决。近年来，基于学习的方法在几何感知中展现出强大的表示能力，并在实践中越来越多地用于增强以几何为中心的经典系统。然而，学习组件是否应直接替代几何估计，还是应作为此类流程中的中间模块，仍是一个开放性问题。
  在本工作中，我们针对这一问题，研究了一种端到端的模块化框架，用于有效的空间推理，其中学习部分提出几何假设，而几何算法则处理估计决策。具体而言，我们在RGB-D序列的相对相机姿态估计背景下研究了这一原则。使用VGGT作为代表性学习模型，我们在不同运动幅度和场景动态下评估了基于学习的姿态和深度提案，并随后采用经典点对平面RGB-D ICP作为几何后端。我们在TUM RGB-D基准上的实验揭示了三个一致的发现：(1) 仅依靠基于学习的姿态提案是不可靠的；(2) 当学习提出的几何未正确对齐相机内参时，性能会下降；(3) 当学习提出的深度在几何上对齐并随后经过几何处理阶段时，在中等挑战性的刚性场景中会出现一致的性能提升。
  这些结果表明，几何不仅仅是优化组件，而是验证和吸收基于学习的几何观测的关键仲裁者。我们的研究强调了模块化、几何感知系统设计在实现稳健空间感知中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of learning-based components in spatial perception systems, specifically focusing on relative camera pose estimation using RGB-D sequences. The study proposes a modular framework where learning models generate geometric hypotheses, while classical geometric algorithms are responsible for refining and validating these proposals. Experimental results on the TUM RGB-D benchmark show that learning-based pose proposals alone are unreliable, improper alignment of learned geometry with camera intrinsics can reduce performance, but when properly aligned and followed by a geometric disposal stage, consistent improvements are observed in rigid environments. These findings emphasize the necessity of integrating geometric validation into learning-based pipelines for robust spatial reasoning.</div>
<div class="mono" style="margin-top:8px">本文探讨了学习组件在空间推理系统中的作用，特别是针对RGB-D序列中的相对相机位姿估计问题。研究提出了一种模块化框架，其中学习模型生成几何假设，而经典几何算法负责验证和优化这些估计。在TUM RGB-D基准测试中，实验结果表明仅依靠学习模型生成的位姿估计是不可靠的，若学习得到的几何信息与相机内参对齐不当，可能降低性能，但在几何对齐后结合几何优化阶段，可以在中等挑战性的刚性场景中实现一致的性能提升。这些结果表明几何验证对于可靠的空间感知至关重要，学习与几何应作为互补模块进行整合，而非相互替代。</div>
</details>
</div>
<div class="card">
<div class="title">AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines</div>
<div class="meta-line">Authors: Yifan Wu, Yiran Peng, Yiyu Chen, Jianhao Ruan, Zijie Zhuang, Cheng Yang, Jiayi Zhang, Man Chen, Yenchi Tseng, Zhaoyang Yu, Liang Chen, Yuyao Zhai, Bang Liu, Chenglin Wu, Yuyu Luo</div>
<div class="meta-line">First: 2026-02-15T20:03:19+00:00 · Latest: 2026-02-15T20:03:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14296v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoWebWorld: 通过有限状态机合成无限可验证的Web环境</div>
<div class="mono" style="margin-top:8px">自主Web GUI代理的性能严重依赖于其训练数据的质量和数量。然而，一个根本性的瓶颈依然存在：从真实网站中收集交互轨迹既昂贵又难以验证。底层状态转换是隐藏的，导致需要依赖不一致且成本高昂的外部验证器来评估每一步的正确性。为了解决这一问题，我们提出了AutoWebWorld，一个通过将Web环境建模为有限状态机（FSMs）来合成可控且可验证的Web环境的新框架，并利用编码代理将FSMs转换为可交互的网站。与真实网站中隐式的状态转换不同，AutoWebWorld明确定义了所有状态、动作和转换规则。这使得可以进行程序化验证：动作的正确性通过预定义的规则进行检查，任务成功则通过在FSM图中达到目标状态来确认。AutoWebWorld实现了完全自动化的搜索与验证流程，仅以每轨迹0.04美元的成本，从29个多样化的Web环境中生成超过11,663条验证轨迹。在这些合成数据上进行训练显著提升了真实环境中的性能。我们的7B参数Web GUI代理在WebVoyager上仅用15步就超越了所有基线。此外，我们观察到一个清晰的扩展定律：随着合成数据量的增加，AutoWebWorld在WebVoyager和Online-Mind2Web上的性能持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to overcome the challenges of collecting and verifying interaction trajectories for training autonomous Web GUI agents, which is both costly and error-prone in real-world settings. AutoWebWorld introduces a framework that models web environments as Finite State Machines (FSMs), allowing for explicit definition of states, actions, and transitions. This enables programmatic verification of agent actions and task completion. The framework generates a large number of verified trajectories efficiently, with over 11,663 trajectories produced from 29 diverse web environments at a low cost. Training on this synthetic data leads to improved real-world performance, as demonstrated by the 7B Web GUI agent outperforming baselines on WebVoyager within 15 steps, and a clear scaling law showing performance improvements with increased synthetic data volume.</div>
<div class="mono" style="margin-top:8px">自主Web GUI代理的性能受到真实世界交互数据收集和验证难度与成本的限制。AutoWebWorld通过使用有限状态机（FSM）合成可控且可验证的Web环境，解决了这一问题，其中所有状态、动作和转移规则都被显式定义。这使得代理动作和任务完成可以通过预定义规则进行程序化验证。该框架从29个多样化的Web环境中生成超过11,663条验证过的交互轨迹，成本低廉。在这些合成数据上训练的Web GUI代理显著提升了真实场景下的表现，其中7B参数的代理在WebVoyager上15步内优于所有基线模型，且性能随合成数据量的增加而提升。</div>
</details>
</div>
<div class="card">
<div class="title">KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Kris Shengjun Dong, Sahil Modi, Dima Nikiforov, Sana Damani, Edward Lin, Siva Kumar Sastry Hari, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-15T19:48:43+00:00 · Latest: 2026-02-15T19:48:43+00:00</div>
<div class="meta-line">Comments: 15 pages, 33 pages with appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KernelBlaster: 通过记忆增强的上下文强化学习实现持续跨任务CUDA优化</div>
<div class="mono" style="margin-top:8px">在多个GPU架构世代上优化CUDA代码具有挑战性，因为实现最佳性能需要在日益复杂且特定于硬件的优化空间中进行广泛探索。传统编译器受限于固定的启发式方法，而对大型语言模型（LLMs）进行微调则成本较高。然而，CUDA代码优化的代理工作流在整合先前探索的知识方面能力有限，导致采样偏差和次优解。我们提出了KernelBlaster，这是一个基于记忆增强的上下文强化学习（MAIC-RL）框架，旨在提升基于LLM的GPU编码代理的CUDA优化搜索能力。KernelBlaster通过将知识积累到一个可检索的持久CUDA知识库中，使代理能够从经验中学习，并在未来的任务中做出系统性的决策。我们提出了一种新颖的基于配置文件引导和文本梯度的代理流程，用于CUDA生成和优化，以在不同世代的GPU架构上实现高性能。KernelBlaster引导LLM代理系统地探索超越简单重写的高潜力优化策略。与PyTorch基线相比，我们的方法在KernelBench Level 1、2、3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。我们发布了KernelBlaster作为一个开源代理框架，附带测试框架、验证组件和可复现的评估流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of KernelBlaster is to address the challenges of optimizing CUDA code across different GPU architectures by improving the search capabilities of LLM-based coding agents. The framework employs a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) approach, enabling agents to accumulate and retrieve optimization knowledge from a Persistent CUDA Knowledge Base. This allows for more systematic and informed decision-making in future tasks. Experimental results show that KernelBlaster achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively, compared to the PyTorch baseline.</div>
<div class="mono" style="margin-top:8px">KernelBlaster的研究动机是解决在不同GPU架构上优化CUDA代码的挑战，传统编译器和LLM微调方法在此方面存在局限。该方法提出了一种基于记忆增强的上下文强化学习（MAIC-RL）框架，使LLM代理能够积累并检索过去的优化经验，从而提升未来任务的决策能力。KernelBlaster采用了一种基于性能分析和文本梯度的代理流程，系统性地探索高潜力优化策略，超越简单的代码重写。实验结果显示，与PyTorch基线相比，KernelBlaster在KernelBench Levels 1、2和3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。</div>
</details>
</div>
<div class="card">
<div class="title">SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement</div>
<div class="meta-line">Authors: Xiaojun Jia, Jie Liao, Simeng Qin, Jindong Gu, Wenqi Ren, Xiaochun Cao, Yang Liu, Philip Torr</div>
<div class="meta-line">First: 2026-02-15T16:09:48+00:00 · Latest: 2026-02-15T16:09:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14211v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SkillJect：面向编码代理的基于技能的隐蔽提示注入自动化框架</div>
<div class="mono" style="margin-top:8px">代理技能正逐渐成为编码代理的核心抽象，通过封装长文本指令和辅助脚本来扩展工具增强的行为。这种抽象引入了一个未被充分衡量的攻击面：基于技能的提示注入，其中被污染的技能可能引导代理偏离用户的意图和安全策略。在实践中，简单的注入往往失败，因为恶意意图过于明显或偏离原始技能太远，导致代理忽略或拒绝这些注入；现有的攻击方法也大多是手工构建的。我们提出了首个针对代理技能的隐蔽提示注入自动化框架。该框架由三个代理组成：一个攻击代理，在显式的隐蔽约束下合成注入技能；一个代码代理，在现实的工具环境中使用注入的技能执行任务；一个评估代理，记录操作轨迹（如工具调用和文件操作），并验证是否发生了目标恶意行为。我们还提出了一种恶意负载隐藏策略，通过在辅助脚本中隐藏对抗性操作，同时注入优化的诱导提示以触发工具执行。在多种编码代理设置和现实软件工程任务上的广泛实验表明，我们的方法在现实场景下能够持续实现高攻击成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the security vulnerabilities introduced by the increasing use of skill-based abstractions in coding agents, particularly the risk of prompt injection attacks that can manipulate agent behavior. The proposed method, SkillJect, introduces an automated framework for stealthy prompt injection, utilizing a closed-loop system with three agents: an Attack Agent that generates injection skills under stealth constraints, a Code Agent that executes tasks in a realistic environment, and an Evaluate Agent that monitors and verifies malicious behaviors. Experimental results across various coding-agent scenarios and real-world software engineering tasks demonstrate that the framework achieves high attack success rates while maintaining stealth.</div>
<div class="mono" style="margin-top:8px">随着编码代理中技能抽象的广泛应用，新的漏洞也随之产生：基于技能的提示注入，恶意技能可能引导代理偏离其预期行为。为了解决这一问题，作者提出了SkillJect，一个专门用于自动化隐蔽提示注入的框架，通过包含三个代理的闭环系统实现：攻击代理在隐蔽约束下合成注入技能，代码代理在真实环境中执行任务，评估代理通过操作轨迹验证是否发生恶意行为。该框架还引入了一种恶意负载隐藏策略，将对抗性操作隐藏在辅助脚本中，并注入优化的诱导提示以触发工具执行。在多种编码代理场景和现实软件工程任务中的广泛实验表明，SkillJect在现实环境中实现了高攻击成功率。</div>
</details>
</div>
<div class="card">
<div class="title">EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming</div>
<div class="meta-line">Authors: Sen Fang, Weiyuan Ding, Mengshi Zhang, Zihao Chen, Bowen Xu</div>
<div class="meta-line">First: 2025-05-18T01:02:33+00:00 · Latest: 2026-02-15T05:28:25+00:00</div>
<div class="meta-line">Comments: 27 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12185v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.12185v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI&#x27;s o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVALOOOP：一种以自一致性为中心的框架，用于评估大型语言模型在编程中的鲁棒性</div>
<div class="mono" style="margin-top:8px">评估大型语言模型（LLMs）在编程中的鲁棒性对于确保其在基于人工智能的软件开发中的可靠性至关重要。然而，对抗性攻击存在根本性局限，影响了公平的鲁棒性评估：它们在不同攻击策略下会产生矛盾的评估结果，并且更关键的是，它们仅通过外部扰动进行操作，无法捕捉到自主编码代理所需的内在稳定性，因为后续输入是由模型自身生成的。我们引入了EVALOOOP，这是一种新颖的评估框架，从自一致性角度评估鲁棒性，利用软件工程任务中固有的自然二元性（例如代码生成和代码摘要）。EVALOOOP建立了一个自包含的反馈循环，其中LLM在代码和自然语言之间反复转换，直至功能失效，鲁棒性通过一种新的平均可持续循环（ASL）指标进行量化——即在基准任务中保持功能正确性的平均迭代次数。这种循环策略无需依赖外部攻击配置，内在地评估鲁棒性，提供了一个统一的指标，揭示了LLM在持续自指转换中如何有效保持语义完整性。我们在EVALOOOP中使用MBPP Plus基准评估了96个流行的LLM，参数范围从0.5B到685B，发现EVALOOOP通常在十次循环内导致pass@1准确率绝对下降2.65%-47.62%。有趣的是，鲁棒性并不总是与初始性能（即单次查询）一致；例如，Qwen3-235B-A22B-Instruct-2507，尽管在初始代码生成方面不如OpenAI的o系列模型和DeepSeek-V3，但表现出更优的鲁棒性（ASL得分）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of adversarial attacks in assessing the robustness of large language models (LLMs) in programming tasks, as these attacks fail to capture intrinsic model stability. EVALOOOP is introduced as a self-consistency-centered framework that evaluates robustness through a feedback loop where an LLM iteratively transforms code and natural language until functional failure. The framework uses a novel metric, Average Sustainable Loops (ASL), to quantify robustness by measuring the mean number of iterations maintaining correctness. Experimental results on 96 LLMs with the MBPP Plus benchmark show that EVALOOOP typically leads to a 2.65%-47.62% drop in pass@1 accuracy within ten loops, and robustness is not always correlated with initial performance, as demonstrated by Qwen3-235B-A22B-Instruct-2507 showing superior robustness despite lower initial code generation performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有对抗攻击在评估大型语言模型（LLMs）编程鲁棒性方面的局限性，因为这些攻击无法捕捉自主编码代理所需的内在稳定性。EVALOOOP 是一种以自洽性为中心的评估框架，通过模型在代码和自然语言之间迭代转换直至功能失效的反馈循环来评估 LLM 的鲁棒性。该框架引入了新的指标 Average Sustainable Loops（ASL），用于量化模型在保持功能正确性下的平均迭代次数。在 MBPP Plus 基准上对 96 个 LLM 进行的实验表明，EVALOOOP 通常会导致 pass@1 准确率下降 2.65%-47.62%。有趣的是，鲁棒性并不总是与初始性能相关，例如 Qwen3-235B-A22B-Instruct-2507 虽然在初始代码生成上不如 OpenAI 的 o 系列模型和 DeepSeek-V3，但其 ASL 得分却更优。</div>
</details>
</div>
<div class="card">
<div class="title">From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design</div>
<div class="meta-line">Authors: Sha Li, Stefano Petrangeli, Yu Shen, Xiang Chen</div>
<div class="meta-line">First: 2026-02-14T22:31:49+00:00 · Latest: 2026-02-14T22:31:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13912v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13912v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs&#x27; limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从像素到政策：为内容感知布局设计增强语言模型的空间推理</div>
<div class="mono" style="margin-top:8px">我们引入了LaySPA，这是一个强化学习框架，为大型语言模型（LLMs）赋予明确且可解释的空间推理能力，以实现内容感知的图形布局设计。LaySPA解决了两个关键挑战：LLMs在空间推理方面的局限性以及设计决策过程中的不透明性。我们不以像素级操作为基础，而是将布局设计重新表述为一个在结构化文本空间环境上的策略学习问题，该环境明确编码了画布几何、元素属性和元素间关系。LaySPA生成包含可解释推理轨迹和结构化布局规范的双层级输出，从而实现透明且可控的设计决策。布局设计策略通过多目标空间批评进行优化，将布局质量分解为几何有效性、关系一致性与审美一致性，并采用相对群体优化进行训练，以稳定开放设计空间中的学习过程。实验表明，LaySPA在结构有效性与视觉质量方面均有提升，优于更大的专有LLMs，并在使用更少标注样本和更低延迟的情况下，其性能与专门的SOTA布局生成器相当。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces LaySPA, a reinforcement learning framework designed to enhance the spatial reasoning capabilities of large language models (LLMs) for content-aware graphic layout design. The motivation stems from the limitations of LLMs in understanding spatial relationships and the opacity of design decisions. LaySPA reformulates layout design as a policy learning problem in a structured textual spatial environment, encoding canvas geometry, element attributes, and inter-element relationships. The framework produces interpretable reasoning traces and structured layout specifications, enabling transparent design decisions. It optimizes layout policies using a multi-objective spatial critique that evaluates geometric validity, relational coherence, and aesthetic consistency, and employs relative group optimization for stable training. Experimental results show that LaySPA improves structural validity and visual quality, surpassing larger proprietary LLMs and matching the performance of specialized state-of-the-art layout generators with fewer annotated samples and lower latency.</div>
<div class="mono" style="margin-top:8px">本文提出LaySPA，一种强化学习框架，旨在提升大型语言模型（LLMs）在内容感知图形布局设计中的空间推理能力。研究动机源于LLMs在空间推理方面的局限性以及设计决策过程的不透明性。LaySPA将布局设计问题转化为结构化文本空间环境中的策略学习任务，显式编码画布几何、元素属性及元素间关系。该框架生成双层级输出，包括可解释的推理轨迹和结构化的布局规范，以实现透明可控的设计决策。通过多目标空间批评机制，将布局质量分解为几何有效性、关系一致性与审美一致性，并采用相对群体优化方法稳定训练过程。实验结果表明，LaySPA在结构有效性和视觉质量方面表现优异，超越了更大规模的专有LLMs，并在标注样本更少、延迟更低的情况下达到与专用顶级布局生成器相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Xiaoyu Li, Yitao Wu, Xian Wu, Haolin Zhuo, Lijun Zhao, Lining Sun</div>
<div class="meta-line">First: 2026-02-14T13:34:21+00:00 · Latest: 2026-02-14T13:34:21+00:00</div>
<div class="meta-line">Comments: Based on this work, we achieved 1st place on the KITTI tracking leaderboard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13772v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13772v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Offline-Poly：面向离线三维多目标跟踪的多面体框架</div>
<div class="mono" style="margin-top:8px">离线三维多目标跟踪（MOT）是4D自动标注（4DAL）流程中的关键组成部分。它通过引入时间上下文来增强高性能检测器生成的伪标签。然而，现有的离线三维MOT方法通常是在线框架的直接扩展，未能充分利用离线设置的优势。此外，这些方法往往依赖于固定的上游检测器和定制化架构，限制了其适应性。为了解决这些问题，我们提出了Offline-Poly，一种基于以跟踪为中心设计的通用离线三维MOT方法。我们引入了一种标准化范式，称为Tracking-by-Tracking（TBT），该范式仅基于任意现成的跟踪输出，并生成离线优化的轨迹片段。这种设计将离线跟踪器与特定的上游检测器或跟踪器解耦。在TBT范式下，Offline-Poly接受一个或多个粗略的跟踪结果，并通过包含预处理、分层匹配与融合以及轨迹片段优化的结构化流程进行处理。每个模块都旨在利用离线跟踪的两个基本特性：资源不受限性，允许在实时限制之外进行全局优化；以及未来可观测性，使得可以在完整的时间范围内进行轨迹推理。Offline-Poly首先利用全局场景上下文消除短期鬼轨迹片段，并通过重新识别碎片化片段进行轨迹重建。然后，它通过构建场景级相似性，将多个输入源的轨迹片段进行关联。最后，Offline-Poly通过联合利用局部和全局运动模式对轨迹片段进行优化。在nuScenes数据集上，我们实现了77.6%的AMOTA最优性能；在KITTI数据集上，取得了83.00%的HOTA领先结果。全面的实验进一步验证了Offline-Poly的灵活性、通用性和模块化有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Offline-Poly is proposed to address the limitations of existing offline 3D multi-object tracking methods, which are often direct extensions of online frameworks and lack adaptability. The method introduces a tracking-centric design called Tracking-by-Tracking (TBT), which operates on arbitrary off-the-shelf tracking outputs to generate refined tracklets. Offline-Poly utilizes the advantages of the offline setting, such as resource unconstrainedness and future observability, through a structured pipeline that includes pre-processing, hierarchical matching and fusion, and tracklet refinement. It achieves state-of-the-art performance on nuScenes with 77.6% AMOTA and leading results on KITTI with 83.00% HOTA, demonstrating its flexibility and effectiveness.</div>
<div class="mono" style="margin-top:8px">Offline-Poly 是为了解决现有离线 3D 多目标跟踪方法的局限性而提出的，这些方法通常直接扩展自在线框架，缺乏适应性。该方法引入了一种以跟踪为中心的框架，称为 Tracking-by-Tracking (TBT)，它仅依赖于任意现成的跟踪输出，生成经过离线优化的轨迹片段，而不依赖特定的检测器或跟踪器。Offline-Poly 采用包含预处理、分层匹配与融合以及轨迹片段优化的结构化流程，充分利用离线跟踪的两个基本特性：资源不受限的全局优化和未来可观测性。在 nuScenes 和 KITTI 数据集上的实验结果表明，Offline-Poly 分别达到了 77.6% 的 AMOTA 和 83.00% 的 HOTA，证明了其灵活性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird&#x27;s-Eye View with Deformable Attention and Sparse Goal Proposals</div>
<div class="meta-line">Authors: Minsang Kong, Myeongjun Kim, Sang Gu Kang, Hejiu Lu, Yupeng Zhong, Sang Hun Lee</div>
<div class="meta-line">First: 2025-09-12T09:17:54+00:00 · Latest: 2026-02-14T08:37:57+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Intelligent Transportation Systems (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10080v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10080v2">PDF</a> · <a href="https://github.com/Kongminsang/bevtraj">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In autonomous driving, trajectory prediction is essential for safe and efficient navigation. While recent methods often rely on high-definition (HD) maps to provide structured environmental priors, such maps are costly to maintain, geographically limited, and unreliable in dynamic or unmapped scenarios. Directly leveraging raw sensor data in Bird&#x27;s-Eye View (BEV) space offers greater flexibility, but BEV features are dense and unstructured, making agent-centric spatial reasoning challenging and computationally inefficient. To address this, we propose Bird&#x27;s-Eye View Trajectory Prediction (BEVTraj), a map-free framework that employs deformable attention to adaptively aggregate task-relevant context from sparse locations in dense BEV features. We further introduce a Sparse Goal Candidate Proposal (SGCP) module that predicts a small set of realistic goals, enabling fully end-to-end multimodal forecasting without heuristic post-processing. Extensive experiments show that BEVTraj achieves performance comparable to state-of-the-art HD map-based methods while providing greater robustness and flexibility without relying on pre-built maps. The source code is available at https://github.com/Kongminsang/bevtraj.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BEVTraj: 无需地图的鸟瞰视角端到端轨迹预测方法，结合可变形注意力与稀疏目标提案</div>
<div class="mono" style="margin-top:8px">在自动驾驶中，轨迹预测对于安全和高效导航至关重要。尽管近期方法常依赖高精度（HD）地图来提供结构化的环境先验，但此类地图维护成本高、地理范围有限，并且在动态或未映射场景中不可靠。直接利用鸟瞰视角（BEV）空间中的原始传感器数据提供了更大的灵活性，但BEV特征密集且无结构，使得以代理为中心的空间推理具有挑战性且计算效率低下。为了解决这一问题，我们提出了鸟瞰视角轨迹预测（BEVTraj），这是一种无需地图的框架，通过可变形注意力机制从密集的BEV特征中自适应地聚合任务相关的上下文信息。我们进一步引入了稀疏目标候选提案（SGCP）模块，用于预测一组现实的目标，从而实现完全端到端的多模态预测，无需启发式后处理。大量实验表明，BEVTraj在性能上与基于HD地图的最先进方法相当，同时在不依赖预构建地图的情况下提供了更高的鲁棒性和灵活性。源代码可在 https://github.com/Kongminsang/bevtraj 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trajectory prediction is crucial for autonomous driving systems to navigate safely and efficiently. Traditional approaches often depend on high-definition maps, which are expensive, limited in scope, and unreliable in dynamic or unmapped environments. BEVTraj introduces a map-free framework that uses deformable attention to selectively gather relevant context from dense Bird&#x27;s-Eye View features, and incorporates a Sparse Goal Candidate Proposal module to generate realistic future goals. This enables end-to-end trajectory forecasting without heuristic post-processing. Experimental results demonstrate that BEVTraj matches the performance of HD map-based methods while offering better robustness and adaptability in diverse scenarios.</div>
<div class="mono" style="margin-top:8px">轨迹预测对于自动驾驶系统的安全高效导航至关重要。传统方法通常依赖高精度地图，但这些地图成本高、地理范围有限且在动态或未标注环境中不可靠。BEVTraj提出了一种无需地图的框架，利用可变形注意力机制从密集的鸟瞰图特征中选择性地聚合任务相关的上下文信息，从而实现更灵活高效的空间推理。同时，该框架引入了稀疏目标候选提案模块，生成少量现实的目标，支持端到端的多模态预测。实验结果表明，BEVTraj在无需地图的情况下，其性能与基于高精度地图的最先进方法相当，且具备更强的鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories</div>
<div class="meta-line">Authors: Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen</div>
<div class="meta-line">First: 2025-04-29T22:22:44+00:00 · Latest: 2026-02-14T01:32:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21205v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.21205v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 29 standalone LLMs and 15 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SecRepoBench：在现实仓库中评估代码代理的安全代码补全基准</div>
<div class="mono" style="margin-top:8px">本文介绍了SecRepoBench，这是一个用于评估代码代理在现实仓库中进行安全代码补全的基准。SecRepoBench包含27个C/C++仓库中的318个代码补全任务，涵盖15种CWE漏洞。我们使用该基准评估了29个独立的LLMs和15个代码代理，覆盖了三种最先进的代理框架。我们发现，最先进的LLMs在生成正确且安全的代码补全方面存在困难。然而，代码代理显著优于独立LLMs。我们展示了SecRepoBench比之前的最先进的基准更具挑战性。最后，我们的全面分析为提升代码代理在现实仓库中编写正确且安全代码的能力提供了潜在方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents SecRepoBench, a benchmark designed to assess code agents&#x27; performance in secure code completion within real-world repositories. The benchmark includes 318 tasks across 27 C/C++ repositories, targeting 15 common CWE vulnerabilities. Evaluating 29 standalone LLMs and 15 code agents using three state-of-the-art agent frameworks, the study reveals that while advanced LLMs struggle with generating secure and correct code, code agents demonstrate significantly better performance. The results indicate that SecRepoBench is more challenging than existing benchmarks, highlighting the need for improved code agent capabilities in real-world software development.</div>
<div class="mono" style="margin-top:8px">本文提出了SecRepoBench，这是一个用于评估代码代理在真实仓库中安全代码补全能力的基准测试。该基准包含27个C/C++仓库中的318个代码补全任务，涵盖15种常见的软件漏洞。研究评估了29个独立的大型语言模型（LLMs）和15个代码代理，使用三种最先进的代理框架。结果表明，尽管先进的LLMs在生成安全代码方面表现不佳，但代码代理显著优于独立模型。该基准被证明比之前的基准更具挑战性，为提升代码代理在实际环境中的安全代码生成能力提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Spear-Phishing Website Generation in Large Language Model Coding Agents</div>
<div class="meta-line">Authors: Tailia Malloy, Tegawende F. Bissyande</div>
<div class="meta-line">First: 2026-02-13T12:12:53+00:00 · Latest: 2026-02-13T12:12:53+00:00</div>
<div class="meta-line">Comments: 18 Pages, 7 Figures, 1 Table. Accepted to the conference Human Computer Interaction International</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are expanding beyond being a tool humans use and into independent agents that can observe an environment, reason about solutions to problems, make changes that impact those environments, and understand how their actions impacted their environment. One of the most common applications of these LLM Agents is in computer programming, where agents can successfully work alongside humans to generate code while controlling programming environments or networking systems. However, with the increasing ability and complexity of these agents comes dangers about the potential for their misuse. A concerning application of LLM agents is in the domain cybersecurity, where they have the potential to greatly expand the threat imposed by attacks such as social engineering. This is due to the fact that LLM Agents can work autonomously and perform many tasks that would normally require time and effort from skilled human programmers. While this threat is concerning, little attention has been given to assessments of the capabilities of LLM coding agents in generating code for social engineering attacks. In this work we compare different LLMs in their ability and willingness to produce potentially dangerous code bases that could be misused by cyberattackers. The result is a dataset of 200 website code bases and logs from 40 different LLM coding agents. Analysis of models shows which metrics of LLMs are more and less correlated with performance in generating spear-phishing sites. Our analysis and the dataset we present will be of interest to researchers and practitioners concerned in defending against the potential misuse of LLMs in spear-phishing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型编码代理在钓鱼网站生成中的能力</div>
<div class="mono" style="margin-top:8px">大型语言模型正在从人类使用的工具扩展为能够观察环境、推理解决问题、对环境产生影响并理解自身行为影响的独立代理。这些LLM代理最常见的应用之一是计算机编程，它们可以与人类协作生成代码，同时控制编程环境或网络系统。然而，随着这些代理的能力和复杂性不断增加，其被滥用的潜在风险也随之而来。在网络安全领域，LLM代理的一个令人担忧的应用是它们可能大大扩展社会工程攻击的威胁。这是因为LLM代理可以自主工作，并执行通常需要熟练人类程序员投入时间和精力的任务。尽管这种威胁令人担忧，但对LLM编码代理在生成社会工程攻击代码方面的能力评估却很少受到关注。在本研究中，我们比较了不同LLM在生成可能被网络攻击者滥用的危险代码库方面的能力和意愿。结果是一个包含200个网站代码库和40个不同LLM编码代理日志的数据集。模型分析展示了哪些LLM指标与生成精准钓鱼网站的性能更为或更不相关。我们的分析以及所呈现的数据集将对关注如何防范LLM在钓鱼攻击中被滥用的研究人员和实践者具有参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential misuse of large language models (LLMs) in generating spear-phishing websites, a critical concern in cybersecurity. The research compares the ability and willingness of different LLMs to produce malicious code bases that could be used in social engineering attacks. By collecting and analyzing 200 website code bases and logs from 40 LLM coding agents, the authors identify which model metrics are more or less correlated with the effectiveness of spear-phishing site generation. The findings provide insights into the risks posed by LLMs in cybersecurity and offer a dataset for further research and defense strategies.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLM）编码代理在生成钓鱼网站方面的潜在滥用风险，这是网络安全领域的重要关注点。通过比较不同LLM在生成可能被攻击者利用的恶意代码库方面的能力和意愿，研究构建了一个包含200个网站代码库及40个LLM代理日志的数据集，揭示了哪些模型指标与生成精准钓鱼网站的效果更为相关。研究结果对致力于防范LLM在钓鱼攻击中被滥用的学者和实践者具有重要参考价值。</div>
</details>
</div>
<div class="card">
<div class="title">3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset</div>
<div class="meta-line">Authors: Mehran Advand, Zahra Dehghanian, Navid Faraji, Reza Barati, Seyed Amir Ahmad Safavi-Naini, Hamid R. Rabiee</div>
<div class="meta-line">First: 2026-02-13T11:08:15+00:00 · Latest: 2026-02-13T11:08:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mehrn79.github.io/3DLAND">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DLAND：腹部异常病变三维定位数据集</div>
<div class="mono" style="margin-top:8px">现有的腹部CT医学影像数据集通常缺乏三维标注、多器官覆盖或精确的病灶-器官关联，阻碍了稳健的表示学习和临床应用。为解决这一问题，我们引入了3DLAND，这是一个大规模基准数据集，包含超过6,000个增强CT体积，配有超过20,000个高保真度的三维病灶标注，涵盖七个腹部器官：肝脏、肾脏、胰腺、脾脏、胃和胆囊。我们的三阶段简化流程集成了自动化空间推理、提示优化的2D分割以及记忆引导的3D传播，经专家放射科医生验证，表面Dice评分超过0.75。通过提供多样化的病灶类型和患者人口统计信息，3DLAND支持对医学AI异常检测、定位和跨器官迁移学习的可扩展评估。我们的数据集为评估器官感知的三维分割模型设立了新的基准，为面向医疗的AI发展铺平了道路。为促进可重复性和进一步研究，3DLAND数据集及实现代码已公开在https://mehrn79.github.io/3DLAND。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing abdominal CT datasets, which often lack comprehensive 3D annotations, multi-organ coverage, and accurate lesion-to-organ associations. The 3DLAND dataset introduces a large-scale benchmark with over 6,000 contrast-enhanced CT volumes and more than 20,000 high-fidelity 3D lesion annotations across seven abdominal organs. The dataset is constructed using a three-phase pipeline that combines automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores above 0.75. This enables scalable evaluation of medical AI models for anomaly detection, localization, and cross-organ transfer learning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有腹部CT数据集在三维标注、多器官覆盖以及病变与器官关联方面的不足。作者提出了3DLAND数据集，包含超过6000例增强CT影像，覆盖肝脏、肾脏、胰腺、脾脏、胃和胆囊等七个腹部器官，共有超过20000个高保真度的三维病变标注。该数据集通过一个三阶段流程构建，结合了自动化空间推理、提示优化的2D分割以及记忆引导的3D传播，经放射科专家验证，表面Dice分数超过0.75。3DLAND为医学人工智能中的异常检测、定位和跨器官迁移学习提供了可扩展的评估平台。</div>
</details>
</div>
<div class="card">
<div class="title">Easy-Poly: An Easy Polyhedral Framework For 3D Multi-Object Tracking</div>
<div class="meta-line">Authors: Peng Zhang, Xin Li, Xin Lin, Liang He</div>
<div class="meta-line">First: 2025-02-25T04:01:25+00:00 · Latest: 2026-02-13T08:05:15+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17822v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.17822v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent 3D multi-object tracking (3D MOT) methods mainly follow tracking-by-detection pipelines, but often suffer from high false positives, missed detections, and identity switches, especially in crowded and small-object scenarios. To address these challenges, we propose Easy-Poly, a filter-based 3D MOT framework with four key innovations: (1) CNMSMM, a novel Camera-LiDAR fusion detection method combining multi-modal augmentation and an efficient NMS with a new loss function to improve small target detection; (2) Dynamic Track-Oriented (DTO) data association that robustly handles uncertainties and occlusions via class-aware optimal assignment and parallel processing strategies; (3) Dynamic Motion Modeling (DMM) using a confidence-weighted Kalman filter with adaptive noise covariance to enhance tracking accuracy; and (4) an extended life-cycle management system reducing identity switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 65.65% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 75.6%), while also running in real-time. Our framework advances robustness and adaptability in complex driving environments, paving the way for safer autonomous driving perception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Easy-Poly：一种用于3D多目标跟踪的简单多面体框架</div>
<div class="mono" style="margin-top:8px">最近的3D多目标跟踪(3D MOT)方法主要采用基于检测的跟踪流程，但在密集和小目标场景中常出现高误检率、漏检和身份切换的问题。为了解决这些挑战，我们提出了Easy-Poly，一个基于滤波的3D MOT框架，包含四项关键创新：(1) CNMSMM，一种结合多模态增强和高效NMS的新损失函数的相机-激光雷达融合检测方法，以提升小目标检测性能；(2) 动态目标导向(DTO)数据关联，通过类感知最优分配和并行处理策略，鲁棒地处理不确定性与遮挡；(3) 动态运动建模(DMM)，使用带有自适应噪声协方差的置信度加权卡尔曼滤波器以提高跟踪精度；(4) 扩展生命周期管理系统，减少身份切换和误终止。实验结果表明，Easy-Poly在mAP（如LargeKernel3D中从63.30%提升至65.65%）和AMOTA（如从73.1%提升至75.6%）方面优于Poly-MOT和Fast-Poly等先进方法，同时还能实现实时运行。我们的框架提升了复杂驾驶环境下的鲁棒性和适应性，为更安全的自动驾驶感知铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this paper is to improve the performance of 3D multi-object tracking in challenging scenarios such as crowded environments and small-object detection. The proposed Easy-Poly framework introduces four key innovations: a novel Camera-LiDAR fusion detection method with multi-modal augmentation and an efficient NMS, a class-aware data association strategy for handling uncertainties and occlusions, a confidence-weighted Kalman filter with adaptive noise covariance for motion modeling, and an extended life-cycle management system to reduce identity switches and false terminations. Experimental results demonstrate that Easy-Poly achieves significant improvements in mAP and AMOTA metrics compared to existing methods like Poly-MOT and Fast-Poly, while maintaining real-time performance.</div>
<div class="mono" style="margin-top:8px">Easy-Poly的研究动机是提升3D多目标跟踪在拥挤场景和小目标检测中的性能。该框架引入了四项关键创新：一种结合多模态增强和高效NMS的新型相机-激光雷达融合检测方法，一种基于类别的数据关联策略以处理不确定性与遮挡，一种基于置信度的卡尔曼滤波器用于动态运动建模，以及一个扩展的生命管理机制以减少身份切换和错误终止。实验结果表明，Easy-Poly在mAP和AMOTA指标上显著优于现有方法如Poly-MOT和Fast-Poly，同时保持实时运行能力。</div>
</details>
</div>
<div class="card">
<div class="title">Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models</div>
<div class="meta-line">Authors: Joshua Ong Jun Leang, Yu Zhao, Mihaela Cătălina Stoian, Wenda Li, Shay B. Cohen, Eleonora Giunchiglia</div>
<div class="meta-line">First: 2026-02-13T03:56:22+00:00 · Latest: 2026-02-13T03:56:22+00:00</div>
<div class="meta-line">Comments: 8 pages, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12586v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我可以点单吗？扩散语言模型中用于槽填充顺序的蒙特卡洛树搜索</div>
<div class="mono" style="margin-top:8px">尽管在掩码扩散模型（MDMs）中，计划-填充解码在数学和代码推理方面表现出潜力，但性能仍高度依赖于槽填充顺序，通常导致输出方差显著。我们引入了McDiffuSE框架，将槽选择建模为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。McDiffuSE利用前瞻模拟，在承诺之前评估部分完成情况，系统地探索生成顺序的组合空间。实验表明，与自回归基线相比平均提升3.2%，与基线计划-填充方法相比提升8.0%，在MBPP和MATH500上分别取得19.5%和4.9%的显著提升。我们的分析表明，虽然McDiffuSE主要遵循顺序填充，但引入非顺序生成对于最大化性能是必要的。我们观察到，为了克服模型置信度偏差并发现有效的顺序，需要更大的探索常数，而不是增加模拟次数。这些发现确立了基于MCTS的规划作为提升MDMs生成质量的有效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of slot infilling order sensitivity in Masked Diffusion Models (MDMs), which affects the consistency and quality of generated outputs, particularly in mathematical and code reasoning tasks. The authors propose McDiffuSE, a framework that uses Monte Carlo Tree Search (MCTS) to optimize the order of slot filling by treating it as a decision-making process. Through look-ahead simulations, McDiffuSE evaluates partial completions before finalizing them, enabling systematic exploration of generation orders. Experimental results demonstrate that McDiffuSE improves performance by 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill methods, with significant gains of 19.5% on MBPP and 4.9% on MATH500. The analysis shows that while sequential ordering is common, non-sequential generation is crucial for optimal performance, and larger exploration constants are more effective than more simulations in mitigating model confidence biases.</div>
<div class="mono" style="margin-top:8px">本文针对Masked Diffusion Models（MDMs）中槽填充顺序对生成质量影响较大的问题，特别是在数学和代码推理任务中。作者提出McDiffuSE框架，将槽选择视为决策过程，并通过蒙特卡洛树搜索（MCTS）优化填充顺序。该方法利用前瞻模拟，在最终确定生成顺序前评估部分完成情况，从而系统地探索生成顺序的组合空间。实验结果显示，McDiffuSE在自回归基线模型上平均提升3.2%，在基线计划-填充模型上提升8.0%，在MBPP和MATH500数据集上分别取得19.5%和4.9%的显著提升。分析表明，尽管多数情况下采用顺序填充，但引入非顺序生成对性能提升至关重要，且更大的探索常数比增加模拟次数更能克服模型置信偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation</div>
<div class="meta-line">Authors: Benyu Zhang, Qiang Zhang, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan</div>
<div class="meta-line">First: 2026-02-07T01:15:15+00:00 · Latest: 2026-02-12T21:47:09+00:00</div>
<div class="meta-line">Comments: added more results on scaling law analysis</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07298v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于原则的合成数据使大型语言模型在推荐系统中首次具备扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）代表了推荐系统的一个有前景的前沿领域，但其发展受到缺乏可预测的扩展定律的阻碍，而这些定律对于指导研究和优化资源分配至关重要。我们假设这可能是由于以往持续预训练（CPT）工作中原始用户交互数据的固有噪声、偏差和不完整性所致。本文提出了一种新颖的分层框架，用于生成高质量的合成数据，通过为LLM创建一个精心设计的、教学性的课程来规避这些问题。我们通过展示在我们的原则性合成数据上训练的标准序列模型在下游排序任务中显著优于在真实数据上训练的模型（例如SasRec在recall@100上提升130%），提供了有力且直接的证据，证明了我们课程的有效性，展示了其在学习可泛化的用户偏好模式方面的优越性。在此基础上，我们首次实证展示了在我们的高质量、推荐特定数据上持续预训练的LLM具备稳健的幂律扩展特性。我们的实验表明，在多种合成数据模式下，困惑度的降低是一致且可预测的。这些发现为在推荐领域可靠地扩展LLM能力奠定了基础方法论，从而将研究重点从缓解数据不足转向利用高质量、结构化的信息。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of large language models (LLMs) in recommender systems has been hindered by the lack of predictable scaling laws, which are essential for research and resource optimization. This paper proposes a principled synthetic data generation framework that addresses the noise, bias, and incompleteness issues in real user interaction data by creating a curated, pedagogical curriculum. Experimental results show that models trained on this synthetic data significantly outperform those trained on real data in ranking tasks, with a $+130\%$ improvement in recall@100 for SasRec. Furthermore, the study demonstrates robust power-law scaling for LLMs pre-trained on the synthetic data, revealing consistent perplexity reduction across different data modalities, thereby establishing a reliable foundation for scaling LLM capabilities in recommendation systems.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在推荐系统中的应用受到缺乏可预测的扩展定律的阻碍，这限制了研究方向和资源分配的优化。本文提出了一种基于原则的合成数据生成框架，通过构建结构化的教学课程来解决真实用户交互数据中的噪声、偏差和不完整性问题。实验结果表明，使用该合成数据训练的模型在排序任务中显著优于基于真实数据训练的模型，SasRec在recall@100指标上提升了130%。此外，研究首次展示了在推荐领域持续预训练的LLMs具有稳健的幂律扩展特性，不同数据模态下均表现出一致且可预测的困惑度下降，为LLMs在推荐系统中的可靠扩展奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study</div>
<div class="meta-line">Authors: Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim</div>
<div class="meta-line">First: 2025-10-26T16:49:06+00:00 · Latest: 2026-02-12T21:11:14+00:00</div>
<div class="meta-line">Comments: Accepted at LREC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22747v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22747v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Québec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Québec French LLMs on HuggingFace.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型的低资源方言适配：一项法语方言案例研究</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）已被广泛采用，但其最强能力仍主要局限于少数高资源语言，这些语言拥有大量训练数据。最近，持续预训练（CPT）作为一种方法被提出，用于微调这些模型以适应低资源的区域方言。在本文中，我们研究了在数据和计算资源受限的情况下使用CPT进行方言学习的效果。通过低秩适配（LoRA）和计算高效的持续预训练，我们使用一个非常小的数据集将三个LLMs适配为魁北克法语方言，并在COLE数据集上进行基准测试。实验结果表明，在不到1%的模型参数更新的情况下，对少数方言基准的提升显著，而对标准语言基准的退化则非常有限。结果分析表明，性能提升高度依赖于语料库的构成。这些发现表明，结合参数高效微调（PEFT）的持续预训练可以以低成本和可持续的方式缩小方言差距，使高质量LLM能够惠及少数语言群体。我们发布了首个魁北克法语LLMs。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of adapting large language models to low-resource dialects, focusing on Québec French. The motivation stems from the fact that LLMs are predominantly trained on high-resource languages, limiting their utility for minority dialects. The authors employ continual pre-training (CPT) combined with low-rank adaptation (LoRA) to fine-tune three LLMs using a minimal dataset. Their experiments show that the adapted models achieve performance improvements on Québec French benchmarks while maintaining strong results on the prestige language benchmarks, with less than 1% of model parameters updated. The results highlight the importance of corpus composition in achieving effective dialect adaptation through parameter-efficient methods.</div>
<div class="mono" style="margin-top:8px">本文探讨了在低资源环境下对大型语言模型进行方言适配的挑战，以魁北克法语为例。动机源于当前大型语言模型主要依赖高资源语言训练，限制了其在少数方言中的应用。作者采用持续预训练（CPT）结合低秩适配（LoRA）技术，使用少量数据对三个语言模型进行适配。实验结果显示，适配后的模型在魁北克法语基准测试中表现提升，同时在标准法语基准测试中几乎没有性能下降，仅更新了不到1%的模型参数。结果分析表明，语料库的构成对方言适配效果有显著影响，表明通过参数高效微调（PEFT）方法可以有效缩小方言差距，实现低成本且可持续的语言资源创建。</div>
</details>
</div>
<div class="card">
<div class="title">LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens</div>
<div class="meta-line">Authors: Zekun Li, Sizhe An, Chengcheng Tang, Chuan Guo, Ivan Shugurov, Linguang Zhang, Amy Zhao, Srinath Sridhar, Lingling Tao, Abhay Mittal</div>
<div class="meta-line">First: 2026-02-12T20:02:21+00:00 · Latest: 2026-02-12T20:02:21+00:00</div>
<div class="meta-line">Comments: Project page: https://kunkun0w0.github.io/project/LLaMo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kunkun0w0.github.io/project/LLaMo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (&gt;30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaMo：通过连续自回归标记统一运动理解与生成的预训练语言模型扩展</div>
<div class="mono" style="margin-top:8px">近年来，大模型在统一多模态生成与理解方面取得了显著进展。然而，将运动与语言生成和理解统一的模型开发仍被广泛忽视。现有方法通常在配对的运动-文本数据上微调大型语言模型（LLMs），由于可用的文本-运动对规模有限，这可能导致语言能力的灾难性遗忘。此外，先前的方法通常通过量化将运动转换为离散表示以与语言模型集成，从而引入了显著的抖动伪影。为了解决这些挑战，我们提出了LLaMo，一个统一框架，通过一种特定模态的混合变换器（MoT）架构扩展预训练的LLMs。该设计在保留基础模型语言理解能力的同时，实现了可扩展的多模态适应。我们将人类运动编码为因果连续的潜在空间，并通过一个轻量级的流匹配头在解码器骨干中保持下一个标记预测范式，从而支持实时流式运动生成（&gt;30 FPS）。通过利用预训练LLMs的全面语言理解和大规模运动-文本预训练，我们的实验表明，LLaMo在一般场景下实现了高保真度的文本到运动生成和运动到文本描述，尤其是在零样本运动生成方面，标志着向通用统一运动-语言大模型迈出的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop a unified model for both motion understanding and generation, addressing the limitations of existing approaches that suffer from catastrophic forgetting and jitter artifacts. LLaMo introduces a modality-specific Mixture-of-Transformers (MoT) architecture to extend pretrained language models, preserving their linguistic capabilities while enabling scalable multimodal adaptation. By encoding human motion into a continuous latent space and using a lightweight flow-matching head in the decoder-only backbone, LLaMo supports real-time motion generation with over 30 FPS. Experimental results show that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, particularly in zero-shot scenarios, demonstrating its effectiveness in unifying motion and language understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一个统一的模型，用于运动理解和生成，以解决现有方法在灾难性遗忘和抖动伪影方面的局限性。LLaMo提出了一种模态特定的Mixture-of-Transformers（MoT）架构，扩展了预训练语言模型，同时保留其语言理解能力，并实现可扩展的多模态适应。该模型将人体运动编码为连续的潜在空间，并通过轻量级的流匹配头维持下一个token预测范式，支持实时运动生成（&gt;30 FPS）。实验结果表明，LLaMo在零样本运动生成等一般场景中实现了高质量的文本到运动生成和运动到文本描述，展示了其在统一运动与语言理解方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态通常未能充分与语言语义对齐，影响了跨模态的对齐效果，尤其是在需要跨远距离区域或多张图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，这是一种新的框架，将视觉操作重新定义为语言引导的特征调制。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重新编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们在ViLaVT中实例化了这一范式，ViLaVT是一个新型的LVLM，配备了专门用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的双阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current large vision-language models (LVLMs) that rely on single-pass visual encoding, leading to the loss of fine-grained visual information. The proposed method, &#x27;&#x27;chatting with images&#x27;&#x27;, introduces a framework where visual manipulation is guided by language prompts, enabling dynamic joint re-encoding across multiple image regions. This approach enhances the alignment between linguistic and visual modalities, particularly in tasks requiring reasoning over distant visual regions or multiple images. The model, ViLaVT, is trained using a two-stage curriculum involving supervised fine-tuning and reinforcement learning, and achieves significant improvements in complex spatial reasoning tasks across eight benchmarks.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型视觉-语言模型（LVLMs）在单次视觉编码过程中丢失细粒度视觉信息的问题，提出了一种新的框架&#x27;chatting with images&#x27;。该方法通过语言引导的特征调制动态地对多个图像区域进行联合重编码，从而加强视觉与语言模态之间的对齐。在八个基准测试中，所提出的ViLaVT模型表现出显著的性能提升，尤其在涉及多图像或多视频的复杂空间推理任务中效果更为突出。</div>
</details>
</div>
<div class="card">
<div class="title">3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu</div>
<div class="meta-line">First: 2026-02-12T16:41:26+00:00 · Latest: 2026-02-12T16:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aczheng-cai.github.io/3dgsnav.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DGSNav：通过主动3D高斯点云增强视觉-语言模型的物体导航推理</div>
<div class="mono" style="margin-top:8px">物体导航是具身智能的核心能力，使代理能够在未知环境中定位目标物体。近年来，视觉-语言模型（VLMs）的进步推动了零样本物体导航（ZSON）的发展。然而，现有方法通常依赖于场景抽象，将环境转换为语义地图或文本表示，导致高层决策受限于低层感知的准确性。在本工作中，我们提出了3DGSNav，一个新颖的ZSON框架，通过将3D高斯点云（3DGS）作为持久记忆嵌入VLMs，以增强空间推理能力。通过主动感知，3DGSNav逐步构建环境的3DGS表示，从而实现前沿感知的第一人称视角轨迹引导自由视角渲染。此外，我们设计了结构化的视觉提示，并将其与思维链（Chain-of-Thought, CoT）提示相结合，进一步提升VLM推理能力。在导航过程中，实时物体检测器过滤潜在目标，而由VLM驱动的主动视角切换则执行目标重新验证，确保高效且可靠的识别。在多个基准测试和四足机器人上的实验证明，我们的方法在鲁棒性和竞争力方面均优于最先进的方法。项目页面：https://aczheng-cai.github.io/3dgsnav.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of zero-shot object navigation in unknown environments by proposing 3DGSNav, a framework that integrates 3D Gaussian Splatting as a persistent memory mechanism for vision-language models. The method employs active perception to incrementally build a 3DGS representation, allowing for trajectory-guided rendering of first-person views. It also introduces structured visual prompts combined with Chain-of-Thought prompting to enhance VLM reasoning. Experimental results across multiple benchmarks and real-world robot testing show that 3DGSNav achieves robust and competitive performance compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改进视觉语言模型（VLMs）的空间推理能力，提升零样本物体导航的性能，克服传统场景抽象方法的局限性。3DGSNav提出了一种新颖的框架，将3D高斯点云作为持久记忆机制，通过主动感知逐步构建环境的3D表示，从而实现轨迹引导的自由视角渲染。此外，设计的结构化视觉提示与链式思维提示相结合，进一步提升模型推理效果。在多个基准测试和真实世界四足机器人导航实验中，该方法展现出稳健且具有竞争力的性能，优于现有最先进的技术。</div>
</details>
</div>
<div class="card">
<div class="title">On the Adoption of AI Coding Agents in Open-source Android and iOS Development</div>
<div class="meta-line">Authors: Muhammad Ahmad Khan, Hasnain Ali, Muneeb Rana, Muhammad Saqib Ilyas, Abdul Ali Bangash</div>
<div class="meta-line">First: 2026-02-12T16:30:29+00:00 · Latest: 2026-02-12T16:30:29+00:00</div>
<div class="meta-line">Comments: Accepted at MSR 2026 Mining Challenge track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理在开源Android和iOS开发中的采用</div>
<div class="mono" style="margin-top:8px">AI编码代理在软件开发中的贡献日益增加，但其对移动开发的影响却鲜有实证研究。本文提出了首个针对开源移动应用项目中代理生成代码的类别级实证研究。我们利用AIDev数据集中的193个已验证的Android和iOS开源GitHub仓库中的2,901个AI撰写的拉取请求（PRs），分析了跨移动平台、代理和任务类别的PR接受行为。我们发现，Android项目接收的AI撰写的PR数量是iOS的两倍，并且其PR接受率（71%）高于iOS（63%），在Android上还存在显著的代理间差异。在任务类别方面，常规任务（功能、修复和UI）的PR接受率最高，而重构和构建等结构性变更的PR成功率较低且解决时间更长。此外，我们的演化分析表明，Android的PR解决时间在2025年中期有所改善，之后又开始下降。我们的研究结果为AI代理对开源移动项目的影响提供了首个基于实证的描述，并为评估代理生成贡献对设计平台感知代理系统的影响建立了实证基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the adoption and impact of AI coding agents in open-source mobile development, focusing on Android and iOS platforms. By analyzing 2,901 AI-authored pull requests across 193 verified repositories, the research identifies differences in PR acceptance rates between Android (71%) and iOS (63%), with notable variation among agents on Android. It also reveals that routine tasks such as feature additions and bug fixes are more likely to be accepted, while structural changes like refactoring and build updates face lower success rates and longer resolution times. The study further observes a trend of decreasing PR resolution time on Android after mid-2025, highlighting the evolving role of AI agents in open-source mobile ecosystems.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在开源移动开发中的采用及其影响，重点关注Android和iOS项目。通过对193个仓库中2,901个AI生成的拉取请求（PR）进行分析，研究发现Android项目的PR接受率（71%）高于iOS（63%），且在Android平台上存在显著的代理间差异。此外，常规任务如功能添加和修复的PR接受率较高，而重构和构建等结构性任务则接受率较低且解决时间较长。研究还观察到，自2025年中以来，Android项目的PR解决时间呈现下降趋势，表明AI生成代码在开源移动生态中的整合正在发生变化。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?</div>
<div class="meta-line">Authors: Thibaud Gloaguen, Niels Mündler, Mark Müller, Veselin Raychev, Martin Vechev</div>
<div class="meta-line">First: 2026-02-12T14:15:22+00:00 · Latest: 2026-02-12T14:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11988v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents&#x27; task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估AGENTS.md：仓库级上下文文件对编码代理是否有帮助？</div>
<div class="mono" style="margin-top:8px">在软件开发中，一种普遍的做法是通过手动或自动生成上下文文件（如AGENTS.md）来针对特定仓库定制编码代理。尽管这种做法被代理开发者强烈推荐，但目前尚无严谨的研究探讨此类上下文文件是否对实际任务真正有效。在本研究中，我们探讨了这一问题，并在两种互补的设置下评估了编码代理的任务完成性能：一是使用LLM生成的上下文文件（遵循代理开发者建议）进行流行仓库中的SWE-bench任务，二是使用包含开发者提交的上下文文件的仓库中的新问题集合。我们发现，在多个编码代理和LLM中，上下文文件往往会降低任务的成功率，同时增加推理成本超过20%。行为上，无论是LLM生成的还是开发者提供的上下文文件，都会鼓励更广泛的探索（例如更彻底的测试和文件遍历），而编码代理往往遵循这些指令。最终，我们得出结论：上下文文件中不必要的要求会使任务更加困难，而人工撰写的上下文文件应仅描述最低限度的要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the effectiveness of repository-level context files, such as AGENTS.md, in improving the performance of coding agents. The research is motivated by the common practice of using such files to tailor agents to specific repositories, yet there has been no rigorous evaluation of their real-world impact. The authors evaluate coding agents on two sets of tasks: one using LLM-generated context files based on developer recommendations, and the other using issues from repositories with manually committed context files. Their findings show that context files generally lower task success rates and increase inference costs, suggesting that they may introduce unnecessary complexity rather than enhance agent performance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了诸如AGENTS.md等仓库级上下文文件对编码代理性能的影响。研究动机源于编码代理常用这些文件进行定制，但缺乏对其实际效果的实证分析。作者在两种互补的设置中评估了编码代理的表现：一种是使用基于开发者建议的LLM生成的上下文文件，另一种是使用实际开发者提交的上下文文件。研究结果表明，上下文文件通常会降低任务成功率并增加推理成本超过20%，这表明它们可能引入了不必要的复杂性。实验还发现，编码代理倾向于遵循上下文文件的指令，但过多或不相关的条件会阻碍任务完成。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</div>
<div class="meta-line">Authors: Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang, Hongyang Wei, Yi-Fan Zhang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Long Chen</div>
<div class="meta-line">First: 2026-02-12T14:12:14+00:00 · Latest: 2026-02-12T14:12:14+00:00</div>
<div class="meta-line">Comments: 19 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11980v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11980v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model&#x27;s layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间链式思维：连接理解与生成模型以实现空间推理生成</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在美学图像合成方面表现出色，但它们通常在复杂空间理解和推理方面存在困难。现有方法依赖多模态大语言模型（MLLMs）来增强这一能力。然而，这些方法要么通过联合训练带来高昂的计算成本，要么仅依赖文本提示导致空间信息丢失。为缓解这些限制，我们提出了一种空间链式思维（SCoT）框架，这是一种即插即用的方法，能够有效连接MLLMs的推理能力与扩散模型的生成能力。具体而言，我们首先通过交错文本-坐标指令格式训练扩散模型，以增强其布局感知能力。随后，我们利用最先进的MLLMs作为规划器，生成全面的布局计划，并将它们的空间规划能力直接转移到生成过程中。大量实验表明，我们的方法在图像生成基准测试中达到了最先进的性能，并在复杂推理任务中显著优于基线方法，同时在图像编辑场景中也表现出强大的效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of diffusion models in spatial understanding and reasoning by proposing the Spatial Chain-of-Thought (SCoT) framework. The motivation stems from the fact that while diffusion models excel in image generation, they often fail to incorporate complex spatial information effectively. The method combines the reasoning abilities of Multimodal Large Language Models (MLLMs) with the generative strength of diffusion models through a plug-and-play approach. By training diffusion models on interleaved text-coordinate instructions and using MLLMs to generate layout plans, the framework enhances spatial awareness. Experimental results show that SCoT achieves state-of-the-art performance on image generation benchmarks and outperforms existing methods in complex reasoning tasks and image editing scenarios.</div>
<div class="mono" style="margin-top:8px">本文旨在解决扩散模型在空间理解和推理方面的不足，提出了空间链式思维（SCoT）框架。动机源于扩散模型虽在图像生成方面表现出色，但难以有效理解与推理空间结构。方法通过在扩散模型上使用交错的文本-坐标指令进行训练，增强其布局感知能力，并利用先进的多模态大语言模型（MLLMs）作为规划器生成详细的布局计划，将MLLMs的空间规划能力直接引入生成过程。实验结果表明，该方法在图像生成基准测试中达到最先进水平，在复杂空间推理任务中显著优于基线方法，并在图像编辑场景中展现出良好的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Suraj Ranganath, Anish Patnaik, Vaishak Menon</div>
<div class="meta-line">First: 2026-02-12T12:32:51+00:00 · Latest: 2026-02-12T12:32:51+00:00</div>
<div class="meta-line">Comments: Workshop submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11882v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11882v1">PDF</a> · <a href="https://github.com/suraj-ranganath/DINO-MBQuant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>位数分配在世界模型规划中的重要性：一种用于高效空间推理的混合位数配对研究</div>
<div class="mono" style="margin-top:8px">高效的空问推理需要在有限精度预算下仍保持可靠的世界模型。我们研究低位数规划行为主要由总位宽决定，还是由位数在模块间的分配决定。使用DINO-WM在Wall规划任务上，我们在两种规划预算下对均匀、混合、非对称和逐层变体进行了配对目标混合位数评估。我们观察到一个一致的三阶段模式：8位和6位设置接近FP16，3位设置崩溃，而4位设置则对分配敏感。在这一过渡区域，保持编码器精度的规划效果优于均匀量化，且接近大小的非对称变体也表现出相同的编码器侧方向。在后续的严格22单元复制实验中，每单元的回合数较少，混合与均匀INT4的差异变得依赖预算，这进一步突显了该过渡区域的敏感性。这些发现为高效空间推理的模块感知和预算感知量化策略提供了研究方向。代码和运行产物可在https://github.com/suraj-ranganath/DINO-MBQuant获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of bit allocation on the reliability of world models in spatial reasoning tasks under limited precision budgets. By evaluating DINO-WM on the Wall planning task with different quantization strategies, including uniform, mixed, asymmetric, and layerwise variants, the research identifies a three-regime pattern in performance based on bitwidth. It finds that 8-bit and 6-bit settings maintain performance close to FP16, while 3-bit settings experience significant degradation. The 4-bit regime shows sensitivity to bit allocation, with encoder precision preservation leading to better planning outcomes. These results suggest that quantization policies should be both module-aware and budget-aware to optimize spatial reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究探讨了位宽分配对世界模型在空间推理任务中性能的影响，旨在在不牺牲可靠性的情况下优化效率。通过在Wall规划任务上评估DINO-WM的不同量化策略，包括均匀、混合、非对称和逐层分配方式，在两个规划预算下观察到性能呈现三种模式：8位和6位设置保持接近FP16的水平，3位设置出现显著下降，而4位设置则对分配方式敏感。结果表明，在过渡区域中，保持编码器精度和采用非对称量化可以提升规划效果。这些发现支持开发更智能的量化策略，以兼顾模块需求和整体预算限制。</div>
</details>
</div>
<div class="card">
<div class="title">Code2Worlds: Empowering Coding LLMs for 4D World Generation</div>
<div class="meta-line">Authors: Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang</div>
<div class="meta-line">First: 2026-02-12T09:34:28+00:00 · Latest: 2026-02-12T09:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11757v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11757v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/Code2Worlds">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://aigeeksgroup.github.io/Code2Worlds">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Code2Worlds: 为4D世界生成赋能的编码大语言模型</div>
<div class="mono" style="margin-top:8px">实现空间智能需要超越视觉合理性，构建基于物理定律的世界模拟器。尽管编码大语言模型在静态3D场景生成方面取得了进展，但将其范式扩展到4D动态仍然是一个关键的前沿领域。该任务面临两个根本性挑战：多尺度上下文纠缠，其中整体生成无法平衡局部物体结构与全局环境布局；以及语义-物理执行鸿沟，其中开环代码生成会导致缺乏动态保真的物理幻觉。我们提出了Code2Worlds框架，将4D生成建模为语言到模拟代码生成。首先，我们提出了一种双流架构，将检索增强的物体生成与分层环境编排解耦。其次，为确保动态保真度，我们建立了一种物理感知的闭环机制，其中PostProcess Agent编写动态，同时VLM-Motion Critic进行自我反思，以迭代优化模拟代码。在Code4D基准上的评估表明，Code2Worlds在SGS指标上比基线方法提升了41%，在丰富度上提高了49%，并且能够生成此前静态方法所不具备的物理感知动态。代码：https://github.com/AIGeeksGroup/Code2Worlds。网站：https://aigeeksgroup.github.io/Code2Worlds。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance coding LLMs&#x27; ability to generate 4D dynamic worlds by incorporating physical laws into the generation process. The proposed method, Code2Worlds, introduces a dual-stream architecture that separates object generation from environmental orchestration, and employs a physics-aware closed-loop mechanism involving a PostProcess Agent and a VLM-Motion Critic for iterative refinement. Experimental results on the Code4D benchmark demonstrate significant improvements, with a 41% increase in spatial grounding score (SGS) and 49% higher richness compared to existing methods, showcasing the framework&#x27;s effectiveness in generating physically accurate dynamics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升编码大语言模型在生成4D动态世界方面的能力，通过引入物理定律来增强生成过程的合理性。所提出的方法Code2Worlds采用双流架构，将物体生成与环境编排分离，并引入了物理感知的闭环机制，结合PostProcess Agent和VLM-Motion Critic进行迭代优化。在Code4D基准测试中，实验结果表明该框架在SGS指标上提升了41%，Richness指标提高了49%，展示了其在生成物理准确动态方面的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation</div>
<div class="meta-line">Authors: Shuo Lu, Jianjie Cheng, Yinuo Xu, Yongcan Yu, Lijun Sheng, Peijie Wang, Siru Jiang, Yongguan Hu, Run Ling, Yihua Shao, Ao Ma, Wei Feng, Lingxiao He, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang</div>
<div class="meta-line">First: 2026-02-12T06:37:55+00:00 · Latest: 2026-02-12T06:37:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态大语言模型真的理解空间吗？一项数学推理评估</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在面向感知的任务中表现出色，但其进行数学空间推理的能力，即解析和操作二维和三维关系的能力，仍不清楚。人类在解决教科书风格的空间推理问题时准确率超过95%，但我们发现大多数领先的MLLMs在相同任务上的准确率甚至无法达到60%。这种显著的差距突显了空间推理是当前模型的基本弱点。为研究这一差距，我们提出了MathSpatial，一个用于评估和提升MLLMs空间推理能力的统一框架。MathSpatial包含三个互补的组成部分：(i) MathSpatial-Bench，一个涵盖三个类别和十一种子类型的2000个问题的基准，旨在将推理难度与感知噪声分离；(ii) MathSpatial-Corpus，一个包含8000个已验证解的训练数据集；以及(iii) MathSpatial-SRT，它将推理建模为由三个原子操作——关联、约束和推断——组成的结构化轨迹。实验表明，使用MathSpatial对Qwen2.5-VL-7B进行微调可实现竞争力的准确率，同时减少25%的token使用。MathSpatial提供了首个大规模分离感知与推理的资源，使我们能够精确测量和全面理解MLLMs中的数学空间推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the limitations of multimodal large language models (MLLMs) in mathematical spatial reasoning, a critical yet underexplored capability. The researchers developed MathSpatial, a framework consisting of a benchmark, a training corpus, and a structured reasoning trace model, to evaluate and enhance spatial reasoning performance. Experimental results show that fine-tuning Qwen2.5-VL-7B on MathSpatial improves accuracy while reducing token usage by 25%, indicating the framework&#x27;s effectiveness in isolating reasoning challenges from perceptual noise.</div>
<div class="mono" style="margin-top:8px">本研究探讨了多模态大语言模型（MLLMs）在数学空间推理方面的局限性，这是一种关键但较少被研究的能力。研究人员提出了MathSpatial框架，包含2000个空间推理问题的基准测试集、8000个带有验证解的训练数据集，以及将推理建模为由三个基本操作组成的结构化推理轨迹。实验结果显示，对Qwen2.5-VL-7B进行MathSpatial微调后，准确率得到提升，同时减少了25%的token使用量，表明该框架在增强空间推理能力方面具有有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</div>
<div class="meta-line">Authors: Dewi S. W. Gould, Bruno Mlodozeniec, Samuel F. Brown</div>
<div class="meta-line">First: 2025-08-08T08:16:40+00:00 · Latest: 2026-02-12T04:11:52+00:00</div>
<div class="meta-line">Comments: 7 pages and appendices</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06111v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06111v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the capabilities and risks of foundation models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others&#x27; weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that: (1) weaker models can reliably differentiate and score stronger ones, (2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and (3) SKATE automatically surfaces fine-grained capability differences between models. Our findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SKATE：一种可扩展的竞赛评估框架：较弱的LLM通过可验证挑战区分更强的LLM</div>
<div class="mono" style="margin-top:8px">评估基础模型的能力和风险至关重要，但当前方法需要大量领域专业知识，阻碍了其可扩展性，因为这些模型迅速发展。我们引入SKATE：一种新颖的评估框架，其中大语言模型（LLMs）通过为彼此生成和解决可验证任务进行竞争。我们的核心洞察是将评估视为一种游戏：模型同时扮演任务设置者和解决者，被激励创建能够突出自身优势并暴露他人弱点的问题。SKATE提供了几个关键优势，平衡了可扩展性、开放性和客观性。它完全自动化，无需数据，且可扩展，不需要人工输入或领域专业知识。通过使用可验证任务而非LLM裁判，评分具有客观性。与领域受限的程序生成基准（例如国际象棋或空间推理）不同，让LLM创造性地提出挑战使评估更加开放和可扩展。作为概念验证，我们引入了由LLM设定的代码输出预测（COP）挑战，作为测试我们方法的可验证且可扩展的框架。使用基于TrueSkill的排名系统，我们评估了六个前沿LLM，并发现：（1）较弱的模型可以可靠地区分和评分更强的模型；（2）基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题；（3）SKATE能够自动揭示模型之间的细粒度能力差异。我们的发现是迈向通用、可扩展评估框架的重要一步，这些框架能够跟上LLM的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind SKATE is to develop a scalable and objective method for evaluating large language models (LLMs) without relying on domain expertise or human input. The framework treats evaluation as a competitive game where LLMs generate and solve verifiable tasks for each other, allowing them to highlight their strengths and expose others&#x27; weaknesses. The main experimental results show that weaker models can effectively differentiate and score stronger ones, LLMs exhibit self-preferencing behavior by generating challenges aligned with their own capabilities, and SKATE can automatically reveal detailed capability differences between models.</div>
<div class="mono" style="margin-top:8px">开发SKATE的动机是创建一个无需依赖领域专业知识或人工输入的可扩展且客观的大型语言模型（LLM）评估方法。该框架将评估视为一种竞争游戏，其中LLM既作为任务生成者又作为解题者，从而突出自身优势并揭示他人的不足。主要实验结果表明，较弱的模型能够可靠地区分并评分更强的模型，LLM表现出自我偏好行为，生成与其自身能力相符的挑战，SKATE还能自动揭示模型之间的细粒度能力差异，证明其作为通用且可扩展评估方法的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">GameDevBench: Evaluating Agentic Capabilities Through Game Development</div>
<div class="meta-line">Authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten, Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar, Chris Donahue</div>
<div class="meta-line">First: 2026-02-11T18:15:11+00:00 · Latest: 2026-02-11T18:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5&#x27;s performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GameDevBench：通过游戏开发评估代理能力</div>
<div class="mono" style="margin-top:8px">尽管在代码代理方面取得了快速进展，但其多模态对应物的进展却相对滞后。一个关键挑战是缺乏结合软件开发复杂性和深度多模态理解需求的评估测试平台。游戏开发提供了这样的测试平台，因为代理必须在视觉游戏场景中处理大量密集的代码库，并操控内在多模态的资产，如着色器、精灵和动画。我们提出了GameDevBench，这是首个用于评估代理在游戏开发任务上的基准。GameDevBench包含从网络和视频教程中提取的132个任务。这些任务需要显著的多模态理解，并且复杂度较高——平均解决方案所需的代码行数和文件更改量是之前软件开发基准的三倍以上。代理在游戏开发任务上仍然表现不佳，最好的代理仅能完成54.5%的任务。我们发现任务难度与多模态复杂度之间存在强相关性，成功率从面向游戏玩法的任务的46.9%下降到2D图形任务的31.6%。为了提升多模态能力，我们引入了两种基于图像和视频的简单反馈机制。尽管这些方法简单，但它们持续提升了代理的性能，其中最大的提升是Claude Sonnet 4.5的性能从33.3%提升到47.7%。我们公开发布GameDevBench，以支持进一步的代理游戏开发研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GameDevBench is to address the lack of comprehensive evaluation frameworks for multimodal coding agents, which are essential for tasks requiring both code generation and deep understanding of visual elements. The benchmark is designed to assess agents&#x27; abilities in game development, a domain that involves complex codebases and intrinsic multimodal assets like shaders, sprites, and animations. GameDevBench includes 132 tasks from web and video tutorials, with the average solution requiring over three times more code and file changes than previous benchmarks. Experimental results show that even the best agent only solves 54.5% of the tasks, with success rates dropping significantly to 31.6% for 2D graphics tasks. The study introduces two simple feedback mechanisms based on images and videos, which improve agent performance, notably increasing Claude Sonnet 4.5&#x27;s success rate from 33.3% to 47.7%.</div>
<div class="mono" style="margin-top:8px">开发GameDevBench的动机是解决多模态编码代理缺乏全面评估框架的问题，这些代理在进展上落后于单模态代理。该基准旨在评估代理在游戏开发任务中的能力，这些任务需要处理复杂的代码和视觉资产。GameDevBench包含132个来自网页和视频教程的任务，要求较强的多模态理解能力。实验结果显示，即使是最优的代理也只能完成54.5%的任务，其中2D图形任务的成功率降至31.6%。研究引入了两种基于图像和视频的简单反馈机制，显著提升了代理性能，特别是Claude Sonnet 4.5的成功率从33.3%提升至47.7%。</div>
</details>
</div>
<div class="card">
<div class="title">CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning</div>
<div class="meta-line">Authors: Hang Wu, Yujun Cai, Zehao Li, Haonan Ge, Bowen Sun, Junsong Yuan, Yiwei Wang</div>
<div class="meta-line">First: 2026-01-30T04:45:43+00:00 · Latest: 2026-02-11T17:26:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00181v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00181v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamReasoner：通过结构化空间推理强化摄像机运动理解</div>
<div class="mono" style="margin-top:8px">理解摄像机动态是视频空间智能的基本支柱。然而，现有的多模态模型大多将此任务视为黑箱分类，往往依赖于表面的视觉模式而非几何线索，从而混淆了物理上不同的运动。我们提出了CamReasoner框架，将摄像机运动理解重新表述为一个结构化的推理过程，以弥合感知与电影逻辑之间的差距。我们的方法以观察-思考-回答（O-T-A）范式为核心，迫使模型在一个显式的推理模块中解码轨迹和视锥等时空线索。为了培养这一能力，我们构建了一个大规模推理轨迹套件，包含18,000个SFT推理链和38,000个RL反馈样本。值得注意的是，我们是首个在该领域使用强化学习进行逻辑对齐的团队，确保运动推理基于物理几何而非上下文猜测。通过将强化学习应用于观察-思考-回答（O-T-A）推理范式，CamReasoner有效抑制了幻觉现象，并在多个基准测试中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the understanding of camera movements in videos by moving beyond superficial visual patterns and incorporating geometric reasoning. CamReasoner introduces a structured inference framework based on the Observation-Thinking-Answer (O-T-A) paradigm, which requires the model to explicitly reason about spatio-temporal cues like trajectories and view frustums. The framework is trained using a large-scale inference trajectory suite with 18k SFT reasoning chains and 38k RL feedback samples, and it is the first to apply reinforcement learning for logical alignment in this domain. Experimental results show that CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance on multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">CamReasoner的研究动机是突破现有视频空间智能模型对相机运动理解的表面视觉模式，转而采用几何推理。该框架引入了观察-思考-回答（O-T-A）范式，要求模型在显式的推理模块中处理轨迹和视锥等时空线索。实验结果表明，通过在O-T-A推理过程中应用强化学习，CamReasoner在多个基准测试中实现了最先进的性能，并有效减少了运动理解中的幻觉现象。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting</div>
<div class="meta-line">Authors: Rishikesh Bhyri, Brian R Quaranto, Philip J Seger, Kaity Tung, Brendan Fox, Gene Yang, Steven D. Schwaitzberg, Junsong Yuan, Nan Xi, Peter C W Kim</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-11T16:49:37+00:00 · Latest: 2026-02-11T16:49:37+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026. This version includes additional authors who contributed during the rebuttal phase</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11024v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于密集手术器械计数的链式观察空间推理</div>
<div class="mono" style="margin-top:8px">准确计数手术室（OR）中的手术器械是确保手术期间患者安全的关键前提。尽管大型视觉-语言模型和代理AI取得了近期进展，但准确计数这些器械仍然极具挑战性，尤其是在器械密集排列的场景中。为了解决这一问题，我们引入了Chain-of-Look，这是一种新颖的视觉推理框架，通过强制执行结构化的视觉链来模拟人类的顺序计数过程，而不是依赖传统的无序目标检测方法。这种视觉链引导模型沿着连贯的空间轨迹进行计数，从而在复杂场景中提高计数的准确性。为了进一步增强视觉链的物理合理性，我们引入了邻近损失函数，该函数显式建模了密集排列手术器械所固有的空间约束。我们还提出了SurgCount-HD，一个包含1,464张高密度手术器械图像的新数据集。大量实验表明，我们的方法在密集手术器械计数这一具有挑战性的任务中，优于现有的计数方法（如CountGD、REC）以及多模态大语言模型（如Qwen、ChatGPT）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate counting of surgical instruments in operating rooms is essential for patient safety, yet remains challenging in dense scenarios due to the tight clustering of objects. The Chain-of-Look framework addresses this by simulating the sequential human counting process through a structured visual chain, which guides the model to count along a coherent spatial path. This approach improves accuracy in complex scenes by incorporating a neighboring loss function that enforces spatial constraints. Evaluated on SurgCount-HD, a new high-density surgical instrument dataset, the method achieves superior performance compared to existing object detection and multimodal large language models.</div>
<div class="mono" style="margin-top:8px">手术器械在手术室中的准确计数对于保障患者安全至关重要，但在密集场景下由于器械紧密排列，这一任务仍极具挑战性。为解决这一问题，本文提出Chain-of-Look框架，通过模拟人类按顺序计数的过程，构建结构化的视觉链来引导模型进行空间轨迹上的计数。该方法结合了邻近损失函数，以显式建模密集排列器械的空间约束，从而提升复杂场景下的计数精度。在新构建的SurgCount-HD数据集上进行的广泛实验表明，该方法在密集手术器械计数任务中优于现有技术，包括CountGD、REC等目标检测方法以及Qwen、ChatGPT等多模态大语言模型。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning GPT-5 for GPU Kernel Generation</div>
<div class="meta-line">Authors: Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah</div>
<div class="meta-line">First: 2026-02-11T16:22:54+00:00 · Latest: 2026-02-11T16:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora&#x27;s environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在GPU内核生成中微调GPT-5</div>
<div class="mono" style="margin-top:8px">开发高效的GPU内核对于扩展现代AI系统至关重要，但由于复杂的硬件架构和需要专门的优化专业知识，这一任务仍然具有挑战性。尽管大型语言模型（LLMs）在通用顺序代码生成方面表现出强大的能力，但在GPU代码生成方面却面临显著挑战，原因包括高质量标记训练数据的稀缺、合成解决方案生成时的编译器偏见，以及在不同硬件世代间的泛化能力有限。这使得监督微调（SFT）无法作为一种可扩展的方法来改进当前的LLMs。相比之下，强化学习（RL）提供了一种数据效率高且适应性强的替代方案，但需要访问相关工具、仔细选择训练问题，并具备一个强大的评估环境。我们介绍了Makora的环境和工具，用于前沿模型的强化学习微调，并报告了我们对GPT-5进行微调以生成Triton代码的结果。在单次尝试设置下，我们的微调模型将内核正确性从43.7%提升至77.0%（+33.3个百分点），并使在KernelBench基准测试中优于TorchInductor的问题比例从14.8%提升至21.8%（+7个百分点）。与基线GPT-5相比，我们的模型在KernelBench上超过了先前的最先进模型。当集成到完整的编码代理中时，它能够在扩展的KernelBench套件中解决高达97.4%的问题，并在72.9%的问题上优于PyTorch的TorchInductor编译器，几何平均速度提升2.12倍。我们的工作表明，针对特定任务的强化学习后训练可以释放LLMs在高度专业化的技术领域中的能力，而传统监督学习则受限于数据可用性，从而为AI辅助的加速器编程开辟了新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenges in generating efficient GPU kernels using Large Language Models (LLMs), particularly due to the lack of high-quality training data and limitations in supervised fine-tuning. The authors propose using reinforcement learning (RL) with a custom environment and tools, named Makora, to fine-tune GPT-5 for Triton code generation. Their experiments show that the fine-tuned model significantly improves kernel correctness, achieving 77.0% compared to 43.7% in the baseline, and outperforms TorchInductor on a broader set of problems when integrated into a coding agent, solving 97.4% of the expanded KernelBench suite and achieving a geometric mean speedup of 2.12x.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升大型语言模型（LLMs）在生成高效GPU内核方面的能力，这对于现代AI系统的扩展至关重要，但因硬件复杂性和训练数据不足而具有挑战性。作者提出使用强化学习（RL）对GPT-5进行微调，以实现Triton代码生成，并利用Makora的环境和工具。实验结果显示，微调后的模型在内核正确性方面显著提升，达到77.0%，相比基线模型提高了33.3个百分点，并在21.8%的问题上优于TorchInductor。当集成到编码代理中时，该模型能在扩展版的KernelBench套件中解决97.4%的问题，相比PyTorch的编译器在72.9%的问题上表现更优，几何平均加速比为2.12倍。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260217_0354.html">20260217_0354</a>
<a href="archive/20260216_0344.html">20260216_0344</a>
<a href="archive/20260215_0344.html">20260215_0344</a>
<a href="archive/20260213_0409.html">20260213_0409</a>
<a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
