<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-16 03:44</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260216_0344</div>
    <div class="row"><div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图像对话进行内省式视觉思维</div>
<div class="mono" style="margin-top:8px">当前的大型视觉-语言模型（LVLMs）通常依赖于基于单次视觉编码的纯文本推理，这往往导致细粒度视觉信息的丢失。最近提出的『图像思考』方法试图通过外部工具或代码操作图像来缓解这一限制；然而，由此生成的视觉状态通常在语言语义上缺乏足够的关联，影响了跨模态对齐的效果，尤其是在需要跨远距离区域或多个图像进行视觉语义或几何关系推理时。为了解决这些挑战，我们提出了『图像对话』，一种新的框架，将视觉操作重新定义为语言引导的特征调制。在富有表现力的语言提示指导下，模型动态地对多个图像区域进行联合重编码，从而实现语言推理与视觉状态更新之间的更紧密耦合。我们在ViLaVT中实例化了这一范式，ViLaVT是一个新型的LVLM，配备了专门用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程进行训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT实现了显著且一致的性能提升，尤其在复杂的多图像和基于视频的空间推理任务中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current large vision-language models (LVLMs) that rely on single-pass visual encoding and lose fine-grained visual information during reasoning. The authors propose &#x27;chatting with images&#x27;, a framework that integrates language-guided feature modulation into visual manipulation, allowing the model to dynamically re-encode multiple image regions based on expressive language prompts. This approach enhances cross-modal alignment by tightly coupling linguistic reasoning with visual state updates. The framework is instantiated in ViLaVT, a novel LVLM with a dynamic vision encoder, trained using a two-stage curriculum of supervised fine-tuning and reinforcement learning. Experimental results across eight benchmarks show that ViLaVT achieves significant improvements, especially in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型视觉-语言模型（LVLMs）在单次视觉编码过程中丢失细粒度视觉信息的问题，提出了一种新的框架&#x27;chatting with images&#x27;，将视觉操作重构为语言引导的特征调制。该方法通过表达性语言提示动态地对多个图像区域进行联合重编码，从而加强语言推理与视觉状态更新之间的耦合。该框架在ViLaVT中实现，ViLaVT是一个新型的LVLM，配备了专门用于交互式视觉推理的动态视觉编码器，并采用监督微调与强化学习相结合的双阶段课程进行训练。在八个基准测试中，ViLaVT表现出显著且一致的性能提升，尤其在涉及多图像和视频的复杂空间推理任务中效果更为突出。</div>
</details>
</div>
<div class="card">
<div class="title">3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu</div>
<div class="meta-line">First: 2026-02-12T16:41:26+00:00 · Latest: 2026-02-12T16:41:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aczheng-cai.github.io/3dgsnav.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3DGSNav：通过主动3D高斯点云增强视觉-语言模型的物体导航推理</div>
<div class="mono" style="margin-top:8px">物体导航是具身智能的核心能力，使代理能够在未知环境中定位目标物体。近年来，视觉-语言模型（VLMs）的进步推动了零样本物体导航（ZSON）的发展。然而，现有方法通常依赖于场景抽象，将环境转换为语义地图或文本表示，导致高层决策受限于低层感知的准确性。在本工作中，我们提出了3DGSNav，一个新颖的ZSON框架，通过将3D高斯点云（3DGS）作为持久记忆嵌入VLMs，以增强空间推理能力。通过主动感知，3DGSNav逐步构建环境的3DGS表示，从而实现前沿感知的第一视角视图的轨迹引导自由视角渲染。此外，我们设计了结构化的视觉提示，并将其与思维链（Chain-of-Thought, CoT）提示相结合，进一步提升VLM推理能力。在导航过程中，实时物体检测器过滤潜在目标，而由VLM驱动的主动视角切换则执行目标重新验证，确保高效且可靠的识别。在多个基准测试和四足机器人上的实验证明，我们的方法在鲁棒性和竞争力方面均优于最先进的方法。项目页面：https://aczheng-cai.github.io/3dgsnav.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this work is to improve the spatial reasoning capabilities of vision-language models (VLMs) for zero-shot object navigation (ZSON) by addressing limitations in scene abstraction. 3DGSNav introduces a novel framework that integrates 3D Gaussian Splatting (3DGS) as persistent memory, allowing VLMs to maintain a detailed 3D representation of the environment through active perception. This enables trajectory-guided rendering and structured visual prompts combined with Chain-of-Thought prompting to enhance reasoning. The method also incorporates a real-time object detector and active viewpoint switching to verify targets, leading to robust and competitive performance across multiple benchmarks and real-world experiments on a quadruped robot.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过改进场景抽象方法的局限性，提升视觉语言模型（VLM）在零样本物体导航（ZSON）中的空间推理能力。3DGSNav提出了一种新颖的框架，将3D高斯点云（3DGS）作为持久记忆机制，通过主动感知逐步构建环境的3D表示，从而支持轨迹引导的自由视角渲染和前沿感知的第一人称视角。该方法结合了结构化的视觉提示与链式思维（CoT）提示以增强推理能力，并利用实时物体检测器与VLM驱动的主动视角切换确保目标识别的准确性和可靠性。在多个基准测试和真实世界四足机器人导航实验中，结果表明3DGSNav在鲁棒性和性能上均优于现有最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Adoption of AI Coding Agents in Open-source Android and iOS Development</div>
<div class="meta-line">Authors: Muhammad Ahmad Khan, Hasnain Ali, Muneeb Rana, Muhammad Saqib Ilyas, Abdul Ali Bangash</div>
<div class="meta-line">First: 2026-02-12T16:30:29+00:00 · Latest: 2026-02-12T16:30:29+00:00</div>
<div class="meta-line">Comments: Accepted at MSR 2026 Mining Challenge track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理在开源Android和iOS开发中的采用</div>
<div class="mono" style="margin-top:8px">AI编码代理在软件开发中日益发挥重要作用，但其对移动开发的影响却鲜有实证研究。本文提出了首个针对开源移动应用项目中代理生成代码的类别级实证研究。我们利用AIDev数据集中的193个已验证的Android和iOS开源GitHub仓库中的2,901个AI撰写的拉取请求（PRs），分析了跨移动平台、代理和任务类别的PR接受行为。我们发现，Android项目接收的AI撰写的PR数量是iOS的两倍，并且其PR接受率（71%）高于iOS（63%），在Android平台上还存在显著的代理间差异。在任务类别方面，常规任务（功能、修复和UI）的PR接受率最高，而重构和构建等结构性变更的PR成功率较低且解决时间较长。此外，我们的演化分析表明，Android的PR解决时间在2025年中期有所改善，之后又开始下降。我们的研究结果为AI代理对开源移动项目的影响提供了首个基于实证的描述，并为评估代理生成的贡献以构建平台感知的代理系统建立了实证基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the adoption and impact of AI coding agents in open-source mobile development, focusing on Android and iOS projects. By analyzing 2,901 AI-authored pull requests across 193 repositories, the research identifies platform-specific differences in PR acceptance rates, with Android projects showing higher acceptance (71%) compared to iOS (63%). It also reveals that routine tasks such as feature additions and UI updates are more likely to be accepted, while structural changes like refactoring and build modifications face lower success rates and longer resolution times. The study further observes a trend of decreasing PR resolution time on Android after mid-2025, suggesting evolving dynamics in the integration of AI-generated code into open-source mobile ecosystems.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI编码代理在开源移动开发中的采用情况，重点关注Android和iOS项目。研究动机源于AI在软件开发中日益增长的作用以及其在移动生态系统中影响的缺乏实证分析。通过对193个开源仓库中2,901个AI生成的拉取请求（PR）进行分析，研究发现Android项目接受的AI生成PR数量是iOS的两倍，且接受率（71%）高于iOS（63%）。功能型、修复型和UI相关的PR接受率最高，而重构和构建等结构性任务则成功率较低且处理时间较长。此外，研究还显示Android项目在2025年中旬前PR处理时间有所改善，之后又出现下降趋势，表明AI代理在移动开发中的整合正在经历变化。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?</div>
<div class="meta-line">Authors: Thibaud Gloaguen, Niels Mündler, Mark Müller, Veselin Raychev, Martin Vechev</div>
<div class="meta-line">First: 2026-02-12T14:15:22+00:00 · Latest: 2026-02-12T14:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11988v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents&#x27; task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估AGENTS.md：仓库级上下文文件对编码代理是否有帮助？</div>
<div class="mono" style="margin-top:8px">在软件开发中，一种普遍的做法是通过手动或自动生成上下文文件（如AGENTS.md）来定制编码代理以适应特定仓库。尽管这种做法被代理开发者强烈推荐，但目前尚无严谨的研究探讨此类上下文文件是否对实际任务有效。在本工作中，我们研究了这一问题，并在两个互补的设置中评估编码代理的任务完成性能：一是使用LLM生成的上下文文件（遵循代理开发者建议）进行流行仓库中的SWE-bench任务，二是使用包含开发者提交的上下文文件的仓库中的新问题集合。我们发现，在多个编码代理和LLM中，上下文文件往往会降低任务成功率，同时增加推理成本超过20%。行为上，无论是LLM生成的还是开发者提供的上下文文件，都会鼓励更广泛的探索（例如更彻底的测试和文件遍历），而编码代理往往遵循这些指令。最终，我们得出结论：上下文文件中不必要的要求会使任务更加困难，而人工撰写的上下文文件应仅描述最低限度的要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the effectiveness of repository-level context files, such as AGENTS.md, in improving the performance of coding agents. The research motivation stems from the common practice of tailoring coding agents with context files, yet there is a lack of empirical evidence on their real-world impact. The authors evaluate coding agents on two settings: one using LLM-generated context files based on developer recommendations, and the other using actual developer-committed context files. Their findings show that context files generally lower task success rates and increase inference costs by more than 20%, suggesting that they may introduce unnecessary complexity. The results indicate that coding agents tend to follow context file instructions, but excessive or irrelevant requirements can hinder task completion.</div>
<div class="mono" style="margin-top:8px">本研究探讨了诸如AGENTS.md等仓库级上下文文件对编码代理性能的影响。研究动机源于编码代理常用这些文件来适应特定仓库的实践，但缺乏对其实际效果的实证分析。作者在两个互补的场景中评估了编码代理的表现：使用LLM生成的上下文文件处理现有SWE-bench任务，以及从包含开发者提交的上下文文件的仓库中收集的新问题。研究结果显示，上下文文件通常会降低任务的成功率，并使推理成本增加超过20%。这表明，不必要的上下文要求可能使任务更加困难。研究还指出，无论是LLM生成还是开发者编写的上下文文件，都会促使代理进行更广泛的探索，但若包含过多无关信息则可能适得其反。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</div>
<div class="meta-line">Authors: Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang, Hongyang Wei, Yi-Fan Zhang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Long Chen</div>
<div class="meta-line">First: 2026-02-12T14:12:14+00:00 · Latest: 2026-02-12T14:12:14+00:00</div>
<div class="meta-line">Comments: 19 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11980v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11980v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model&#x27;s layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空间链式思维：连接理解与生成模型以实现空间推理生成</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在美学图像合成方面表现出色，但它们通常在复杂空间理解和推理方面存在困难。现有方法依赖多模态大语言模型（MLLMs）来增强这一能力。然而，这些方法要么通过联合训练带来高昂的计算成本，要么仅依赖文本提示导致空间信息丢失。为了解决这些限制，我们提出了一种空间链式思维（SCoT）框架，这是一种即插即用的方法，能够有效连接MLLMs的推理能力与扩散模型的生成能力。具体而言，我们首先通过交错文本-坐标指令格式训练扩散模型，以增强其布局感知能力。随后，我们利用最先进的MLLMs作为规划器，生成全面的布局计划，并将它们的空间规划能力直接转移到生成过程中。大量实验表明，我们的方法在图像生成基准测试中达到了最先进的性能，并在复杂推理任务中显著优于基线方法，同时在图像编辑场景中也表现出强大的效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of diffusion models in spatial understanding and reasoning by proposing a Spatial Chain-of-Thought (SCoT) framework. The framework integrates Multimodal Large Language Models (MLLMs) with diffusion models through a plug-and-play approach, where the diffusion model is trained on interleaved text-coordinate instructions to improve layout awareness, and MLLMs generate detailed spatial plans to guide the generation process. Experimental results show that the proposed method achieves state-of-the-art performance on image generation benchmarks and outperforms existing baselines in complex reasoning tasks, while also demonstrating effectiveness in image editing scenarios.</div>
<div class="mono" style="margin-top:8px">本文旨在解决扩散模型在空间理解和推理方面的不足，提出了空间链式思维（SCoT）框架。动机源于扩散模型虽在图像生成方面表现出色，但缺乏对空间结构的有效推理能力。该方法通过插件式设计，将多模态大语言模型（MLLMs）的推理能力与扩散模型的生成能力相结合。首先，通过交错的文本-坐标指令格式训练扩散模型以增强其布局感知能力，然后利用MLLMs生成详细的布局计划，并将其空间规划能力直接应用于生成过程。实验结果表明，SCoT在图像生成基准测试中达到最先进的性能，在复杂的空间推理任务中显著优于基线方法，并在图像编辑场景中展现出良好的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning</div>
<div class="meta-line">Authors: Suraj Ranganath, Anish Patnaik, Vaishak Menon</div>
<div class="meta-line">First: 2026-02-12T12:32:51+00:00 · Latest: 2026-02-12T12:32:51+00:00</div>
<div class="meta-line">Comments: Workshop submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11882v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11882v1">PDF</a> · <a href="https://github.com/suraj-ranganath/DINO-MBQuant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在世界模型规划中比特的重要性：一种用于高效空间推理的配对混合比特研究</div>
<div class="mono" style="margin-top:8px">高效的空问推理需要在有限精度预算下仍保持可靠的世界模型。我们研究低比特规划行为是否主要由总比特宽度决定，还是由各模块中比特的分配位置决定。在Wall规划任务中，我们使用DINO-WM对均匀、混合、非对称和逐层变体进行了配对目标混合比特评估，并在两种规划预算下观察到一致的三种状态模式：8比特和6比特设置接近FP16，3比特设置崩溃，而4比特设置则对分配敏感。在这一过渡区域，保持编码器精度的规划效果优于均匀量化，且接近大小的非对称变体表现出相同的编码器侧方向。在后续的严格22单元复制实验中，使用更少每单元场景数，混合与均匀INT4的符号变得依赖预算，这进一步突显了该过渡区域的敏感性。这些发现为高效空间推理的模块感知和预算感知量化策略提供了研究方向。代码和运行产物可在https://github.com/suraj-ranganath/DINO-MBQuant获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of bit allocation on the reliability of world models in spatial reasoning tasks under limited precision budgets. By evaluating DINO-WM on the Wall planning task with different quantization strategies, including uniform, mixed, asymmetric, and layerwise, across two planner budgets, the research identifies a three-regime pattern in performance. At 8-bit and 6-bit settings, the model&#x27;s behavior closely resembles FP16, while 3-bit settings lead to significant performance collapse. The 4-bit regime shows sensitivity to bit allocation, with encoder precision preservation improving planning performance. These results suggest that effective quantization for spatial reasoning should consider both module-specific and budget constraints.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在有限精度预算下，位宽分配对世界模型空间推理可靠性的影响。通过在Wall规划任务上评估DINO-WM的不同量化策略，包括均匀、混合、非对称和分层量化，发现性能呈现三种状态：8位和6位设置保持接近FP16的水平，3位设置出现显著退化，而4位设置则对位宽分配敏感。结果表明，在过渡区域中，保持编码器精度和采用非对称量化可以提升规划性能，提示需要更细致的量化策略，以模块需求和预算条件为依据。</div>
</details>
</div>
<div class="card">
<div class="title">Code2Worlds: Empowering Coding LLMs for 4D World Generation</div>
<div class="meta-line">Authors: Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang</div>
<div class="meta-line">First: 2026-02-12T09:34:28+00:00 · Latest: 2026-02-12T09:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11757v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11757v1">PDF</a> · <a href="https://github.com/AIGeeksGroup/Code2Worlds">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://aigeeksgroup.github.io/Code2Worlds">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Code2Worlds: 为4D世界生成赋能的编码大语言模型</div>
<div class="mono" style="margin-top:8px">实现空间智能需要超越视觉合理性，构建基于物理定律的世界模拟器。尽管编码大语言模型在静态3D场景生成方面取得了进展，但将其范式扩展到4D动态仍然是一个关键的前沿领域。该任务面临两个根本性挑战：多尺度上下文纠缠，其中整体生成无法在局部物体结构与全局环境布局之间取得平衡；以及语义-物理执行差距，其中开环代码生成会导致缺乏动态保真的物理幻觉。我们提出了Code2Worlds框架，将4D生成建模为语言到模拟代码生成。首先，我们提出了一种双流架构，将检索增强的物体生成与分层环境编排解耦。其次，为确保动态保真度，我们建立了一种物理感知的闭环机制，其中PostProcess Agent编写动态，同时结合VLM-Motion Critic进行自我反思，以迭代优化模拟代码。在Code4D基准上的评估表明，Code2Worlds在SGS指标上比基线方法提升了41%，在丰富度上提高了49%，并且能够生成此前静态方法中缺失的物理感知动态。代码：https://github.com/AIGeeksGroup/Code2Worlds。网站：https://aigeeksgroup.github.io/Code2Worlds。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to enhance coding LLMs&#x27; ability to generate 4D dynamic worlds by incorporating physical laws into the generation process. The proposed method, Code2Worlds, introduces a dual-stream architecture that separates object generation from environmental orchestration, and employs a physics-aware closed-loop mechanism with a PostProcess Agent and a VLM-Motion Critic for iterative refinement. Experimental results on the Code4D benchmark demonstrate significant improvements, with a 41% increase in SGS and 49% higher Richness compared to existing baselines, and the unique capability to generate physics-aware dynamics not achievable by prior static methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升编码大语言模型在生成4D动态世界方面的能力，突破静态3D场景生成的局限。提出的方法Code2Worlds采用双流架构，将物体生成与环境编排分离，并引入物理感知的闭环机制，结合PostProcess Agent和VLM-Motion Critic进行迭代优化。在Code4D基准上的实验结果显示，该框架在SGS指标上提升了41%，Richness指标提高了49%，有效实现了物理准确的动态生成。</div>
</details>
</div>
<div class="card">
<div class="title">Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation</div>
<div class="meta-line">Authors: Shuo Lu, Jianjie Cheng, Yinuo Xu, Yongcan Yu, Lijun Sheng, Peijie Wang, Siru Jiang, Yongguan Hu, Run Ling, Yihua Shao, Ao Ma, Wei Feng, Lingxiao He, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang</div>
<div class="meta-line">First: 2026-02-12T06:37:55+00:00 · Latest: 2026-02-12T06:37:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态大语言模型真的理解空间吗？数学推理评估</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在面向感知的任务中表现出色，但其进行数学空间推理的能力，即解析和操作二维和三维关系的能力，仍不清楚。人类在解决教科书风格的空间推理问题时准确率超过95\%，但我们发现大多数领先的MLLMs在相同任务上的准确率甚至无法达到60\%。这种显著的差距突显了空间推理是当前模型的基本弱点。为研究这一差距，我们提出了MathSpatial，一个用于评估和提升MLLMs空间推理能力的统一框架。MathSpatial包含三个互补的组成部分：(i) MathSpatial-Bench，一个涵盖三个类别和十一个子类型的2000个问题的基准测试集，旨在将推理难度与感知噪声分离；(ii) MathSpatial-Corpus，一个包含8000个额外问题且有验证解的训练数据集；以及(iii) MathSpatial-SRT，它将推理建模为由三个原子操作——关联、约束和推断——组成的结构化轨迹。实验表明，使用MathSpatial对Qwen2.5-VL-7B进行微调可实现竞争力的准确率，同时减少25\%的token数量。MathSpatial提供了首个大规模分离感知与推理的资源，使我们能够对MLLMs中的数学空间推理进行精确测量和全面理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the limitations of multimodal large language models (MLLMs) in mathematical spatial reasoning, a critical yet underexplored capability. The researchers introduce MathSpatial, a framework that includes a benchmark with 2K problems, a training dataset of 8K problems, and a structured reasoning trace model. Their experiments demonstrate that fine-tuning Qwen2.5-VL-7B on MathSpatial improves accuracy while reducing token usage by 25%, highlighting the framework&#x27;s effectiveness in evaluating and enhancing spatial reasoning in MLLMs.</div>
<div class="mono" style="margin-top:8px">本研究探讨了多模态大语言模型（MLLMs）在数学空间推理任务中的表现，发现其与人类在类似任务上的准确率存在显著差距。研究人员提出了MathSpatial框架，包含基准数据集、训练语料库和结构化推理轨迹模型，用于评估和提升MLLMs的空间推理能力。实验结果表明，对Qwen2.5-VL-7B进行MathSpatial微调后，准确率得到提升，同时减少了25%的token使用量，证明了该框架在分离感知与推理难度方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</div>
<div class="meta-line">Authors: Dewi S. W. Gould, Bruno Mlodozeniec, Samuel F. Brown</div>
<div class="meta-line">First: 2025-08-08T08:16:40+00:00 · Latest: 2026-02-12T04:11:52+00:00</div>
<div class="meta-line">Comments: 7 pages and appendices</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06111v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06111v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the capabilities and risks of foundation models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others&#x27; weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that: (1) weaker models can reliably differentiate and score stronger ones, (2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and (3) SKATE automatically surfaces fine-grained capability differences between models. Our findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SKATE：一种可扩展的竞赛评估框架：较弱的LLM通过可验证挑战区分更强的LLM</div>
<div class="mono" style="margin-top:8px">评估基础模型的能力和风险至关重要，但当前方法需要大量领域专业知识，阻碍了其可扩展性，因为这些模型迅速发展。我们引入SKATE：一种新颖的评估框架，其中大语言模型（LLMs）通过为彼此生成和解决可验证任务进行竞争。我们的核心观点是将评估视为一种游戏：模型同时扮演任务设置者和解决者，被激励创建能够突出自身优势并暴露他人弱点的问题。SKATE提供了几个关键优势，在可扩展性、开放性和客观性之间取得平衡。它完全自动化，无需数据，且可扩展，不需要人工输入或领域专业知识。通过使用可验证任务而非LLM裁判，评分具有客观性。与领域受限的程序生成基准（例如国际象棋或空间推理）不同，让LLM创造性地提出挑战使评估更加开放和可扩展。作为概念验证，我们引入了由LLM设定的代码输出预测（COP）挑战，作为测试我们方法的可验证且可扩展框架。使用基于TrueSkill的排名系统，我们评估了六个前沿LLM，并发现：(1) 较弱的模型可以可靠地区分和评分更强的模型；(2) 基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题；(3) SKATE能够自动揭示模型之间的细粒度能力差异。我们的发现是迈向通用、可扩展评估框架的重要一步，这些框架能够跟上LLM的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to develop a scalable and objective method for evaluating large language models (LLMs) without requiring domain expertise or human input. The proposed framework, SKATE, treats evaluation as a game where LLMs generate and solve verifiable tasks for each other, allowing them to highlight their own strengths while identifying weaknesses in others. The main experimental results show that weaker models can effectively differentiate and score stronger ones, LLMs exhibit self-preferencing behavior by generating challenges aligned with their own capabilities, and SKATE can automatically reveal detailed capability differences between models.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种无需领域专业知识和人工干预的可扩展且客观的大型语言模型（LLM）评估方法。提出的框架SKATE将评估视为一种竞争游戏，其中LLM既作为任务生成者又作为解题者，从而突出自身优势并暴露他人的不足。主要实验结果表明，较弱的模型能够可靠地区分并评分更强的模型，LLM表现出自我偏好行为，生成与其自身能力相符的挑战，且SKATE能够有效揭示模型间的细微能力差异。</div>
</details>
</div>
<div class="card">
<div class="title">GameDevBench: Evaluating Agentic Capabilities Through Game Development</div>
<div class="meta-line">Authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten, Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar, Chris Donahue</div>
<div class="meta-line">First: 2026-02-11T18:15:11+00:00 · Latest: 2026-02-11T18:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5&#x27;s performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GameDevBench：通过游戏开发评估代理能力</div>
<div class="mono" style="margin-top:8px">尽管在代码代理方面取得了快速进展，但其多模态对应物的进展却相对滞后。一个关键挑战是缺乏结合软件开发复杂性与深度多模态理解需求的评估测试平台。游戏开发提供了这样的测试平台，因为代理必须在视觉游戏场景中处理诸如着色器、精灵和动画等本质上多模态的资产，同时导航庞大的代码库。我们提出了GameDevBench，这是首个用于评估代理在游戏开发任务上的基准。GameDevBench包含132个任务，来源于网络和视频教程。这些任务需要显著的多模态理解，并且复杂度较高——平均解决方案所需的代码行数和文件更改量是之前软件开发基准的三倍以上。代理在游戏开发任务上仍存在困难，最佳代理仅能完成54.5%的任务。我们发现任务难度与多模态复杂度之间存在强相关性，成功率从面向游戏玩法的任务的46.9%下降到2D图形任务的31.6%。为了提升多模态能力，我们引入了两种基于图像和视频的简单反馈机制。尽管这些方法简单，但它们持续提升了代理的性能，其中最大的提升是Claude Sonnet 4.5的性能从33.3%提升到47.7%。我们公开发布GameDevBench，以支持进一步的代理游戏开发研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind GameDevBench is to address the lack of comprehensive evaluation frameworks for multimodal coding agents, which are less advanced than unimodal ones. The benchmark is designed to assess agents&#x27; ability to handle complex tasks in game development, which involve both code and multimodal assets like shaders, sprites, and animations. GameDevBench includes 132 tasks from web and video tutorials, requiring significant multimodal understanding. The main experimental results show that even the best agent only solves 54.5% of the tasks, with success rates dropping to 31.6% for 2D graphics tasks. The study also introduces two simple feedback mechanisms based on images and videos, which significantly improve agent performance, particularly for Claude Sonnet 4.5, increasing its success rate from 33.3% to 47.7%.</div>
<div class="mono" style="margin-top:8px">GameDevBench的提出旨在解决多模态编码代理缺乏全面评估框架的问题。该基准通过游戏开发任务来测试代理对视觉和文本元素的综合理解能力，例如着色器、精灵和动画。GameDevBench包含132个来自网页和视频教程的任务，结果显示即使最好的代理也只能完成54.5%的任务。任务难度与多模态复杂度密切相关，游戏导向任务的成功率为46.9%，而2D图形任务仅为31.6%。研究引入了两种基于图像和视频的简单反馈机制，显著提升了代理的表现，其中Claude Sonnet 4.5的成功率从33.3%提升至47.7%。</div>
</details>
</div>
<div class="card">
<div class="title">CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning</div>
<div class="meta-line">Authors: Hang Wu, Yujun Cai, Zehao Li, Haonan Ge, Bowen Sun, Junsong Yuan, Yiwei Wang</div>
<div class="meta-line">First: 2026-01-30T04:45:43+00:00 · Latest: 2026-02-11T17:26:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00181v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00181v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamReasoner：通过结构化空间推理强化相机运动理解</div>
<div class="mono" style="margin-top:8px">理解相机动态是视频空间智能的核心支柱。然而，现有的多模态模型大多将此任务视为黑箱分类，往往依赖表面视觉模式而非几何线索，从而混淆物理上不同的运动。我们提出了CamReasoner框架，将相机运动理解重新表述为结构化推理过程，以弥合感知与电影逻辑之间的鸿沟。我们的方法围绕观察-思考-回答（O-T-A）范式展开，迫使模型在一个显式的推理模块中解码轨迹和视锥等时空线索。为了赋予模型这一能力，我们构建了一个大规模推理轨迹套件，包含18,000个SFT推理链和38,000个RL反馈样本。值得注意的是，我们是首个在该领域使用强化学习进行逻辑对齐的团队，确保运动推理基于物理几何而非情境猜测。通过将强化学习应用于观察-思考-回答（O-T-A）推理范式，CamReasoner有效抑制了幻觉现象，并在多个基准测试中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to improve the understanding of camera movements in videos by moving beyond superficial visual patterns and incorporating geometric reasoning. CamReasoner introduces an Observation-Thinking-Answer (O-T-A) framework that forces the model to perform structured spatial inference, decoding spatio-temporal cues like trajectories and view frustums. The framework is trained using a Large-scale Inference Trajectory Suite with 18k SFT reasoning chains and 38k RL feedback samples, and it is the first to apply reinforcement learning for logical alignment in this domain. Experimental results show that CamReasoner effectively reduces hallucinations and achieves state-of-the-art performance on multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过引入几何推理来提升视频中相机运动的理解，突破以往依赖表面视觉模式的局限。CamReasoner提出了一种观察-思考-回答（O-T-A）框架，要求模型在显式的推理模块中对轨迹和视锥等时空线索进行结构化推理。该框架基于包含18k个SFT推理链和38k个RL反馈样本的大规模推理轨迹集进行训练，并首次在该领域应用强化学习进行逻辑对齐。实验结果表明，CamReasoner有效抑制了幻觉现象，在多个基准测试中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting</div>
<div class="meta-line">Authors: Rishikesh Bhyri, Brian R Quaranto, Philip J Seger, Kaity Tung, Brendan Fox, Gene Yang, Steven D. Schwaitzberg, Junsong Yuan, Nan Xi, Peter C W Kim</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-11T16:49:37+00:00 · Latest: 2026-02-11T16:49:37+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026. This version includes additional authors who contributed during the rebuttal phase</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11024v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于密集手术器械计数的链式观察空间推理</div>
<div class="mono" style="margin-top:8px">准确计数手术室（OR）中的手术器械是确保手术期间患者安全的关键前提。尽管大型视觉-语言模型和代理AI取得了近期进展，但准确计数这些器械仍然极具挑战性，尤其是在器械密集排列的场景中。为了解决这一问题，我们引入了Chain-of-Look，这是一种新颖的视觉推理框架，通过强制执行结构化的视觉链来模拟人类的顺序计数过程，而不是依赖传统的无序目标检测方法。这种视觉链引导模型沿着连贯的空间轨迹进行计数，从而在复杂场景中提高计数准确性。为了进一步增强视觉链的物理合理性，我们引入了邻近损失函数，该函数显式建模了密集排列手术器械所固有的空间约束。我们还提出了SurgCount-HD，一个包含1,464张高密度手术器械图像的新数据集。大量实验表明，在密集手术器械计数这一具有挑战性的任务中，我们的方法优于现有的计数方法（如CountGD、REC）以及多模态大语言模型（如Qwen、ChatGPT）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate counting of surgical instruments in operating rooms is essential for patient safety, yet remains challenging in dense scenarios due to the tight clustering of objects. The Chain-of-Look framework addresses this by simulating the sequential human counting process through a structured visual chain, which allows the model to follow a coherent spatial trajectory for improved accuracy. This approach is further enhanced with a neighboring loss function that enforces spatial constraints among closely packed instruments. The proposed method achieves superior performance compared to existing techniques such as CountGD, REC, and large language models like Qwen and ChatGPT, as validated by extensive experiments on the SurgCount-HD dataset.</div>
<div class="mono" style="margin-top:8px">手术器械在手术室中的准确计数对于保障患者安全至关重要，但在密集场景下仍面临挑战。本文提出的Chain-of-Look框架通过模拟人类按顺序计数的过程，采用结构化的视觉链来实现计数，不同于传统的无序目标检测方法。引入邻近损失函数以增强视觉链的物理合理性，从而更好地建模密集排列的手术器械之间的空间约束。在SurgCount-HD数据集上的大量实验表明，该方法在密集手术器械计数任务中优于现有方法，如CountGD、REC以及大型语言模型Qwen和ChatGPT。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning GPT-5 for GPU Kernel Generation</div>
<div class="meta-line">Authors: Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah</div>
<div class="meta-line">First: 2026-02-11T16:22:54+00:00 · Latest: 2026-02-11T16:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora&#x27;s environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在GPU内核生成中微调GPT-5</div>
<div class="mono" style="margin-top:8px">开发高效的GPU内核对于扩展现代AI系统至关重要，但由于复杂的硬件架构和对专门优化知识的需求，这一任务仍然具有挑战性。尽管大型语言模型（LLMs）在通用顺序代码生成方面表现出强大的能力，但在GPU代码生成方面却面临显著挑战，原因包括高质量标记训练数据的稀缺、合成解决方案生成时的编译器偏见，以及在不同硬件世代间的泛化能力有限。这使得监督微调（SFT）难以作为一种可扩展的方法来改进当前的LLMs。相比之下，强化学习（RL）提供了一种数据效率高且适应性强的替代方案，但需要访问相关工具、仔细选择训练问题，并具备强大的评估环境。我们介绍了Makora的环境和工具，用于前沿模型的强化学习微调，并报告了我们对GPT-5进行微调以生成Triton代码的结果。在单次尝试设置下，我们的微调模型将内核正确性从43.7%提升至77.0%（+33.3个百分点），并使在KernelBench基准测试中优于TorchInductor的问题比例从14.8%提升至21.8%（+7个百分点）。与基线GPT-5相比，我们的模型在KernelBench上超过了先前的最先进模型。当集成到完整的编码代理中时，它能够在扩展的KernelBench套件中解决高达97.4%的问题，并在72.9%的问题上优于PyTorch的TorchInductor编译器，几何平均速度提升2.12倍。我们的工作表明，针对特定任务的强化学习后训练可以释放LLMs在高度专业化的技术领域中的能力，这些领域传统监督学习受限于数据可用性，从而为AI辅助的加速器编程开辟了新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenges of generating efficient GPU kernels using Large Language Models (LLMs), as supervised fine-tuning is not scalable due to limited high-quality training data and hardware-specific biases. The authors propose a reinforcement learning (RL) approach, utilizing Makora&#x27;s environment and tools, to fine-tune GPT-5 for Triton code generation. Their experiments show that the fine-tuned model significantly improves kernel correctness, increasing it from 43.7% to 77.0%, and enhances performance over TorchInductor on a broader set of problems, achieving a 97.4% success rate in an expanded KernelBench suite with a geometric mean speedup of 2.12x.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决使用大语言模型（LLMs）生成高效GPU内核的挑战，尤其是由于高质量训练数据的缺乏和监督微调的局限性。作者提出了一种基于强化学习（RL）的方法，利用Makora的环境和工具对GPT-5进行微调，以适应Triton代码生成任务。实验结果显示，微调后的模型显著提升了内核正确性，达到77.0%，相比基线模型的43.7%提高了33.3个百分点。当集成到完整的编码代理中时，该模型能够在扩展的KernelBench套件中解决97.4%的问题，并在72.9%的问题上超越PyTorch的TorchInductor编译器，实现几何平均速度提升2.12倍。</div>
</details>
</div>
<div class="card">
<div class="title">City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</div>
<div class="meta-line">Authors: Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</div>
<div class="meta-line">First: 2025-12-17T19:59:31+00:00 · Latest: 2026-02-11T06:31:53+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026 (ORAL)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15933v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.15933v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dwipddalal.github.io/AgentNav/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environment. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs, reasoning techniques (e.g., GEPA, chain-of-thought, reflection) and competitive baseline PReP significantly underperform in this challenging setting. To address this, we propose Verbalization of Path(VoP), which explicitly grounds the agent&#x27;s internal reasoning by probing city-scale cognitive maps (key landmarks and directions toward the destination) from the MLLM, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外城市导航：从大规模知识中探索MLLMs的涌现导航</div>
<div class="mono" style="margin-top:8px">利用多模态大语言模型（MLLMs）开发具身智能体为解决复杂的现实世界任务提供了巨大潜力。然而，当前的评估基准主要以语言为中心或严重依赖模拟环境，很少涉及实际场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们引入了稀疏锚定视觉导航任务，专门设计用于评估MLLMs在具有挑战性和知识密集型的现实环境中进行序列决策的能力。我们通过CityNav这一涵盖四个不同全球城市的综合性基准来实现该任务，专门用于评估原始的MLLM驱动智能体在城市导航中的表现。智能体仅需依赖视觉输入和内部多模态推理，无需额外的环境标注或专门的架构修改，即可在50多个决策点中进行序列导航。关键的是，智能体必须通过解读城市特定线索和识别地标自主实现定位，进行空间推理，并战略性地规划和执行前往目标的路线。通过广泛的评估，我们证明当前最先进的MLLMs、推理技术（如GEPA、思维链、反思）以及具有竞争力的基线PReP在这一挑战性场景中表现显著不足。为了解决这一问题，我们提出了路径语言化（VoP），通过从MLLM中探查城市级认知地图（关键地标和朝向目标的方向）来显式锚定智能体的内部推理，从而显著提升导航成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of current MLLM evaluation benchmarks, which are often language-centric or simulated, by introducing the task of Sparsely Grounded Visual Navigation. The proposed CityNav benchmark evaluates MLLMs&#x27; ability to navigate real-world urban environments using only visual inputs and internal multimodal reasoning, without additional annotations or architectural changes. Experimental results show that state-of-the-art MLLMs and existing techniques like GEPA, chain-of-thought, and reflection fail to perform well in this task, highlighting the need for better reasoning mechanisms. The authors propose VoP, a method that explicitly grounds the agent&#x27;s reasoning through city-scale cognitive maps, significantly improving navigation success.</div>
<div class="mono" style="margin-top:8px">本文旨在解决当前评估多模态大语言模型（MLLMs）在现实世界导航任务中的不足，特别是在复杂城市环境中的表现。作者提出了稀疏视觉导航任务，并通过CityNav基准进行实现，该基准测试MLLMs仅依赖视觉输入和内部推理进行城市导航的能力。实验结果表明，现有的方法，包括最先进的MLLMs和推理技术如GEPA、思维链和反思，以及基线模型PReP，在此任务中表现不佳。为此，他们提出了VoP方法，通过提取城市级认知地图来显式地锚定代理的推理过程，从而显著提升导航成功率。</div>
</details>
</div>
<div class="card">
<div class="title">ContextBench: A Benchmark for Context Retrieval in Coding Agents</div>
<div class="meta-line">Authors: Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Federica Sarro, Zhaoyang Chu, He Ye</div>
<div class="meta-line">First: 2026-02-05T17:10:26+00:00 · Latest: 2026-02-11T04:58:49+00:00</div>
<div class="meta-line">Comments: 36 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05892v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.05892v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (&quot;The Bitter Lesson&quot; of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextBench：面向编码代理的上下文检索基准</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的编码代理在自动化问题解决基准上表现出色，但现有评估主要关注最终任务的成功率，对代理在解决问题过程中如何检索和使用代码上下文的洞察有限。我们引入了ContextBench，这是一个面向过程的编码代理上下文检索评估基准。ContextBench包含来自8种编程语言的66个仓库中的1,136个问题解决任务，每个任务都附加了人工标注的黄金上下文。我们进一步实现了一个自动化评估框架，用于追踪代理的行为轨迹，并在问题解决过程中测量上下文的召回率、精确率和效率。通过ContextBench，我们评估了四个前沿的LLM和五个编码代理。我们的结果表明，复杂的代理框架在上下文检索方面仅带来边际收益（编码代理的“苦涩教训”），LLM倾向于优先召回而非精确，且探索与实际使用的上下文之间存在显著差距。ContextBench通过添加中间的黄金上下文指标，增强了现有端到端基准，揭示了问题解决过程中的关键环节。这些上下文为指导LLM在软件任务中的推理提供了有价值的中间信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to evaluate how coding agents retrieve and utilize code context during issue resolution, rather than just focusing on final task success. ContextBench introduces a process-oriented benchmark with 1,136 tasks from 66 repositories across eight programming languages, each annotated with human-verified code contexts. The authors developed an automated framework to track agent behavior and assess context recall, precision, and efficiency. Experimental results reveal that advanced agent structures provide only minor improvements in context retrieval, LLMs tend to prioritize recall over precision, and there is a significant gap between the context explored and the context actually used.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前编码代理评估的不足，即主要关注最终任务的成功率，而忽视了代码上下文的检索过程。ContextBench引入了一个面向过程的基准测试，包含来自66个仓库、涵盖八种编程语言的1136个问题解决任务，每个任务均附有人工标注的黄金上下文。作者开发了一个自动化评估框架，用于追踪代理的行为并评估问题解决过程中上下文的召回率、精确率和效率。实验结果表明，高级代理结构在上下文检索方面仅带来微小提升，LLMs倾向于优先召回而非精确，且存在显著的探索上下文与实际使用上下文之间的差距。这些发现强调了在编码代理评估中引入中间上下文指标的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps</div>
<div class="meta-line">Authors: Sharat Bhat, Harshita Khandelwal, Tushar Kataria, Vivek Gupta</div>
<div class="meta-line">First: 2026-02-11T04:36:14+00:00 · Latest: 2026-02-11T04:36:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10518v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10518v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapVerse：一个用于多样化真实地图的地理问答基准</div>
<div class="mono" style="margin-top:8px">地图是结构化和上下文知识的强大载体，涵盖地理、人口、基础设施和环境模式。对这类知识进行推理需要模型能够整合空间关系、视觉线索、现实世界背景和领域专业知识——这些能力当前的大语言模型（LLMs）和视觉-语言模型（VLMs）仍难以一致展现。然而，用于评估VLMs在地图推理上的数据集范围狭窄，局限于特定领域，并且严重依赖人工生成内容（如LLMs或基于流水线的方法的输出），难以深入评估真实的地理推理能力。为了解决这一问题，我们提出了MapVerse，一个基于真实地图的大规模基准数据集。该数据集包含1,025张地图上的11,837对人工撰写的问答对，涵盖十种多样的地图类别以及每种地图下的多种问题类别。该数据集为评估地图阅读、解释和多模态推理提供了丰富的场景。我们评估了十种最先进的模型，以建立基准并量化推理能力的差距。除了整体表现外，我们还进行了细粒度的类别分析，以评估模型推理在多个维度上的表现，并探讨影响推理结果的视觉因素。我们的研究发现，尽管当前的VLMs在分类式任务上表现具有竞争力，但在需要复杂空间推理的高级任务上，无论是开源还是闭源模型都存在不足。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of existing datasets for evaluating geospatial reasoning in vision-language models (VLMs), which are often narrow in scope and rely on artificial content. MapVerse is introduced as a large-scale benchmark built on real-world maps, containing 11,837 human-authored question-answer pairs across 1,025 maps, covering ten diverse map categories and multiple question types. The main experimental results show that while current VLMs perform well on classification tasks, they struggle with more complex spatial reasoning tasks, highlighting significant gaps in their capabilities.</div>
<div class="mono" style="margin-top:8px">该研究提出了MapVerse，这是一个用于地理空间问答的全面基准，旨在解决现有数据集范围狭窄且依赖人工生成内容的不足。MapVerse包含1,025幅真实地图上的11,837对人工撰写的问答对，涵盖十种地图类型和多种问题类别。研究人员在该基准上评估了十种最先进的模型，发现尽管当前的视觉-语言模型在分类任务上表现良好，但在需要复杂空间推理的任务上仍存在明显不足。</div>
</details>
</div>
<div class="card">
<div class="title">CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</div>
<div class="meta-line">Authors: Tianjiao Yu, Xinzhuo Li, Yifan Shen, Yuanzhe Liu, Ismini Lourentzou</div>
<div class="meta-line">First: 2025-12-14T17:05:11+00:00 · Latest: 2026-02-10T22:27:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12768v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12768v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoRe3D: 以协作推理为基础的三维智能</div>
<div class="mono" style="margin-top:8px">近期在大型多模态模型方面的进展表明，显式的推理机制在提高模型可靠性、可解释性和跨模态对齐方面起着关键作用。尽管这种以推理为中心的方法在语言和视觉任务中已被证明是有效的，但其在三维领域的扩展仍不成熟。CoRe3D引入了一个统一的三维理解和生成推理框架，该框架在语义和空间抽象上联合运作，使从语言中推断出的高层意图能够直接指导低层三维内容的生成。该设计的核心是一个基于空间的推理表示，将三维潜在空间分解为局部区域，使模型能够以组合和程序化的方式对几何结构进行推理。通过紧密耦合语义链式推理与结构化空间推理，CoRe3D生成的三维输出表现出强局部一致性和与语言描述的忠实对齐。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of CoRe3D stems from the need to enhance the reliability, interpretability, and cross-modal alignment of models in 3D understanding and generation tasks. The method introduces a unified reasoning framework that integrates semantic and spatial abstractions, enabling high-level language-based intentions to guide low-level 3D content creation. The main experimental results demonstrate that CoRe3D produces 3D outputs with strong local consistency and faithful alignment with linguistic descriptions, highlighting its effectiveness in generating coherent and semantically accurate 3D representations.</div>
<div class="mono" style="margin-top:8px">CoRe3D的研究动机是通过引入显式的推理机制来提升3D智能的可靠性、可解释性和跨模态对齐能力。该方法提出了一种统一的3D理解和生成推理框架，能够在语义和空间抽象层面协同运作，使语言中的高层意图直接引导3D内容的生成。关键实验结果表明，CoRe3D通过其基于空间的推理表示，能够生成具有强局部一致性和忠实语言描述对齐的3D输出。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning</div>
<div class="meta-line">Authors: Yancheng Long, Yankai Yang, Hongyang Wei, Wei Chen, Tianke Zhang, Haonan fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</div>
<div class="meta-line">First: 2026-02-07T09:23:34+00:00 · Latest: 2026-02-10T19:38:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07458v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term &quot;Attention Collapse,&quot; where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialReward：通过显式空间推理弥合在线强化学习在图像编辑中的感知鸿沟</div>
<div class="mono" style="margin-top:8px">在线强化学习（RL）为复杂的图像编辑提供了有前景的途径，但目前受到可靠且细粒度奖励信号稀缺的限制。现有评估器经常面临我们称之为『注意力崩溃』的关键感知鸿沟，其中模型忽略了跨图像比较，无法捕捉细粒度细节，导致感知不准确和评分失调。为了解决这些限制，我们提出了SpatialReward，这是一种通过显式空间推理强制进行精确验证的奖励模型。通过将推理锚定在预测的编辑区域，SpatialReward在像素级证据基础上进行语义判断，显著提高了评估的准确性。我们的模型在精心挑选的26万条空间感知数据集上进行训练，在MMRB2和EditReward-Bench上取得了最先进的性能，并在我们提出的MultiEditReward-Bench上优于专有评估器。此外，SpatialReward在在线RL中作为强大的信号，使OmniGen2在GEdit-Bench上提升+0.90，超越了领先的判别模型，并将GPT-4.1的提升效果翻倍（+0.45）。这些结果表明，空间推理对于在图像编辑中实现有效的对齐至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to overcome the limitations of online reinforcement learning (RL) in image editing, particularly the issue of unreliable and coarse reward signals. The proposed method, SpatialReward, introduces explicit spatial reasoning to address the &quot;Attention Collapse&quot; problem, where models fail to compare across images and miss fine-grained details. By focusing on predicted edit regions and grounding semantic judgments in pixel-level evidence, SpatialReward improves evaluative accuracy. Experimental results show that it achieves state-of-the-art performance on MMRB2 and EditReward-Bench, outperforms proprietary evaluators on MultiEditReward-Bench, and enhances OmniGen2 by +0.90 on GEdit-Bench, surpassing the leading discriminative model and doubling the gain of GPT-4.1.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决在线强化学习（RL）在图像编辑中的局限性，尤其是奖励信号的不可靠和粗糙问题。提出的方法SpatialReward通过引入显式的空间推理来应对&quot;注意力崩溃&quot;现象，即模型无法跨图像比较并忽略细节。该方法通过将语义评估锚定在预测编辑区域的像素级证据上，提升了奖励估计的准确性。实验结果表明，它在MMRB2和EditReward-Bench上达到了最先进的性能，在MultiEditReward-Bench上优于专有评估器，并在GEdit-Bench上使OmniGen2提升+0.90，超越了领先的判别模型，且GPT-4.1的提升幅度翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到动作：在空间基础先验中构建视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-动作（VLA）模型在三维真实世界中进行操作，但通常基于二维编码器，这导致了空间推理的缺失，限制了其泛化能力和适应性。近期的VLA三维集成技术要么需要专用传感器且跨模态迁移效果差，要么注入的线索较弱，缺乏几何信息并损害了视觉-语言对齐。在本工作中，我们引入了FALCON（从空间到动作），一种新颖范式，它将丰富的三维空间标记注入到动作头中。FALCON利用空间基础模型，仅通过RGB图像即可提供强大的几何先验，并包含一个可选的具身空间模型，能够融合深度或姿态信息以提高保真度，而无需重新训练或改变架构。为了保持语言推理能力，空间标记被输入到空间增强的动作头中，而不是简单地连接到视觉-语言主干网络中。这些设计使FALCON能够解决空间表示、模态迁移性和对齐方面的局限性。在三个模拟基准和十一项真实世界任务的全面评估中，我们提出的FALCON取得了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化下保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing vision-language-action (VLA) models that operate in 3D environments but rely on 2D encoders, leading to a gap in spatial reasoning. The authors propose FALCON, a novel approach that integrates rich 3D spatial tokens into the action head without modifying the vision-language backbone. By leveraging spatial foundation models, FALCON extracts strong geometric priors from RGB data and optionally fuses depth or pose information for enhanced accuracy. Experimental results across three simulation benchmarks and eleven real-world tasks demonstrate that FALCON achieves state-of-the-art performance, outperforms existing baselines, and maintains robustness in challenging conditions such as clutter and varying object scales.</div>
<div class="mono" style="margin-top:8px">本文针对现有视觉-语言-动作（VLA）模型在3D环境中操作但依赖2D编码器导致的空间推理不足问题。FALCON模型通过在动作头中注入丰富的3D空间标记，提出了一种新范式，利用空间基础模型从RGB数据中提取强几何先验。此外，该模型包含一个可选的具身空间模型，能够融合深度或姿态信息以提高精度，而无需重新训练或改变架构。空间增强的动作头保留了语言推理能力，不修改视觉-语言主干。在三个模拟基准和十一项现实任务的综合评估中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化等挑战性条件下保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Mindset: Reasoning with Adaptive Cognitive Modes</div>
<div class="meta-line">Authors: Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen</div>
<div class="meta-line">First: 2026-02-10T18:31:47+00:00 · Latest: 2026-02-10T18:31:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10063v1">PDF</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset">Code1</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维模式链：基于自适应认知模式的推理</div>
<div class="mono" style="margin-top:8px">人类解决问题的过程从不是单一思维模式的重复，这里的思维模式指的是不同的认知处理方式。在解决特定任务时，我们并不依赖单一思维模式，而是将多种思维模式整合到同一个解决方案中。然而，现有的LLM推理方法陷入了一个常见误区：它们在所有步骤中都使用相同的固定思维模式，忽视了解决同一问题的不同阶段需要根本不同的思维模式。这种单一假设阻碍了模型向更高层次智能的发展。为了解决这一局限性，我们提出了思维模式链（Chain of Mindset, CoM），这是一种无需训练的代理框架，能够实现步骤级的自适应思维模式协调。CoM将推理分解为四种功能上异质的思维模式：空间思维、收敛思维、发散思维和算法思维。一个元代理根据推理状态的演变动态选择最优的思维模式，同时双向上下文门控机制过滤跨模块的信息流动，以保持推理的有效性和效率。我们在涵盖数学、代码生成、科学问答和空间推理的六个具有挑战性的基准上进行了实验，结果表明CoM在整体准确率上分别比最强基线模型Qwen3-VL-32B-Instruct和Gemini-2.0-Flash高出4.96\%和4.72\%，同时保持推理效率。我们的代码可在\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitation of existing large language models (LLMs) in reasoning tasks, where they often apply a fixed mindset across all steps, failing to adapt to the varying cognitive requirements of different problem-solving stages. The proposed Chain of Mindset (CoM) framework introduces an agentic approach that enables adaptive mindset orchestration at the step level without requiring additional training. It decomposes reasoning into four distinct cognitive modes—Spatial, Convergent, Divergent, and Algorithmic—and employs a Meta-Agent to dynamically select the most suitable mindset based on the current reasoning state. A bidirectional Context Gate is used to manage information flow between modules, ensuring both effectiveness and efficiency. Experimental results on six challenging benchmarks show that CoM achieves state-of-the-art performance, improving overall accuracy by 4.96\% and 4.72\% on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash respectively, while maintaining reasoning efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有大型语言模型在推理任务中的局限性，通过模拟人类认知的灵活性来提升推理能力。提出的Chain of Mindset（CoM）框架采用一种代理方法，允许在问题解决的每一步进行适应性的认知模式调度，将推理分解为四种功能不同的认知模式：空间、收敛、发散和算法模式。一个元代理根据当前推理状态动态选择最合适的认知模式，而双向上下文门控机制则管理模块间的信息流动。实验结果在数学、代码生成、科学问答和空间推理等六个基准测试中显示，CoM达到了最先进的性能，分别在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上将总体准确率提升了4.96\%和4.72\%，同时保持推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge</div>
<div class="meta-line">Authors: Yijie Lin, Guofeng Ding, Haochen Zhou, Haobin Li, Mouxing Yang, Xi Peng</div>
<div class="meta-line">First: 2026-02-10T14:45:02+00:00 · Latest: 2026-02-10T14:45:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARK：一个结合推理与知识的双轴多模态检索基准</div>
<div class="mono" style="margin-top:8px">现有的多模态检索基准主要强调日常图像的语义匹配，并对专业知识和复杂推理的诊断能力有限。为解决这一问题，我们引入了ARK，一个从两个互补视角分析多模态检索的基准：(i) 知识领域（五个领域，17个子类型），这些领域描述了检索所依赖的内容和专业知识；(ii) 推理能力（六个类别），这些类别描述了在识别正确候选时所需进行的多模态证据推理类型。具体而言，ARK评估了单模态和多模态查询与候选的检索，涵盖16种异构视觉数据类型。为了避免评估中的捷径匹配，大多数查询都与需要多步推理的目标困难负样本配对。我们在ARK上评估了23种代表性的基于文本和多模态的检索器，并观察到知识密集型与推理密集型检索之间存在显著差距，其中细粒度视觉和空间推理成为持续的瓶颈。我们进一步表明，简单的增强方法如重排序和重写能够带来一致的改进，但仍存在大量提升空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ARK is to address the limitations of existing multimodal retrieval benchmarks, which focus primarily on semantic matching in daily-life images and neglect professional knowledge and complex reasoning. ARK introduces a dual-axis benchmark that evaluates retrieval based on knowledge domains (five domains with 17 subtypes) and reasoning skills (six categories). The benchmark includes 16 heterogeneous visual data types and employs targeted hard negatives to prevent shortcut matching. Evaluation of 23 retrievers reveals a significant performance gap between knowledge-intensive and reasoning-intensive tasks, with fine-grained visual and spatial reasoning being major bottlenecks. Simple techniques like re-ranking and rewriting show consistent improvements, but there is still substantial room for advancement.</div>
<div class="mono" style="margin-top:8px">本研究旨在弥补现有多模态检索基准在专业领域知识和复杂推理能力上的不足，这些基准主要关注日常图像的语义匹配。提出的ARK基准从两个互补角度评估多模态检索：知识领域（五个领域，17个子类）和推理能力（六个类别）。它包含16种异构视觉数据类型，并采用针对性的困难负样本以避免捷径匹配。实验结果表明，知识密集型与推理密集型检索之间存在显著性能差距，细粒度视觉和空间推理成为持续存在的瓶颈。简单的改进方法如重排序和重写能带来一致的提升，但仍存在较大的优化空间。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-10T14:22:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v2">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型（MLLMs）在空间推理方面取得了进展，越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这常常导致语义与几何的不匹配以及冗余信号。我们提出了GeoThinker框架，将整合范式从被动融合转向主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控进行校准，以偏重任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面设定了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力和显著提升的空间感知能力，包括具身指称和自动驾驶。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of passive geometric integration in spatial reasoning tasks for Multimodal Large Language Models (MLLMs), where geometric information is often misaligned with semantic content and leads to redundant signals. The proposed GeoThinker framework introduces an active perception approach by enabling the model to selectively retrieve geometric evidence based on its internal reasoning needs. It achieves this through Spatial-Grounded Fusion at specific VLM layers, using frame-strict cross-attention and Importance Gating to focus on task-relevant geometric structures. Experimental results on VSI-Bench show that GeoThinker achieves a state-of-the-art score of 72.6 and demonstrates strong performance in complex downstream tasks such as embodied referring and autonomous driving.</div>
<div class="mono" style="margin-top:8px">本文针对多模态大语言模型（MLLMs）在空间推理任务中被动融合几何信息的局限性进行了探讨。现有方法常因无差别融合3D编码器的几何信息而导致语义与几何的错位以及冗余信号。为解决这些问题，作者提出了GeoThinker框架，通过根据内部推理需求选择性地检索几何证据实现主动感知。该方法在特定VLM层应用空间基础融合，结合帧严格交叉注意力和重要性门控机制，以聚焦任务相关的几何结构。实验结果表明，GeoThinker在VSI-Bench上取得72.6的峰值得分，优于现有方法，并在复杂场景如具身指称和自动驾驶中展现出强大的泛化能力与空间感知提升。</div>
</details>
</div>
<div class="card">
<div class="title">EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems</div>
<div class="meta-line">Authors: Wentao Zhang, Jianfeng Wang, Liheng Liang, Yilei Zhao, HaiBin Wen, Zhe Zhao</div>
<div class="meta-line">First: 2026-02-10T14:04:22+00:00 · Latest: 2026-02-10T14:04:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10171v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvoCodeBench：一种用于自进化的LLM驱动编码系统的类人表现基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在编程任务中的持续进步，LLM驱动的编码系统已从一次性代码生成演进为能够在推理过程中进行迭代改进的复杂系统。然而，现有的代码基准主要强调静态正确性，并隐式地假设推理过程中模型能力是固定的。因此，它们无法捕捉推理时的自我进化特性，例如代理在迭代优化解决方案时准确性与效率是否提升。此外，这些基准对资源成本的考量有限，并且很少将模型性能与人类程序员进行校准。而且，许多基准主要针对高资源语言，忽略了跨语言鲁棒性和长尾语言的稳定性。因此，我们提出了EvoCodeBench，这是一个用于评估自进化LLM驱动编码系统在多种编程语言中的表现基准，并与人类表现进行直接比较。EvoCodeBench追踪性能动态，测量解决方案的正确性以及解决时间、内存消耗等效率指标，并在重复问题解决尝试中评估算法设计的改进。为了将评估建立在以人类为中心的参考框架上，我们直接在相同任务上比较模型性能与人类程序员的性能，从而在人类能力分布中实现相对性能评估。此外，EvoCodeBench支持多种编程语言，能够在统一协议下进行系统性的跨语言和长尾语言稳定性分析。我们的结果表明，自进化系统在时间上表现出可衡量的效率提升，而与人类相对的多语言分析提供了仅凭准确性无法获得的见解。EvoCodeBench为评估进化LLM驱动系统中的编码智能奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind EvoCodeBench is to address the limitations of existing code benchmarks that focus on static correctness and ignore the dynamic self-evolution of LLM-driven coding systems during inference. The benchmark evaluates the iterative improvement of these systems by tracking performance dynamics across multiple programming languages, measuring both solution correctness and efficiency metrics like solving time, memory consumption, and algorithmic design refinement. Experimental results show that self-evolving systems achieve measurable efficiency gains over time, and comparing their performance to human programmers provides deeper insights into their capabilities beyond mere accuracy.</div>
<div class="mono" style="margin-top:8px">EvoCodeBench的提出动机是为了解决现有代码基准测试主要关注静态正确性，而忽视了LLM驱动编码系统在推理过程中自我演进的问题。该基准通过跟踪多次问题解决尝试中的性能动态，评估系统在正确性、解决时间、内存消耗和算法设计优化方面的表现。同时，它直接将模型性能与人类程序员进行对比，实现基于人类能力分布的相对性能评估，并支持多种编程语言以分析跨语言鲁棒性和长尾语言稳定性。实验结果表明，自我演进系统在时间推移中可实现可衡量的效率提升，且基于人类相对性和多语言分析的评估方式提供了仅靠准确率无法获得的深入见解。</div>
</details>
</div>
<div class="card">
<div class="title">From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</div>
<div class="meta-line">Authors: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki</div>
<div class="meta-line">First: 2026-02-09T14:39:43+00:00 · Latest: 2026-02-10T08:48:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08735v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08735v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从对应关系到行动：多模态大语言模型中类人多图像空间推理</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在单图像空间推理方面取得了显著进展，但需要整合多视角信息的多图像空间推理仍具有挑战性。认知研究指出，人类通过两种机制解决此类任务：跨视角对应关系，用于识别不同视角中对应同一物理位置的区域；以及逐步视角转换，通过依次组合相对视角变化来完成推理。然而，现有研究仅部分且通常隐式地引入了这些机制，缺乏对两者显式的监督。我们提出了一种名为HATCH（用于跨视角对应关系和视角变化的人类感知训练）的训练框架，包含两个互补的目标：（1）块级空间对齐，鼓励块表示在空间对应区域中对齐；（2）行动后回答推理，要求模型在预测最终答案前生成显式的视角转换动作。在三个基准测试上的实验表明，HATCH在保持单图像推理能力的同时，显著优于同等规模的基线模型，并且在与更大模型的对比中也取得了具有竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of multi-image spatial reasoning in multimodal large language models (MLLMs), which is less developed compared to single-image reasoning. The authors propose HATCH, a training framework that explicitly incorporates two human-like mechanisms: cross-view correspondence and stepwise viewpoint transformation. By introducing Patch-Level Spatial Alignment and Action-then-Answer Reasoning as complementary objectives, HATCH enables the model to align spatial information across views and generate sequential viewpoint transitions. Experimental results on three benchmarks show that HATCH significantly outperforms existing baselines of similar size and achieves performance comparable to much larger models, while maintaining strong single-image reasoning abilities.</div>
<div class="mono" style="margin-top:8px">本文针对多图像空间推理在多模态大语言模型（MLLMs）中的挑战，指出其相较于单图像推理发展不足。作者提出HATCH框架，显式地引入了两种人类式的机制：跨视图对应和逐步视角变换。该框架包含两个目标——图像块级空间对齐，以对齐不同视角中对应物理位置的图像块表示；以及动作-答案推理，要求模型在生成最终答案前显式生成视角变换动作。在三个基准测试上的实验结果表明，HATCH在性能上显著优于同类规模的基线模型，并且在与更大模型的对比中表现竞争力，同时保持了良好的单图像推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs</div>
<div class="meta-line">Authors: Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-23T09:17:07+00:00 · Latest: 2026-02-10T07:17:25+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17653v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17653v2">PDF</a> · <a href="https://github.com/LiAuto-DSR/GeoGramBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored. In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning. To evaluate this capability, we present GeoGramBench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity. Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level. These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoGramBench：现代大语言模型几何程序推理基准测试</div>
<div class="mono" style="margin-top:8px">几何空间推理是许多人工智能应用的基础，但大语言模型（LLMs）在处理以过程代码形式表达的几何空间信息方面的能力仍被忽视。本文通过形式化程序到几何任务来解决这一问题，该任务挑战模型将程序化绘图代码转化为准确且抽象的几何推理。为了评估这一能力，我们提出了GeoGramBench，这是一个包含500个精心优化问题的基准，这些问题按照一个定制的三级分类法组织，考虑的是几何复杂性而非传统的数学推理复杂性。我们对17个前沿LLM的全面评估揭示了其在这一任务中存在一致且显著的不足：即使最先进的模型在最高抽象层级上的准确率也低于50%。这些结果突显了程序驱动空间推理所面临的独特挑战，并确立了GeoGramBench作为推进符号到空间几何推理研究的宝贵资源。项目页面：https://github.com/LiAuto-DSR/GeoGramBench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the underexplored ability of large language models (LLMs) to process geometric spatial information expressed in procedural code. The authors introduce GeoGramBench, a benchmark consisting of 500 carefully curated problems organized into a three-level taxonomy based on geometric complexity. Through comprehensive evaluation of 17 state-of-the-art LLMs, they find that even the most advanced models perform poorly, achieving less than 50% accuracy at the highest abstraction level. These findings underscore the challenges of program-driven geometric reasoning and position GeoGramBench as a critical resource for future research in this domain.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLMs）处理以程序代码形式表达的几何空间信息的能力进行研究，该能力尚未得到充分探索。作者提出了GeoGramBench基准测试，包含500个精心整理的问题，并按照几何复杂度构建了三级分类体系。通过对17个前沿LLM的全面评估，发现即使最先进的模型在最高抽象层级的准确率也低于50%。这些结果突显了程序驱动几何推理的独特挑战，并确立了GeoGramBench作为推动符号到空间几何推理研究的重要资源。</div>
</details>
</div>
<div class="card">
<div class="title">AgentCgroup: Understanding and Controlling OS Resources of AI Agents</div>
<div class="meta-line">Authors: Yusheng Zheng, Jiakun Fan, Quanzhi Fu, Yiwei Yang, Wei Zhang, Andi Quinn</div>
<div class="meta-line">First: 2026-02-10T02:37:42+00:00 · Latest: 2026-02-10T02:37:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09345v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09345v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed in multi-tenant cloud environments, where they execute diverse tool calls within sandboxed containers, each call with distinct resource demands and rapid fluctuations. We present a systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Our measurements reveal that (1) OS-level execution (tool calls, container and agent initialization) accounts for 56-74% of end-to-end task latency; (2) memory, not CPU, is the concurrency bottleneck; (3) memory spikes are tool-call-driven with a up to 15.4x peak-to-average ratio; and (4) resource demands are highly unpredictable across tasks, runs, and models. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three mismatches in existing resource controls: a granularity mismatch (container-level policies vs. tool-call-level dynamics), a responsiveness mismatch (user-space reaction vs. sub-second unpredictable bursts), and an adaptability mismatch (history-based prediction vs. non-deterministic stateful execution). We propose AgentCgroup , an eBPF-based resource controller that addresses these mismatches through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies driven by in-kernel monitoring. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentCgroup：理解与控制AI代理的OS资源</div>
<div class="mono" style="margin-top:8px">AI代理越来越多地部署在多租户云环境中，在沙箱容器中执行多样化的工具调用，每个调用具有不同的资源需求和快速波动。我们系统地分析了在沙箱AI编码代理中的OS级资源动态，基于SWE-rebench基准的144个软件工程任务，覆盖两个LLM模型。我们的测量结果表明：(1) OS级执行（工具调用、容器和代理初始化）占端到端任务延迟的56-74%；(2) 内存而非CPU是并发瓶颈；(3) 内存峰值由工具调用驱动，其峰值与平均值的比值高达15.4倍；(4) 资源需求在任务、运行和模型之间高度不可预测。通过将这些特性与无服务器、微服务和批处理工作负载进行比较，我们识别出现有资源控制存在三个不匹配：粒度不匹配（容器级策略与工具调用级动态）、响应性不匹配（用户空间反应与亚秒级不可预测的突发）、以及适应性不匹配（基于历史的预测与非确定性状态执行）。我们提出了AgentCgroup，一种基于eBPF的资源控制器，通过与工具调用边界对齐的分层cgroup结构、内核级强制执行（通过sched_ext和memcg_bpf_ops）以及由内核监控驱动的运行时自适应策略来解决这些问题。初步评估表明，该方案提升了多租户隔离性并减少了资源浪费。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing deployment of AI agents in multi-tenant cloud environments necessitates a deeper understanding of their OS-level resource usage patterns. This study characterizes the resource dynamics of sandboxed AI coding agents by analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Key findings include that OS-level execution accounts for 56-74% of task latency, memory is the main concurrency bottleneck, and resource demands are highly unpredictable. To address these challenges, the paper proposes AgentCgroup, an eBPF-based resource controller that employs hierarchical cgroup structures, in-kernel enforcement mechanisms, and runtime-adaptive policies. Preliminary results show enhanced multi-tenant isolation and reduced resource waste.</div>
<div class="mono" style="margin-top:8px">随着AI代理在多租户云环境中的广泛应用，对其操作系统级资源使用模式的研究变得尤为重要。本文通过分析SWE-rebench基准中的144个软件工程任务，研究了沙箱环境中AI编码代理的资源动态。结果表明，操作系统级执行占任务总延迟的56%-74%，内存是并发瓶颈，且资源需求具有高度不可预测性。为此，本文提出了基于eBPF的AgentCgroup资源控制器，通过与工具调用边界对齐的层级cgroup结构、内核级强制执行机制以及运行时自适应策略，有效解决了现有资源控制机制的三个不匹配问题，提升了多租户隔离度并减少了资源浪费。</div>
</details>
</div>
<div class="card">
<div class="title">Automated QoR improvement in OpenROAD with coding agents</div>
<div class="meta-line">Authors: Amur Ghose, Junyeong Jang, Andrew B. Kahng, Jakang Lee</div>
<div class="meta-line">First: 2026-01-09T19:30:02+00:00 · Latest: 2026-02-09T22:54:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06268v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06268v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, effective clock period reductions of up to 10.0%, and power reductions of up to 19.4%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在OpenROAD中使用编码代理实现自动化QoR改进</div>
<div class="mono" style="margin-top:8px">EDA开发和创新一直受到专家工程资源稀缺的限制。尽管领先的LLMs在编码和科学推理任务中表现出色，但它们在推动EDA技术进步方面的能力尚未经过充分验证。我们提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主、基于仓库的编码系统，能够读取OpenROAD，提出研究方向，将其扩展为实施步骤，并提交可执行的代码差异。我们的贡献包括：(i) 一个用于EDA代码修改的闭环LLM框架；(ii) 针对PPA导向改进的OpenROAD任务套件和评估协议；以及 (iii) 需要最少人工监督的端到端演示。在OpenROAD上的实验实现了最多5.9%的布线线长减少、10.0%的有效时钟周期减少和19.4%的功耗减少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of expert engineering resources in EDA development and innovation. The authors introduce AuDoPEDA, an autonomous coding system based on OpenAI models and a Codex-class agent that analyzes OpenROAD, suggests research directions, generates implementation steps, and submits executable code changes. Experimental results show that AuDoPEDA achieves up to 5.9% reduction in routed wirelength, 10.0% improvement in effective clock period, and 19.4% decrease in power consumption.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决电子设计自动化（EDA）开发与创新中专家工程资源稀缺的问题。作者提出了AuDoPEDA，这是一个基于OpenAI模型和Codex类代理的自主编码系统，能够分析OpenROAD，提出研究方向，生成实现步骤，并提交可执行的代码修改。实验结果表明，AuDoPEDA在布线长度、时钟周期和功耗方面分别实现了最高5.9%、10.0%和19.4%的优化。</div>
</details>
</div>
<div class="card">
<div class="title">AIDev: Studying AI Coding Agents on GitHub</div>
<div class="meta-line">Authors: Hao Li, Haoxiang Zhang, Ahmed E. Hassan</div>
<div class="meta-line">First: 2026-02-09T20:45:58+00:00 · Latest: 2026-02-09T20:45:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.
  &gt; AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIDev: 在 GitHub 上研究 AI 编码代理</div>
<div class="mono" style="margin-top:8px">AI 编码代理正在迅速改变软件工程，执行诸如功能开发、调试和测试等任务。尽管其影响日益扩大，研究界仍缺乏一个全面的数据集，以捕捉这些代理在实际项目中的使用情况。为了解决这一问题，我们引入了 AIDev，这是一个大规模的数据集，专注于实际 GitHub 仓库中由代理生成的拉取请求（Agentic-PRs）。AIDev 聚合了由五个代理（OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code）生成的 932,791 个 Agentic-PRs，这些 PRs 涉及 116,211 个仓库和 72,189 名开发者。此外，AIDev 还包含一个精选的 33,596 个 Agentic-PRs 子集，来自拥有超过 100 星标评分的 2,807 个仓库，提供了诸如评论、审查、提交和相关问题等更多信息。该数据集为未来关于 AI 接纳、开发者生产力和人机协作在软件工程新时代的研究奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of AI coding agents in software engineering has created a need for a comprehensive dataset to understand their real-world usage. AIDev is introduced as a large-scale dataset containing 932,791 agent-authored pull requests from five AI coding agents, including OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code, across 116,211 repositories and involving 72,189 developers. Additionally, it includes a curated subset of 33,596 Agentic-PRs from popular repositories with over 100 stars, offering detailed information such as comments, reviews, and related issues. This dataset provides a valuable resource for studying AI adoption, developer productivity, and human-AI collaboration in modern software engineering practices.</div>
<div class="mono" style="margin-top:8px">随着AI编码代理在软件工程领域的快速发展，研究其实际应用的全面数据集变得尤为重要。AIDev通过收集五个AI编码代理（包括OpenAI Codex、Devin、GitHub Copilot、Cursor和Claude Code）在GitHub上生成的932,791个代理拉取请求（Agentic-PRs），覆盖了116,211个仓库和72,189名开发者，填补了这一空白。此外，AIDev还包含一个精选的33,596个Agentic-PRs子集，来自拥有超过100颗星标的2,807个仓库，并附有评论、审查、提交和相关问题等元数据。该数据集为未来研究AI采纳、开发者效率和人机协作提供了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development</div>
<div class="meta-line">Authors: Pengrui Lu, Shiqi Zhang, Yunzhong Hou, Lyumanshan Ye, Chaoyi Huang, Zixi Chen, Ji Zeng, Hantao Jiang, Pengfei Liu, Yiwei Wang, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2026-02-02T05:17:23+00:00 · Latest: 2026-02-09T15:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01655v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01655v2">PDF</a> · <a href="https://github.com/zsworld6/projdevbench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProjDevBench：对AI编码代理进行端到端项目开发基准测试</div>
<div class="mono" style="margin-top:8px">最近的编码代理可以从简单提示生成完整的代码库，但现有的评估主要集中在问题级别的错误修复，未能跟上端到端开发的需求。我们引入了ProjDevBench，这是一个端到端基准测试，为编码代理提供项目需求并评估生成的代码库。该基准结合了在线判题（OJ）测试与大语言模型（LLM）辅助的代码审查，评估代理在（1）系统架构设计、（2）功能正确性以及（3）迭代解决方案优化方面的表现。我们整理了涵盖8个类别的20个编程问题，包括概念导向任务和现实应用场景，并评估了基于不同LLM后端构建的六个编码代理。我们的评估结果显示整体接受率为27.38%：代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。我们的基准测试可在https://github.com/zsworld6/projdevbench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind ProjDevBench is to address the limitations of current coding agent evaluations, which primarily focus on bug fixing rather than end-to-end project development. The benchmark introduces a comprehensive evaluation framework that combines Online Judge testing with LLM-assisted code review to assess coding agents on system architecture design, functional correctness, and iterative solution refinement. It includes 20 programming problems across 8 categories, evaluating six agents based on different LLM backends. The main experimental results show that while agents perform well on basic functionality and data structures, they struggle with complex system design, time complexity optimization, and resource management, leading to an overall acceptance rate of 27.38%.</div>
<div class="mono" style="margin-top:8px">本研究提出了ProjDevBench，这是一个用于评估AI编码代理在端到端项目开发能力的基准测试平台，而非局限于单个问题修复。该基准结合了在线评测与大语言模型辅助的代码审查，从系统架构设计、功能正确性以及迭代解决方案优化三个方面对代理进行评估。包含8个类别下的20个编程问题，涵盖概念性任务和现实应用场景。对基于不同大语言模型后端的六个编码代理进行评估，结果显示整体接受率为27.38%，表明代理在处理基本功能和数据结构时表现尚可，但在复杂系统设计、时间复杂度优化和资源管理方面存在明显困难。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Quantity: Trajectory Diversity Scaling for Code Agents</div>
<div class="meta-line">Authors: Guhong Chen, Chenghao Sun, Cheng Fu, Qiyao Wang, Zhihong Huang, Chaopeng Wei, Guangxu Chen, Feiteng Fang, Ahmadreza Argha, Bing Zhao, Xander Xu, Qi Han, Hamid Alinejad-Rokny, Qiang Qu, Binhua Li, Shiwen Ni, Min Yang, Hu Wei, Yongbin Li</div>
<div class="meta-line">First: 2026-02-03T07:43:03+00:00 · Latest: 2026-02-09T14:24:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03219v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越数量：面向代码代理的轨迹多样性扩展</div>
<div class="mono" style="margin-top:8px">随着代码大语言模型（LLMs）通过模型上下文协议（MCP）演进为工具交互代理，其泛化能力正受到低质量合成数据和数量扩展边际效益递减的限制。此外，以数量为中心的扩展方法在早期就遇到了瓶颈，未能充分利用轨迹数据。我们提出TDScaling，这是一种基于轨迹多样性的数据合成框架，通过提升多样性而非原始数据量来扩展代码代理的性能。在固定训练预算下，增加轨迹多样性所带来的性能提升比增加轨迹数量更大，从而优化了代理训练的性能成本权衡。TDScaling集成了四项创新：（1）业务聚类机制，用于捕捉真实服务中的逻辑依赖关系；（2）蓝图驱动的多代理范式，确保轨迹的一致性；（3）自适应演化机制，利用领域熵、推理模式熵和累积动作复杂度引导合成过程，防止模式坍缩；（4）沙盒环境中的代码工具，以减轻内在编码能力的灾难性遗忘。我们在通用工具使用基准（BFCL、tau^2-Bench）和代码代理任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling在提升工具使用泛化能力和内在编码能力方面均取得了显著成效。我们计划在论文发表后公开完整的代码库和合成数据集（包含30,000多个工具集群）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of code large language models (LLMs) as they transition into tool-interactive agents, where generalization is hindered by low-quality synthetic data and the diminishing returns of increasing data quantity. The proposed TDScaling framework focuses on enhancing trajectory diversity instead of data volume to improve performance under a fixed training budget. It introduces four key innovations: a Business Cluster mechanism to model real-service dependencies, a blueprint-driven multi-agent paradigm for coherent trajectory generation, an adaptive evolution mechanism using entropy metrics to avoid mode collapse, and a sandboxed code tool to preserve coding capabilities. Experimental results on multiple benchmarks and code agent tasks show that TDScaling significantly enhances both tool-use generalization and coding proficiency.</div>
<div class="mono" style="margin-top:8px">本文针对代码大语言模型（LLMs）演变为工具交互代理时所面临的合成数据质量低和数据量增长收益递减的问题，提出了一种基于轨迹多样性扩展的TDScaling框架，通过提升轨迹多样性而非单纯增加数据量来增强模型性能。TDScaling包含四项创新：业务聚类机制用于捕捉实际服务中的逻辑依赖关系，蓝图驱动的多代理范式确保轨迹的一致性，自适应演化机制利用领域熵、推理模式熵和累积动作复杂度避免模式坍缩，以及一个沙箱环境的代码工具以防止内在编码能力的遗忘。实验结果表明，TDScaling在多个基准测试和代码代理任务中有效提升了工具使用泛化能力和编码能力，实现了更好的性能与成本平衡。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260215_0344.html">20260215_0344</a>
<a href="archive/20260213_0409.html">20260213_0409</a>
<a href="archive/20260212_0416.html">20260212_0416</a>
<a href="archive/20260211_0417.html">20260211_0417</a>
<a href="archive/20260210_0423.html">20260210_0423</a>
<a href="archive/20260209_0349.html">20260209_0349</a>
<a href="archive/20260208_0340.html">20260208_0340</a>
<a href="archive/20260207_0358.html">20260207_0358</a>
<a href="archive/20260206_0359.html">20260206_0359</a>
<a href="archive/20260205_0404.html">20260205_0404</a>
<a href="archive/20260204_0407.html">20260204_0407</a>
<a href="archive/20260202_0344.html">20260202_0344</a>
<a href="archive/20260201_0339.html">20260201_0339</a>
<a href="archive/20260131_0354.html">20260131_0354</a>
<a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
