<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-31 03:54</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260131_0354</div>
    <div class="row"><div class="card">
<div class="title">StepShield: When, Not Whether to Intervene on Rogue Agents</div>
<div class="meta-line">Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 2 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepShield：何时干预而非是否干预 rogue agents</div>
<div class="mono" style="margin-top:8px">现有的智能体安全基准测试报告二元准确率，将早期干预与事后分析混为一谈。一个在第8步标记违规的检测器可以实现干预；而一个在第48步报告违规的检测器则仅具有取证价值。这一区别至关重要，但当前基准测试无法衡量。我们引入了StepShield，这是首个评估违规检测时机而非仅仅是否检测的基准测试。StepShield包含9,213个代码智能体轨迹，其中包括1,278对精心标注的训练样本和一个包含7,935个轨迹、真实 rogue 行为率为8.1%的测试集。这些 rogue 行为基于六个类别中的现实世界安全事件。我们提出了三个新颖的时间度量指标：早期干预率（EIR）、干预间隔和节省的token数。令人惊讶的是，我们的评估结果显示，基于LLM的裁判模型实现了59%的EIR，而静态分析器仅达到26%，性能差距高达2.3倍，这在标准准确率指标中完全不可见。我们进一步证明，早期检测具有直接的经济效益：我们的级联 HybridGuard 检测器可将监控成本降低75%，并在企业级规模下预计五年内累计节省1.08亿美元。通过将评估重点从是否检测转向何时检测，StepShield为构建更安全且更具经济可行性的AI智能体提供了新的基础。代码和数据在Apache 2.0许可下发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">StepShield addresses the limitation of existing agent safety benchmarks that only report binary accuracy, failing to distinguish between early intervention and post-mortem analysis. The benchmark introduces three novel temporal metrics—Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved—to evaluate when violations are detected. Experimental results show that an LLM-based judge achieves a 59% EIR, significantly outperforming a static analyzer with 26%, highlighting a 2.3x performance gap. Additionally, the cascaded HybridGuard detector demonstrates economic benefits by reducing monitoring costs by 75% and projecting $108M in savings over five years at enterprise scale.</div>
<div class="mono" style="margin-top:8px">StepShield的动机是解决现有智能体安全基准仅报告二元准确率的问题，无法区分早期干预与事后分析。该方法提出三个新的时间指标——早期干预率（EIR）、干预间隔和节省的标记数，用于评估违规行为被检测的时间而非是否发生。实验结果显示，基于大语言模型的裁判达到59%的EIR，显著优于仅26%的静态分析器，显示出2.3倍的性能差距。此外，级联的HybridGuard检测器在企业规模下展现出经济优势，可将监控成本降低75%，并预计在五年内节省1.08亿美元。</div>
</details>
</div>
<div class="card">
<div class="title">MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</div>
<div class="meta-line">Authors: Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen</div>
<div class="meta-line">First: 2026-01-29T17:52:41+00:00 · Latest: 2026-01-29T17:52:41+00:00</div>
<div class="meta-line">Comments: Project Page: https://metric-anything.github.io/metric-anything-io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://metric-anything.github.io/metric-anything-io/">Project1</a> · <a href="http://metric-anything.github.io/metric-anything-io/">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetricAnything: 通过噪声异构源扩展度量深度预训练</div>
<div class="mono" style="margin-top:8px">扩展已推动了视觉基础模型的近期进展，但将这一范式扩展到度量深度估计仍面临挑战，原因包括异构传感器噪声、相机依赖性偏差以及噪声跨源3D数据中的度量模糊性。我们引入了Metric Anything，这是一个简单且可扩展的预训练框架，无需手动设计提示、相机特定建模或任务特定架构，即可从噪声且多样的3D源中学习度量深度。我们的方法核心是稀疏度量提示，通过随机掩码深度图创建，它作为通用接口，将空间推理与传感器和相机偏差解耦。利用约2000万张图像-深度对，涵盖10000个相机模型的重建、捕获和渲染3D数据，我们首次展示了度量深度领域中的明确扩展趋势。预训练模型在提示驱动的任务如深度补全、超分辨率和雷达-相机融合中表现出色，而其蒸馏后的无提示学生模型在单目深度估计、相机内参恢复、单/多视角度量3D重建和VLA规划中达到了最先进的结果。我们还表明，使用Metric Anything预训练的ViT作为视觉编码器，显著提升了多模态大语言模型在空间智能方面的能力。这些结果表明，度量深度估计可以从驱动现代基础模型的相同扩展规律中受益，为可扩展且高效的现实世界度量感知开辟了新路径。我们开源了MetricAnything，网址为http://metric-anything.github.io/metric-anything-io/，以支持社区研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenges of scaling metric depth estimation using noisy and heterogeneous 3D data sources. The authors propose Metric Anything, a pretraining framework that learns metric depth without relying on manual prompts, camera-specific models, or task-specific architectures. A key component of their method is the Sparse Metric Prompt, which masks depth maps to create a universal interface for decoupling spatial reasoning from sensor and camera biases. Using approximately 20 million image-depth pairs across 10,000 camera models, they demonstrate a clear scaling trend in metric depth estimation. The pretrained model performs well in various downstream tasks, and its distilled version achieves state-of-the-art results in monocular depth estimation and related 3D perception tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过利用噪声和异构的3D数据源来解决度量深度估计的扩展难题。提出的Metric Anything方法引入了一个可扩展的预训练框架，能够在不依赖手动提示、相机特定建模或任务特定架构的情况下学习度量深度。其核心组件是Sparse Metric Prompt，通过随机掩码深度图来解耦空间推理与传感器和相机偏差。使用来自多种3D数据类型和相机模型的约2000万张图像-深度对，该框架首次展示了度量深度估计中的明确扩展趋势。预训练模型在深度补全、雷达-相机融合等提示驱动任务中表现优异，而其蒸馏后的无提示版本在单目深度估计、相机内参恢复、单/多视角度量3D重建以及VLA规划任务中达到了最先进的结果。此外，将Metric Anything作为视觉编码器用于多模态大语言模型，显著提升了其空间智能能力。</div>
</details>
</div>
<div class="card">
<div class="title">Causal World Modeling for Robot Control</div>
<div class="meta-line">Authors: Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</div>
<div class="meta-line">First: 2026-01-29T17:07:43+00:00 · Latest: 2026-01-29T17:07:43+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-va Code: https://github.com/robbyant/lingbot-va</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21998v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-va">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于机器人控制的因果世界建模</div>
<div class="mono" style="margin-top:8px">本工作强调，视频世界建模与视觉-语言预训练共同为机器人学习建立了新颖且独立的基础。直观来说，视频世界模型通过理解动作与视觉动态之间的因果关系，提供了对近未来进行想象的能力。受此启发，我们引入了LingBot-VA，这是一种自回归扩散框架，能够同时学习帧预测和策略执行。我们的模型包含三个精心设计的组件：(1) 由混合变换器（MoT）架构驱动的共享潜在空间，整合了视觉和动作标记；(2) 闭环滚动机制，允许持续获取环境反馈和真实观测；(3) 异步推理流水线，通过并行化动作预测和运动执行来支持高效控制。我们在模拟基准和现实场景中评估了该模型，结果显示其在长时序操作、训练后数据效率以及对新配置的强泛化能力方面具有显著潜力。代码和模型已公开，以促进社区发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to enhance robot control by leveraging video world modeling in conjunction with vision-language pre-training, offering a new approach to robot learning. The proposed LingBot-VA framework employs an autoregressive diffusion model that integrates frame prediction and policy execution through a shared latent space using a Mixture-of-Transformers architecture. It also incorporates a closed-loop rollout mechanism for continuous environmental feedback and an asynchronous inference pipeline to parallelize action prediction and motor execution. Experimental results on both simulated and real-world tasks demonstrate the model&#x27;s effectiveness in long-term manipulation, data efficiency, and adaptability to new environments.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过结合视频世界建模与视觉语言预训练，为机器人学习提供一种新的方法。提出的LingBot-VA框架采用自回归扩散模型，通过共享潜在空间、Mixture-of-Transformers架构和闭环回滚机制，同时学习帧预测和策略执行。异步推理管道实现了动作预测与运动执行的并行处理，提高了控制效率。实验结果表明，该模型在长期操作任务中表现出色，训练后数据效率高，并且能够良好地适应新环境。</div>
</details>
</div>
<div class="card">
<div class="title">From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation</div>
<div class="meta-line">Authors: Javier Argota Sánchez-Vaquerizo, Luis Borunda Monsivais</div>
<div class="meta-line">First: 2026-01-29T16:54:18+00:00 · Latest: 2026-01-29T16:54:18+00:00</div>
<div class="meta-line">Comments: Paper selected for the workshop Human Cognition, AI, and the Future of HCI: Navigating the Disruptive and Wild Landscape of Large Language Models and Agentic AI as part of the Human-Computer Interaction (HCI) conference of the Alpine region (AlpCHI 2026) hosted at the Congressi Stefano Franscini, March 1st to March 5th, 2026 on Monte Verità in Ascona, Switzerland</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21977v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based &quot;particles&quot; rather than cognitive &quot;agents&quot;. To bridge this, we introduce \textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \textbf{Cognitive Friction} ($C_f$) it is possible to reveal &quot;Phantom Affordances&quot;, i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从粒子到智能体：幻觉作为空间模拟中认知摩擦的度量</div>
<div class="mono" style="margin-top:8px">传统的建筑模拟（如计算流体动力学、疏散模拟、结构分析）将元素建模为基于确定性物理的&quot;粒子&quot;，而非具有认知能力的&quot;智能体&quot;。为弥合这一差距，我们引入\textbf{智能体环境模拟}，其中大型多模态生成模型基于语义预期主动预测空间环境的下一状态。借助面向无障碍性的增强现实（AR）流程和多模态数字孪生的实例，我们提出从时间步进（time-steps）向事件驱动的空间推理（Episodic Spatial Reasoning）转变，其中模拟通过有意义且由惊讶触发的事件推进。在此框架下，我们提出将人工智能幻觉作为诊断工具。通过形式化\textbf{认知摩擦}（$C_f$），可以揭示&quot;幽灵可操作性&quot;（Phantom Affordances），即建筑空间中的语义模糊性。最后，我们通过将环境视为动态的认知伙伴来挑战当前人机交互（HCI）范式，并提出以人类为中心的认知编排框架，用于设计AI驱动的模拟，以保持自主性、情感清晰度和认知完整性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of traditional architectural simulations that treat elements as deterministic particles rather than cognitive agents. It introduces Agentic Environmental Simulations, which leverage large multimodal generative models to predict spatial environment states based on semantic expectations. The study proposes a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations progress through meaningful events triggered by surprisal. By formalizing Cognitive Friction ($C_f$), the research identifies Phantom Affordances—semiotic ambiguities in built environments—and suggests using AI hallucinations as diagnostic tools. The findings challenge current HCI paradigms by framing environments as dynamic cognitive partners and advocate for a human-centered framework of cognitive orchestration to design AI-driven simulations that maintain autonomy, affective clarity, and cognitive integrity.</div>
<div class="mono" style="margin-top:8px">本文探讨了传统建筑模拟方法的局限性，这些方法将元素视为确定性的物理粒子，而非具有认知能力的智能体。研究提出了Agentic Environmental Simulations，利用大型多模态生成模型基于语义期望预测空间环境的下一状态。通过将模拟从时间步进式推进转变为基于有意义事件的Episodic Spatial Reasoning，文章引入了Cognitive Friction（$C_f$）这一概念，以识别建筑空间中的Phantom Affordances，即语义上的模糊性。研究还挑战了现有的人机交互范式，提出将环境视为动态认知伙伴，并倡导以人类为中心的认知编排框架，用于设计AI驱动的模拟系统，以保持自主性、情感清晰度和认知完整性。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning</div>
<div class="meta-line">Authors: Jinjun Peng, Magnus Saebo, Tianjun Zhong, Yi-Jie Cheng, Junfeng Yang, Baishakhi Ray, Simin Chen, Yangruibo Ding</div>
<div class="meta-line">First: 2026-01-29T12:49:25+00:00 · Latest: 2026-01-29T12:49:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21649v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the &quot;physics&quot; of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Spot：通过以仓库为中心的学习构建小型仓库专家</div>
<div class="mono" style="margin-top:8px">在隐私敏感和资源受限的环境中部署编码代理，推动了具备强大能力的开放权重小型语言模型（SLMs）的需求。然而，它们存在一个根本的能力差距：与前沿大模型不同，它们缺乏推理时的强泛化能力，难以处理复杂且陌生的代码库。我们发现，当前以任务为中心的学习（TCL）范式，通过跨不同仓库的暴露来扩展模型，未能解决这一限制。为此，我们提出了以仓库为中心的学习（RCL），这是一种范式转变，强调垂直仓库深度而非水平任务广度，建议SLMs必须通过参数化知识获取来内化目标软件环境的“物理特性”，而不是通过昂贵的推理时搜索来恢复。基于这一新范式，我们设计了一个四单元的以仓库为中心的学习体验，将静态代码库转化为交互式学习信号，以训练SWE-Spot-4B，这是一组高度紧凑的模型，作为仓库专用专家构建，突破了已建立的扩展趋势，在多个软件工程任务中优于开放权重模型（如Meta的CWM、Qwen3-Coder-30B），并超越或匹配以效率为导向的商业模型（如GPT-4.1-mini、GPT-5-nano）。进一步分析表明，RCL在训练样本效率和推理成本方面均表现更优，强调了在构建高效智能时，仓库掌握是与通用编码能力相辅相成的一个独特且必要的维度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing need for coding agents in privacy-sensitive and resource-constrained settings highlights the importance of developing capable small language models (SLMs). Traditional Task-Centric Learning (TCL) approaches, which spread training across diverse repositories, fail to address the SLMs&#x27; inability to generalize effectively to complex and unfamiliar codebases. To overcome this, the authors introduce Repository-Centric Learning (RCL), which focuses on deep understanding of specific codebases rather than broad task coverage. They design a four-unit RCL experience to train SWE-Spot-4B, a compact model family that outperforms open-weight models and matches or exceeds efficiency-focused commercial models in multiple software engineering tasks, demonstrating higher training efficiency and lower inference costs.</div>
<div class="mono" style="margin-top:8px">随着在隐私敏感和资源受限环境中对编码代理的需求增加，开发具备能力的小型语言模型（SLMs）变得尤为重要。传统的任务中心学习（TCL）方法在处理复杂和陌生代码库时存在根本性的能力差距。为此，本文提出了以仓库为中心的学习（RCL）范式，强调对特定仓库的深入理解而非广泛任务覆盖。通过四单元的RCL学习体验，作者训练了SWE-Spot-4B这一紧凑型模型家族，在多个软件工程任务中表现优于开放权重模型，并与专注于效率的商业模型相匹配或超越，显示出更高的训练样本效率和更低的推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">RSGround-R1: Rethinking Remote Sensing Visual Grounding through Spatial Reasoning</div>
<div class="meta-line">Authors: Shiqi Huang, Shuting He, Bihan Wen</div>
<div class="meta-line">First: 2026-01-29T12:35:57+00:00 · Latest: 2026-01-29T12:35:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21634v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote Sensing Visual Grounding (RSVG) aims to localize target objects in large-scale aerial imagery based on natural language descriptions. Owing to the vast spatial scale and high semantic ambiguity of remote sensing scenes, these descriptions often rely heavily on positional cues, posing unique challenges for Multimodal Large Language Models (MLLMs) in spatial reasoning. To leverage this unique feature, we propose a reasoning-guided, position-aware post-training framework, dubbed \textbf{RSGround-R1}, to progressively enhance spatial understanding. Specifically, we first introduce Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) using synthetically generated RSVG reasoning data to establish explicit position awareness. Reinforcement Fine-Tuning (RFT) is then applied, augmented by our newly designed positional reward that provides continuous and distance-aware guidance toward accurate localization. Moreover, to mitigate incoherent localization behaviors across rollouts, we introduce a spatial consistency guided optimization scheme that dynamically adjusts policy updates based on their spatial coherence, ensuring stable and robust convergence. Extensive experiments on RSVG benchmarks demonstrate superior performance and generalization of our model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RSGround-R1: 通过空间推理重新思考遥感视觉定位</div>
<div class="mono" style="margin-top:8px">遥感视觉定位（RSVG）旨在根据自然语言描述在大规模航拍图像中定位目标物体。由于遥感场景具有广阔的空间尺度和高度语义模糊性，这些描述通常高度依赖于位置线索，给多模态大语言模型（MLLMs）的空间推理带来了独特挑战。为利用这一特性，我们提出了一种推理引导、位置感知的后训练框架，称为\textbf{RSGround-R1}，以逐步增强空间理解能力。具体而言，我们首先引入基于合成生成RSVG推理数据的思维链监督微调（CoT-SFT）来建立明确的位置感知。随后，我们应用强化微调（RFT），并结合我们新设计的位置奖励机制，以提供连续且距离感知的指导，实现精准定位。此外，为缓解不同 rollout 之间的不一致定位行为，我们引入了一种空间一致性引导的优化方案，根据其空间一致性动态调整策略更新，确保稳定且鲁棒的收敛。在RSVG基准上的大量实验表明，我们的模型在性能和泛化能力上均表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of localizing target objects in large-scale aerial imagery using natural language descriptions, a task known as Remote Sensing Visual Grounding (RSVG). The study highlights the difficulties posed by the vast spatial scale and high semantic ambiguity of remote sensing scenes, which often require heavy reliance on positional cues. To tackle these issues, the authors propose RSGround-R1, a reasoning-guided, position-aware post-training framework. It incorporates Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) with synthetic RSVG data to build position awareness, followed by Reinforcement Fine-Tuning (RFT) with a novel positional reward mechanism that provides continuous, distance-aware feedback. Additionally, a spatial consistency guided optimization is introduced to ensure coherent and stable localization across different rollouts. Experimental results on RSVG benchmarks show that the model achieves superior performance and better generalization compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">本文针对遥感视觉定位（RSVG）任务中的挑战，即在大规模航拍图像中根据自然语言描述定位目标物体。由于遥感场景空间尺度大且语义模糊，描述通常依赖于位置线索，这对多模态大语言模型（MLLMs）的空间推理能力提出了特殊要求。作者提出了一种名为RSGround-R1的推理引导、位置感知的后训练框架，通过合成生成的RSVG推理数据进行链式思维监督微调（CoT-SFT），建立明确的位置感知能力，并结合强化微调（RFT）和新设计的位置奖励机制，提供连续且距离感知的定位指导。此外，引入了空间一致性引导的优化方案，动态调整策略更新以确保不同 rollout 的定位行为一致且稳定。在多个 RSVG 数据集上的实验表明，RSGround-R1 在性能和泛化能力上均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization</div>
<div class="meta-line">Authors: Alireza Nadaf, Alireza Mohammadshahi, Majid Yazdani</div>
<div class="meta-line">First: 2026-01-29T10:40:54+00:00 · Latest: 2026-01-29T10:40:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21526v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21526v1">PDF</a> · <a href="https://github.com/Leeroo-AI/kapso">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAPSO：一种基于知识的自主程序合成与优化框架</div>
<div class="mono" style="margin-top:8px">我们引入了KAPSO，一种用于自主程序合成与优化的模块化框架。给定一个自然语言目标和评估方法，KAPSO迭代地执行构思、代码合成与编辑、执行、评估和学习，以改进可运行的产物，使其朝着可衡量的目标发展。KAPSO不将合成视为终点，而是将其作为长周期优化循环中的一个操作符，其中进展由评估器的结果定义。
KAPSO通过整合三个紧密耦合的组件，针对编码代理中常见的长周期失败问题，包括实验状态丢失、脆弱的调试和领域专业知识的弱复用。首先，一个原生支持Git的实验引擎将每次尝试隔离为一个分支，生成可复现的产物并保留迭代间的溯源信息。其次，一个知识系统吸收异构来源，包括仓库、内部操作手册以及经过整理的外部资源，如文档、科学论文和网络搜索结果，并将它们组织成结构化的表示，以支持对工作流、实现和环境约束的检索。最后，一个认知记忆层协调检索，并维护一个可复用的事件记忆库，该记忆库从实验轨迹（运行日志、差异和评估反馈）中提炼出可复用的经验教训，减少重复的错误模式并加快收敛速度。
我们在MLE-Bench（Kaggle风格的机器学习竞赛）和ALE-Bench（AtCoder启发式优化）上评估了KAPSO，并报告了端到端的性能表现。
代码地址：https://github.com/Leeroo-AI/kapso</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind KAPSO is to address long-horizon failures in coding agents, such as lost experimental state, brittle debugging, and weak reuse of domain knowledge. KAPSO introduces a modular framework that treats program synthesis as an iterative operator within an optimization loop, combining ideation, code synthesis and editing, execution, evaluation, and learning. The framework integrates three key components: a git-native experimentation engine for reproducibility and provenance tracking, a knowledge system that organizes heterogeneous sources into a structured representation, and a cognitive memory layer that stores reusable lessons from past experiments. Experimental results on MLE-Bench and ALE-Bench demonstrate improved end-to-end performance in achieving measurable objectives through autonomous program synthesis and optimization.</div>
<div class="mono" style="margin-top:8px">KAPSO的提出旨在解决编码代理中常见的长周期失败问题，如实验状态丢失、调试脆弱性和领域知识重用不足。该框架采用模块化设计，融合了git原生的实验引擎、整合异构信息源的知识系统以及一个从实验轨迹中提取可重用经验的认知记忆层。这些组件协同工作，支持程序的自主合成与优化，通过反复执行、评估和学习，依据评估器反馈逐步提升可运行产物的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</div>
<div class="meta-line">Authors: Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T08:15:00+00:00 · Latest: 2026-01-29T08:38:27+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026, URL: https://github.com/AMAP-ML/SpatialGenEval</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20354v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20354v2">PDF</a> · <a href="https://github.com/AMAP-ML/SpatialGenEval">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>各就各位：文本到图像模型空间智能基准测试</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型在生成高保真图像方面取得了显著成功，但在处理复杂的空间关系（如空间感知、推理或交互）时往往表现不佳。这些关键方面由于当前基准测试的提示设计较短或信息稀疏而被忽视。本文我们引入了SpatialGenEval，这是一个新的基准测试，旨在系统评估T2I模型的空间智能，涵盖两个关键方面：(1) SpatialGenEval包含25个现实场景中的1,230个长且信息密集的提示，每个提示整合了10个空间子领域和对应的10个多选问答对，涵盖从物体位置和布局到遮挡和因果关系等多个方面。我们对21个最先进的模型进行了广泛评估，发现高阶空间推理仍然是主要瓶颈。(2) 为了证明我们信息密集设计的实用性不仅限于简单的评估，我们还构建了SpatialT2I数据集。该数据集包含15,400个文本-图像对，通过重写提示确保图像一致性，同时保持信息密度。在当前基础模型（如Stable Diffusion-XL、Uniworld-V1、OmniGen2）上微调的结果显示出一致的性能提升（+4.2%、+5.7%、+4.4%），并增强了空间关系的现实效果，突显了以数据为中心的范式在实现T2I模型空间智能中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing benchmarks in evaluating the spatial intelligence of text-to-image models, which often neglect complex spatial relationships such as perception, reasoning, and interaction. The authors introduce SpatialGenEval, a comprehensive benchmark with 1,230 long, information-dense prompts across 25 real-world scenes, each incorporating 10 spatial sub-domains and 10 multi-choice questions. Their evaluation of 21 state-of-the-art models shows that higher-order spatial reasoning remains a significant challenge. Additionally, they propose the SpatialT2I dataset, which contains 15,400 text-image pairs with rewritten prompts to maintain information density while ensuring image consistency. Fine-tuning on this dataset leads to consistent performance improvements across models, demonstrating the effectiveness of a data-centric approach in enhancing spatial intelligence.</div>
<div class="mono" style="margin-top:8px">本文针对当前文本到图像模型在处理复杂空间关系方面的不足，指出现有基准测试因提示语简短或信息稀疏而忽视了这些关键能力。作者提出了SpatialGenEval，这是一个包含25个现实场景、1230个信息密集型提示语的全面基准测试，每个提示语整合了10个空间子领域和10组多选问答对。对21个最先进的模型的评估表明，高阶空间推理仍是主要瓶颈。此外，本文还构建了SpatialT2I数据集，通过重写提示语确保图像一致性，同时保持信息密度，使Stable Diffusion-XL、Uniworld-V1和OmniGen2等模型的性能提升了4.2%至5.7%。</div>
</details>
</div>
<div class="card">
<div class="title">NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents</div>
<div class="meta-line">Authors: Yang Song, Anoushka Vyas, Zirui Wei, Sina Khoshfetrat Pakazad, Henrik Ohlsson, Graham Neubig</div>
<div class="meta-line">First: 2026-01-29T07:57:23+00:00 · Latest: 2026-01-29T07:57:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21372v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21372v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEMO：通过自主编码代理实现执行感知的优化建模</div>
<div class="mono" style="margin-top:8px">本文中，我们提出了NEMO，一个将决策问题自然语言描述转换为可执行的数学优化形式的系统，能够与用户协作或自主运行。现有方法通常依赖于专门的大型语言模型（LLMs）或定制化的任务特定代理。这些方法往往脆弱、复杂，并且经常生成语法错误或不可执行的代码。
NEMO则专注于与自主编码代理（ACAs）的远程交互，将其视为类似于基于API与LLMs交互的第一类抽象。这种设计使得围绕ACAs构建更高层次的系统成为可能，这些系统能够结构化、整合并迭代地优化任务规范。由于ACAs在沙箱环境中执行，NEMO生成的代码在构建时即具备可执行性，从而支持自动验证和修复。
在此基础上，我们引入了与ACAs之间以及跨ACAs的新颖协调模式，包括独立生成的优化器和模拟器实现之间的非对称验证循环（作为高级验证机制）、用于经验重用的外部记忆，以及通过最小贝叶斯风险（MBR）解码和自一致性增强的鲁棒性。我们在九个已有的优化基准上评估了NEMO。如图1所示，它在大多数任务上实现了最先进的性能，在多个数据集上具有显著优势，展示了执行感知代理架构在自动化优化建模中的强大能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of NEMO is to address the limitations of existing methods in translating natural language into executable optimization models, which often result in syntactically invalid or non-executable code. NEMO introduces a system that collaborates with or operates autonomously using autonomous coding agents (ACAs), treating them as a first-class abstraction similar to API-based interactions with large language models. This approach allows for structured, consolidated, and iteratively refined task specifications. The system employs asymmetric validation loops, external memory for experience reuse, and robustness improvements through minimum Bayes risk decoding and self-consistency. Evaluation on nine optimization benchmarks shows that NEMO achieves state-of-the-art performance on most tasks, with significant improvements on several datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有方法在将自然语言描述的决策问题转化为可执行优化模型时存在的不足，这些方法常导致语法错误或不可执行的代码。NEMO提出了一种系统，通过与自主编码代理（ACAs）协作或自主运行，将ACAs视为类似与大型语言模型交互的API的一级抽象。这使得任务规范能够被结构化、整合并逐步优化。系统引入了新颖的ACAs协调模式，包括不对称验证循环、外部记忆用于经验复用以及基于最小贝叶斯风险（MBR）的解码以增强鲁棒性。在九个优化基准上的评估表明，NEMO在大多数任务上达到了最先进的性能，在多个数据集上取得了显著的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Large Vision Models Can Solve Mental Rotation Problems</div>
<div class="meta-line">Authors: Sebastian Ray Mason, Anders Gjølbye, Phillip Chavarria Højbjerg, Lenka Tětková, Lars Kai Hansen</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-18T11:18:28+00:00 · Latest: 2026-01-29T05:53:48+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15271v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.15271v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mental rotation is a key test of spatial reasoning in humans and has been central to understanding how perception supports cognition. Despite the success of modern vision transformers, it is still unclear how well these models develop similar abilities. In this work, we present a systematic evaluation of ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from simple block structures similar to those used by Shepard and Metzler to study human cognition, to more complex block figures, three types of text, and photo-realistic objects. By probing model representations layer by layer, we examine where and how these networks succeed. We find that i) self-supervised ViTs capture geometric structure better than supervised ViTs; ii) intermediate layers perform better than final layers; iii) task difficulty increases with rotation complexity and occlusion, mirroring human reaction times and suggesting similar constraints in embedding space representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型视觉模型可以解决心理旋转问题</div>
<div class="mono" style="margin-top:8px">心理旋转是衡量人类空间推理能力的关键测试，也是理解知觉如何支持认知的核心。尽管现代视觉变换器取得了成功，但这些模型在发展类似能力方面表现如何仍不清楚。在本研究中，我们系统地评估了ViT、CLIP、DINOv2和DINOv3在一系列心理旋转任务中的表现，从类似Shepard和Metzler用于研究人类认知的简单积木结构，到更复杂的积木图形、三种类型的文本以及逼真的照片对象。通过逐层探测模型表示，我们分析了这些网络在何处以及如何成功。我们发现：i) 自监督的ViT比监督的ViT更能捕捉几何结构；ii) 中间层的表现优于最终层；iii) 任务难度随着旋转复杂度和遮挡程度的增加而增加，与人类反应时间相呼应，表明嵌入空间表示中存在类似的约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the ability of large vision models to solve mental rotation problems, a fundamental aspect of spatial reasoning. The researchers evaluate ViT, CLIP, DINOv2, and DINOv3 across various mental rotation tasks, ranging from simple block structures to complex figures and photo-realistic objects. Their analysis reveals that self-supervised ViTs outperform supervised ones in capturing geometric structure, intermediate layers show better performance than final layers, and task difficulty correlates with rotation complexity and occlusion, reflecting human cognitive constraints.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型视觉模型在执行心理旋转任务中的能力，这些任务对于评估空间推理至关重要。研究人员对ViT、CLIP、DINOv2和DINOv3等视觉变换器模型在多种心理旋转任务中的表现进行了系统评估，任务范围从简单的块状结构到复杂的图形和逼真照片中的物体。分析结果表明，自监督的ViT在捕捉几何结构方面优于监督模型，中间层的表现优于最终层，任务难度随着旋转复杂度和遮挡程度的增加而上升，与人类反应时间相呼应，暗示模型表示空间中存在与人类认知相似的限制。</div>
</details>
</div>
<div class="card">
<div class="title">Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs</div>
<div class="meta-line">Authors: Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang</div>
<div class="meta-line">First: 2026-01-29T03:53:25+00:00 · Latest: 2026-01-29T03:53:25+00:00</div>
<div class="meta-line">Comments: 24 pages, 6 figures, 17 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>只需提问：好奇的代码代理在前沿大语言模型中揭示系统提示</div>
<div class="mono" style="margin-top:8px">基于大语言模型的自主代码代理通过工具使用、长视野推理和自主交互正在重塑软件和人工智能开发。然而，这种自主性引入了一种之前未被认识的安全风险：代理交互从根本上扩大了大语言模型的攻击面，使得系统提示的系统性探测和恢复成为可能。我们识别出系统提示提取是一种内嵌于代码代理中的新兴漏洞，并提出了\textbf{\textsc{JustAsk}}，一个自我演进的框架，它仅通过交互就能自主发现有效的提取策略。与以往基于提示工程或数据集的攻击不同，\textsc{JustAsk} 不需要人工编写的提示、标记的监督或超出标准用户交互的特权访问。它将提取过程建模为一个在线探索问题，使用基于上置信界（Upper Confidence Bound）的策略选择方法，并在一个涵盖原子探测和高级编排的分层技能空间中进行操作。这些技能利用了系统指令泛化中的不完美性以及助人与安全之间的固有张力。我们在\textbf{41}个黑盒商业模型上进行了评估，这些模型来自多个提供商，\textsc{JustAsk} 一致实现了完整或接近完整的系统提示恢复，揭示了在现代代理系统中反复出现的设计和架构层面的漏洞。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of autonomous code agents based on large language models (LLMs) in software and AI development has raised concerns about security. This study identifies a new vulnerability where these agents can be systematically probed to reveal hidden system prompts that influence their behavior. To address this, the authors introduce JustAsk, a self-evolving framework that autonomously discovers extraction strategies through interaction without requiring handcrafted prompts or privileged access. Experimental results show that JustAsk successfully recovers full or near-complete system prompts across 41 black-box commercial models, highlighting recurring design and architecture-level weaknesses in modern agent systems.</div>
<div class="mono" style="margin-top:8px">随着基于大语言模型（LLM）的自主代码代理在软件和人工智能开发中的广泛应用，其安全性问题日益受到关注。这些代理依赖系统提示来指导行为，而其交互过程可能被系统性地利用以恢复隐藏的提示信息。为此，作者提出了JustAsk，这是一种能够通过自主交互发现提取策略的自我演进框架，无需人工设计提示或特权访问。实验在41个黑盒商业模型上进行，结果表明JustAsk能够稳定地恢复完整的或接近完整的系统提示，揭示了现代代理系统中设计和架构层面的常见漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs</div>
<div class="meta-line">Authors: Artem Dementyev, Wazeer Zulfikar, Sinan Hersek, Pascal Getreuer, Anurag Kumar, Vivek Kumar</div>
<div class="meta-line">First: 2026-01-28T23:39:31+00:00 · Latest: 2026-01-28T23:39:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21124v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over &quot;Spatial Audio Tokens&quot; produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhaseCoder：面向多模态大语言模型的麦克风几何无关空间音频理解</div>
<div class="mono" style="margin-top:8px">当前多模态大语言模型将音频处理为单声道流，忽略了对具身AI至关重要的丰富空间信息。相反，现有的空间音频模型受限于固定的麦克风几何结构，无法在多种设备上部署。我们提出了PhaseCoder，这是一种仅基于Transformer的空间音频编码器，与麦克风几何无关。PhaseCoder以原始多通道音频和麦克风坐标作为输入，执行定位并生成稳健的空间嵌入。我们展示了Gemma 3n大语言模型可以微调以推理由PhaseCoder生成的&quot;空间音频标记&quot;。我们在麦克风不变定位基准测试中展示了我们的编码器达到最先进的结果，并首次实现了大语言模型从任意麦克风阵列进行复杂空间推理和定向转录任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance the spatial audio understanding capabilities of multimodal large language models (LLMs) by overcoming the limitations of processing audio as a mono stream. PhaseCoder, a transformer-based spatial audio encoder, is designed to be independent of microphone geometry, allowing it to handle raw multichannel audio and microphone coordinates to generate spatial embeddings. The main experimental results show that PhaseCoder achieves state-of-the-art performance on microphone-invariant localization tasks and enables LLMs like Gemma 3n to perform complex spatial reasoning and targeted transcription from arbitrary microphone arrays.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提升多模态大语言模型（LLMs）对空间音频的理解能力，克服现有方法将音频视为单声道以及空间音频模型受限于固定麦克风配置的不足。提出了一种名为PhaseCoder的基于Transformer的空间音频编码器，该编码器不依赖麦克风几何结构。PhaseCoder通过接收原始多通道音频和麦克风坐标，生成可用于定位的空间嵌入。实验结果表明，PhaseCoder在麦克风不变定位基准测试中达到了最先进的性能，并首次实现了LLM通过PhaseCoder生成的空间音频标记进行复杂空间推理和定向转录任务。</div>
</details>
</div>
<div class="card">
<div class="title">Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve</div>
<div class="meta-line">Authors: Hongzheng Chen, Alexander Novikov, Ngân Vũ, Hanna Alam, Zhiru Zhang, Aiden Grossman, Mircea Trofin, Amir Yazdanbakhsh</div>
<div class="meta-line">First: 2026-01-28T22:34:56+00:00 · Latest: 2026-01-28T22:34:56+00:00</div>
<div class="meta-line">Comments: Accepted to C4ML@CGO&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21096v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Magellan：使用AlphaEvolve自主发现新型编译器优化启发式方法</div>
<div class="mono" style="margin-top:8px">现代编译器依赖人工设计的启发式方法来指导优化过程。这些由人类设计的规则往往难以适应现代软件和硬件的复杂性，导致维护负担沉重。为了解决这一挑战，我们提出了Magellan，一个代理框架，通过合成可执行的C++决策逻辑来进化编译器本身。Magellan在一个生成、用户提供的宏基准评估和优化的闭环中，将大型语言模型编码代理与进化搜索和自动调优结合，生成可以直接集成到现有编译器中的紧凑启发式方法。在多个生产优化任务中，Magellan发现的策略在匹配或超越专家基线方面表现出色。在LLVM函数内联中，Magellan合成的新启发式方法在二进制大小缩减和端到端性能方面均优于数十年的人工工程成果。在寄存器分配中，它学习了一个简洁的优先规则，用于处理活跃区间，在大规模工作负载中与复杂的人工设计策略表现一致。我们还报告了XLA问题的初步结果，展示了在减少工程工作量的情况下，Magellan在LLVM之外的可移植性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the limitations of hand-crafted heuristics in modern compilers, which are difficult to maintain and adapt to evolving software and hardware complexities. Magellan introduces an agentic framework that combines an LLM coding agent with evolutionary search and autotuning to automatically generate and refine compiler optimization heuristics. The framework operates in a closed loop, generating decision logic, evaluating it on macro-benchmarks, and iteratively improving it. Experimental results show that Magellan discovers heuristics that match or exceed expert-designed ones in LLVM function inlining and register allocation, achieving better binary-size reduction and end-to-end performance. It also demonstrates promising results in XLA problems, highlighting its potential for broader applicability with minimal engineering effort.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现代编译器中手工设计启发式规则的局限性，这些规则难以适应复杂的软件和硬件环境，并且维护成本高。Magellan提出了一种代理框架，结合LLM编码代理、进化搜索和自动调优，以自动产生和优化编译器优化启发式规则。该框架通过生成、评估和迭代改进的闭环流程运作。实验结果表明，在LLVM函数内联和寄存器分配等任务中，Magellan发现的启发式规则在性能和二进制大小缩减方面不亚于甚至优于专家设计的规则，且工程投入较少。</div>
</details>
</div>
<div class="card">
<div class="title">City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</div>
<div class="meta-line">Authors: Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</div>
<div class="meta-line">First: 2025-12-17T19:59:31+00:00 · Latest: 2026-01-28T19:40:38+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15933v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15933v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dwipddalal.github.io/AgentNav/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environment. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs, reasoning techniques (e.g., GEPA, chain-of-thought, reflection) and competitive baseline PReP significantly underperform in this challenging setting. To address this, we propose Verbalization of Path(VoP), which explicitly grounds the agent&#x27;s internal reasoning by probing city-scale cognitive maps (key landmarks and directions toward the destination) from the MLLM, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外城市导航：从大规模知识中探索MLLMs的涌现导航</div>
<div class="mono" style="margin-top:8px">利用多模态大语言模型（MLLMs）开发具身智能体为解决复杂的现实世界任务提供了巨大潜力。然而，当前的评估基准主要以语言为中心或严重依赖模拟环境，很少涉及实际场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们引入了稀疏锚定视觉导航任务，专门设计用于评估MLLMs在具有挑战性和知识密集型的现实环境中进行序列决策的能力。我们通过CityNav这一涵盖四个不同全球城市的综合性基准来实现该任务，专门用于评估原始的MLLM驱动智能体在城市导航中的表现。智能体必须仅依靠视觉输入和内部多模态推理，完成50多个决策点的序列导航，而无需额外的环境标注或专门的架构修改。关键的是，智能体必须通过解读城市特定线索和识别地标自主实现定位，进行空间推理，并战略性地规划和执行前往目标的路线。通过广泛的评估，我们表明当前最先进的MLLMs、推理技术（如GEPA、思维链、反思）以及有竞争力的基线模型PReP在这一具有挑战性的环境中表现显著不足。为了解决这一问题，我们提出了路径语言化（Verbalization of Path, VoP），通过从MLLM中探查城市级认知地图（关键地标和朝向目标的方向）来显式锚定智能体的内部推理，从而显著提升导航成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of evaluating multimodal large language models (MLLMs) in real-world navigation tasks, where current benchmarks are insufficient. The authors introduce the Sparsely Grounded Visual Navigation task, implemented through the CityNav benchmark, which tests MLLMs&#x27; ability to navigate complex urban environments using only visual inputs and internal reasoning. Their experiments show that existing methods, including state-of-the-art MLLMs and techniques like GEPA, chain-of-thought, and reflection, as well as the baseline PReP, struggle significantly in this setting. To improve performance, they propose the Verbalization of Path (VoP) method, which explicitly grounds the agent&#x27;s reasoning by extracting city-scale cognitive maps, leading to better navigation outcomes.</div>
<div class="mono" style="margin-top:8px">本研究针对当前多模态大语言模型（MLLMs）评估基准的不足，即过于依赖语言或模拟环境，提出了稀疏视觉导航任务。该基准CityNav专门用于评估MLLMs在真实城市环境中的序列决策能力，仅依赖视觉输入和内部多模态推理，无需额外标注或架构调整。实验结果表明，现有的顶级MLLMs和基线模型如PReP在该任务中表现不佳，凸显了更优推理机制的必要性。为此，作者提出了VoP方法，通过构建城市级认知地图来显式地锚定代理的内部推理过程，从而显著提升导航成功率。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-01-28T17:27:08+00:00</div>
<div class="meta-line">Comments: 21 main pages, 7 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效仓库代理</div>
<div class="mono" style="margin-top:8px">开放权重的编码代理应该比闭源系统具有根本性的优势：它们可以专门针对私有代码库，直接在权重中编码特定仓库的信息。然而，训练的成本和复杂性使这一优势一直停留在理论层面。我们证明现在这一优势已经可以实现。我们提出了软验证高效仓库代理（SERA），这是一种高效的编码代理训练方法，使得能够快速且低成本地创建专门针对私有代码库的代理。仅使用监督微调（SFT），SERA在完全开源（开放数据、方法和代码）模型中实现了最先进的结果，同时其性能与前沿的开放权重模型（如Devstral-Small-2）相当。创建SERA模型的成本比强化学习低26倍，比之前合成数据方法低57倍，以达到同等性能。我们的方法——软验证生成（SVG）——可以从单个代码仓库生成数以千计的轨迹。结合成本效益，这使得专门针对私有代码库成为可能。除了仓库专门化，我们还将SVG应用于更大的代码库集合，生成超过200,000条合成轨迹。我们使用该数据集对编码代理的训练中的扩展定律、消融实验和混淆因素进行了详细分析。总体而言，我们相信我们的工作将大大加速开放编码代理的研究，并展示能够专门针对私有代码库的开源模型的优势。我们发布了SERA作为Ai2开放编码代理系列的第一个模型，同时发布了所有代码、数据和Claude Code集成，以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SERA, an efficient method for training coding agents that can be specialized to private codebases. The motivation is to leverage open-weight models to gain the flexibility of private data without the high cost and complexity of traditional approaches. The proposed method, Soft Verified Generation (SVG), generates a large number of synthetic trajectories from a single code repository using only supervised fine-tuning, achieving state-of-the-art results on open-source models and matching the performance of advanced open-weight models like Devstral-Small-2. The approach is significantly more cost-effective, being 26 times cheaper than reinforcement learning and 57 times cheaper than prior synthetic data methods. Additionally, the paper applies SVG to a broader corpus, producing over 200,000 synthetic trajectories for further analysis of scaling laws and training factors.</div>
<div class="mono" style="margin-top:8px">本文的动机是使开放权重的编码代理能够有效专精于私有代码库，这被认为是相较于封闭系统的核心优势。作者提出了SERA方法，通过仅使用监督微调（SFT）来训练编码代理，实现了在开放数据、方法和代码下的最先进性能。其核心方法Soft Verified Generation（SVG）能够从单个代码库生成数千条合成轨迹，大幅降低了训练成本，相较于强化学习和以往的合成数据方法分别降低了26倍和57倍。此外，该方法还被应用于更大规模的代码库集合，生成超过20万个合成轨迹，用于深入分析编码代理的扩展规律和训练因素。</div>
</details>
</div>
<div class="card">
<div class="title">On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents</div>
<div class="meta-line">Authors: Jai Lal Lulla, Seyedmoein Mohsenimofidi, Matthias Galster, Jie M. Zhang, Sebastian Baltes, Christoph Treude</div>
<div class="meta-line">First: 2026-01-28T09:09:30+00:00 · Latest: 2026-01-28T09:09:30+00:00</div>
<div class="meta-line">Comments: 5 pages, 1 figure, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20404v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于AGENTS.md文件对AI编码代理效率的影响</div>
<div class="mono" style="margin-top:8px">诸如Codex和Claude Code等AI编码代理正越来越多地用于自主贡献软件仓库。然而，关于仓库级配置工件如何影响代理操作效率的研究仍很有限。本文研究了AGENTS.md文件对在GitHub拉取请求上运行的AI编码代理的运行时间和令牌消耗的影响。我们分析了10个仓库和124个拉取请求，在有和无AGENTS.md文件的两种条件下执行代理。我们测量了代理执行期间的实时时钟执行时间和令牌使用情况。我们的结果表明，存在AGENTS.md文件与较低的中位数运行时间（Δ28.64%）和减少的输出令牌消耗（Δ16.58%）相关，同时保持了相当的任务完成行为。基于这些结果，我们讨论了在实践中配置和部署AI编码代理的即时影响，并概述了关于仓库级指令在塑造AI编码代理行为、效率和集成方面作用的更广泛研究议程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AGENTS.md files influence the efficiency of AI coding agents in software development workflows. As AI coding agents like Codex and Claude Code become more prevalent in contributing to software repositories, understanding the impact of repository-level configuration artifacts is crucial. The research analyzes 10 repositories and 124 pull requests, comparing agent performance with and without AGENTS.md files. Results indicate that the presence of AGENTS.md reduces median runtime by 28.64% and lowers output token consumption by 16.58%, without compromising task completion. These findings highlight the importance of proper configuration in optimizing the performance of AI coding agents.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AGENTS.md文件对AI编码代理在GitHub拉取请求中运行效率的影响。随着Codex和Claude Code等AI编码代理在软件开发中的广泛应用，了解其在仓库级配置文件下的表现变得尤为重要。研究分析了10个仓库和124个拉取请求，比较了代理在有无AGENTS.md文件情况下的运行效果。结果表明，AGENTS.md的存在可使中位运行时间减少28.64%，输出令牌消耗降低16.58%，同时保持任务完成率相近。这些发现强调了仓库级指令在优化AI编码代理部署与集成中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding</div>
<div class="meta-line">Authors: Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao</div>
<div class="meta-line">First: 2024-02-24T08:57:12+00:00 · Latest: 2026-01-28T08:05:32+00:00</div>
<div class="meta-line">Comments: Accepted by Empirical Software Engineering (EMSE) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.15769v3">Abs</a> · <a href="https://arxiv.org/pdf/2402.15769v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenCode：一种通用的数据增强框架，用于提升基于深度学习的代码理解</div>
<div class="mono" style="margin-top:8px">预训练代码模型引领了代码智能的时代，多个模型展现出令人印象深刻的表现。然而，一个重要的问题——用于自动帮助开发者准备训练数据的代码数据增强方法——在该领域尚未得到充分研究。本文介绍了一种通用的数据增强框架GenCode，以增强代码理解模型的训练效果。简而言之，GenCode遵循生成与选择的范式来准备有用的训练代码数据。具体来说，它首先使用代码增强技术生成新的代码候选，然后通过影响力评分识别重要的候选作为训练数据。为了评估GenCode的有效性，我们在四个代码理解任务（如代码克隆检测）和三个预训练代码模型（如CodeT5）以及两个最近发布的代码专用大语言模型（LLMs）（如Qwen2.5-Coder）上进行了实验。与最先进的代码增强方法MixCode相比，GenCode在预训练代码模型上平均提高了2.92%的准确率和4.90%的对抗鲁棒性。对于代码专用的LLMs，GenCode在准确率和自然鲁棒性上平均分别提升了0.93%和0.98%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the lack of effective data augmentation methods for code data in the context of deep learning-based code understanding. GenCode introduces a generic framework that follows a generation-and-selection paradigm, first generating new code candidates through code augmentation techniques and then selecting the most informative ones using influence scores. Experimental results on four code understanding tasks and three pre-trained code models, as well as two code-specific large language models, show that GenCode improves model accuracy by 2.92% and adversarial robustness by 4.90% compared to the state-of-the-art method MixCode, with additional gains in natural robustness for code-specific LLMs.</div>
<div class="mono" style="margin-top:8px">本文针对代码数据在深度学习代码理解领域中缺乏有效的数据增强方法的问题，提出了一种通用的代码数据增强框架GenCode。该框架采用生成与选择的范式，首先通过代码增强技术生成新的代码候选，再利用影响力评分筛选出最有价值的样本作为训练数据。在四个代码理解任务和两个预训练模型上的实验表明，与现有最优方法MixCode相比，GenCode在准确率上提升了2.92%，对抗鲁棒性提高了4.90%，同时在代码专用大语言模型上也带来了平均0.93%的准确率提升和0.98%的自然鲁棒性增强。</div>
</details>
</div>
<div class="card">
<div class="title">Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction</div>
<div class="meta-line">Authors: Genyuan Zhang, Zihao Wang, Zhifan Gao, Lei Xu, Zhen Zhou, Haijun Yu, Jianjia Zhang, Xiujian Liu, Weiwei Zhang, Shaoyu Wang, Huazhu Fu, Fenglin Liu, Weiwen Wu</div>
<div class="meta-line">First: 2026-01-28T06:54:06+00:00 · Latest: 2026-01-28T06:54:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20304v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构约束的语言引导扩散模型用于非配对低剂量CT血管造影重建</div>
<div class="mono" style="margin-top:8px">碘对比剂（ICM）的应用提高了CT在多种临床指征中的敏感性和特异性。然而，过量使用ICM可能导致肾损伤和危及生命的过敏反应。深度学习方法可以从低剂量ICM生成正常剂量ICM的CT图像，从而在保持诊断能力的同时减少所需剂量。然而，现有方法在使用不完全配对图像进行准确增强时存在困难，主要原因是模型识别特定结构的能力有限。为克服这一限制，我们提出了一种结构约束的语言引导扩散模型（SLDM），这是一种集成了结构协同和空间智能的统一医学生成模型。首先，有效提取图像的结构先验信息以约束模型推理过程，从而确保增强过程中的结构一致性。随后，引入具有空间智能的语义监督策略，该策略集成了视觉感知和空间推理的功能，从而促使模型实现准确增强。最后，应用减影血管造影增强模块，其作用是提高ICM剂区域的对比度，使其适合观察。视觉比较的定性分析和多个指标的定量结果表明，我们的方法在低剂量对比剂CT血管造影的血管重建中是有效的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of accurately reconstructing high-quality CT angiography images from low-dose iodinated contrast media (ICM) data, which is critical for reducing patient exposure to harmful doses while maintaining diagnostic accuracy. The proposed Structure-constrained Language-informed Diffusion Model (SLDM) integrates structural prior information and semantic supervision with spatial intelligence to enhance image reconstruction. The model constrains inference using structural features, incorporates semantic guidance for accurate enhancement, and applies a subtraction angiography module to improve contrast. Experimental results show that the SLDM outperforms existing methods in both qualitative visual analysis and quantitative metrics, demonstrating its effectiveness in low-dose CT angiography reconstruction.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决从低剂量碘对比剂CT血管造影扫描中准确重建高质量CT血管造影图像的挑战，以减少患者对有害剂量的暴露同时保持诊断准确性。所提出的方法——结构约束的语言引导扩散模型（SLDM）——结合了结构先验信息和语义监督策略，以空间智能引导扩散过程，确保结构一致性并增强模型对特定解剖结构的识别能力。此外，减影血管造影增强模块进一步提升了对比剂区域的对比度，使其更适合观察。实验结果，包括定性视觉比较和多种定量指标，表明SLDM在低剂量CT血管造影重建中优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</div>
<div class="meta-line">Authors: Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long</div>
<div class="meta-line">First: 2026-01-27T17:40:07+00:00 · Latest: 2026-01-27T17:40:07+00:00</div>
<div class="meta-line">Comments: Project page: https://thuml.github.io/Reasoning-Visual-World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://thuml.github.io/Reasoning-Visual-World">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉生成通过多模态世界模型实现类人推理</div>
<div class="mono" style="margin-top:8px">人类通过构建内部世界模型并操作其中的概念来进行推理。最近的AI进展，特别是链式推理（CoT），在某种程度上模拟了这种人类认知能力，其中世界模型被认为嵌入在大型语言模型中。当前系统主要依赖语言推理，在数学和编程等正式和抽象领域已达到专家级表现。然而，在需要更丰富表示和先验知识的物理和空间智能领域，它们仍远不如人类。因此，统一多模态模型（UMMs）的出现，引发了人们对基于互补多模态路径的更类人推理方式的兴趣，尽管其优势尚不明确。从世界模型的角度来看，本文首次系统地研究了视觉生成何时以及如何促进推理。我们的核心观点是视觉优势假说：对于某些任务，特别是与物理世界相关的任务，视觉生成更自然地作为世界模型，而纯语言世界模型则因表示限制或先验知识不足而遇到瓶颈。理论上，我们将内部世界建模形式化为链式推理的核心组成部分，并分析不同形式世界模型之间的区别。实证上，我们识别出需要交替使用视觉和语言链式推理的任务，并构建了一个新的评估套件VisWorld-Eval。在最先进的UMM上进行的受控实验表明，在有利于视觉世界建模的任务中，交替链式推理显著优于纯语言链式推理，但在其他任务中则没有明显优势。综上，本工作阐明了多模态世界建模在构建更强大、更类人的多模态AI中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how visual generation can enhance human-like reasoning in AI systems, particularly in domains requiring physical and spatial understanding. It proposes the visual superiority hypothesis, suggesting that visual generation more naturally supports world modeling compared to purely verbal approaches. Through a new evaluation framework called VisWorld-Eval, the study demonstrates that interleaving visual and verbal reasoning significantly improves performance on tasks that benefit from visual world modeling, while offering no clear advantage in purely verbal domains.</div>
<div class="mono" style="margin-top:8px">本文探讨了视觉生成如何通过多模态世界模型提升人工智能系统的人类类比推理能力。由于纯语言推理在需要空间和物理理解的领域存在局限，研究提出了视觉优越性假说，认为视觉生成在这些任务中更自然地支持世界建模。通过在先进统一多模态模型上的受控实验，作者发现交错的视觉与语言推理在需要视觉世界建模的任务中显著提升性能，而在其他领域则没有明显优势。</div>
</details>
</div>
<div class="card">
<div class="title">daVinci-Dev: Agent-native Mid-training for Software Engineering</div>
<div class="meta-line">Authors: Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu</div>
<div class="meta-line">First: 2026-01-26T12:20:18+00:00 · Latest: 2026-01-27T12:16:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18418v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model&#x27;s agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>daVinci-Dev：面向软件工程的代理原生中间训练</div>
<div class="mono" style="margin-top:8px">最近，大型语言模型（LLM）的能力前沿已从单轮代码生成转向代理软件工程——一种模型能够自主导航、编辑和测试复杂仓库的范式。尽管后训练方法已成为代码代理的主流方法，但由于资源需求巨大，**代理中间训练**（agentic mid-training）——即在模拟真实代理工作流的大规模数据上进行中间训练——仍被严重忽视。与仅依赖昂贵的强化学习相比，代理中间训练提供了更可扩展的路径来培养基础的代理行为。实现有效的代理中间训练的核心挑战在于静态训练数据与真实开发中动态且反馈丰富的环境之间的分布不匹配。为了解决这一问题，我们系统地研究了代理中间训练，建立了适用于大规模代理开发的数据合成原则和训练方法。我们的方法核心在于**代理原生数据**——包含两种互补轨迹的监督数据：**上下文原生轨迹**，保留代理经历的完整信息流，提供广泛覆盖和多样性；以及**环境原生轨迹**，从可执行仓库中收集，其观察来源于实际工具调用和测试执行，提供深度和交互的真实性。我们在 `SWE-Bench Verified` 上验证了模型的代理能力。在使用对齐基础模型和代理框架的两种后训练设置下，我们的方法在使用不到一半中间训练令牌（73.1B）的情况下，优于之前的开源软件工程中间训练方案 `Kimi-Dev`。除了相对优势外，我们的表现最佳的32B和72B模型分别实现了**56.1%**和**58.5%**的解决率，分别...</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of training effective code agents by exploring agentic mid-training, which aims to instill foundational agentic behaviors through training on data that reflects real development workflows. The authors introduce agent-native data, consisting of contextually-native and environmentally-native trajectories, to bridge the gap between static training data and dynamic development environments. Their experiments on SWE-Bench Verified show that their approach outperforms Kimi-Dev with fewer training tokens, achieving resolution rates of 56.1% and 58.5% for their 32B and 72B models, respectively.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型在代理式软件工程中的训练问题，提出了一种模拟真实开发流程的中训练方法。该方法引入了代理原生数据，包含上下文原生轨迹和环境原生轨迹，以弥合静态训练数据与动态开发环境之间的差距。研究结果表明，该方法在SWE-Bench Verified基准测试中优于现有方法如Kimi-Dev，最佳模型在使用不到一半训练标记的情况下分别达到了56.1%和58.5%的解决率。</div>
</details>
</div>
<div class="card">
<div class="title">How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests</div>
<div class="meta-line">Authors: Daniel Ogenrwot, John Businge</div>
<div class="meta-line">First: 2026-01-24T20:27:04+00:00 · Latest: 2026-01-27T06:14:36+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17581v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff&#x27;s $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理如何修改代码：GitHub拉取请求的大型研究</div>
<div class="mono" style="margin-top:8px">AI编码代理正越来越多地作为自主贡献者，通过生成和提交拉取请求（PRs）参与开发。然而，我们缺乏关于这些由代理生成的PR与人类贡献之间差异的实证证据，尤其是在代码修改方式和PR描述方面。理解这些差异对于评估其可靠性和对开发流程的影响至关重要。我们使用MSR 2026 Mining Challenge版本的AIDev数据集，分析了24,014个已合并的Agentic PR（440,295次提交）和5,081个已合并的人类PR（23,242次提交）。我们考察了新增内容、删除内容、提交次数以及涉及的文件，并通过词法和语义相似性评估PR描述与代码差异的一致性。在提交次数方面，Agentic PR与Human PR存在显著差异（Cliff&#x27;s δ=0.5429），在涉及的文件和删除行方面表现出中等差异。它们在所有衡量标准下也显示出略高的描述与代码差异的相似性。这些发现提供了关于AI编码代理如何参与开源开发的大型实证特征描述。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how AI coding agents contribute to open source development by analyzing their pull requests compared to those made by humans. The researchers used the MSR 2026 Mining Challenge version of the AIDev dataset to examine 24,014 merged Agentic PRs and 5,081 merged Human PRs, focusing on code modifications, commit patterns, and the alignment between PR descriptions and code changes. The results show that Agentic PRs have significantly higher commit counts and exhibit moderate differences in the number of files touched and deleted lines, while also demonstrating slightly higher lexical and semantic similarity between descriptions and diffs.</div>
<div class="mono" style="margin-top:8px">本研究通过分析AI编码代理生成的拉取请求与人类贡献的差异，探讨其在开源开发中的贡献模式。研究动机源于对AI生成代码修改方式和描述一致性缺乏实证证据的需求。利用AIDev数据集，作者对比了24,014个合并的Agentic PR和5,081个合并的人类PR，关注提交次数、修改文件及描述与代码变更的一致性。实验结果表明，Agentic PR的提交次数显著高于人类PR，修改的文件和删除行存在中等差异，且在描述与代码变更的一致性方面表现略优。</div>
</details>
</div>
<div class="card">
<div class="title">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</div>
<div class="meta-line">Authors: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou</div>
<div class="meta-line">First: 2025-10-13T05:51:22+00:00 · Latest: 2026-01-27T05:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vlaser：具有协同具身推理能力的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管已有大量研究致力于利用视觉-语言模型（VLMs）开发具身推理能力，或将其集成到视觉-语言-动作（VLA）模型中以实现端到端的机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。在本工作中，我们通过引入Vlaser——一种具有协同具身推理能力的视觉-语言-动作模型，迈出初步一步，弥合具身推理与VLA策略学习之间的鸿沟。Vlaser是一种基础的视觉-语言模型，旨在将高层次推理与低层次控制相结合，以支持具身智能体。基于高质量的Vlaser-6M数据集，Vlaser在一系列具身推理基准测试中取得了最先进的性能，包括空间推理、具身锚定、具身问答和任务规划。此外，我们系统地研究了不同VLM初始化方式对监督VLA微调的影响，提供了关于缓解互联网规模预训练数据与具身特定策略学习数据之间领域偏移的新见解。基于这些见解，我们的方法在WidowX基准测试中取得了最先进的结果，并在Google Robot基准测试中表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating high-level vision-language reasoning with low-level action control in embodied agents, aiming to bridge the gap between VLM-based reasoning and VLA policy learning. The authors propose Vlaser, a Vision-Language-Action model that incorporates synergistic embodied reasoning, built on a high-quality dataset of 6 million samples. Experimental results show that Vlaser achieves state-of-the-art performance on several embodied reasoning benchmarks, including spatial reasoning, embodied grounding, and task planning. Additionally, the study provides insights into the impact of different VLM initializations on VLA fine-tuning, leading to improved performance on the WidowX and Google Robot benchmarks.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将高阶视觉语言推理与低阶动作控制相结合的问题，以弥合上游视觉语言模型与下游视觉语言动作策略学习之间的差距。提出的Vlaser模型引入了协同具身推理机制，将基础的视觉语言模型与任务特定的控制机制相结合。在多个具身推理基准测试中，Vlaser表现出最先进的性能，并提供了关于不同视觉语言模型初始化对视觉语言动作微调影响的新见解，从而在WidowX和Google Robot基准测试中取得了优异的结果。</div>
</details>
</div>
<div class="card">
<div class="title">m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</div>
<div class="meta-line">Authors: Yosub Shin, Michael Buriek, Igor Molybog</div>
<div class="meta-line">First: 2026-01-27T02:01:56+00:00 · Latest: 2026-01-27T02:01:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>m2sv：一种用于地图到街景空间推理的可扩展基准</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在许多多模态基准上表现出色，但在需要将抽象的俯视图表示与以自我为中心视角对齐的空间推理任务中仍显得脆弱。我们引入了m2sv，这是一个用于地图到街景空间推理的可扩展基准，要求模型通过将北向俯视地图与同一现实路口拍摄的街景图像对齐，推断相机的观看方向。我们发布了m2sv-20k，这是一个地理多样性且具有可控模糊性的基准，以及m2sv-sft-11k，一个用于监督微调的精心整理的结构化推理轨迹集。尽管在现有多模态基准上表现强劲，但评估最好的VLM在m2sv上的准确率仅为65.2%，远低于人类基线的95%。虽然监督微调和强化学习带来了持续的性能提升，但跨基准评估显示其迁移能力有限。除了整体准确率外，我们还通过结构信号和人类努力系统地分析了地图到街景推理的难度，并对适应的开放模型进行了广泛的失败分析。我们的研究结果突显了几何对齐、证据聚合和推理一致性方面的持续差距，为未来多视角的基于场景的空间推理研究提供了动力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces m2sv, a scalable benchmark designed to evaluate vision--language models&#x27; ability to perform spatial reasoning by aligning abstract overhead maps with egocentric street-view images. The benchmark includes m2sv-20k, a geographically diverse dataset with controlled ambiguity, and m2sv-sft-11k, a set of structured reasoning traces for supervised fine-tuning. Despite strong performance on other multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, significantly lower than the human baseline of 95%. The study shows that both supervised fine-tuning and reinforcement learning improve performance, but transfer across benchmarks is limited. The authors also conduct a detailed analysis of the challenges in map-to-street-view reasoning, identifying issues in geometric alignment, evidence aggregation, and reasoning consistency.</div>
<div class="mono" style="margin-top:8px">本文提出m2sv，这是一个用于评估视觉语言模型在地图到街景空间推理能力的可扩展基准。该基准要求模型通过将北向俯视地图与同一现实路口拍摄的街景图像对齐，推断相机的视角方向。研究发布了m2sv-20k，一个地理多样性且具有可控模糊性的数据集，以及m2sv-sft-11k，用于监督微调的结构化推理轨迹集。尽管在现有多模态基准上表现优异，但最佳评估的视觉语言模型在m2sv上的准确率仅为65.2%，远低于人类基线的95%。研究还发现，虽然监督微调和强化学习能提升性能，但跨基准评估显示迁移能力有限。进一步分析揭示了在几何对齐、证据聚合和推理一致性方面仍存在显著挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward</div>
<div class="meta-line">Authors: Dipendra Misra, Aldo Pacchiano, Ta-Chung Chi, Ge Gao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-27T00:31:04+00:00 · Latest: 2026-01-27T00:31:04+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19055v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent&#x27;s response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains adapted from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于用户编辑的LLM原理化微调：偏好、监督与奖励的融合</div>
<div class="mono" style="margin-top:8px">我们研究如何利用由用户编辑生成的部署数据来微调LLM，该数据包含上下文、代理的响应以及用户的编辑。这种部署数据自然地出现在基于LLM的写作助手和编码代理等应用中。由于用户编辑的自然来源，它成为适应和个性化LLM的理想数据源。在此设置下，各种通常在文献中独立研究的反馈类型（偏好、监督标签和成本）得以统一。在本文中，我们启动了从用户编辑中学习的理论研究。我们首先推导出从这些反馈类型中学习的算法的学习界。我们证明这些算法根据用户、数据分布和模型类别具有不同的权衡。随后，我们提出了一种简单的集成方法，以联合学习这些反馈类型。在两个改编自Gao等人2024年的领域中，我们展示了我们的集成方法优于仅从单一反馈类型学习的方法。此外，我们还证明了所提出的程序在测试时能够稳健地适应不同的用户编辑分布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the fine-tuning of large language models (LLMs) using user-edit deployment data, which includes context, agent responses, and user modifications. The motivation stems from the natural and diverse feedback provided by users in applications like writing assistants and coding agents, making it a valuable resource for personalization. The authors unify preference, supervision, and reward signals into a single learning framework and derive theoretical bounds for each feedback type. They propose an ensembling approach that combines these signals, demonstrating improved performance over individual methods on two domains adapted from Gao et al. 2024. The results show that their procedure is robust across different user-edit distributions during testing.</div>
<div class="mono" style="margin-top:8px">本文研究了如何利用用户编辑部署数据对大语言模型（LLMs）进行微调，该数据包含上下文、代理响应和用户修改。研究动机源于用户在写作助手和代码代理等应用中提供的自然且多样的反馈，为模型的适应与个性化提供了宝贵资源。作者分析了从不同反馈类型（偏好、监督和成本）学习的理论特性，并证明每种反馈类型在不同用户、数据分布和模型类别下具有不同的性能权衡。随后，他们提出了一种简单的集成方法，联合利用这些反馈类型，在两个改编自Gao等人2024年的领域上展示了优于单独反馈方法的性能。此外，该方法在测试时对不同的用户编辑分布表现出良好的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Foundation Models for Radar Scene Understanding</div>
<div class="meta-line">Authors: Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia</div>
<div class="meta-line">First: 2025-11-26T06:41:00+00:00 · Latest: 2026-01-26T20:56:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21105v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21105v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>雷达场景理解的基础模型扩展</div>
<div class="mono" style="margin-top:8px">雷达传感器在恶劣天气、光照和远距离条件下提供了可靠的感知能力。近年来，基础模型在视觉和语言理解方面取得了重大进展，但其与雷达感知的结合仍处于初步探索阶段。现有的雷达方法往往是碎片化的且任务特定的；每个下游任务都采用不同的架构和训练目标，阻碍了任务间的迁移能力。在本文中，我们提出了RadarFM：一种通过结构化空间语言监督学习统一场景级表示的雷达基础模型。我们做出了两项关键贡献：(1) 一种结构化描述框架，能够以原生雷达坐标编码车辆分布；(2) 一种哈希感知的对比学习目标，通过量化连续场景相似性而非二元匹配，实现细粒度的空间推理。我们利用CARLA模拟器，在多样化的驾驶场景中生成大规模且标注良好的雷达数据集。此外，我们还提出了定位感知的评估指标，以超越传统检测度量的方式评估空间准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating foundation models into radar scene understanding, where existing methods are task-specific and lack transferability. The authors propose RadarFM, a foundation model that learns unified scene-level representations using structured spatial language supervision. Two key innovations include a structured caption framework for encoding vehicle distributions in radar coordinates and a hash-aware contrastive learning objective that captures continuous scene similarity for fine-grained spatial reasoning. Experimental results on large-scale radar datasets generated via CARLA demonstrate improved performance in spatial understanding compared to traditional detection-based metrics.</div>
<div class="mono" style="margin-top:8px">本文旨在解决雷达场景理解中基础模型整合的挑战，现有方法多为任务特定且缺乏迁移能力。作者提出RadarFM，一种通过结构化空间语言监督学习统一场景表示的基础模型。主要贡献包括：一种结构化描述框架，用于在原生雷达坐标中编码车辆分布；以及一种基于哈希的对比学习目标，通过量化连续场景相似性实现细粒度空间推理。在CARLA模拟器生成的大规模雷达数据集上进行实验，结果表明RadarFM在空间理解方面优于传统基于检测的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests</div>
<div class="meta-line">Authors: Jingzhi Gong, Giovanni Pinna, Yixin Bian, Jie M. Zhang</div>
<div class="meta-line">First: 2026-01-08T12:31:02+00:00 · Latest: 2026-01-26T17:05:34+00:00</div>
<div class="meta-line">Comments: Accepted by MSR&#x27;26 Mining Challenge Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04886v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04886v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that &quot;descriptions claim unimplemented changes&quot; was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5 times longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分析AI编码代理撰写的拉取请求中的消息-代码不一致问题</div>
<div class="mono" style="margin-top:8px">由AI编码代理生成的拉取请求（PR）描述是向人类审阅者传达代码变更的主要渠道。然而，这些消息与实际变更之间的对齐情况尚未被研究，引发了对AI代理可信度的担忧。为填补这一空白，我们使用PR消息-代码不一致（PR-MCI）指标，分析了五个代理生成的23,247个PR。我们贡献了974个手动标注的PR，发现其中406个（1.7%）表现出高PR-MCI，并识别出八种PR-MCI类型，其中最常见的问题是“描述声称未实现的变更”（占45.4%）。统计检验表明，高PR-MCI的PR被接受率低51.7%（28.3% vs. 80.0%），且合并所需时间是普通PR的3.5倍（55.8小时 vs. 16.0小时）。我们的研究结果表明，不可靠的PR描述会削弱对AI代理的信任，强调了需要PR-MCI验证机制和改进PR生成以实现可信的人机协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the issue of message-code inconsistency in pull requests authored by AI coding agents, as PR descriptions are critical for human reviewers to understand code changes. By analyzing 23,247 PRs across five agents, the researchers identified 406 PRs with high inconsistency, contributing 974 manually annotated examples. They found that 45.4% of these inconsistencies were due to descriptions claiming unimplemented changes, and statistical analysis showed that high-MCI PRs had significantly lower acceptance rates and longer merge times, indicating the importance of improving PR description reliability for trustworthy AI-human collaboration.</div>
<div class="mono" style="margin-top:8px">该研究探讨了由AI编码代理生成的拉取请求（PR）中消息与代码不一致的问题，因为这些描述对人类审阅者理解代码变更至关重要。通过对五个代理生成的23,247个PR进行分析，研究者发现了406个存在高不一致性的PR，并贡献了974个手动标注的案例。结果显示，45.4%的不一致源于描述中声称实现了未完成的变更，统计分析表明高不一致性的PR接受率低51.7%，合并时间长3.5倍，强调了提升PR描述可靠性以实现可信人机协作的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shared Spatial Memory Through Predictive Coding</div>
<div class="meta-line">Authors: Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Wenbo Ding, Yuguang Fang</div>
<div class="meta-line">First: 2025-11-06T10:12:46+00:00 · Latest: 2026-01-26T11:24:30+00:00</div>
<div class="meta-line">Comments: We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04235v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04235v3">PDF</a> · <a href="http://github.com/fangzr/SSM-PC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners&#x27; locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过预测编码实现共享空间记忆</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，构建一致的共享空间记忆是一个关键挑战，其中部分可观测性和有限带宽常常导致协调中的灾难性失败。我们引入了一种多智能体预测编码框架，将协调建模为智能体之间相互不确定性的最小化。通过信息瓶颈目标，该框架促使智能体不仅学习应向谁、传达什么信息，还学习何时进行通信。该框架的基础是一种类似网格细胞的度量，作为内部空间编码用于自我定位，这种编码自发地从自监督运动预测中产生。在此基础上，智能体逐步发展出一种带宽高效的通信机制，并形成专门的神经群体以编码合作伙伴的位置——这是海马体社会位置细胞（SPCs）的人工类比。这些社会表征进一步被分层强化学习策略所利用，该策略主动探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法表现出对带宽限制的卓越鲁棒性：当带宽从128位/步减少到4位/步时，成功率从73.5%平稳下降至64.4%，而全广播基线则从67.6%骤降至28.6%。我们的研究结果为复杂社会表征如何从统一的预测驱动中涌现提供了理论上有原则且生物上合理的基础，从而实现集体智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to address the challenge of maintaining a consistent shared spatial memory in multi-agent systems, which is often hindered by partial observability and limited communication bandwidth. The proposed method introduces a predictive coding framework that minimizes mutual uncertainty among agents through an information bottleneck objective, enabling them to learn optimal communication strategies in terms of content, timing, and recipient. The framework uses a grid-cell-like metric for internal spatial coding, which arises from self-supervised motion prediction, and supports the development of efficient communication and specialized neural representations for partner locations. Experimental results on the Memory-Maze benchmark demonstrate that the approach maintains high performance under bandwidth constraints, with success rates dropping only slightly from 73.5% to 64.4% as bandwidth decreases from 128 to 4 bits per step, compared to a sharp decline in a full-broadcast baseline.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决多智能体系统中保持一致共享空间记忆的挑战，特别是在部分可观测性和通信带宽受限的情况下。提出的方法引入了一种基于预测编码的框架，通过最小化智能体间的相互不确定性来实现协调，学习谁、什么以及何时进行通信。该框架利用类似网格细胞的度量作为自定位的内部空间编码，源自自监督运动预测。智能体随后发展出高效的通信机制和专门的神经群体，用于编码其他智能体的位置，模拟海马体的社会位置细胞。主要实验结果表明，在带宽受限的情况下，该方法表现出良好的鲁棒性，在4 bits/step时仍能保持64.4%的成功率，而全广播基线则骤降至28.6%。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</div>
<div class="meta-line">Authors: Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui</div>
<div class="meta-line">First: 2025-12-20T19:08:15+00:00 · Latest: 2026-01-26T10:49:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18470v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.18470v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, SWE-EVO comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on SWE-EVO, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-EVO：在长周期软件演进场景中对编码代理的基准测试</div>
<div class="mono" style="margin-top:8px">现有的AI编码代理基准主要关注孤立的、单一问题的任务，如修复错误或实现小功能。然而，现实中的软件工程本质上是长周期的：开发者必须理解高层需求，协调多个文件的更改，并在多次迭代中演进代码库，同时保持现有功能。我们引入了SWE-EVO，这是一个评估代理在长周期软件演进挑战上的基准。SWE-EVO基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演进任务，要求代理在平均涉及21个文件的多步骤修改中进行操作，并通过平均每个实例874个测试的全面测试套件进行验证。对最先进的模型的实验揭示了一个显著的能力差距：即使GPT-5结合OpenHands也只能在SWE-EVO上达到21%的解决率，而相比之下在单一问题的SWE-Bench Verified上达到65%。这表明当前的代理在持续的、多文件的推理方面存在困难。我们还提出了一项细粒度指标Fix Rate，用于捕捉解决这些复杂长周期任务的部分进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SWE-EVO, a benchmark designed to evaluate AI coding agents in long-horizon software evolution scenarios, which are more representative of real-world software development. Unlike existing benchmarks that focus on isolated tasks, SWE-EVO is constructed from the release notes and version histories of seven mature Python projects, containing 48 tasks that require multi-step modifications across an average of 21 files. The results show that even advanced models like GPT-5 with OpenHands achieve only a 21% resolution rate on SWE-EVO, significantly lower than the 65% on the single-issue SWE-Bench Verified, highlighting the difficulty agents face in sustained, multi-file reasoning. The paper also proposes a new metric, Fix Rate, to measure partial progress in solving these complex tasks.</div>
<div class="mono" style="margin-top:8px">本研究提出了SWE-EVO，这是一个用于评估AI编码代理在长期软件演进任务中表现的基准测试。与现有的专注于单一问题的基准不同，SWE-EVO基于七个成熟开源Python项目的发布说明和版本历史，包含48个需要跨多个文件进行多步骤修改的任务，并通过平均每个实例874个测试的全面测试套件进行验证。实验结果显示，即使是GPT-5与OpenHands结合的先进模型，在SWE-EVO上的解决率也只有21%，远低于SWE-Bench Verified上的65%，表明当前代理在持续的多文件推理方面存在显著不足。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Conditioned Reasoning in Long-Egocentric Videos</div>
<div class="meta-line">Authors: James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi</div>
<div class="meta-line">First: 2026-01-26T03:21:35+00:00 · Latest: 2026-01-26T03:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时域第一视角视频中的空间条件推理</div>
<div class="mono" style="margin-top:8px">由于视角漂移和缺乏持久的几何上下文，长时域第一视角视频对视觉导航提出了重大挑战。尽管近期的视觉-语言模型在图像和短视频推理上表现良好，但它们在长第一视角序列中的空间推理能力仍有限。在本工作中，我们研究了显式空间信号如何影响基于VLM的视频理解，而无需修改模型架构或推理过程。我们引入了Sanpo-D，这是Google Sanpo数据集的细粒度重新标注版本，并在面向导航的空间查询上对多个VLM进行了基准测试。为了检验输入级别的归纳偏置，我们进一步将深度图与RGB帧融合，并评估其对空间推理的影响。我们的结果揭示了通用准确性与空间特化之间的权衡，表明具有深度感知和空间基础的表示方法可以提高对行人和障碍物检测等关键安全任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenges of visual navigation in long-horizon egocentric videos, where viewpoint drift and lack of persistent geometric context hinder spatial understanding. The authors investigate how explicit spatial signals can enhance video understanding using vision-language models (VLMs) without altering model structures or inference processes. They introduce Sanpo-D, a refined annotation of the Google Sanpo dataset, and evaluate various VLMs on spatial navigation tasks. By integrating depth maps with RGB frames, they demonstrate that spatially grounded representations improve performance in critical tasks like pedestrian and obstruction detection, albeit at the cost of reduced general-purpose accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对长时地缘视角视频中因视角漂移和缺乏持久几何信息导致的视觉导航难题。作者探讨了在不修改模型结构或推理流程的前提下，显式空间信号如何提升基于视觉语言模型的视频理解能力。他们提出了Sanpo-D，这是对Google Sanpo数据集的精细化标注，并在导航相关的空间查询上评估了多种VLM的表现。通过融合深度图与RGB帧，实验表明深度感知的表示方法在行人和障碍物检测等关键任务中提升了性能，但以牺牲通用准确性为代价。</div>
</details>
</div>
<div class="card">
<div class="title">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</div>
<div class="meta-line">Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</div>
<div class="meta-line">First: 2026-01-19T18:48:37+00:00 · Latest: 2026-01-26T00:36:33+00:00</div>
<div class="meta-line">Comments: https://cooperbench.com First two authors contribute equally. The 3th - 6th authors contribute equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13295v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others&#x27; plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CooperBench：为何代码代理还不能成为你的队友</div>
<div class="mono" style="margin-top:8px">解决团队冲突不仅需要任务相关的专业能力，还需要社交智能来寻找共同点并建立共识。随着AI代理越来越多地协作处理复杂工作，它们必须发展协调能力，才能有效作为队友。然而，我们假设当前的代理缺乏这些能力。为此，我们引入了CooperBench，这是一个涵盖4种编程语言、12个库的超过600个协作编码任务的基准测试。每个任务为两个代理分配不同的功能，这些功能可以独立实现，但若缺乏适当协调则可能产生冲突。任务基于真实的开源仓库，并包含专家编写的测试用例。在评估最先进的编码代理时，我们观察到协调的诅咒：代理协作完成任务的成功率平均比各自独立完成任务低30%。这与人类团队中增加队友通常能提高生产力的情况形成鲜明对比。我们的分析揭示了三个关键问题：(1) 通信渠道充斥着模糊、时机不当和不准确的信息；(2) 即使有有效的沟通，代理也会偏离其承诺；(3) 代理常常对他人计划和沟通持有错误的预期。通过大规模模拟，我们还观察到一些罕见但有趣的协调行为，包括角色分工、资源分配和协商。我们的研究提出了一个协作编码的新基准，并呼吁从追求单个代理能力转向发展社交智能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this study is to investigate the limitations of current coding agents in collaborative settings, particularly their inability to effectively coordinate with each other. The authors introduce CooperBench, a benchmark consisting of over 600 collaborative coding tasks across four programming languages and twelve libraries, designed to test agents&#x27; coordination abilities. The main experimental results show that state-of-the-art coding agents perform significantly worse when working together, achieving on average 30% lower success rates than when completing tasks individually, highlighting the &#x27;curse of coordination.&#x27; The analysis identifies three key challenges: ineffective communication, unfulfilled commitments, and misaligned expectations. The study also observes some emergent coordination behaviors in large-scale simulations, such as role division and negotiation, suggesting potential pathways for future improvements.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探讨当前编码代理在协作环境中的局限性，强调团队协调中社交智能的重要性。作者提出了CooperBench，一个包含超过600个跨四种编程语言和十二个库的协作编码任务基准，旨在测试代理在实现独立功能时的协调能力。主要实验结果表明，最先进的编码代理在协作时平均成功率比单独完成任务低30%。这一现象被称为&#x27;协调的诅咒&#x27;，揭示了代理在沟通、承诺履行和预期管理方面存在困难。然而，通过大规模模拟，研究还观察到了一些罕见但有趣的协调行为，如角色分工、资源共享和协商。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260130_0352.html">20260130_0352</a>
<a href="archive/20260129_0350.html">20260129_0350</a>
<a href="archive/20260128_0350.html">20260128_0350</a>
<a href="archive/20260127_0346.html">20260127_0346</a>
<a href="archive/20260126_0339.html">20260126_0339</a>
<a href="archive/20260125_0339.html">20260125_0339</a>
<a href="archive/20260124_0345.html">20260124_0345</a>
<a href="archive/20260123_0348.html">20260123_0348</a>
<a href="archive/20260122_0349.html">20260122_0349</a>
<a href="archive/20260121_0436.html">20260121_0436</a>
<a href="archive/20260120_0340.html">20260120_0340</a>
<a href="archive/20260119_0337.html">20260119_0337</a>
<a href="archive/20260118_0338.html">20260118_0338</a>
<a href="archive/20260117_2150.html">20260117_2150</a>
<a href="archive/20260117_0320.html">20260117_0320</a>
<a href="archive/20260117_0151.html">20260117_0151</a>
<a href="archive/20260117_0143.html">20260117_0143</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
